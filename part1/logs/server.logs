time="2024-03-21T21:38:15Z" level=info msg="Starting k3s v1.28.7+k3s1 (051b14b2)"
time="2024-03-21T21:38:16Z" level=info msg="Configuring sqlite3 database connection pooling: maxIdleConns=2, maxOpenConns=0, connMaxLifetime=0s"
time="2024-03-21T21:38:16Z" level=info msg="Configuring database table schema and indexes, this may take a moment..."
time="2024-03-21T21:38:16Z" level=info msg="Database tables and indexes are up to date"
time="2024-03-21T21:38:16Z" level=info msg="Kine available at unix://kine.sock"
time="2024-03-21T21:38:16Z" level=info msg="generated self-signed CA certificate CN=k3s-client-ca@1711057096: notBefore=2024-03-21 21:38:16.007043059 +0000 UTC notAfter=2034-03-19 21:38:16.007043059 +0000 UTC"
time="2024-03-21T21:38:16Z" level=info msg="certificate CN=system:admin,O=system:masters signed by CN=k3s-client-ca@1711057096: notBefore=2024-03-21 21:38:16 +0000 UTC notAfter=2025-03-21 21:38:16 +0000 UTC"
time="2024-03-21T21:38:16Z" level=info msg="certificate CN=system:k3s-supervisor,O=system:masters signed by CN=k3s-client-ca@1711057096: notBefore=2024-03-21 21:38:16 +0000 UTC notAfter=2025-03-21 21:38:16 +0000 UTC"
time="2024-03-21T21:38:16Z" level=info msg="certificate CN=system:kube-controller-manager signed by CN=k3s-client-ca@1711057096: notBefore=2024-03-21 21:38:16 +0000 UTC notAfter=2025-03-21 21:38:16 +0000 UTC"
time="2024-03-21T21:38:16Z" level=info msg="certificate CN=system:kube-scheduler signed by CN=k3s-client-ca@1711057096: notBefore=2024-03-21 21:38:16 +0000 UTC notAfter=2025-03-21 21:38:16 +0000 UTC"
time="2024-03-21T21:38:16Z" level=info msg="certificate CN=system:apiserver,O=system:masters signed by CN=k3s-client-ca@1711057096: notBefore=2024-03-21 21:38:16 +0000 UTC notAfter=2025-03-21 21:38:16 +0000 UTC"
time="2024-03-21T21:38:16Z" level=info msg="certificate CN=system:kube-proxy signed by CN=k3s-client-ca@1711057096: notBefore=2024-03-21 21:38:16 +0000 UTC notAfter=2025-03-21 21:38:16 +0000 UTC"
time="2024-03-21T21:38:16Z" level=info msg="certificate CN=system:k3s-controller signed by CN=k3s-client-ca@1711057096: notBefore=2024-03-21 21:38:16 +0000 UTC notAfter=2025-03-21 21:38:16 +0000 UTC"
time="2024-03-21T21:38:16Z" level=info msg="certificate CN=k3s-cloud-controller-manager signed by CN=k3s-client-ca@1711057096: notBefore=2024-03-21 21:38:16 +0000 UTC notAfter=2025-03-21 21:38:16 +0000 UTC"
time="2024-03-21T21:38:16Z" level=info msg="generated self-signed CA certificate CN=k3s-server-ca@1711057096: notBefore=2024-03-21 21:38:16.013738448 +0000 UTC notAfter=2034-03-19 21:38:16.013738448 +0000 UTC"
time="2024-03-21T21:38:16Z" level=info msg="certificate CN=kube-apiserver signed by CN=k3s-server-ca@1711057096: notBefore=2024-03-21 21:38:16 +0000 UTC notAfter=2025-03-21 21:38:16 +0000 UTC"
time="2024-03-21T21:38:16Z" level=info msg="generated self-signed CA certificate CN=k3s-request-header-ca@1711057096: notBefore=2024-03-21 21:38:16.01499251 +0000 UTC notAfter=2034-03-19 21:38:16.01499251 +0000 UTC"
time="2024-03-21T21:38:16Z" level=info msg="certificate CN=system:auth-proxy signed by CN=k3s-request-header-ca@1711057096: notBefore=2024-03-21 21:38:16 +0000 UTC notAfter=2025-03-21 21:38:16 +0000 UTC"
time="2024-03-21T21:38:16Z" level=info msg="generated self-signed CA certificate CN=etcd-server-ca@1711057096: notBefore=2024-03-21 21:38:16.015997395 +0000 UTC notAfter=2034-03-19 21:38:16.015997395 +0000 UTC"
time="2024-03-21T21:38:16Z" level=info msg="certificate CN=etcd-client signed by CN=etcd-server-ca@1711057096: notBefore=2024-03-21 21:38:16 +0000 UTC notAfter=2025-03-21 21:38:16 +0000 UTC"
time="2024-03-21T21:38:16Z" level=info msg="generated self-signed CA certificate CN=etcd-peer-ca@1711057096: notBefore=2024-03-21 21:38:16.016963919 +0000 UTC notAfter=2034-03-19 21:38:16.016963919 +0000 UTC"
time="2024-03-21T21:38:16Z" level=info msg="certificate CN=etcd-peer signed by CN=etcd-peer-ca@1711057096: notBefore=2024-03-21 21:38:16 +0000 UTC notAfter=2025-03-21 21:38:16 +0000 UTC"
time="2024-03-21T21:38:16Z" level=info msg="certificate CN=etcd-server signed by CN=etcd-server-ca@1711057096: notBefore=2024-03-21 21:38:16 +0000 UTC notAfter=2025-03-21 21:38:16 +0000 UTC"
time="2024-03-21T21:38:16Z" level=info msg="Saving cluster bootstrap data to datastore"
time="2024-03-21T21:38:16Z" level=info msg="certificate CN=k3s,O=k3s signed by CN=k3s-server-ca@1711057096: notBefore=2024-03-21 21:38:16 +0000 UTC notAfter=2025-03-21 21:38:16 +0000 UTC"
time="2024-03-21T21:38:16Z" level=warning msg="dynamiclistener [::]:6443: no cached certificate available for preload - deferring certificate load until storage initialization or first client request"
time="2024-03-21T21:38:16Z" level=info msg="Active TLS secret / (ver=) (count 10): map[listener.cattle.io/cn-10.43.0.1:10.43.0.1 listener.cattle.io/cn-127.0.0.1:127.0.0.1 listener.cattle.io/cn-192.168.56.110:192.168.56.110 listener.cattle.io/cn-__1-f16284:::1 listener.cattle.io/cn-kubernetes:kubernetes listener.cattle.io/cn-kubernetes.default:kubernetes.default listener.cattle.io/cn-kubernetes.default.svc:kubernetes.default.svc listener.cattle.io/cn-kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local listener.cattle.io/cn-localhost:localhost listener.cattle.io/cn-server:server listener.cattle.io/fingerprint:SHA1=964613B919200B56262F0CCBF8300E6C3C82149A]"
time="2024-03-21T21:38:16Z" level=info msg="Running kube-apiserver --advertise-address=192.168.56.110 --advertise-port=6443 --allow-privileged=true --anonymous-auth=false --api-audiences=https://kubernetes.default.svc.cluster.local,k3s --authorization-mode=Node,RBAC --bind-address=127.0.0.1 --cert-dir=/var/lib/rancher/k3s/server/tls/temporary-certs --client-ca-file=/var/lib/rancher/k3s/server/tls/client-ca.crt --egress-selector-config-file=/var/lib/rancher/k3s/server/etc/egress-selector-config.yaml --enable-admission-plugins=NodeRestriction --enable-aggregator-routing=true --enable-bootstrap-token-auth=true --etcd-servers=unix://kine.sock --kubelet-certificate-authority=/var/lib/rancher/k3s/server/tls/server-ca.crt --kubelet-client-certificate=/var/lib/rancher/k3s/server/tls/client-kube-apiserver.crt --kubelet-client-key=/var/lib/rancher/k3s/server/tls/client-kube-apiserver.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --profiling=false --proxy-client-cert-file=/var/lib/rancher/k3s/server/tls/client-auth-proxy.crt --proxy-client-key-file=/var/lib/rancher/k3s/server/tls/client-auth-proxy.key --requestheader-allowed-names=system:auth-proxy --requestheader-client-ca-file=/var/lib/rancher/k3s/server/tls/request-header-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6444 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/var/lib/rancher/k3s/server/tls/service.key --service-account-signing-key-file=/var/lib/rancher/k3s/server/tls/service.current.key --service-cluster-ip-range=10.43.0.0/16 --service-node-port-range=30000-32767 --storage-backend=etcd3 --tls-cert-file=/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.crt --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 --tls-private-key-file=/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.key"
time="2024-03-21T21:38:16Z" level=info msg="Running kube-scheduler --authentication-kubeconfig=/var/lib/rancher/k3s/server/cred/scheduler.kubeconfig --authorization-kubeconfig=/var/lib/rancher/k3s/server/cred/scheduler.kubeconfig --bind-address=127.0.0.1 --kubeconfig=/var/lib/rancher/k3s/server/cred/scheduler.kubeconfig --leader-elect=false --profiling=false --secure-port=10259"
I0321 21:38:16.364179    2292 options.go:220] external host was not specified, using 192.168.56.110
time="2024-03-21T21:38:16Z" level=info msg="Running kube-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/var/lib/rancher/k3s/server/cred/controller.kubeconfig --authorization-kubeconfig=/var/lib/rancher/k3s/server/cred/controller.kubeconfig --bind-address=127.0.0.1 --cluster-cidr=10.42.0.0/16 --cluster-signing-kube-apiserver-client-cert-file=/var/lib/rancher/k3s/server/tls/client-ca.nochain.crt --cluster-signing-kube-apiserver-client-key-file=/var/lib/rancher/k3s/server/tls/client-ca.key --cluster-signing-kubelet-client-cert-file=/var/lib/rancher/k3s/server/tls/client-ca.nochain.crt --cluster-signing-kubelet-client-key-file=/var/lib/rancher/k3s/server/tls/client-ca.key --cluster-signing-kubelet-serving-cert-file=/var/lib/rancher/k3s/server/tls/server-ca.nochain.crt --cluster-signing-kubelet-serving-key-file=/var/lib/rancher/k3s/server/tls/server-ca.key --cluster-signing-legacy-unknown-cert-file=/var/lib/rancher/k3s/server/tls/server-ca.nochain.crt --cluster-signing-legacy-unknown-key-file=/var/lib/rancher/k3s/server/tls/server-ca.key --configure-cloud-routes=false --controllers=*,tokencleaner,-service,-route,-cloud-node-lifecycle --kubeconfig=/var/lib/rancher/k3s/server/cred/controller.kubeconfig --leader-elect=false --profiling=false --root-ca-file=/var/lib/rancher/k3s/server/tls/server-ca.crt --secure-port=10257 --service-account-private-key-file=/var/lib/rancher/k3s/server/tls/service.current.key --service-cluster-ip-range=10.43.0.0/16 --use-service-account-credentials=true"
I0321 21:38:16.365014    2292 server.go:156] Version: v1.28.7+k3s1
I0321 21:38:16.365038    2292 server.go:158] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
time="2024-03-21T21:38:16Z" level=info msg="Running cloud-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/var/lib/rancher/k3s/server/cred/cloud-controller.kubeconfig --authorization-kubeconfig=/var/lib/rancher/k3s/server/cred/cloud-controller.kubeconfig --bind-address=127.0.0.1 --cloud-config=/var/lib/rancher/k3s/server/etc/cloud-config.yaml --cloud-provider=k3s --cluster-cidr=10.42.0.0/16 --configure-cloud-routes=false --controllers=*,-route --feature-gates=CloudDualStackNodeIPs=true --kubeconfig=/var/lib/rancher/k3s/server/cred/cloud-controller.kubeconfig --leader-elect=false --leader-elect-resource-name=k3s-cloud-controller-manager --node-status-update-frequency=1m0s --profiling=false"
time="2024-03-21T21:38:16Z" level=info msg="Server node token is available at /var/lib/rancher/k3s/server/token"
time="2024-03-21T21:38:16Z" level=info msg="To join server node to cluster: k3s server -s https://10.0.2.15:6443 -t ${SERVER_NODE_TOKEN}"
time="2024-03-21T21:38:16Z" level=info msg="Agent node token is available at /var/lib/rancher/k3s/server/agent-token"
time="2024-03-21T21:38:16Z" level=info msg="To join agent node to cluster: k3s agent -s https://10.0.2.15:6443 -t ${AGENT_NODE_TOKEN}"
time="2024-03-21T21:38:16Z" level=info msg="Waiting for API server to become available"
time="2024-03-21T21:38:16Z" level=info msg="Wrote kubeconfig /etc/rancher/k3s/k3s.yaml"
time="2024-03-21T21:38:16Z" level=info msg="Run: k3s kubectl"
I0321 21:38:16.702988    2292 shared_informer.go:311] Waiting for caches to sync for node_authorizer
I0321 21:38:16.704765    2292 plugins.go:158] Loaded 12 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,TaintNodesByCondition,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,MutatingAdmissionWebhook.
I0321 21:38:16.704844    2292 plugins.go:161] Loaded 13 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,ClusterTrustBundleAttest,CertificateSubjectRestriction,ValidatingAdmissionPolicy,ValidatingAdmissionWebhook,ResourceQuota.
I0321 21:38:16.705416    2292 instance.go:298] Using reconciler: lease
I0321 21:38:16.716222    2292 handler.go:275] Adding GroupVersion apiextensions.k8s.io v1 to ResourceManager
W0321 21:38:16.716246    2292 genericapiserver.go:744] Skipping API apiextensions.k8s.io/v1beta1 because it has no resources.
I0321 21:38:16.814805    2292 handler.go:275] Adding GroupVersion  v1 to ResourceManager
I0321 21:38:16.817187    2292 instance.go:709] API group "internal.apiserver.k8s.io" is not enabled, skipping.
I0321 21:38:16.956901    2292 instance.go:709] API group "resource.k8s.io" is not enabled, skipping.
I0321 21:38:16.963963    2292 handler.go:275] Adding GroupVersion authentication.k8s.io v1 to ResourceManager
W0321 21:38:16.964275    2292 genericapiserver.go:744] Skipping API authentication.k8s.io/v1beta1 because it has no resources.
W0321 21:38:16.964342    2292 genericapiserver.go:744] Skipping API authentication.k8s.io/v1alpha1 because it has no resources.
I0321 21:38:16.964911    2292 handler.go:275] Adding GroupVersion authorization.k8s.io v1 to ResourceManager
W0321 21:38:16.964971    2292 genericapiserver.go:744] Skipping API authorization.k8s.io/v1beta1 because it has no resources.
I0321 21:38:16.965635    2292 handler.go:275] Adding GroupVersion autoscaling v2 to ResourceManager
I0321 21:38:16.966306    2292 handler.go:275] Adding GroupVersion autoscaling v1 to ResourceManager
W0321 21:38:16.966434    2292 genericapiserver.go:744] Skipping API autoscaling/v2beta1 because it has no resources.
W0321 21:38:16.966485    2292 genericapiserver.go:744] Skipping API autoscaling/v2beta2 because it has no resources.
I0321 21:38:16.967602    2292 handler.go:275] Adding GroupVersion batch v1 to ResourceManager
W0321 21:38:16.967788    2292 genericapiserver.go:744] Skipping API batch/v1beta1 because it has no resources.
I0321 21:38:16.968408    2292 handler.go:275] Adding GroupVersion certificates.k8s.io v1 to ResourceManager
W0321 21:38:16.968563    2292 genericapiserver.go:744] Skipping API certificates.k8s.io/v1beta1 because it has no resources.
W0321 21:38:16.968616    2292 genericapiserver.go:744] Skipping API certificates.k8s.io/v1alpha1 because it has no resources.
I0321 21:38:16.969115    2292 handler.go:275] Adding GroupVersion coordination.k8s.io v1 to ResourceManager
W0321 21:38:16.969265    2292 genericapiserver.go:744] Skipping API coordination.k8s.io/v1beta1 because it has no resources.
W0321 21:38:16.969433    2292 genericapiserver.go:744] Skipping API discovery.k8s.io/v1beta1 because it has no resources.
I0321 21:38:16.969893    2292 handler.go:275] Adding GroupVersion discovery.k8s.io v1 to ResourceManager
I0321 21:38:16.971035    2292 handler.go:275] Adding GroupVersion networking.k8s.io v1 to ResourceManager
W0321 21:38:16.971242    2292 genericapiserver.go:744] Skipping API networking.k8s.io/v1beta1 because it has no resources.
W0321 21:38:16.971360    2292 genericapiserver.go:744] Skipping API networking.k8s.io/v1alpha1 because it has no resources.
I0321 21:38:16.971758    2292 handler.go:275] Adding GroupVersion node.k8s.io v1 to ResourceManager
W0321 21:38:16.971888    2292 genericapiserver.go:744] Skipping API node.k8s.io/v1beta1 because it has no resources.
W0321 21:38:16.972037    2292 genericapiserver.go:744] Skipping API node.k8s.io/v1alpha1 because it has no resources.
I0321 21:38:16.972659    2292 handler.go:275] Adding GroupVersion policy v1 to ResourceManager
W0321 21:38:16.972833    2292 genericapiserver.go:744] Skipping API policy/v1beta1 because it has no resources.
I0321 21:38:16.973990    2292 handler.go:275] Adding GroupVersion rbac.authorization.k8s.io v1 to ResourceManager
W0321 21:38:16.974164    2292 genericapiserver.go:744] Skipping API rbac.authorization.k8s.io/v1beta1 because it has no resources.
W0321 21:38:16.974170    2292 genericapiserver.go:744] Skipping API rbac.authorization.k8s.io/v1alpha1 because it has no resources.
I0321 21:38:16.974460    2292 handler.go:275] Adding GroupVersion scheduling.k8s.io v1 to ResourceManager
W0321 21:38:16.974467    2292 genericapiserver.go:744] Skipping API scheduling.k8s.io/v1beta1 because it has no resources.
W0321 21:38:16.974470    2292 genericapiserver.go:744] Skipping API scheduling.k8s.io/v1alpha1 because it has no resources.
I0321 21:38:16.975952    2292 handler.go:275] Adding GroupVersion storage.k8s.io v1 to ResourceManager
W0321 21:38:16.975959    2292 genericapiserver.go:744] Skipping API storage.k8s.io/v1beta1 because it has no resources.
W0321 21:38:16.975962    2292 genericapiserver.go:744] Skipping API storage.k8s.io/v1alpha1 because it has no resources.
I0321 21:38:16.976990    2292 handler.go:275] Adding GroupVersion flowcontrol.apiserver.k8s.io v1beta3 to ResourceManager
I0321 21:38:16.977922    2292 handler.go:275] Adding GroupVersion flowcontrol.apiserver.k8s.io v1beta2 to ResourceManager
W0321 21:38:16.977930    2292 genericapiserver.go:744] Skipping API flowcontrol.apiserver.k8s.io/v1beta1 because it has no resources.
W0321 21:38:16.977934    2292 genericapiserver.go:744] Skipping API flowcontrol.apiserver.k8s.io/v1alpha1 because it has no resources.
I0321 21:38:16.981456    2292 handler.go:275] Adding GroupVersion apps v1 to ResourceManager
W0321 21:38:16.981686    2292 genericapiserver.go:744] Skipping API apps/v1beta2 because it has no resources.
W0321 21:38:16.981763    2292 genericapiserver.go:744] Skipping API apps/v1beta1 because it has no resources.
I0321 21:38:16.982795    2292 handler.go:275] Adding GroupVersion admissionregistration.k8s.io v1 to ResourceManager
W0321 21:38:16.982872    2292 genericapiserver.go:744] Skipping API admissionregistration.k8s.io/v1beta1 because it has no resources.
W0321 21:38:16.983074    2292 genericapiserver.go:744] Skipping API admissionregistration.k8s.io/v1alpha1 because it has no resources.
I0321 21:38:16.983692    2292 handler.go:275] Adding GroupVersion events.k8s.io v1 to ResourceManager
W0321 21:38:16.983825    2292 genericapiserver.go:744] Skipping API events.k8s.io/v1beta1 because it has no resources.
I0321 21:38:16.986623    2292 handler.go:275] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
W0321 21:38:16.988460    2292 genericapiserver.go:744] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
time="2024-03-21T21:38:16Z" level=info msg="Password verified locally for node server"
time="2024-03-21T21:38:16Z" level=info msg="certificate CN=server signed by CN=k3s-server-ca@1711057096: notBefore=2024-03-21 21:38:16 +0000 UTC notAfter=2025-03-21 21:38:16 +0000 UTC"
time="2024-03-21T21:38:17Z" level=info msg="certificate CN=system:node:server,O=system:nodes signed by CN=k3s-client-ca@1711057096: notBefore=2024-03-21 21:38:16 +0000 UTC notAfter=2025-03-21 21:38:17 +0000 UTC"
I0321 21:38:17.493145    2292 secure_serving.go:213] Serving securely on 127.0.0.1:6444
I0321 21:38:17.493292    2292 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/rancher/k3s/server/tls/request-header-ca.crt"
I0321 21:38:17.493567    2292 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.crt::/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.key"
I0321 21:38:17.493678    2292 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0321 21:38:17.494295    2292 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/rancher/k3s/server/tls/client-auth-proxy.crt::/var/lib/rancher/k3s/server/tls/client-auth-proxy.key"
I0321 21:38:17.494441    2292 aggregator.go:164] waiting for initial CRD sync...
I0321 21:38:17.494456    2292 controller.go:78] Starting OpenAPI AggregationController
I0321 21:38:17.494475    2292 controller.go:80] Starting OpenAPI V3 AggregationController
I0321 21:38:17.494516    2292 handler_discovery.go:412] Starting ResourceDiscoveryManager
I0321 21:38:17.494972    2292 controller.go:116] Starting legacy_token_tracking_controller
I0321 21:38:17.494986    2292 shared_informer.go:311] Waiting for caches to sync for configmaps
I0321 21:38:17.495395    2292 available_controller.go:423] Starting AvailableConditionController
I0321 21:38:17.495407    2292 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0321 21:38:17.495644    2292 customresource_discovery_controller.go:289] Starting DiscoveryController
I0321 21:38:17.499622    2292 gc_controller.go:78] Starting apiserver lease garbage collector
I0321 21:38:17.500201    2292 apf_controller.go:374] Starting API Priority and Fairness config controller
I0321 21:38:17.500457    2292 gc_controller.go:78] Starting apiserver lease garbage collector
I0321 21:38:17.500522    2292 system_namespaces_controller.go:67] Starting system namespaces controller
I0321 21:38:17.500777    2292 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0321 21:38:17.500789    2292 shared_informer.go:311] Waiting for caches to sync for cluster_authentication_trust_controller
I0321 21:38:17.500844    2292 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0321 21:38:17.500850    2292 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0321 21:38:17.500940    2292 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/rancher/k3s/server/tls/client-ca.crt"
I0321 21:38:17.501461    2292 crdregistration_controller.go:111] Starting crd-autoregister controller
I0321 21:38:17.501481    2292 shared_informer.go:311] Waiting for caches to sync for crd-autoregister
I0321 21:38:17.513059    2292 controller.go:134] Starting OpenAPI controller
I0321 21:38:17.513101    2292 controller.go:85] Starting OpenAPI V3 controller
I0321 21:38:17.513339    2292 naming_controller.go:291] Starting NamingConditionController
I0321 21:38:17.513378    2292 establishing_controller.go:76] Starting EstablishingController
I0321 21:38:17.513390    2292 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0321 21:38:17.513399    2292 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0321 21:38:17.513412    2292 crd_finalizer.go:266] Starting CRDFinalizer
I0321 21:38:17.513917    2292 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/rancher/k3s/server/tls/client-ca.crt"
I0321 21:38:17.514000    2292 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/rancher/k3s/server/tls/request-header-ca.crt"
time="2024-03-21T21:38:17Z" level=info msg="Module overlay was already loaded"
time="2024-03-21T21:38:17Z" level=info msg="Module br_netfilter was already loaded"
time="2024-03-21T21:38:17Z" level=info msg="Set sysctl 'net/ipv4/conf/all/forwarding' to 1"
time="2024-03-21T21:38:17Z" level=info msg="Set sysctl 'net/netfilter/nf_conntrack_max' to 131072"
time="2024-03-21T21:38:17Z" level=info msg="Set sysctl 'net/netfilter/nf_conntrack_tcp_timeout_established' to 86400"
time="2024-03-21T21:38:17Z" level=info msg="Set sysctl 'net/netfilter/nf_conntrack_tcp_timeout_close_wait' to 3600"
time="2024-03-21T21:38:17Z" level=info msg="Logging containerd to /var/lib/rancher/k3s/agent/containerd/containerd.log"
time="2024-03-21T21:38:17Z" level=info msg="Running containerd -c /var/lib/rancher/k3s/agent/etc/containerd/config.toml -a /run/k3s/containerd/containerd.sock --state /run/k3s/containerd --root /var/lib/rancher/k3s/agent/containerd"
E0321 21:38:17.679013    2292 controller.go:146] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
I0321 21:38:17.696985    2292 shared_informer.go:318] Caches are synced for configmaps
I0321 21:38:17.697120    2292 cache.go:39] Caches are synced for AvailableConditionController controller
I0321 21:38:17.700885    2292 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0321 21:38:17.700951    2292 shared_informer.go:318] Caches are synced for cluster_authentication_trust_controller
I0321 21:38:17.700984    2292 apf_controller.go:379] Running API Priority and Fairness config worker
I0321 21:38:17.700993    2292 apf_controller.go:382] Running API Priority and Fairness periodic rebalancing process
I0321 21:38:17.702233    2292 shared_informer.go:318] Caches are synced for crd-autoregister
I0321 21:38:17.702274    2292 aggregator.go:166] initial CRD sync complete...
I0321 21:38:17.702280    2292 autoregister_controller.go:141] Starting autoregister controller
I0321 21:38:17.702284    2292 cache.go:32] Waiting for caches to sync for autoregister controller
I0321 21:38:17.702288    2292 cache.go:39] Caches are synced for autoregister controller
I0321 21:38:17.702851    2292 controller.go:624] quota admission added evaluator for: namespaces
I0321 21:38:17.704620    2292 shared_informer.go:318] Caches are synced for node_authorizer
E0321 21:38:17.754442    2292 controller.go:95] Unable to perform initial Kubernetes service initialization: namespaces "default" not found
I0321 21:38:17.887491    2292 controller.go:624] quota admission added evaluator for: leases.coordination.k8s.io
I0321 21:38:18.531825    2292 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0321 21:38:18.542537    2292 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0321 21:38:18.543707    2292 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
time="2024-03-21T21:38:18Z" level=info msg="containerd is now running"
time="2024-03-21T21:38:18Z" level=info msg="Running kubelet --address=0.0.0.0 --allowed-unsafe-sysctls=net.ipv4.ip_forward,net.ipv6.conf.all.forwarding --anonymous-auth=false --authentication-token-webhook=true --authorization-mode=Webhook --cgroup-driver=systemd --client-ca-file=/var/lib/rancher/k3s/agent/client-ca.crt --cloud-provider=external --cluster-dns=10.43.0.10 --cluster-domain=cluster.local --container-runtime-endpoint=unix:///run/k3s/containerd/containerd.sock --containerd=/run/k3s/containerd/containerd.sock --eviction-hard=imagefs.available<5%,nodefs.available<5% --eviction-minimum-reclaim=imagefs.available=10%,nodefs.available=10% --fail-swap-on=false --feature-gates=CloudDualStackNodeIPs=true --healthz-bind-address=127.0.0.1 --hostname-override=server --kubeconfig=/var/lib/rancher/k3s/agent/kubelet.kubeconfig --node-ip=192.168.56.110 --node-labels= --pod-infra-container-image=rancher/mirrored-pause:3.6 --pod-manifest-path=/var/lib/rancher/k3s/agent/pod-manifests --read-only-port=0 --resolv-conf=/run/systemd/resolve/resolv.conf --serialize-image-pulls=false --tls-cert-file=/var/lib/rancher/k3s/agent/serving-kubelet.crt --tls-private-key-file=/var/lib/rancher/k3s/agent/serving-kubelet.key"
time="2024-03-21T21:38:18Z" level=info msg="Connecting to proxy" url="wss://127.0.0.1:6443/v1-k3s/connect"
time="2024-03-21T21:38:18Z" level=info msg="Handling backend connection request [server]"
time="2024-03-21T21:38:18Z" level=info msg="Waiting to retrieve kube-proxy configuration; server is not ready: https://127.0.0.1:6443/v1-k3s/readyz: 500 Internal Server Error"
I0321 21:38:18.812079    2292 controller.go:624] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0321 21:38:18.835710    2292 controller.go:624] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0321 21:38:18.882157    2292 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.43.0.1"}
W0321 21:38:18.886082    2292 lease.go:263] Resetting endpoints for master service "kubernetes" to [192.168.56.110]
I0321 21:38:18.886927    2292 controller.go:624] quota admission added evaluator for: endpoints
I0321 21:38:18.889897    2292 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
time="2024-03-21T21:38:19Z" level=info msg="Creating k3s-supervisor event broadcaster"
time="2024-03-21T21:38:19Z" level=info msg="Waiting for cloud-controller-manager privileges to become available"
time="2024-03-21T21:38:19Z" level=info msg="Kube API server is now running"
time="2024-03-21T21:38:19Z" level=info msg="ETCD server is now running"
time="2024-03-21T21:38:19Z" level=info msg="k3s is up and running"
time="2024-03-21T21:38:19Z" level=info msg="Applying CRD addons.k3s.cattle.io"
time="2024-03-21T21:38:19Z" level=info msg="Applying CRD etcdsnapshotfiles.k3s.cattle.io"
time="2024-03-21T21:38:19Z" level=info msg="Applying CRD helmcharts.helm.cattle.io"
Flag --cloud-provider has been deprecated, will be removed in 1.25 or later, in favor of removing cloud provider code from Kubelet.
Flag --containerd has been deprecated, This is a cadvisor flag that was mistakenly registered with the Kubelet. Due to legacy concerns, it will follow the standard CLI deprecation timeline before being removed.
Flag --pod-infra-container-image has been deprecated, will be removed in a future release. Image garbage collector will get sandbox image information from CRI.
I0321 21:38:19.741843    2292 server.go:202] "--pod-infra-container-image will not be pruned by the image garbage collector in kubelet and should also be set in the remote runtime"
I0321 21:38:19.746266    2292 server.go:462] "Kubelet version" kubeletVersion="v1.28.7+k3s1"
I0321 21:38:19.746345    2292 server.go:464] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0321 21:38:19.748576    2292 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/rancher/k3s/agent/client-ca.crt"
I0321 21:38:19.768130    2292 handler.go:275] Adding GroupVersion k3s.cattle.io v1 to ResourceManager
I0321 21:38:19.774633    2292 server.go:720] "--cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /"
I0321 21:38:19.776457    2292 container_manager_linux.go:265] "Container manager verified user specified cgroup-root exists" cgroupRoot=[]
I0321 21:38:19.776776    2292 container_manager_linux.go:270] "Creating Container Manager object based on Node Config" nodeConfig={"RuntimeCgroupsName":"","SystemCgroupsName":"","KubeletCgroupsName":"","KubeletOOMScoreAdj":-999,"ContainerRuntime":"","CgroupsPerQOS":true,"CgroupRoot":"/","CgroupDriver":"systemd","KubeletRootDir":"/var/lib/kubelet","ProtectKernelDefaults":false,"KubeReservedCgroupName":"","SystemReservedCgroupName":"","ReservedSystemCPUs":{},"EnforceNodeAllocatable":{"pods":{}},"KubeReserved":null,"SystemReserved":null,"HardEvictionThresholds":[{"Signal":"imagefs.available","Operator":"LessThan","Value":{"Quantity":null,"Percentage":0.05},"GracePeriod":0,"MinReclaim":null},{"Signal":"nodefs.available","Operator":"LessThan","Value":{"Quantity":null,"Percentage":0.05},"GracePeriod":0,"MinReclaim":null}],"QOSReserved":{},"CPUManagerPolicy":"none","CPUManagerPolicyOptions":null,"TopologyManagerScope":"container","CPUManagerReconcilePeriod":10000000000,"ExperimentalMemoryManagerPolicy":"None","ExperimentalMemoryManagerReservedMemory":null,"PodPidsLimit":-1,"EnforceCPULimits":true,"CPUCFSQuotaPeriod":100000000,"TopologyManagerPolicy":"none","TopologyManagerPolicyOptions":null}
I0321 21:38:19.777319    2292 topology_manager.go:138] "Creating topology manager with none policy"
I0321 21:38:19.777406    2292 container_manager_linux.go:301] "Creating device plugin manager"
I0321 21:38:19.777950    2292 state_mem.go:36] "Initialized new in-memory state store"
I0321 21:38:19.778919    2292 kubelet.go:393] "Attempting to sync node with API server"
I0321 21:38:19.779015    2292 kubelet.go:298] "Adding static pod path" path="/var/lib/rancher/k3s/agent/pod-manifests"
I0321 21:38:19.779102    2292 kubelet.go:309] "Adding apiserver pod source"
I0321 21:38:19.779179    2292 apiserver.go:42] "Waiting for node sync before watching apiserver pods"
I0321 21:38:19.787708    2292 kuberuntime_manager.go:257] "Container runtime initialized" containerRuntime="containerd" version="v1.7.11-k3s2" apiVersion="v1"
W0321 21:38:19.790811    2292 probe.go:268] Flexvolume plugin directory at /usr/libexec/kubernetes/kubelet-plugins/volume/exec/ does not exist. Recreating.
I0321 21:38:19.792124    2292 server.go:1227] "Started kubelet"
I0321 21:38:19.796125    2292 fs_resource_analyzer.go:67] "Starting FS ResourceAnalyzer"
I0321 21:38:19.803192    2292 handler.go:275] Adding GroupVersion k3s.cattle.io v1 to ResourceManager
I0321 21:38:19.810118    2292 server.go:162] "Starting to listen" address="0.0.0.0" port=10250
I0321 21:38:19.811515    2292 server.go:462] "Adding debug handlers to kubelet server"
I0321 21:38:19.812108    2292 ratelimit.go:65] "Setting rate limiting for podresources endpoint" qps=100 burstTokens=10
I0321 21:38:19.812217    2292 server.go:233] "Starting to serve the podresources API" endpoint="unix:/var/lib/kubelet/pod-resources/kubelet.sock"
I0321 21:38:19.818651    2292 volume_manager.go:291] "Starting Kubelet Volume Manager"
I0321 21:38:19.819220    2292 desired_state_of_world_populator.go:151] "Desired state populator starts to run"
I0321 21:38:19.819730    2292 reconciler_new.go:29] "Reconciler: start to sync state"
E0321 21:38:19.820073    2292 cri_stats_provider.go:448] "Failed to get the info of the filesystem with mountpoint" err="unable to find data in memory cache" mountpoint="/var/lib/rancher/k3s/agent/containerd/io.containerd.snapshotter.v1.overlayfs"
E0321 21:38:19.820189    2292 kubelet.go:1431] "Image garbage collection failed once. Stats initialization may not have completed yet" err="invalid capacity 0 on image filesystem"
time="2024-03-21T21:38:19Z" level=info msg="Applying CRD helmchartconfigs.helm.cattle.io"
I0321 21:38:19.869434    2292 serving.go:355] Generated self-signed cert in-memory
E0321 21:38:19.913845    2292 nodelease.go:49] "Failed to get node when trying to set owner ref to the node lease" err="nodes \"server\" not found" node="server"
I0321 21:38:19.921082    2292 cpu_manager.go:214] "Starting CPU manager" policy="none"
I0321 21:38:19.921258    2292 cpu_manager.go:215] "Reconciling" reconcilePeriod="10s"
I0321 21:38:19.921356    2292 state_mem.go:36] "Initialized new in-memory state store"
I0321 21:38:19.926355    2292 kubelet_network_linux.go:50] "Initialized iptables rules." protocol="IPv4"
I0321 21:38:19.922312    2292 kubelet_node_status.go:70] "Attempting to register node" node="server"
I0321 21:38:19.924262    2292 handler.go:275] Adding GroupVersion helm.cattle.io v1 to ResourceManager
I0321 21:38:19.928976    2292 policy_none.go:49] "None policy: Start"
I0321 21:38:19.946803    2292 kubelet_network_linux.go:50] "Initialized iptables rules." protocol="IPv6"
I0321 21:38:19.947358    2292 status_manager.go:217] "Starting to sync pod status with apiserver"
I0321 21:38:19.947413    2292 kubelet.go:2303] "Starting kubelet main sync loop"
E0321 21:38:19.947516    2292 kubelet.go:2327] "Skipping pod synchronization" err="[container runtime status check may not have completed yet, PLEG is not healthy: pleg has yet to be successful]"
I0321 21:38:19.947725    2292 memory_manager.go:169] "Starting memorymanager" policy="None"
I0321 21:38:19.947816    2292 state_mem.go:35] "Initializing new in-memory state store"
I0321 21:38:19.962719    2292 kubelet_node_status.go:73] "Successfully registered node" node="server"
time="2024-03-21T21:38:19Z" level=info msg="Waiting for CRD helmchartconfigs.helm.cattle.io to become available"
I0321 21:38:19.973069    2292 handler.go:275] Adding GroupVersion helm.cattle.io v1 to ResourceManager
E0321 21:38:19.973580    2292 kubelet_node_status.go:540] "Error updating node status, will retry" err="error getting node \"server\": nodes \"server\" not found"
time="2024-03-21T21:38:19Z" level=info msg="Annotations and labels have been set successfully on node: server"
time="2024-03-21T21:38:20Z" level=info msg="Starting flannel with backend vxlan"
I0321 21:38:20.029959    2292 manager.go:471] "Failed to read data from checkpoint" checkpoint="kubelet_internal_checkpoint" err="checkpoint is not found"
I0321 21:38:20.031960    2292 plugin_manager.go:118] "Starting Kubelet Plugin Manager"
I0321 21:38:20.035769    2292 kubelet_node_status.go:493] "Fast updating node status as it just became ready"
W0321 21:38:20.040686    2292 watcher.go:93] Error while processing event ("/sys/fs/cgroup/devices/kubepods.slice/kubepods-besteffort.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/devices/kubepods.slice/kubepods-besteffort.slice: no such file or directory
I0321 21:38:20.269826    2292 controllermanager.go:189] "Starting" version="v1.28.7+k3s1"
I0321 21:38:20.269845    2292 controllermanager.go:191] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0321 21:38:20.279314    2292 secure_serving.go:213] Serving securely on 127.0.0.1:10257
I0321 21:38:20.279417    2292 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0321 21:38:20.279696    2292 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0321 21:38:20.279486    2292 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I0321 21:38:20.279762    2292 shared_informer.go:311] Waiting for caches to sync for RequestHeaderAuthRequestController
I0321 21:38:20.279494    2292 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0321 21:38:20.280060    2292 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0321 21:38:20.279558    2292 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0321 21:38:20.286040    2292 shared_informer.go:311] Waiting for caches to sync for tokens
I0321 21:38:20.290121    2292 controller.go:624] quota admission added evaluator for: serviceaccounts
I0321 21:38:20.291935    2292 controllermanager.go:642] "Started controller" controller="persistentvolume-attach-detach-controller"
I0321 21:38:20.292139    2292 attach_detach_controller.go:337] "Starting attach detach controller"
I0321 21:38:20.292255    2292 shared_informer.go:311] Waiting for caches to sync for attach detach
I0321 21:38:20.297908    2292 controllermanager.go:642] "Started controller" controller="persistentvolumeclaim-protection-controller"
I0321 21:38:20.297996    2292 pvc_protection_controller.go:102] "Starting PVC protection controller"
I0321 21:38:20.298075    2292 shared_informer.go:311] Waiting for caches to sync for PVC protection
I0321 21:38:20.303304    2292 controllermanager.go:642] "Started controller" controller="persistentvolume-protection-controller"
I0321 21:38:20.303488    2292 pv_protection_controller.go:78] "Starting PV protection controller"
I0321 21:38:20.303498    2292 shared_informer.go:311] Waiting for caches to sync for PV protection
I0321 21:38:20.308250    2292 controllermanager.go:642] "Started controller" controller="replicationcontroller-controller"
I0321 21:38:20.308459    2292 replica_set.go:214] "Starting controller" name="replicationcontroller"
I0321 21:38:20.308529    2292 shared_informer.go:311] Waiting for caches to sync for ReplicationController
I0321 21:38:20.316156    2292 controllermanager.go:642] "Started controller" controller="garbage-collector-controller"
I0321 21:38:20.316949    2292 garbagecollector.go:155] "Starting controller" controller="garbagecollector"
I0321 21:38:20.316972    2292 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0321 21:38:20.317014    2292 graph_builder.go:294] "Running" component="GraphBuilder"
I0321 21:38:20.326060    2292 controllermanager.go:642] "Started controller" controller="horizontal-pod-autoscaler-controller"
I0321 21:38:20.326688    2292 horizontal.go:200] "Starting HPA controller"
I0321 21:38:20.326742    2292 shared_informer.go:311] Waiting for caches to sync for HPA
I0321 21:38:20.331977    2292 controllermanager.go:642] "Started controller" controller="certificatesigningrequest-approving-controller"
I0321 21:38:20.332059    2292 certificate_controller.go:115] "Starting certificate controller" name="csrapproving"
I0321 21:38:20.332067    2292 shared_informer.go:311] Waiting for caches to sync for certificate-csrapproving
I0321 21:38:20.339547    2292 node_lifecycle_controller.go:431] "Controller will reconcile labels"
I0321 21:38:20.339694    2292 controllermanager.go:642] "Started controller" controller="node-lifecycle-controller"
I0321 21:38:20.339786    2292 node_lifecycle_controller.go:465] "Sending events to api server"
I0321 21:38:20.339951    2292 node_lifecycle_controller.go:476] "Starting node controller"
I0321 21:38:20.339988    2292 shared_informer.go:311] Waiting for caches to sync for taint
I0321 21:38:20.382434    2292 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0321 21:38:20.382758    2292 shared_informer.go:318] Caches are synced for RequestHeaderAuthRequestController
I0321 21:38:20.382815    2292 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0321 21:38:20.387052    2292 shared_informer.go:318] Caches are synced for tokens
time="2024-03-21T21:38:20Z" level=info msg="Done waiting for CRD helmchartconfigs.helm.cattle.io to become available"
time="2024-03-21T21:38:20Z" level=info msg="Writing static file: /var/lib/rancher/k3s/server/static/charts/traefik-crd-25.0.2+up25.0.0.tgz"
time="2024-03-21T21:38:20Z" level=info msg="Writing static file: /var/lib/rancher/k3s/server/static/charts/traefik-25.0.2+up25.0.0.tgz"
I0321 21:38:20.495302    2292 controllermanager.go:642] "Started controller" controller="ephemeral-volume-controller"
I0321 21:38:20.495398    2292 controller.go:169] "Starting ephemeral volume controller"
I0321 21:38:20.495413    2292 shared_informer.go:311] Waiting for caches to sync for ephemeral
time="2024-03-21T21:38:20Z" level=info msg="Failed to get existing traefik HelmChart" error="helmcharts.helm.cattle.io \"traefik\" not found"
time="2024-03-21T21:38:20Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/rolebindings.yaml"
time="2024-03-21T21:38:20Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/traefik.yaml"
time="2024-03-21T21:38:20Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/auth-reader.yaml"
time="2024-03-21T21:38:20Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/metrics-apiservice.yaml"
time="2024-03-21T21:38:20Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-service.yaml"
time="2024-03-21T21:38:20Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/resource-reader.yaml"
time="2024-03-21T21:38:20Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/ccm.yaml"
time="2024-03-21T21:38:20Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/coredns.yaml"
time="2024-03-21T21:38:20Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/auth-delegator.yaml"
time="2024-03-21T21:38:20Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-deployment.yaml"
time="2024-03-21T21:38:20Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/local-storage.yaml"
time="2024-03-21T21:38:20Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/aggregated-metrics-reader.yaml"
time="2024-03-21T21:38:20Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/runtimes.yaml"
time="2024-03-21T21:38:20Z" level=info msg="Starting dynamiclistener CN filter node controller"
time="2024-03-21T21:38:20Z" level=info msg="Tunnel server egress proxy mode: agent"
time="2024-03-21T21:38:20Z" level=info msg="Starting k3s.cattle.io/v1, Kind=Addon controller"
time="2024-03-21T21:38:20Z" level=info msg="Creating deploy event broadcaster"
time="2024-03-21T21:38:20Z" level=info msg="Creating helm-controller event broadcaster"
time="2024-03-21T21:38:20Z" level=info msg="Starting /v1, Kind=Node controller"
time="2024-03-21T21:38:20Z" level=warning msg="Unable to fetch coredns config map: configmaps \"coredns\" not found"
time="2024-03-21T21:38:20Z" level=info msg="Cluster dns configmap has been set successfully"
time="2024-03-21T21:38:20Z" level=info msg="Labels and annotations have been set successfully on node: server"
time="2024-03-21T21:38:20Z" level=info msg="Starting helm.cattle.io/v1, Kind=HelmChart controller"
time="2024-03-21T21:38:20Z" level=info msg="Starting helm.cattle.io/v1, Kind=HelmChartConfig controller"
time="2024-03-21T21:38:20Z" level=info msg="Starting rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding controller"
time="2024-03-21T21:38:20Z" level=info msg="Starting batch/v1, Kind=Job controller"
time="2024-03-21T21:38:20Z" level=info msg="Starting /v1, Kind=Secret controller"
time="2024-03-21T21:38:20Z" level=info msg="Starting /v1, Kind=ConfigMap controller"
time="2024-03-21T21:38:20Z" level=info msg="Starting /v1, Kind=ServiceAccount controller"
I0321 21:38:20.783267    2292 apiserver.go:52] "Watching apiserver"
I0321 21:38:20.801567    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="rolebindings.rbac.authorization.k8s.io"
I0321 21:38:20.801815    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="roles.rbac.authorization.k8s.io"
I0321 21:38:20.801919    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="leases.coordination.k8s.io"
I0321 21:38:20.802157    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="serviceaccounts"
I0321 21:38:20.802278    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="limitranges"
I0321 21:38:20.802467    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="poddisruptionbudgets.policy"
I0321 21:38:20.802621    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="replicasets.apps"
I0321 21:38:20.802781    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="controllerrevisions.apps"
I0321 21:38:20.802925    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="jobs.batch"
I0321 21:38:20.803289    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="helmchartconfigs.helm.cattle.io"
I0321 21:38:20.803440    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="endpoints"
I0321 21:38:20.803662    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="csistoragecapacities.storage.k8s.io"
I0321 21:38:20.803837    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="endpointslices.discovery.k8s.io"
I0321 21:38:20.804082    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="horizontalpodautoscalers.autoscaling"
I0321 21:38:20.804251    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="podtemplates"
I0321 21:38:20.804381    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="deployments.apps"
I0321 21:38:20.804550    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingresses.networking.k8s.io"
I0321 21:38:20.804661    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="helmcharts.helm.cattle.io"
I0321 21:38:20.804841    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="cronjobs.batch"
I0321 21:38:20.804998    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="networkpolicies.networking.k8s.io"
I0321 21:38:20.805179    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="daemonsets.apps"
I0321 21:38:20.805313    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="statefulsets.apps"
I0321 21:38:20.805461    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="addons.k3s.cattle.io"
I0321 21:38:20.805610    2292 controllermanager.go:642] "Started controller" controller="resourcequota-controller"
I0321 21:38:20.805673    2292 resource_quota_controller.go:294] "Starting resource quota controller"
I0321 21:38:20.805826    2292 shared_informer.go:311] Waiting for caches to sync for resource quota
I0321 21:38:20.805918    2292 resource_quota_monitor.go:305] "QuotaMonitor running"
I0321 21:38:20.824555    2292 desired_state_of_world_populator.go:159] "Finished populating initial desired state of world"
time="2024-03-21T21:38:20Z" level=warning msg="Unable to fetch coredns config map: configmaps \"coredns\" not found"
I0321 21:38:20.951055    2292 controllermanager.go:642] "Started controller" controller="deployment-controller"
I0321 21:38:20.951912    2292 deployment_controller.go:168] "Starting controller" controller="deployment"
I0321 21:38:20.952228    2292 shared_informer.go:311] Waiting for caches to sync for deployment
I0321 21:38:21.119896    2292 controllermanager.go:642] "Started controller" controller="replicaset-controller"
I0321 21:38:21.120148    2292 controllermanager.go:607] "Warning: controller is disabled" controller="node-route-controller"
I0321 21:38:21.121047    2292 replica_set.go:214] "Starting controller" name="replicaset"
I0321 21:38:21.121516    2292 shared_informer.go:311] Waiting for caches to sync for ReplicaSet
time="2024-03-21T21:38:21Z" level=warning msg="Unable to fetch coredns config map: configmaps \"coredns\" not found"
I0321 21:38:21.247197    2292 controllermanager.go:642] "Started controller" controller="persistentvolume-binder-controller"
I0321 21:38:21.247281    2292 controllermanager.go:607] "Warning: controller is disabled" controller="cloud-node-lifecycle-controller"
I0321 21:38:21.247952    2292 pv_controller_base.go:319] "Starting persistent volume controller"
I0321 21:38:21.248022    2292 shared_informer.go:311] Waiting for caches to sync for persistent volume
I0321 21:38:21.513245    2292 endpointslice_controller.go:264] "Starting endpoint slice controller"
I0321 21:38:21.513289    2292 shared_informer.go:311] Waiting for caches to sync for endpoint_slice
I0321 21:38:21.514325    2292 controllermanager.go:642] "Started controller" controller="endpointslice-controller"
time="2024-03-21T21:38:21Z" level=info msg="Updating TLS secret for kube-system/k3s-serving (count: 10): map[listener.cattle.io/cn-10.43.0.1:10.43.0.1 listener.cattle.io/cn-127.0.0.1:127.0.0.1 listener.cattle.io/cn-192.168.56.110:192.168.56.110 listener.cattle.io/cn-__1-f16284:::1 listener.cattle.io/cn-kubernetes:kubernetes listener.cattle.io/cn-kubernetes.default:kubernetes.default listener.cattle.io/cn-kubernetes.default.svc:kubernetes.default.svc listener.cattle.io/cn-kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local listener.cattle.io/cn-localhost:localhost listener.cattle.io/cn-server:server listener.cattle.io/fingerprint:SHA1=964613B919200B56262F0CCBF8300E6C3C82149A]"
I0321 21:38:21.551952    2292 controllermanager.go:642] "Started controller" controller="pod-garbage-collector-controller"
I0321 21:38:21.552040    2292 gc_controller.go:101] "Starting GC controller"
I0321 21:38:21.552096    2292 shared_informer.go:311] Waiting for caches to sync for GC
I0321 21:38:21.697670    2292 controllermanager.go:642] "Started controller" controller="daemonset-controller"
I0321 21:38:21.698609    2292 daemon_controller.go:291] "Starting daemon sets controller"
I0321 21:38:21.698689    2292 shared_informer.go:311] Waiting for caches to sync for daemon sets
I0321 21:38:21.853846    2292 controllermanager.go:642] "Started controller" controller="statefulset-controller"
I0321 21:38:21.853922    2292 controllermanager.go:607] "Warning: controller is disabled" controller="bootstrap-signer-controller"
I0321 21:38:21.854362    2292 stateful_set.go:161] "Starting stateful set controller"
I0321 21:38:21.854381    2292 shared_informer.go:311] Waiting for caches to sync for stateful set
time="2024-03-21T21:38:21Z" level=info msg="Active TLS secret kube-system/k3s-serving (ver=240) (count 10): map[listener.cattle.io/cn-10.43.0.1:10.43.0.1 listener.cattle.io/cn-127.0.0.1:127.0.0.1 listener.cattle.io/cn-192.168.56.110:192.168.56.110 listener.cattle.io/cn-__1-f16284:::1 listener.cattle.io/cn-kubernetes:kubernetes listener.cattle.io/cn-kubernetes.default:kubernetes.default listener.cattle.io/cn-kubernetes.default.svc:kubernetes.default.svc listener.cattle.io/cn-kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local listener.cattle.io/cn-localhost:localhost listener.cattle.io/cn-server:server listener.cattle.io/fingerprint:SHA1=964613B919200B56262F0CCBF8300E6C3C82149A]"
time="2024-03-21T21:38:21Z" level=info msg="Updating TLS secret for kube-system/k3s-serving (count: 10): map[listener.cattle.io/cn-10.43.0.1:10.43.0.1 listener.cattle.io/cn-127.0.0.1:127.0.0.1 listener.cattle.io/cn-192.168.56.110:192.168.56.110 listener.cattle.io/cn-__1-f16284:::1 listener.cattle.io/cn-kubernetes:kubernetes listener.cattle.io/cn-kubernetes.default:kubernetes.default listener.cattle.io/cn-kubernetes.default.svc:kubernetes.default.svc listener.cattle.io/cn-kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local listener.cattle.io/cn-localhost:localhost listener.cattle.io/cn-server:server listener.cattle.io/fingerprint:SHA1=964613B919200B56262F0CCBF8300E6C3C82149A]"
I0321 21:38:21.998671    2292 controllermanager.go:642] "Started controller" controller="token-cleaner-controller"
I0321 21:38:21.999258    2292 controllermanager.go:607] "Warning: controller is disabled" controller="service-lb-controller"
I0321 21:38:21.999892    2292 tokencleaner.go:112] "Starting token cleaner controller"
I0321 21:38:22.000290    2292 shared_informer.go:311] Waiting for caches to sync for token_cleaner
I0321 21:38:22.000625    2292 shared_informer.go:318] Caches are synced for token_cleaner
I0321 21:38:22.139201    2292 controllermanager.go:642] "Started controller" controller="clusterrole-aggregation-controller"
I0321 21:38:22.139393    2292 clusterroleaggregation_controller.go:189] "Starting ClusterRoleAggregator controller"
I0321 21:38:22.139403    2292 shared_informer.go:311] Waiting for caches to sync for ClusterRoleAggregator
I0321 21:38:22.297152    2292 controllermanager.go:642] "Started controller" controller="root-ca-certificate-publisher-controller"
I0321 21:38:22.297499    2292 publisher.go:102] "Starting root CA cert publisher controller"
I0321 21:38:22.297674    2292 shared_informer.go:311] Waiting for caches to sync for crt configmap
I0321 21:38:22.340872    2292 event.go:307] "Event occurred" object="server" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodePasswordValidationComplete" message="Deferred node password secret validation complete"
I0321 21:38:22.596677    2292 controllermanager.go:642] "Started controller" controller="namespace-controller"
I0321 21:38:22.597026    2292 namespace_controller.go:197] "Starting namespace controller"
I0321 21:38:22.597060    2292 shared_informer.go:311] Waiting for caches to sync for namespace
I0321 21:38:22.629468    2292 controller.go:624] quota admission added evaluator for: addons.k3s.cattle.io
I0321 21:38:22.636323    2292 event.go:307] "Event occurred" object="kube-system/ccm" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/ccm.yaml\""
I0321 21:38:22.675179    2292 event.go:307] "Event occurred" object="kube-system/ccm" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/ccm.yaml\""
I0321 21:38:22.687440    2292 event.go:307] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/coredns.yaml\""
I0321 21:38:22.693763    2292 controllermanager.go:642] "Started controller" controller="endpoints-controller"
I0321 21:38:22.693959    2292 endpoints_controller.go:174] "Starting endpoint controller"
I0321 21:38:22.693968    2292 shared_informer.go:311] Waiting for caches to sync for endpoint
I0321 21:38:22.738882    2292 controller.go:624] quota admission added evaluator for: deployments.apps
I0321 21:38:22.760056    2292 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.43.0.10"}
I0321 21:38:22.761004    2292 event.go:307] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/coredns.yaml\""
I0321 21:38:22.773932    2292 event.go:307] "Event occurred" object="kube-system/local-storage" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/local-storage.yaml\""
I0321 21:38:22.808102    2292 event.go:307] "Event occurred" object="kube-system/local-storage" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/local-storage.yaml\""
I0321 21:38:22.813876    2292 event.go:307] "Event occurred" object="kube-system/aggregated-metrics-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/aggregated-metrics-reader.yaml\""
I0321 21:38:22.821730    2292 event.go:307] "Event occurred" object="kube-system/aggregated-metrics-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/aggregated-metrics-reader.yaml\""
I0321 21:38:22.827867    2292 event.go:307] "Event occurred" object="kube-system/auth-delegator" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/auth-delegator.yaml\""
I0321 21:38:22.835075    2292 event.go:307] "Event occurred" object="kube-system/auth-delegator" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/auth-delegator.yaml\""
I0321 21:38:22.840180    2292 controllermanager.go:642] "Started controller" controller="endpointslice-mirroring-controller"
I0321 21:38:22.840284    2292 endpointslicemirroring_controller.go:223] "Starting EndpointSliceMirroring controller"
I0321 21:38:22.840292    2292 shared_informer.go:311] Waiting for caches to sync for endpoint_slice_mirroring
I0321 21:38:22.842772    2292 event.go:307] "Event occurred" object="kube-system/auth-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/auth-reader.yaml\""
I0321 21:38:22.848406    2292 event.go:307] "Event occurred" object="kube-system/auth-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/auth-reader.yaml\""
I0321 21:38:22.997805    2292 controllermanager.go:642] "Started controller" controller="job-controller"
I0321 21:38:22.998037    2292 job_controller.go:226] "Starting job controller"
I0321 21:38:22.998063    2292 shared_informer.go:311] Waiting for caches to sync for job
I0321 21:38:23.050819    2292 certificate_controller.go:115] "Starting certificate controller" name="csrsigning-kubelet-serving"
I0321 21:38:23.050896    2292 shared_informer.go:311] Waiting for caches to sync for certificate-csrsigning-kubelet-serving
I0321 21:38:23.050955    2292 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/var/lib/rancher/k3s/server/tls/server-ca.nochain.crt::/var/lib/rancher/k3s/server/tls/server-ca.key"
I0321 21:38:23.052850    2292 certificate_controller.go:115] "Starting certificate controller" name="csrsigning-kubelet-client"
I0321 21:38:23.052925    2292 shared_informer.go:311] Waiting for caches to sync for certificate-csrsigning-kubelet-client
I0321 21:38:23.052979    2292 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/var/lib/rancher/k3s/server/tls/client-ca.nochain.crt::/var/lib/rancher/k3s/server/tls/client-ca.key"
I0321 21:38:23.054386    2292 certificate_controller.go:115] "Starting certificate controller" name="csrsigning-kube-apiserver-client"
I0321 21:38:23.054460    2292 shared_informer.go:311] Waiting for caches to sync for certificate-csrsigning-kube-apiserver-client
I0321 21:38:23.054510    2292 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/var/lib/rancher/k3s/server/tls/client-ca.nochain.crt::/var/lib/rancher/k3s/server/tls/client-ca.key"
I0321 21:38:23.055585    2292 controllermanager.go:642] "Started controller" controller="certificatesigningrequest-signing-controller"
I0321 21:38:23.055733    2292 certificate_controller.go:115] "Starting certificate controller" name="csrsigning-legacy-unknown"
I0321 21:38:23.056096    2292 shared_informer.go:311] Waiting for caches to sync for certificate-csrsigning-legacy-unknown
I0321 21:38:23.055781    2292 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/var/lib/rancher/k3s/server/tls/server-ca.nochain.crt::/var/lib/rancher/k3s/server/tls/server-ca.key"
I0321 21:38:23.096696    2292 event.go:307] "Event occurred" object="kube-system/metrics-apiservice" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-apiservice.yaml\""
I0321 21:38:23.112244    2292 event.go:307] "Event occurred" object="kube-system/metrics-apiservice" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-apiservice.yaml\""
I0321 21:38:23.112657    2292 handler.go:275] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
W0321 21:38:23.116841    2292 handler_proxy.go:93] no RequestInfo found in the context
E0321 21:38:23.116892    2292 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
E0321 21:38:23.116931    2292 handler_proxy.go:137] error resolving kube-system/metrics-server: service "metrics-server" not found
I0321 21:38:23.196098    2292 controllermanager.go:642] "Started controller" controller="ttl-after-finished-controller"
I0321 21:38:23.196289    2292 ttlafterfinished_controller.go:109] "Starting TTL after finished controller"
I0321 21:38:23.197040    2292 shared_informer.go:311] Waiting for caches to sync for TTL after finished
I0321 21:38:23.347005    2292 controllermanager.go:642] "Started controller" controller="serviceaccount-controller"
I0321 21:38:23.347155    2292 serviceaccounts_controller.go:111] "Starting service account controller"
I0321 21:38:23.347741    2292 shared_informer.go:311] Waiting for caches to sync for service account
I0321 21:38:23.492471    2292 event.go:307] "Event occurred" object="kube-system/metrics-server-deployment" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-deployment.yaml\""
I0321 21:38:23.503541    2292 controllermanager.go:642] "Started controller" controller="persistentvolume-expander-controller"
I0321 21:38:23.503969    2292 expand_controller.go:328] "Starting expand controller"
I0321 21:38:23.504008    2292 shared_informer.go:311] Waiting for caches to sync for expand
I0321 21:38:23.520809    2292 event.go:307] "Event occurred" object="kube-system/metrics-server-deployment" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-deployment.yaml\""
I0321 21:38:23.698284    2292 controllermanager.go:642] "Started controller" controller="disruption-controller"
I0321 21:38:23.698529    2292 disruption.go:433] "Sending events to api server."
I0321 21:38:23.698700    2292 disruption.go:444] "Starting disruption controller"
I0321 21:38:23.698737    2292 shared_informer.go:311] Waiting for caches to sync for disruption
I0321 21:38:23.971739    2292 controllermanager.go:642] "Started controller" controller="cronjob-controller"
I0321 21:38:23.972120    2292 event.go:307] "Event occurred" object="kube-system/metrics-server-service" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-service.yaml\""
I0321 21:38:23.972233    2292 cronjob_controllerv2.go:139] "Starting cronjob controller v2"
I0321 21:38:23.972249    2292 shared_informer.go:311] Waiting for caches to sync for cronjob
I0321 21:38:23.976347    2292 controllermanager.go:642] "Started controller" controller="certificatesigningrequest-cleaner-controller"
I0321 21:38:23.976493    2292 cleaner.go:83] "Starting CSR cleaner controller"
time="2024-03-21T21:38:24Z" level=info msg="Running kube-proxy --cluster-cidr=10.42.0.0/16 --conntrack-max-per-core=0 --conntrack-tcp-timeout-close-wait=0s --conntrack-tcp-timeout-established=0s --healthz-bind-address=127.0.0.1 --hostname-override=server --kubeconfig=/var/lib/rancher/k3s/agent/kubeproxy.kubeconfig --proxy-mode=iptables"
I0321 21:38:24.061948    2292 alloc.go:330] "allocated clusterIPs" service="kube-system/metrics-server" clusterIPs={"IPv4":"10.43.23.223"}
I0321 21:38:24.063182    2292 event.go:307] "Event occurred" object="kube-system/metrics-server-service" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-service.yaml\""
I0321 21:38:24.068198    2292 controllermanager.go:642] "Started controller" controller="ttl-controller"
I0321 21:38:24.068428    2292 ttl_controller.go:124] "Starting TTL controller"
I0321 21:38:24.068473    2292 shared_informer.go:311] Waiting for caches to sync for TTL
W0321 21:38:24.068774    2292 handler_proxy.go:93] no RequestInfo found in the context
E0321 21:38:24.068894    2292 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
E0321 21:38:24.069030    2292 handler_proxy.go:137] error resolving kube-system/metrics-server: endpoints "metrics-server" not found
I0321 21:38:24.076615    2292 node.go:141] Successfully retrieved node IP: 192.168.56.110
I0321 21:38:24.083905    2292 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0321 21:38:24.085395    2292 server_others.go:152] "Using iptables Proxier"
I0321 21:38:24.085418    2292 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0321 21:38:24.085425    2292 server_others.go:438] "Defaulting to no-op detect-local"
I0321 21:38:24.085442    2292 proxier.go:250] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0321 21:38:24.085575    2292 server.go:846] "Version info" version="v1.28.7+k3s1"
I0321 21:38:24.085583    2292 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0321 21:38:24.090179    2292 config.go:188] "Starting service config controller"
I0321 21:38:24.090198    2292 shared_informer.go:311] Waiting for caches to sync for service config
I0321 21:38:24.090219    2292 config.go:97] "Starting endpoint slice config controller"
I0321 21:38:24.090223    2292 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0321 21:38:24.090520    2292 config.go:315] "Starting node config controller"
I0321 21:38:24.090526    2292 shared_informer.go:311] Waiting for caches to sync for node config
W0321 21:38:24.112457    2292 handler_proxy.go:93] no RequestInfo found in the context
E0321 21:38:24.112566    2292 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I0321 21:38:24.112582    2292 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0321 21:38:24.112744    2292 handler_proxy.go:93] no RequestInfo found in the context
E0321 21:38:24.112979    2292 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0321 21:38:24.114130    2292 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0321 21:38:24.191038    2292 shared_informer.go:318] Caches are synced for node config
I0321 21:38:24.191075    2292 shared_informer.go:318] Caches are synced for service config
I0321 21:38:24.191096    2292 shared_informer.go:318] Caches are synced for endpoint slice config
I0321 21:38:24.279081    2292 event.go:307] "Event occurred" object="kube-system/resource-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/resource-reader.yaml\""
I0321 21:38:24.297902    2292 serving.go:355] Generated self-signed cert in-memory
I0321 21:38:24.384030    2292 event.go:307] "Event occurred" object="kube-system/resource-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/resource-reader.yaml\""
I0321 21:38:24.678728    2292 event.go:307] "Event occurred" object="kube-system/rolebindings" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/rolebindings.yaml\""
I0321 21:38:24.696628    2292 event.go:307] "Event occurred" object="kube-system/rolebindings" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/rolebindings.yaml\""
time="2024-03-21T21:38:24Z" level=info msg="Tunnel authorizer set Kubelet Port 10250"
I0321 21:38:25.040942    2292 serving.go:355] Generated self-signed cert in-memory
I0321 21:38:25.079045    2292 event.go:307] "Event occurred" object="kube-system/runtimes" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/runtimes.yaml\""
I0321 21:38:25.283926    2292 event.go:307] "Event occurred" object="kube-system/runtimes" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/runtimes.yaml\""
I0321 21:38:25.429523    2292 controllermanager.go:168] Version: v1.28.7+k3s1
I0321 21:38:25.432345    2292 secure_serving.go:213] Serving securely on 127.0.0.1:10258
I0321 21:38:25.432612    2292 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0321 21:38:25.432657    2292 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I0321 21:38:25.432668    2292 shared_informer.go:311] Waiting for caches to sync for RequestHeaderAuthRequestController
I0321 21:38:25.432694    2292 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0321 21:38:25.432702    2292 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0321 21:38:25.432714    2292 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0321 21:38:25.432720    2292 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
E0321 21:38:25.444857    2292 controllermanager.go:524] unable to get all supported resources from server: unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
time="2024-03-21T21:38:25Z" level=info msg="Creating  event broadcaster"
I0321 21:38:25.481613    2292 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/traefik.yaml\""
I0321 21:38:25.486277    2292 controller.go:624] quota admission added evaluator for: helmcharts.helm.cattle.io
I0321 21:38:25.494415    2292 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/traefik.yaml\""
I0321 21:38:25.507991    2292 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
I0321 21:38:25.510992    2292 event.go:307] "Event occurred" object="kube-system/traefik-crd" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik-crd"
I0321 21:38:25.525997    2292 controller.go:624] quota admission added evaluator for: jobs.batch
I0321 21:38:25.533429    2292 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0321 21:38:25.533486    2292 shared_informer.go:318] Caches are synced for RequestHeaderAuthRequestController
I0321 21:38:25.533530    2292 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
time="2024-03-21T21:38:25Z" level=error msg="error syncing 'kube-system/traefik': handler helm-controller-chart-registration: helmcharts.helm.cattle.io \"traefik\" not found, requeuing"
time="2024-03-21T21:38:25Z" level=error msg="error syncing 'kube-system/traefik-crd': handler helm-controller-chart-registration: helmcharts.helm.cattle.io \"traefik-crd\" not found, requeuing"
I0321 21:38:25.540945    2292 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
I0321 21:38:25.540966    2292 event.go:307] "Event occurred" object="kube-system/traefik-crd" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik-crd"
time="2024-03-21T21:38:25Z" level=info msg="Starting /v1, Kind=Node controller"
time="2024-03-21T21:38:25Z" level=info msg="Starting /v1, Kind=Pod controller"
time="2024-03-21T21:38:25Z" level=info msg="Starting apps/v1, Kind=DaemonSet controller"
time="2024-03-21T21:38:25Z" level=info msg="Starting discovery.k8s.io/v1, Kind=EndpointSlice controller"
I0321 21:38:25.584483    2292 controllermanager.go:337] Started "service-lb-controller"
W0321 21:38:25.584497    2292 controllermanager.go:314] "node-route-controller" is disabled
I0321 21:38:25.584697    2292 controllermanager.go:337] Started "cloud-node-controller"
I0321 21:38:25.584844    2292 controllermanager.go:337] Started "cloud-node-lifecycle-controller"
I0321 21:38:25.585319    2292 controller.go:231] Starting service controller
I0321 21:38:25.585334    2292 shared_informer.go:311] Waiting for caches to sync for service
I0321 21:38:25.585350    2292 node_controller.go:165] Sending events to api server.
I0321 21:38:25.585376    2292 node_controller.go:174] Waiting for informer caches to sync
I0321 21:38:25.585387    2292 node_lifecycle_controller.go:113] Sending events to api server
I0321 21:38:25.685778    2292 node_controller.go:431] Initializing node server with cloud provider
I0321 21:38:25.686146    2292 shared_informer.go:318] Caches are synced for service
time="2024-03-21T21:38:25Z" level=info msg="Stopped tunnel to 127.0.0.1:6443"
time="2024-03-21T21:38:25Z" level=info msg="Connecting to proxy" url="wss://192.168.56.110:6443/v1-k3s/connect"
time="2024-03-21T21:38:25Z" level=info msg="Proxy done" err="context canceled" url="wss://127.0.0.1:6443/v1-k3s/connect"
time="2024-03-21T21:38:25Z" level=info msg="error in remotedialer server [400]: websocket: close 1006 (abnormal closure): unexpected EOF"
I0321 21:38:25.720450    2292 node_controller.go:502] Successfully initialized node server with cloud provider
I0321 21:38:25.721105    2292 event.go:307] "Event occurred" object="kube-system/traefik-crd" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik-crd"
I0321 21:38:25.721201    2292 event.go:307] "Event occurred" object="server" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="Synced" message="Node synced successfully"
time="2024-03-21T21:38:25Z" level=info msg="Handling backend connection request [server]"
I0321 21:38:25.734839    2292 event.go:307] "Event occurred" object="kube-system/traefik-crd" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik-crd"
time="2024-03-21T21:38:25Z" level=info msg="Updated coredns node hosts entry [192.168.56.110 server]"
I0321 21:38:25.907610    2292 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
I0321 21:38:25.961618    2292 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
I0321 21:38:26.556431    2292 serving.go:355] Generated self-signed cert in-memory
I0321 21:38:26.993535    2292 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.7+k3s1"
I0321 21:38:26.993554    2292 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0321 21:38:26.996572    2292 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0321 21:38:26.996629    2292 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I0321 21:38:26.996635    2292 shared_informer.go:311] Waiting for caches to sync for RequestHeaderAuthRequestController
I0321 21:38:26.997067    2292 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0321 21:38:27.001324    2292 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0321 21:38:27.001341    2292 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0321 21:38:27.001360    2292 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0321 21:38:27.001364    2292 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0321 21:38:27.097341    2292 shared_informer.go:318] Caches are synced for RequestHeaderAuthRequestController
I0321 21:38:27.102503    2292 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0321 21:38:27.103179    2292 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0321 21:38:34.117476    2292 range_allocator.go:111] "No Secondary Service CIDR provided. Skipping filtering out secondary service addresses"
I0321 21:38:34.117547    2292 controllermanager.go:642] "Started controller" controller="node-ipam-controller"
I0321 21:38:34.119178    2292 node_ipam_controller.go:162] "Starting ipam controller"
I0321 21:38:34.119314    2292 shared_informer.go:311] Waiting for caches to sync for node
I0321 21:38:34.123757    2292 shared_informer.go:311] Waiting for caches to sync for resource quota
I0321 21:38:34.139509    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0321 21:38:34.139650    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0321 21:38:34.140072    2292 shared_informer.go:318] Caches are synced for taint
I0321 21:38:34.140163    2292 node_lifecycle_controller.go:1225] "Initializing eviction metric for zone" zone=""
I0321 21:38:34.140287    2292 node_lifecycle_controller.go:877] "Missing timestamp for Node. Assuming now as a timestamp" node="server"
I0321 21:38:34.140799    2292 taint_manager.go:205] "Starting NoExecuteTaintManager"
I0321 21:38:34.140867    2292 taint_manager.go:210] "Sending events to api server"
I0321 21:38:34.141482    2292 node_lifecycle_controller.go:1071] "Controller detected that zone is now in new state" zone="" newState="Normal"
I0321 21:38:34.141793    2292 event.go:307] "Event occurred" object="server" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node server event: Registered Node server in Controller"
I0321 21:38:34.144110    2292 actual_state_of_world.go:547] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"server\" does not exist"
I0321 21:38:34.144393    2292 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I0321 21:38:34.148235    2292 shared_informer.go:318] Caches are synced for persistent volume
I0321 21:38:34.151007    2292 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I0321 21:38:34.152129    2292 shared_informer.go:318] Caches are synced for GC
I0321 21:38:34.154811    2292 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0321 21:38:34.154991    2292 shared_informer.go:318] Caches are synced for stateful set
I0321 21:38:34.155207    2292 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I0321 21:38:34.155262    2292 shared_informer.go:318] Caches are synced for deployment
I0321 21:38:34.156271    2292 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I0321 21:38:34.159436    2292 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I0321 21:38:34.162419    2292 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0321 21:38:34.168707    2292 shared_informer.go:318] Caches are synced for TTL
I0321 21:38:34.182688    2292 controller.go:624] quota admission added evaluator for: replicasets.apps
I0321 21:38:34.192324    2292 shared_informer.go:318] Caches are synced for attach detach
I0321 21:38:34.194402    2292 shared_informer.go:318] Caches are synced for endpoint
I0321 21:38:34.195407    2292 event.go:307] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set coredns-6799fbcd5 to 1"
I0321 21:38:34.195431    2292 event.go:307] "Event occurred" object="kube-system/local-path-provisioner" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set local-path-provisioner-6c86858495 to 1"
I0321 21:38:34.195444    2292 event.go:307] "Event occurred" object="kube-system/metrics-server" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set metrics-server-67c658944b to 1"
I0321 21:38:34.195772    2292 shared_informer.go:318] Caches are synced for ephemeral
I0321 21:38:34.197338    2292 shared_informer.go:318] Caches are synced for namespace
I0321 21:38:34.197941    2292 shared_informer.go:318] Caches are synced for crt configmap
I0321 21:38:34.198184    2292 shared_informer.go:318] Caches are synced for PVC protection
I0321 21:38:34.198252    2292 shared_informer.go:318] Caches are synced for TTL after finished
I0321 21:38:34.199209    2292 shared_informer.go:318] Caches are synced for daemon sets
I0321 21:38:34.198224    2292 shared_informer.go:318] Caches are synced for job
I0321 21:38:34.203641    2292 shared_informer.go:318] Caches are synced for PV protection
I0321 21:38:34.204745    2292 shared_informer.go:318] Caches are synced for expand
I0321 21:38:34.208888    2292 shared_informer.go:318] Caches are synced for ReplicationController
I0321 21:38:34.213584    2292 shared_informer.go:318] Caches are synced for endpoint_slice
I0321 21:38:34.219358    2292 shared_informer.go:318] Caches are synced for node
I0321 21:38:34.219463    2292 range_allocator.go:174] "Sending events to api server"
I0321 21:38:34.219481    2292 range_allocator.go:178] "Starting range CIDR allocator"
I0321 21:38:34.219504    2292 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I0321 21:38:34.219509    2292 shared_informer.go:318] Caches are synced for cidrallocator
I0321 21:38:34.221942    2292 shared_informer.go:318] Caches are synced for ReplicaSet
I0321 21:38:34.232181    2292 shared_informer.go:318] Caches are synced for certificate-csrapproving
I0321 21:38:34.241612    2292 range_allocator.go:380] "Set node PodCIDR" node="server" podCIDRs=["10.42.0.0/24"]
time="2024-03-21T21:38:34Z" level=info msg="Flannel found PodCIDR assigned for node server"
time="2024-03-21T21:38:34Z" level=info msg="The interface enp0s3 with ipv4 address 10.0.2.15 will be used by flannel"
I0321 21:38:34.263658    2292 kube.go:139] Waiting 10m0s for node controller to sync
I0321 21:38:34.264219    2292 shared_informer.go:318] Caches are synced for service account
I0321 21:38:34.264395    2292 kube.go:461] Starting kube subnet manager
I0321 21:38:34.272278    2292 shared_informer.go:318] Caches are synced for cronjob
W0321 21:38:34.281083    2292 handler_proxy.go:93] no RequestInfo found in the context
E0321 21:38:34.281139    2292 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
E0321 21:38:34.281222    2292 handler_proxy.go:137] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
I0321 21:38:34.297080    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0321 21:38:34.297620    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0321 21:38:34.297654    2292 event.go:307] "Event occurred" object="kube-system/helm-install-traefik-crd" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: helm-install-traefik-crd-86dbl"
I0321 21:38:34.297662    2292 event.go:307] "Event occurred" object="kube-system/helm-install-traefik" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: helm-install-traefik-qm7rq"
I0321 21:38:34.298988    2292 shared_informer.go:318] Caches are synced for disruption
I0321 21:38:34.311250    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0321 21:38:34.316580    2292 topology_manager.go:215] "Topology Admit Handler" podUID="4cffde12-8965-42e6-8d74-407409fcc2c9" podNamespace="kube-system" podName="helm-install-traefik-qm7rq"
I0321 21:38:34.317262    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0321 21:38:34.318398    2292 event.go:307] "Event occurred" object="kube-system/metrics-server-67c658944b" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: metrics-server-67c658944b-5hcsc"
I0321 21:38:34.318632    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0321 21:38:34.321037    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0321 21:38:34.321390    2292 topology_manager.go:215] "Topology Admit Handler" podUID="e9b5129c-523f-4fa0-9e7b-5b471c643451" podNamespace="kube-system" podName="helm-install-traefik-crd-86dbl"
I0321 21:38:34.326978    2292 shared_informer.go:318] Caches are synced for HPA
I0321 21:38:34.328426    2292 event.go:307] "Event occurred" object="kube-system/local-path-provisioner-6c86858495" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: local-path-provisioner-6c86858495-hz85b"
I0321 21:38:34.328477    2292 event.go:307] "Event occurred" object="kube-system/coredns-6799fbcd5" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: coredns-6799fbcd5-6xgjn"
I0321 21:38:34.341648    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="119.64812ms"
I0321 21:38:34.342027    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="118.497215ms"
I0321 21:38:34.352616    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="129.181952ms"
I0321 21:38:34.381798    2292 topology_manager.go:215] "Topology Admit Handler" podUID="f6265e43-8023-4d38-a0f4-20253de2102d" podNamespace="kube-system" podName="local-path-provisioner-6c86858495-hz85b"
I0321 21:38:34.381910    2292 topology_manager.go:215] "Topology Admit Handler" podUID="d098cc25-e583-4809-a9ed-750e09bc88a7" podNamespace="kube-system" podName="coredns-6799fbcd5-6xgjn"
I0321 21:38:34.381960    2292 topology_manager.go:215] "Topology Admit Handler" podUID="6a8c0d2f-04fa-4241-bc5f-ac5ecdaf159c" podNamespace="kube-system" podName="metrics-server-67c658944b-5hcsc"
I0321 21:38:34.394514    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0321 21:38:34.394980    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="53.296363ms"
I0321 21:38:34.395033    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="29.729s"
I0321 21:38:34.395134    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="53.083305ms"
I0321 21:38:34.395183    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="23.699s"
I0321 21:38:34.395321    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="19.601s"
I0321 21:38:34.397940    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="43.41s"
time="2024-03-21T21:38:34Z" level=info msg="Starting network policy controller version v2.0.1, built on 2024-02-29T20:36:21Z, go1.21.7"
I0321 21:38:34.398458    2292 network_policy_controller.go:164] Starting network policy controller
I0321 21:38:34.406103    2292 shared_informer.go:318] Caches are synced for resource quota
I0321 21:38:34.423947    2292 shared_informer.go:318] Caches are synced for resource quota
I0321 21:38:34.430471    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0321 21:38:34.438063    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="85.404314ms"
I0321 21:38:34.438143    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="46.416s"
W0321 21:38:34.443224    2292 watcher.go:93] Error while processing event ("/sys/fs/cgroup/devices/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podd098cc25_e583_4809_a9ed_750e09bc88a7.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/devices/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podd098cc25_e583_4809_a9ed_750e09bc88a7.slice: no such file or directory
I0321 21:38:34.465763    2292 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/4cffde12-8965-42e6-8d74-407409fcc2c9-klipper-config\") pod \"helm-install-traefik-qm7rq\" (UID: \"4cffde12-8965-42e6-8d74-407409fcc2c9\") " pod="kube-system/helm-install-traefik-qm7rq"
I0321 21:38:34.466074    2292 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"values\" (UniqueName: \"kubernetes.io/secret/4cffde12-8965-42e6-8d74-407409fcc2c9-values\") pod \"helm-install-traefik-qm7rq\" (UID: \"4cffde12-8965-42e6-8d74-407409fcc2c9\") " pod="kube-system/helm-install-traefik-qm7rq"
I0321 21:38:34.466275    2292 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/e9b5129c-523f-4fa0-9e7b-5b471c643451-klipper-cache\") pod \"helm-install-traefik-crd-86dbl\" (UID: \"e9b5129c-523f-4fa0-9e7b-5b471c643451\") " pod="kube-system/helm-install-traefik-crd-86dbl"
I0321 21:38:34.466477    2292 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/e9b5129c-523f-4fa0-9e7b-5b471c643451-klipper-config\") pod \"helm-install-traefik-crd-86dbl\" (UID: \"e9b5129c-523f-4fa0-9e7b-5b471c643451\") " pod="kube-system/helm-install-traefik-crd-86dbl"
I0321 21:38:34.466638    2292 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"content\" (UniqueName: \"kubernetes.io/configmap/e9b5129c-523f-4fa0-9e7b-5b471c643451-content\") pod \"helm-install-traefik-crd-86dbl\" (UID: \"e9b5129c-523f-4fa0-9e7b-5b471c643451\") " pod="kube-system/helm-install-traefik-crd-86dbl"
I0321 21:38:34.466761    2292 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-kvf7h\" (UniqueName: \"kubernetes.io/projected/e9b5129c-523f-4fa0-9e7b-5b471c643451-kube-api-access-kvf7h\") pod \"helm-install-traefik-crd-86dbl\" (UID: \"e9b5129c-523f-4fa0-9e7b-5b471c643451\") " pod="kube-system/helm-install-traefik-crd-86dbl"
I0321 21:38:34.466884    2292 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/4cffde12-8965-42e6-8d74-407409fcc2c9-klipper-helm\") pod \"helm-install-traefik-qm7rq\" (UID: \"4cffde12-8965-42e6-8d74-407409fcc2c9\") " pod="kube-system/helm-install-traefik-qm7rq"
I0321 21:38:34.467218    2292 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"content\" (UniqueName: \"kubernetes.io/configmap/4cffde12-8965-42e6-8d74-407409fcc2c9-content\") pod \"helm-install-traefik-qm7rq\" (UID: \"4cffde12-8965-42e6-8d74-407409fcc2c9\") " pod="kube-system/helm-install-traefik-qm7rq"
I0321 21:38:34.467342    2292 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-j49st\" (UniqueName: \"kubernetes.io/projected/4cffde12-8965-42e6-8d74-407409fcc2c9-kube-api-access-j49st\") pod \"helm-install-traefik-qm7rq\" (UID: \"4cffde12-8965-42e6-8d74-407409fcc2c9\") " pod="kube-system/helm-install-traefik-qm7rq"
I0321 21:38:34.467457    2292 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/e9b5129c-523f-4fa0-9e7b-5b471c643451-klipper-helm\") pod \"helm-install-traefik-crd-86dbl\" (UID: \"e9b5129c-523f-4fa0-9e7b-5b471c643451\") " pod="kube-system/helm-install-traefik-crd-86dbl"
I0321 21:38:34.467562    2292 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"values\" (UniqueName: \"kubernetes.io/secret/e9b5129c-523f-4fa0-9e7b-5b471c643451-values\") pod \"helm-install-traefik-crd-86dbl\" (UID: \"e9b5129c-523f-4fa0-9e7b-5b471c643451\") " pod="kube-system/helm-install-traefik-crd-86dbl"
I0321 21:38:34.467690    2292 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/4cffde12-8965-42e6-8d74-407409fcc2c9-klipper-cache\") pod \"helm-install-traefik-qm7rq\" (UID: \"4cffde12-8965-42e6-8d74-407409fcc2c9\") " pod="kube-system/helm-install-traefik-qm7rq"
I0321 21:38:34.467832    2292 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/e9b5129c-523f-4fa0-9e7b-5b471c643451-tmp\") pod \"helm-install-traefik-crd-86dbl\" (UID: \"e9b5129c-523f-4fa0-9e7b-5b471c643451\") " pod="kube-system/helm-install-traefik-crd-86dbl"
I0321 21:38:34.467948    2292 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/4cffde12-8965-42e6-8d74-407409fcc2c9-tmp\") pod \"helm-install-traefik-qm7rq\" (UID: \"4cffde12-8965-42e6-8d74-407409fcc2c9\") " pod="kube-system/helm-install-traefik-qm7rq"
I0321 21:38:34.473086    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="86.034s"
I0321 21:38:34.494448    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="74s"
I0321 21:38:34.512531    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="81.773s"
I0321 21:38:34.535543    2292 network_policy_controller.go:176] Starting network policy controller full sync goroutine
I0321 21:38:34.568187    2292 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/f6265e43-8023-4d38-a0f4-20253de2102d-config-volume\") pod \"local-path-provisioner-6c86858495-hz85b\" (UID: \"f6265e43-8023-4d38-a0f4-20253de2102d\") " pod="kube-system/local-path-provisioner-6c86858495-hz85b"
I0321 21:38:34.568302    2292 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-qjbdc\" (UniqueName: \"kubernetes.io/projected/f6265e43-8023-4d38-a0f4-20253de2102d-kube-api-access-qjbdc\") pod \"local-path-provisioner-6c86858495-hz85b\" (UID: \"f6265e43-8023-4d38-a0f4-20253de2102d\") " pod="kube-system/local-path-provisioner-6c86858495-hz85b"
I0321 21:38:34.568321    2292 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-nb47m\" (UniqueName: \"kubernetes.io/projected/d098cc25-e583-4809-a9ed-750e09bc88a7-kube-api-access-nb47m\") pod \"coredns-6799fbcd5-6xgjn\" (UID: \"d098cc25-e583-4809-a9ed-750e09bc88a7\") " pod="kube-system/coredns-6799fbcd5-6xgjn"
I0321 21:38:34.568445    2292 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"custom-config-volume\" (UniqueName: \"kubernetes.io/configmap/d098cc25-e583-4809-a9ed-750e09bc88a7-custom-config-volume\") pod \"coredns-6799fbcd5-6xgjn\" (UID: \"d098cc25-e583-4809-a9ed-750e09bc88a7\") " pod="kube-system/coredns-6799fbcd5-6xgjn"
I0321 21:38:34.568564    2292 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/d098cc25-e583-4809-a9ed-750e09bc88a7-config-volume\") pod \"coredns-6799fbcd5-6xgjn\" (UID: \"d098cc25-e583-4809-a9ed-750e09bc88a7\") " pod="kube-system/coredns-6799fbcd5-6xgjn"
I0321 21:38:34.568629    2292 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp-dir\" (UniqueName: \"kubernetes.io/empty-dir/6a8c0d2f-04fa-4241-bc5f-ac5ecdaf159c-tmp-dir\") pod \"metrics-server-67c658944b-5hcsc\" (UID: \"6a8c0d2f-04fa-4241-bc5f-ac5ecdaf159c\") " pod="kube-system/metrics-server-67c658944b-5hcsc"
I0321 21:38:34.568670    2292 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-6zzsr\" (UniqueName: \"kubernetes.io/projected/6a8c0d2f-04fa-4241-bc5f-ac5ecdaf159c-kube-api-access-6zzsr\") pod \"metrics-server-67c658944b-5hcsc\" (UID: \"6a8c0d2f-04fa-4241-bc5f-ac5ecdaf159c\") " pod="kube-system/metrics-server-67c658944b-5hcsc"
I0321 21:38:34.763697    2292 shared_informer.go:318] Caches are synced for garbage collector
I0321 21:38:34.817927    2292 shared_informer.go:318] Caches are synced for garbage collector
I0321 21:38:34.818029    2292 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I0321 21:38:35.264627    2292 kube.go:146] Node controller sync successful
I0321 21:38:35.264819    2292 vxlan.go:141] VXLAN config: VNI=1 Port=0 GBP=false Learning=false DirectRouting=false
I0321 21:38:35.275457    2292 kube.go:621] List of node(server) annotations: map[string]string{"alpha.kubernetes.io/provided-node-ip":"192.168.56.110", "k3s.io/hostname":"server", "k3s.io/internal-ip":"192.168.56.110", "k3s.io/node-args":"[\"server\",\"--write-kubeconfig-mode\",\"644\",\"--node-ip\",\"192.168.56.110\",\"--log\",\"/vagrant/logs/server.logs\"]", "k3s.io/node-config-hash":"3PZPW4FQCHFROJYSVKXEV2FIFKDUB666MSS6DHLJL37T5VMNZEUA====", "k3s.io/node-env":"{\"K3S_DATA_DIR\":\"/var/lib/rancher/k3s/data/a3b46c0299091b71bfcc617b1e1fec1845c13bdd848584ceb39d2e700e702a4b\"}", "node.alpha.kubernetes.io/ttl":"0", "volumes.kubernetes.io/controller-managed-attach-detach":"true"}
W0321 21:38:35.283159    2292 handler_proxy.go:93] no RequestInfo found in the context
E0321 21:38:35.283699    2292 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0321 21:38:35.283897    2292 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0321 21:38:35.284234    2292 handler_proxy.go:93] no RequestInfo found in the context
E0321 21:38:35.284321    2292 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I0321 21:38:35.288540    2292 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0321 21:38:35.344184    2292 kube.go:482] Creating the node lease for IPv4. This is the n.Spec.PodCIDRs: [10.42.0.0/24]
time="2024-03-21T21:38:35Z" level=info msg="Wrote flannel subnet file to /run/flannel/subnet.env"
time="2024-03-21T21:38:35Z" level=info msg="Running flannel backend."
I0321 21:38:35.352319    2292 vxlan_network.go:65] watching for new subnet leases
I0321 21:38:35.352339    2292 iptables.go:290] generated 3 rules
I0321 21:38:35.353297    2292 iptables.go:290] generated 7 rules
I0321 21:38:35.366961    2292 iptables.go:283] bootstrap done
I0321 21:38:35.381005    2292 iptables.go:283] bootstrap done
I0321 21:38:40.579443    2292 kuberuntime_manager.go:1528] "Updating runtime config through cri with podcidr" CIDR="10.42.0.0/24"
I0321 21:38:40.580274    2292 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.42.0.0/24"
I0321 21:38:44.544314    2292 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/coredns-6799fbcd5-6xgjn" containerName="coredns"
I0321 21:38:45.181967    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="2.718023ms"
I0321 21:38:45.250844    2292 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/coredns-6799fbcd5-6xgjn" podStartSLOduration=5.070167765 podCreationTimestamp="2024-03-21 21:38:34 +0000 UTC" firstStartedPulling="2024-03-21 21:38:38.359484001 +0000 UTC m=+22.487123541" lastFinishedPulling="2024-03-21 21:38:44.537276552 +0000 UTC m=+28.664916101" observedRunningTime="2024-03-21 21:38:45.190244015 +0000 UTC m=+29.317883560" watchObservedRunningTime="2024-03-21 21:38:45.247960325 +0000 UTC m=+29.375599868"
I0321 21:38:45.272213    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="24.757997ms"
I0321 21:38:45.272339    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="88.948s"
I0321 21:38:46.267745    2292 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/local-path-provisioner-6c86858495-hz85b" containerName="local-path-provisioner"
I0321 21:38:47.203779    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="14.752077ms"
I0321 21:38:47.204461    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="47.359s"
I0321 21:38:47.210400    2292 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/local-path-provisioner-6c86858495-hz85b" podStartSLOduration=5.32167025 podCreationTimestamp="2024-03-21 21:38:34 +0000 UTC" firstStartedPulling="2024-03-21 21:38:38.37785375 +0000 UTC m=+22.505493292" lastFinishedPulling="2024-03-21 21:38:46.266525043 +0000 UTC m=+30.394164580" observedRunningTime="2024-03-21 21:38:47.186671384 +0000 UTC m=+31.314310965" watchObservedRunningTime="2024-03-21 21:38:47.210341538 +0000 UTC m=+31.337981081"
I0321 21:38:48.091930    2292 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/metrics-server-67c658944b-5hcsc" containerName="metrics-server"
I0321 21:38:49.186814    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="66.239s"
I0321 21:38:49.205511    2292 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/metrics-server-67c658944b-5hcsc" podStartSLOduration=5.501802022 podCreationTimestamp="2024-03-21 21:38:34 +0000 UTC" firstStartedPulling="2024-03-21 21:38:38.385039924 +0000 UTC m=+22.512679463" lastFinishedPulling="2024-03-21 21:38:48.087420989 +0000 UTC m=+32.215060533" observedRunningTime="2024-03-21 21:38:49.186643095 +0000 UTC m=+33.314282642" watchObservedRunningTime="2024-03-21 21:38:49.204183092 +0000 UTC m=+33.331822637"
I0321 21:38:54.780210    2292 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/helm-install-traefik-crd-86dbl" containerName="helm"
I0321 21:38:54.783126    2292 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/helm-install-traefik-qm7rq" containerName="helm"
I0321 21:38:55.289751    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0321 21:38:55.292920    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0321 21:38:55.303693    2292 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/helm-install-traefik-qm7rq" podStartSLOduration=4.886174499 podCreationTimestamp="2024-03-21 21:38:34 +0000 UTC" firstStartedPulling="2024-03-21 21:38:38.363507845 +0000 UTC m=+22.491147379" lastFinishedPulling="2024-03-21 21:38:54.770834402 +0000 UTC m=+38.898473941" observedRunningTime="2024-03-21 21:38:55.264977452 +0000 UTC m=+39.392616994" watchObservedRunningTime="2024-03-21 21:38:55.293501061 +0000 UTC m=+39.421140604"
I0321 21:38:55.304965    2292 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/helm-install-traefik-crd-86dbl" podStartSLOduration=4.897506569 podCreationTimestamp="2024-03-21 21:38:34 +0000 UTC" firstStartedPulling="2024-03-21 21:38:38.363341623 +0000 UTC m=+22.490981156" lastFinishedPulling="2024-03-21 21:38:54.770756801 +0000 UTC m=+38.898396347" observedRunningTime="2024-03-21 21:38:55.293476537 +0000 UTC m=+39.421116079" watchObservedRunningTime="2024-03-21 21:38:55.30492176 +0000 UTC m=+39.432561304"
I0321 21:38:56.230800    2292 scope.go:117] "RemoveContainer" containerID="1549c8ac6d935805c1cd05f38abc2f2eb8198a558b2a61a0ee7c40d77562958f"
I0321 21:38:56.253727    2292 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/helm-install-traefik-qm7rq" containerName="helm"
I0321 21:38:56.335504    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
W0321 21:38:56.353764    2292 aggregator.go:166] failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0321 21:38:56.356910    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0321 21:38:56.385773    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0321 21:38:57.357673    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0321 21:38:57.438642    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0321 21:38:57.651166    2292 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0321 21:38:57.680627    2292 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0321 21:38:57.766962    2292 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0321 21:38:57.794337    2292 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0321 21:38:57.819680    2292 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0321 21:38:57.822707    2292 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0321 21:38:57.835658    2292 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0321 21:38:57.841811    2292 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0321 21:38:57.848385    2292 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0321 21:38:57.854012    2292 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0321 21:38:57.861941    2292 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0321 21:38:57.874134    2292 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0321 21:38:57.879173    2292 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0321 21:38:57.907626    2292 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0321 21:38:57.932184    2292 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0321 21:38:57.949772    2292 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0321 21:38:57.954864    2292 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0321 21:38:58.002398    2292 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0321 21:38:58.008806    2292 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0321 21:38:58.279174    2292 scope.go:117] "RemoveContainer" containerID="dbee519edfde7b85b07e4379f4cf3970322a61cce9e0e2684970fc33240aa346"
E0321 21:38:58.286622    2292 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"helm\" with CrashLoopBackOff: \"back-off 10s restarting failed container=helm pod=helm-install-traefik-qm7rq_kube-system(4cffde12-8965-42e6-8d74-407409fcc2c9)\"" pod="kube-system/helm-install-traefik-qm7rq" podUID="4cffde12-8965-42e6-8d74-407409fcc2c9"
I0321 21:38:58.289191    2292 scope.go:117] "RemoveContainer" containerID="1549c8ac6d935805c1cd05f38abc2f2eb8198a558b2a61a0ee7c40d77562958f"
I0321 21:38:58.306265    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0321 21:38:58.318290    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0321 21:38:59.300227    2292 scope.go:117] "RemoveContainer" containerID="dbee519edfde7b85b07e4379f4cf3970322a61cce9e0e2684970fc33240aa346"
E0321 21:38:59.301292    2292 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"helm\" with CrashLoopBackOff: \"back-off 10s restarting failed container=helm pod=helm-install-traefik-qm7rq_kube-system(4cffde12-8965-42e6-8d74-407409fcc2c9)\"" pod="kube-system/helm-install-traefik-qm7rq" podUID="4cffde12-8965-42e6-8d74-407409fcc2c9"
I0321 21:38:59.332115    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0321 21:38:59.339013    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0321 21:38:59.534688    2292 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/e9b5129c-523f-4fa0-9e7b-5b471c643451-klipper-cache\") pod \"e9b5129c-523f-4fa0-9e7b-5b471c643451\" (UID: \"e9b5129c-523f-4fa0-9e7b-5b471c643451\") "
I0321 21:38:59.534733    2292 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-kvf7h\" (UniqueName: \"kubernetes.io/projected/e9b5129c-523f-4fa0-9e7b-5b471c643451-kube-api-access-kvf7h\") pod \"e9b5129c-523f-4fa0-9e7b-5b471c643451\" (UID: \"e9b5129c-523f-4fa0-9e7b-5b471c643451\") "
I0321 21:38:59.534753    2292 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/e9b5129c-523f-4fa0-9e7b-5b471c643451-klipper-helm\") pod \"e9b5129c-523f-4fa0-9e7b-5b471c643451\" (UID: \"e9b5129c-523f-4fa0-9e7b-5b471c643451\") "
I0321 21:38:59.534770    2292 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"values\" (UniqueName: \"kubernetes.io/secret/e9b5129c-523f-4fa0-9e7b-5b471c643451-values\") pod \"e9b5129c-523f-4fa0-9e7b-5b471c643451\" (UID: \"e9b5129c-523f-4fa0-9e7b-5b471c643451\") "
I0321 21:38:59.534788    2292 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/e9b5129c-523f-4fa0-9e7b-5b471c643451-tmp\") pod \"e9b5129c-523f-4fa0-9e7b-5b471c643451\" (UID: \"e9b5129c-523f-4fa0-9e7b-5b471c643451\") "
I0321 21:38:59.534805    2292 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/e9b5129c-523f-4fa0-9e7b-5b471c643451-klipper-config\") pod \"e9b5129c-523f-4fa0-9e7b-5b471c643451\" (UID: \"e9b5129c-523f-4fa0-9e7b-5b471c643451\") "
I0321 21:38:59.534822    2292 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"content\" (UniqueName: \"kubernetes.io/configmap/e9b5129c-523f-4fa0-9e7b-5b471c643451-content\") pod \"e9b5129c-523f-4fa0-9e7b-5b471c643451\" (UID: \"e9b5129c-523f-4fa0-9e7b-5b471c643451\") "
I0321 21:38:59.537735    2292 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/configmap/e9b5129c-523f-4fa0-9e7b-5b471c643451-content" (OuterVolumeSpecName: "content") pod "e9b5129c-523f-4fa0-9e7b-5b471c643451" (UID: "e9b5129c-523f-4fa0-9e7b-5b471c643451"). InnerVolumeSpecName "content". PluginName "kubernetes.io/configmap", VolumeGidValue ""
I0321 21:38:59.562372    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0321 21:38:59.564513    2292 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/e9b5129c-523f-4fa0-9e7b-5b471c643451-klipper-cache" (OuterVolumeSpecName: "klipper-cache") pod "e9b5129c-523f-4fa0-9e7b-5b471c643451" (UID: "e9b5129c-523f-4fa0-9e7b-5b471c643451"). InnerVolumeSpecName "klipper-cache". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0321 21:38:59.582267    2292 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/e9b5129c-523f-4fa0-9e7b-5b471c643451-kube-api-access-kvf7h" (OuterVolumeSpecName: "kube-api-access-kvf7h") pod "e9b5129c-523f-4fa0-9e7b-5b471c643451" (UID: "e9b5129c-523f-4fa0-9e7b-5b471c643451"). InnerVolumeSpecName "kube-api-access-kvf7h". PluginName "kubernetes.io/projected", VolumeGidValue ""
I0321 21:38:59.582766    2292 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/e9b5129c-523f-4fa0-9e7b-5b471c643451-klipper-config" (OuterVolumeSpecName: "klipper-config") pod "e9b5129c-523f-4fa0-9e7b-5b471c643451" (UID: "e9b5129c-523f-4fa0-9e7b-5b471c643451"). InnerVolumeSpecName "klipper-config". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0321 21:38:59.583394    2292 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/e9b5129c-523f-4fa0-9e7b-5b471c643451-tmp" (OuterVolumeSpecName: "tmp") pod "e9b5129c-523f-4fa0-9e7b-5b471c643451" (UID: "e9b5129c-523f-4fa0-9e7b-5b471c643451"). InnerVolumeSpecName "tmp". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0321 21:38:59.584162    2292 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/e9b5129c-523f-4fa0-9e7b-5b471c643451-values" (OuterVolumeSpecName: "values") pod "e9b5129c-523f-4fa0-9e7b-5b471c643451" (UID: "e9b5129c-523f-4fa0-9e7b-5b471c643451"). InnerVolumeSpecName "values". PluginName "kubernetes.io/secret", VolumeGidValue ""
I0321 21:38:59.591420    2292 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/e9b5129c-523f-4fa0-9e7b-5b471c643451-klipper-helm" (OuterVolumeSpecName: "klipper-helm") pod "e9b5129c-523f-4fa0-9e7b-5b471c643451" (UID: "e9b5129c-523f-4fa0-9e7b-5b471c643451"). InnerVolumeSpecName "klipper-helm". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0321 21:38:59.636301    2292 reconciler_common.go:300] "Volume detached for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/e9b5129c-523f-4fa0-9e7b-5b471c643451-tmp\") on node \"server\" DevicePath \"\""
I0321 21:38:59.636426    2292 reconciler_common.go:300] "Volume detached for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/e9b5129c-523f-4fa0-9e7b-5b471c643451-klipper-config\") on node \"server\" DevicePath \"\""
I0321 21:38:59.636436    2292 reconciler_common.go:300] "Volume detached for volume \"content\" (UniqueName: \"kubernetes.io/configmap/e9b5129c-523f-4fa0-9e7b-5b471c643451-content\") on node \"server\" DevicePath \"\""
I0321 21:38:59.636443    2292 reconciler_common.go:300] "Volume detached for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/e9b5129c-523f-4fa0-9e7b-5b471c643451-klipper-helm\") on node \"server\" DevicePath \"\""
I0321 21:38:59.636452    2292 reconciler_common.go:300] "Volume detached for volume \"values\" (UniqueName: \"kubernetes.io/secret/e9b5129c-523f-4fa0-9e7b-5b471c643451-values\") on node \"server\" DevicePath \"\""
I0321 21:38:59.636461    2292 reconciler_common.go:300] "Volume detached for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/e9b5129c-523f-4fa0-9e7b-5b471c643451-klipper-cache\") on node \"server\" DevicePath \"\""
I0321 21:38:59.636471    2292 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-kvf7h\" (UniqueName: \"kubernetes.io/projected/e9b5129c-523f-4fa0-9e7b-5b471c643451-kube-api-access-kvf7h\") on node \"server\" DevicePath \"\""
I0321 21:39:00.311459    2292 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="36c6921728b5b513a970c2a22640bfa97c84480362a7d05b8126a5389fc5f003"
I0321 21:39:00.355557    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0321 21:39:00.358897    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0321 21:39:00.364792    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0321 21:39:00.373037    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0321 21:39:00.375099    2292 event.go:307] "Event occurred" object="kube-system/helm-install-traefik-crd" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="Completed" message="Job completed"
E0321 21:39:04.469853    2292 resource_quota_controller.go:440] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I0321 21:39:04.476826    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="tlsoptions.traefik.containo.us"
I0321 21:39:04.477068    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressroutes.traefik.containo.us"
I0321 21:39:04.477536    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="middlewaretcps.traefik.containo.us"
I0321 21:39:04.477989    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="tlsoptions.traefik.io"
I0321 21:39:04.478305    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressrouteudps.traefik.containo.us"
I0321 21:39:04.478866    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="middlewares.traefik.io"
I0321 21:39:04.479394    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="serverstransports.traefik.containo.us"
I0321 21:39:04.479728    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="traefikservices.traefik.io"
I0321 21:39:04.479926    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="serverstransporttcps.traefik.io"
I0321 21:39:04.480125    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="middlewares.traefik.containo.us"
I0321 21:39:04.480462    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="serverstransports.traefik.io"
I0321 21:39:04.480735    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressroutetcps.traefik.io"
I0321 21:39:04.480841    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressroutetcps.traefik.containo.us"
I0321 21:39:04.481100    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressrouteudps.traefik.io"
I0321 21:39:04.481329    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="middlewaretcps.traefik.io"
I0321 21:39:04.481514    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="traefikservices.traefik.containo.us"
I0321 21:39:04.481799    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="tlsstores.traefik.containo.us"
I0321 21:39:04.481993    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="tlsstores.traefik.io"
I0321 21:39:04.482733    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressroutes.traefik.io"
I0321 21:39:04.484093    2292 shared_informer.go:311] Waiting for caches to sync for resource quota
I0321 21:39:04.688059    2292 shared_informer.go:318] Caches are synced for resource quota
I0321 21:39:04.789278    2292 garbagecollector.go:816] "failed to discover some groups" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"
I0321 21:39:04.796878    2292 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0321 21:39:04.796921    2292 shared_informer.go:318] Caches are synced for garbage collector
I0321 21:39:04.804168    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="10.090607ms"
I0321 21:39:04.828378    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="49.35s"
I0321 21:39:04.916588    2292 handler.go:275] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0321 21:39:09.961582    2292 scope.go:117] "RemoveContainer" containerID="dbee519edfde7b85b07e4379f4cf3970322a61cce9e0e2684970fc33240aa346"
I0321 21:39:09.980540    2292 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/helm-install-traefik-qm7rq" containerName="helm"
I0321 21:39:10.422633    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0321 21:39:11.451719    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0321 21:39:12.049334    2292 alloc.go:330] "allocated clusterIPs" service="kube-system/traefik" clusterIPs={"IPv4":"10.43.192.107"}
I0321 21:39:12.098893    2292 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Service" apiVersion="v1" type="Normal" reason="EnsuringLoadBalancer" message="Ensuring load balancer"
I0321 21:39:12.248464    2292 endpointslice_controller.go:310] "Error syncing endpoint slices for service, retrying" key="kube-system/traefik" err="EndpointSlice informer cache is out of date"
I0321 21:39:12.305035    2292 controller.go:624] quota admission added evaluator for: ingressroutes.traefik.io
I0321 21:39:12.309401    2292 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set traefik-f4564c4f4 to 1"
I0321 21:39:12.323637    2292 controller.go:624] quota admission added evaluator for: daemonsets.apps
I0321 21:39:12.325007    2292 event.go:307] "Event occurred" object="traefik" fieldPath="" kind="Endpoints" apiVersion="v1" type="Warning" reason="FailedToCreateEndpoint" message="Failed to create endpoint for service kube-system/traefik: endpoints \"traefik\" already exists"
I0321 21:39:12.337405    2292 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Service" apiVersion="v1" type="Normal" reason="AppliedDaemonSet" message="Applied LoadBalancer DaemonSet kube-system/svclb-traefik-25d2c695"
I0321 21:39:12.377856    2292 controller.go:624] quota admission added evaluator for: controllerrevisions.apps
I0321 21:39:12.431345    2292 event.go:307] "Event occurred" object="kube-system/traefik-f4564c4f4" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: traefik-f4564c4f4-2f6w7"
I0321 21:39:12.500107    2292 event.go:307] "Event occurred" object="kube-system/svclb-traefik-25d2c695" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: svclb-traefik-25d2c695-zz6x6"
I0321 21:39:12.544191    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="243.21974ms"
I0321 21:39:12.569059    2292 topology_manager.go:215] "Topology Admit Handler" podUID="f38e0fff-05b6-466b-9169-e56efe37deb9" podNamespace="kube-system" podName="traefik-f4564c4f4-2f6w7"
E0321 21:39:12.576064    2292 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="e9b5129c-523f-4fa0-9e7b-5b471c643451" containerName="helm"
I0321 21:39:12.576834    2292 memory_manager.go:346] "RemoveStaleState removing state" podUID="e9b5129c-523f-4fa0-9e7b-5b471c643451" containerName="helm"
I0321 21:39:12.578828    2292 topology_manager.go:215] "Topology Admit Handler" podUID="6d903700-6b20-41eb-8330-5539ac97fd50" podNamespace="kube-system" podName="svclb-traefik-25d2c695-zz6x6"
I0321 21:39:12.601219    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="56.980994ms"
I0321 21:39:12.601653    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="58.398s"
I0321 21:39:12.630180    2292 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"data\" (UniqueName: \"kubernetes.io/empty-dir/f38e0fff-05b6-466b-9169-e56efe37deb9-data\") pod \"traefik-f4564c4f4-2f6w7\" (UID: \"f38e0fff-05b6-466b-9169-e56efe37deb9\") " pod="kube-system/traefik-f4564c4f4-2f6w7"
I0321 21:39:12.630359    2292 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/f38e0fff-05b6-466b-9169-e56efe37deb9-tmp\") pod \"traefik-f4564c4f4-2f6w7\" (UID: \"f38e0fff-05b6-466b-9169-e56efe37deb9\") " pod="kube-system/traefik-f4564c4f4-2f6w7"
I0321 21:39:12.630481    2292 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-rm5qj\" (UniqueName: \"kubernetes.io/projected/f38e0fff-05b6-466b-9169-e56efe37deb9-kube-api-access-rm5qj\") pod \"traefik-f4564c4f4-2f6w7\" (UID: \"f38e0fff-05b6-466b-9169-e56efe37deb9\") " pod="kube-system/traefik-f4564c4f4-2f6w7"
I0321 21:39:12.765191    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="137.779s"
I0321 21:39:13.493576    2292 scope.go:117] "RemoveContainer" containerID="dbee519edfde7b85b07e4379f4cf3970322a61cce9e0e2684970fc33240aa346"
I0321 21:39:13.544830    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0321 21:39:14.513404    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0321 21:39:14.569799    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0321 21:39:14.798838    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0321 21:39:14.867025    2292 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"content\" (UniqueName: \"kubernetes.io/configmap/4cffde12-8965-42e6-8d74-407409fcc2c9-content\") pod \"4cffde12-8965-42e6-8d74-407409fcc2c9\" (UID: \"4cffde12-8965-42e6-8d74-407409fcc2c9\") "
I0321 21:39:14.867151    2292 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-j49st\" (UniqueName: \"kubernetes.io/projected/4cffde12-8965-42e6-8d74-407409fcc2c9-kube-api-access-j49st\") pod \"4cffde12-8965-42e6-8d74-407409fcc2c9\" (UID: \"4cffde12-8965-42e6-8d74-407409fcc2c9\") "
I0321 21:39:14.867181    2292 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/4cffde12-8965-42e6-8d74-407409fcc2c9-klipper-cache\") pod \"4cffde12-8965-42e6-8d74-407409fcc2c9\" (UID: \"4cffde12-8965-42e6-8d74-407409fcc2c9\") "
I0321 21:39:14.867240    2292 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/4cffde12-8965-42e6-8d74-407409fcc2c9-klipper-config\") pod \"4cffde12-8965-42e6-8d74-407409fcc2c9\" (UID: \"4cffde12-8965-42e6-8d74-407409fcc2c9\") "
I0321 21:39:14.867261    2292 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/4cffde12-8965-42e6-8d74-407409fcc2c9-tmp\") pod \"4cffde12-8965-42e6-8d74-407409fcc2c9\" (UID: \"4cffde12-8965-42e6-8d74-407409fcc2c9\") "
I0321 21:39:14.867586    2292 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"values\" (UniqueName: \"kubernetes.io/secret/4cffde12-8965-42e6-8d74-407409fcc2c9-values\") pod \"4cffde12-8965-42e6-8d74-407409fcc2c9\" (UID: \"4cffde12-8965-42e6-8d74-407409fcc2c9\") "
I0321 21:39:14.867613    2292 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/4cffde12-8965-42e6-8d74-407409fcc2c9-klipper-helm\") pod \"4cffde12-8965-42e6-8d74-407409fcc2c9\" (UID: \"4cffde12-8965-42e6-8d74-407409fcc2c9\") "
I0321 21:39:14.874532    2292 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/configmap/4cffde12-8965-42e6-8d74-407409fcc2c9-content" (OuterVolumeSpecName: "content") pod "4cffde12-8965-42e6-8d74-407409fcc2c9" (UID: "4cffde12-8965-42e6-8d74-407409fcc2c9"). InnerVolumeSpecName "content". PluginName "kubernetes.io/configmap", VolumeGidValue ""
I0321 21:39:14.883436    2292 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/4cffde12-8965-42e6-8d74-407409fcc2c9-kube-api-access-j49st" (OuterVolumeSpecName: "kube-api-access-j49st") pod "4cffde12-8965-42e6-8d74-407409fcc2c9" (UID: "4cffde12-8965-42e6-8d74-407409fcc2c9"). InnerVolumeSpecName "kube-api-access-j49st". PluginName "kubernetes.io/projected", VolumeGidValue ""
I0321 21:39:14.884206    2292 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/4cffde12-8965-42e6-8d74-407409fcc2c9-klipper-helm" (OuterVolumeSpecName: "klipper-helm") pod "4cffde12-8965-42e6-8d74-407409fcc2c9" (UID: "4cffde12-8965-42e6-8d74-407409fcc2c9"). InnerVolumeSpecName "klipper-helm". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0321 21:39:14.884523    2292 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/4cffde12-8965-42e6-8d74-407409fcc2c9-tmp" (OuterVolumeSpecName: "tmp") pod "4cffde12-8965-42e6-8d74-407409fcc2c9" (UID: "4cffde12-8965-42e6-8d74-407409fcc2c9"). InnerVolumeSpecName "tmp". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0321 21:39:14.890823    2292 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/4cffde12-8965-42e6-8d74-407409fcc2c9-values" (OuterVolumeSpecName: "values") pod "4cffde12-8965-42e6-8d74-407409fcc2c9" (UID: "4cffde12-8965-42e6-8d74-407409fcc2c9"). InnerVolumeSpecName "values". PluginName "kubernetes.io/secret", VolumeGidValue ""
I0321 21:39:14.891225    2292 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/4cffde12-8965-42e6-8d74-407409fcc2c9-klipper-cache" (OuterVolumeSpecName: "klipper-cache") pod "4cffde12-8965-42e6-8d74-407409fcc2c9" (UID: "4cffde12-8965-42e6-8d74-407409fcc2c9"). InnerVolumeSpecName "klipper-cache". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0321 21:39:14.894303    2292 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/4cffde12-8965-42e6-8d74-407409fcc2c9-klipper-config" (OuterVolumeSpecName: "klipper-config") pod "4cffde12-8965-42e6-8d74-407409fcc2c9" (UID: "4cffde12-8965-42e6-8d74-407409fcc2c9"). InnerVolumeSpecName "klipper-config". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0321 21:39:14.979468    2292 reconciler_common.go:300] "Volume detached for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/4cffde12-8965-42e6-8d74-407409fcc2c9-tmp\") on node \"server\" DevicePath \"\""
I0321 21:39:14.980315    2292 reconciler_common.go:300] "Volume detached for volume \"values\" (UniqueName: \"kubernetes.io/secret/4cffde12-8965-42e6-8d74-407409fcc2c9-values\") on node \"server\" DevicePath \"\""
I0321 21:39:14.980382    2292 reconciler_common.go:300] "Volume detached for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/4cffde12-8965-42e6-8d74-407409fcc2c9-klipper-helm\") on node \"server\" DevicePath \"\""
I0321 21:39:14.980416    2292 reconciler_common.go:300] "Volume detached for volume \"content\" (UniqueName: \"kubernetes.io/configmap/4cffde12-8965-42e6-8d74-407409fcc2c9-content\") on node \"server\" DevicePath \"\""
I0321 21:39:14.980452    2292 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-j49st\" (UniqueName: \"kubernetes.io/projected/4cffde12-8965-42e6-8d74-407409fcc2c9-kube-api-access-j49st\") on node \"server\" DevicePath \"\""
I0321 21:39:14.980489    2292 reconciler_common.go:300] "Volume detached for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/4cffde12-8965-42e6-8d74-407409fcc2c9-klipper-config\") on node \"server\" DevicePath \"\""
I0321 21:39:14.980517    2292 reconciler_common.go:300] "Volume detached for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/4cffde12-8965-42e6-8d74-407409fcc2c9-klipper-cache\") on node \"server\" DevicePath \"\""
I0321 21:39:15.509073    2292 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="7cfe50db47aed4b7bbc4ef7126c50dda7409aa3affbb94fe787b9d7e20989ceb"
I0321 21:39:15.552772    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0321 21:39:15.580156    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0321 21:39:15.609374    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0321 21:39:15.619817    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0321 21:39:15.623974    2292 event.go:307] "Event occurred" object="kube-system/helm-install-traefik" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="Completed" message="Job completed"
I0321 21:39:18.075020    2292 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/svclb-traefik-25d2c695-zz6x6" containerName="lb-tcp-80"
I0321 21:39:18.428683    2292 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/svclb-traefik-25d2c695-zz6x6" containerName="lb-tcp-443"
I0321 21:39:19.785162    2292 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Service" apiVersion="v1" type="Normal" reason="UpdatedLoadBalancer" message="Updated LoadBalancer with new IPs: [] -> [192.168.56.110]"
I0321 21:39:24.062620    2292 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/traefik-f4564c4f4-2f6w7" containerName="traefik"
I0321 21:39:24.890144    2292 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/traefik-f4564c4f4-2f6w7" podStartSLOduration=2.246116435 podCreationTimestamp="2024-03-21 21:39:12 +0000 UTC" firstStartedPulling="2024-03-21 21:39:13.403469224 +0000 UTC m=+57.531108789" lastFinishedPulling="2024-03-21 21:39:24.043187544 +0000 UTC m=+68.170827092" observedRunningTime="2024-03-21 21:39:24.87434528 +0000 UTC m=+69.001984823" watchObservedRunningTime="2024-03-21 21:39:24.885834738 +0000 UTC m=+69.013474287"
I0321 21:39:24.890372    2292 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/svclb-traefik-25d2c695-zz6x6" podStartSLOduration=8.244059568 podCreationTimestamp="2024-03-21 21:39:12 +0000 UTC" firstStartedPulling="2024-03-21 21:39:13.407502271 +0000 UTC m=+57.535141804" lastFinishedPulling="2024-03-21 21:39:18.05377792 +0000 UTC m=+62.181417456" observedRunningTime="2024-03-21 21:39:19.693721379 +0000 UTC m=+63.821360931" watchObservedRunningTime="2024-03-21 21:39:24.89033522 +0000 UTC m=+69.017974771"
I0321 21:39:24.957295    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="4.746774ms"
I0321 21:39:26.911129    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="39.736158ms"
I0321 21:39:26.913055    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="178.019s"
time="2024-03-21T21:41:37Z" level=info msg="Starting k3s v1.28.7+k3s1 (051b14b2)"
time="2024-03-21T21:41:37Z" level=info msg="Configuring sqlite3 database connection pooling: maxIdleConns=2, maxOpenConns=0, connMaxLifetime=0s"
time="2024-03-21T21:41:37Z" level=info msg="Configuring database table schema and indexes, this may take a moment..."
time="2024-03-21T21:41:37Z" level=info msg="Database tables and indexes are up to date"
time="2024-03-21T21:41:37Z" level=info msg="Kine available at unix://kine.sock"
time="2024-03-21T21:41:37Z" level=info msg="generated self-signed CA certificate CN=k3s-client-ca@1711057297: notBefore=2024-03-21 21:41:37.672631778 +0000 UTC notAfter=2034-03-19 21:41:37.672631778 +0000 UTC"
time="2024-03-21T21:41:37Z" level=info msg="certificate CN=system:admin,O=system:masters signed by CN=k3s-client-ca@1711057297: notBefore=2024-03-21 21:41:37 +0000 UTC notAfter=2025-03-21 21:41:37 +0000 UTC"
time="2024-03-21T21:41:37Z" level=info msg="certificate CN=system:k3s-supervisor,O=system:masters signed by CN=k3s-client-ca@1711057297: notBefore=2024-03-21 21:41:37 +0000 UTC notAfter=2025-03-21 21:41:37 +0000 UTC"
time="2024-03-21T21:41:37Z" level=info msg="certificate CN=system:kube-controller-manager signed by CN=k3s-client-ca@1711057297: notBefore=2024-03-21 21:41:37 +0000 UTC notAfter=2025-03-21 21:41:37 +0000 UTC"
time="2024-03-21T21:41:37Z" level=info msg="certificate CN=system:kube-scheduler signed by CN=k3s-client-ca@1711057297: notBefore=2024-03-21 21:41:37 +0000 UTC notAfter=2025-03-21 21:41:37 +0000 UTC"
time="2024-03-21T21:41:37Z" level=info msg="certificate CN=system:apiserver,O=system:masters signed by CN=k3s-client-ca@1711057297: notBefore=2024-03-21 21:41:37 +0000 UTC notAfter=2025-03-21 21:41:37 +0000 UTC"
time="2024-03-21T21:41:37Z" level=info msg="certificate CN=system:kube-proxy signed by CN=k3s-client-ca@1711057297: notBefore=2024-03-21 21:41:37 +0000 UTC notAfter=2025-03-21 21:41:37 +0000 UTC"
time="2024-03-21T21:41:37Z" level=info msg="certificate CN=system:k3s-controller signed by CN=k3s-client-ca@1711057297: notBefore=2024-03-21 21:41:37 +0000 UTC notAfter=2025-03-21 21:41:37 +0000 UTC"
time="2024-03-21T21:41:37Z" level=info msg="certificate CN=k3s-cloud-controller-manager signed by CN=k3s-client-ca@1711057297: notBefore=2024-03-21 21:41:37 +0000 UTC notAfter=2025-03-21 21:41:37 +0000 UTC"
time="2024-03-21T21:41:37Z" level=info msg="generated self-signed CA certificate CN=k3s-server-ca@1711057297: notBefore=2024-03-21 21:41:37.679153755 +0000 UTC notAfter=2034-03-19 21:41:37.679153755 +0000 UTC"
time="2024-03-21T21:41:37Z" level=info msg="certificate CN=kube-apiserver signed by CN=k3s-server-ca@1711057297: notBefore=2024-03-21 21:41:37 +0000 UTC notAfter=2025-03-21 21:41:37 +0000 UTC"
time="2024-03-21T21:41:37Z" level=info msg="generated self-signed CA certificate CN=k3s-request-header-ca@1711057297: notBefore=2024-03-21 21:41:37.680426738 +0000 UTC notAfter=2034-03-19 21:41:37.680426738 +0000 UTC"
time="2024-03-21T21:41:37Z" level=info msg="certificate CN=system:auth-proxy signed by CN=k3s-request-header-ca@1711057297: notBefore=2024-03-21 21:41:37 +0000 UTC notAfter=2025-03-21 21:41:37 +0000 UTC"
time="2024-03-21T21:41:37Z" level=info msg="generated self-signed CA certificate CN=etcd-server-ca@1711057297: notBefore=2024-03-21 21:41:37.681704948 +0000 UTC notAfter=2034-03-19 21:41:37.681704948 +0000 UTC"
time="2024-03-21T21:41:37Z" level=info msg="certificate CN=etcd-client signed by CN=etcd-server-ca@1711057297: notBefore=2024-03-21 21:41:37 +0000 UTC notAfter=2025-03-21 21:41:37 +0000 UTC"
time="2024-03-21T21:41:37Z" level=info msg="generated self-signed CA certificate CN=etcd-peer-ca@1711057297: notBefore=2024-03-21 21:41:37.682882222 +0000 UTC notAfter=2034-03-19 21:41:37.682882222 +0000 UTC"
time="2024-03-21T21:41:37Z" level=info msg="certificate CN=etcd-peer signed by CN=etcd-peer-ca@1711057297: notBefore=2024-03-21 21:41:37 +0000 UTC notAfter=2025-03-21 21:41:37 +0000 UTC"
time="2024-03-21T21:41:37Z" level=info msg="certificate CN=etcd-server signed by CN=etcd-server-ca@1711057297: notBefore=2024-03-21 21:41:37 +0000 UTC notAfter=2025-03-21 21:41:37 +0000 UTC"
time="2024-03-21T21:41:37Z" level=info msg="Saving cluster bootstrap data to datastore"
time="2024-03-21T21:41:37Z" level=info msg="certificate CN=k3s,O=k3s signed by CN=k3s-server-ca@1711057297: notBefore=2024-03-21 21:41:37 +0000 UTC notAfter=2025-03-21 21:41:37 +0000 UTC"
time="2024-03-21T21:41:37Z" level=warning msg="dynamiclistener [::]:6443: no cached certificate available for preload - deferring certificate load until storage initialization or first client request"
time="2024-03-21T21:41:37Z" level=info msg="Active TLS secret / (ver=) (count 10): map[listener.cattle.io/cn-10.43.0.1:10.43.0.1 listener.cattle.io/cn-127.0.0.1:127.0.0.1 listener.cattle.io/cn-192.168.56.110:192.168.56.110 listener.cattle.io/cn-__1-f16284:::1 listener.cattle.io/cn-kubernetes:kubernetes listener.cattle.io/cn-kubernetes.default:kubernetes.default listener.cattle.io/cn-kubernetes.default.svc:kubernetes.default.svc listener.cattle.io/cn-kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local listener.cattle.io/cn-localhost:localhost listener.cattle.io/cn-server:server listener.cattle.io/fingerprint:SHA1=A9809CD2BE32F035ECEB1CA0D1F006B748463E54]"
time="2024-03-21T21:41:38Z" level=info msg="Running kube-apiserver --advertise-address=192.168.56.110 --advertise-port=6443 --allow-privileged=true --anonymous-auth=false --api-audiences=https://kubernetes.default.svc.cluster.local,k3s --authorization-mode=Node,RBAC --bind-address=127.0.0.1 --cert-dir=/var/lib/rancher/k3s/server/tls/temporary-certs --client-ca-file=/var/lib/rancher/k3s/server/tls/client-ca.crt --egress-selector-config-file=/var/lib/rancher/k3s/server/etc/egress-selector-config.yaml --enable-admission-plugins=NodeRestriction --enable-aggregator-routing=true --enable-bootstrap-token-auth=true --etcd-servers=unix://kine.sock --kubelet-certificate-authority=/var/lib/rancher/k3s/server/tls/server-ca.crt --kubelet-client-certificate=/var/lib/rancher/k3s/server/tls/client-kube-apiserver.crt --kubelet-client-key=/var/lib/rancher/k3s/server/tls/client-kube-apiserver.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --profiling=false --proxy-client-cert-file=/var/lib/rancher/k3s/server/tls/client-auth-proxy.crt --proxy-client-key-file=/var/lib/rancher/k3s/server/tls/client-auth-proxy.key --requestheader-allowed-names=system:auth-proxy --requestheader-client-ca-file=/var/lib/rancher/k3s/server/tls/request-header-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6444 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/var/lib/rancher/k3s/server/tls/service.key --service-account-signing-key-file=/var/lib/rancher/k3s/server/tls/service.current.key --service-cluster-ip-range=10.43.0.0/16 --service-node-port-range=30000-32767 --storage-backend=etcd3 --tls-cert-file=/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.crt --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 --tls-private-key-file=/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.key"
time="2024-03-21T21:41:38Z" level=info msg="Running kube-scheduler --authentication-kubeconfig=/var/lib/rancher/k3s/server/cred/scheduler.kubeconfig --authorization-kubeconfig=/var/lib/rancher/k3s/server/cred/scheduler.kubeconfig --bind-address=127.0.0.1 --kubeconfig=/var/lib/rancher/k3s/server/cred/scheduler.kubeconfig --leader-elect=false --profiling=false --secure-port=10259"
I0321 21:41:38.135783    2292 options.go:220] external host was not specified, using 192.168.56.110
I0321 21:41:38.136449    2292 server.go:156] Version: v1.28.7+k3s1
I0321 21:41:38.136473    2292 server.go:158] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
time="2024-03-21T21:41:38Z" level=info msg="Running kube-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/var/lib/rancher/k3s/server/cred/controller.kubeconfig --authorization-kubeconfig=/var/lib/rancher/k3s/server/cred/controller.kubeconfig --bind-address=127.0.0.1 --cluster-cidr=10.42.0.0/16 --cluster-signing-kube-apiserver-client-cert-file=/var/lib/rancher/k3s/server/tls/client-ca.nochain.crt --cluster-signing-kube-apiserver-client-key-file=/var/lib/rancher/k3s/server/tls/client-ca.key --cluster-signing-kubelet-client-cert-file=/var/lib/rancher/k3s/server/tls/client-ca.nochain.crt --cluster-signing-kubelet-client-key-file=/var/lib/rancher/k3s/server/tls/client-ca.key --cluster-signing-kubelet-serving-cert-file=/var/lib/rancher/k3s/server/tls/server-ca.nochain.crt --cluster-signing-kubelet-serving-key-file=/var/lib/rancher/k3s/server/tls/server-ca.key --cluster-signing-legacy-unknown-cert-file=/var/lib/rancher/k3s/server/tls/server-ca.nochain.crt --cluster-signing-legacy-unknown-key-file=/var/lib/rancher/k3s/server/tls/server-ca.key --configure-cloud-routes=false --controllers=*,tokencleaner,-service,-route,-cloud-node-lifecycle --kubeconfig=/var/lib/rancher/k3s/server/cred/controller.kubeconfig --leader-elect=false --profiling=false --root-ca-file=/var/lib/rancher/k3s/server/tls/server-ca.crt --secure-port=10257 --service-account-private-key-file=/var/lib/rancher/k3s/server/tls/service.current.key --service-cluster-ip-range=10.43.0.0/16 --use-service-account-credentials=true"
time="2024-03-21T21:41:38Z" level=info msg="Running cloud-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/var/lib/rancher/k3s/server/cred/cloud-controller.kubeconfig --authorization-kubeconfig=/var/lib/rancher/k3s/server/cred/cloud-controller.kubeconfig --bind-address=127.0.0.1 --cloud-config=/var/lib/rancher/k3s/server/etc/cloud-config.yaml --cloud-provider=k3s --cluster-cidr=10.42.0.0/16 --configure-cloud-routes=false --controllers=*,-route --feature-gates=CloudDualStackNodeIPs=true --kubeconfig=/var/lib/rancher/k3s/server/cred/cloud-controller.kubeconfig --leader-elect=false --leader-elect-resource-name=k3s-cloud-controller-manager --node-status-update-frequency=1m0s --profiling=false"
time="2024-03-21T21:41:38Z" level=info msg="Server node token is available at /var/lib/rancher/k3s/server/token"
time="2024-03-21T21:41:38Z" level=info msg="To join server node to cluster: k3s server -s https://10.0.2.15:6443 -t ${SERVER_NODE_TOKEN}"
time="2024-03-21T21:41:38Z" level=info msg="Agent node token is available at /var/lib/rancher/k3s/server/agent-token"
time="2024-03-21T21:41:38Z" level=info msg="To join agent node to cluster: k3s agent -s https://10.0.2.15:6443 -t ${AGENT_NODE_TOKEN}"
time="2024-03-21T21:41:38Z" level=info msg="Wrote kubeconfig /etc/rancher/k3s/k3s.yaml"
time="2024-03-21T21:41:38Z" level=info msg="Run: k3s kubectl"
time="2024-03-21T21:41:38Z" level=info msg="Waiting for API server to become available"
I0321 21:41:38.493713    2292 plugins.go:158] Loaded 12 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,TaintNodesByCondition,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,MutatingAdmissionWebhook.
I0321 21:41:38.493733    2292 plugins.go:161] Loaded 13 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,ClusterTrustBundleAttest,CertificateSubjectRestriction,ValidatingAdmissionPolicy,ValidatingAdmissionWebhook,ResourceQuota.
I0321 21:41:38.494305    2292 instance.go:298] Using reconciler: lease
I0321 21:41:38.499891    2292 shared_informer.go:311] Waiting for caches to sync for node_authorizer
I0321 21:41:38.507401    2292 handler.go:275] Adding GroupVersion apiextensions.k8s.io v1 to ResourceManager
W0321 21:41:38.507427    2292 genericapiserver.go:744] Skipping API apiextensions.k8s.io/v1beta1 because it has no resources.
I0321 21:41:38.605598    2292 handler.go:275] Adding GroupVersion  v1 to ResourceManager
I0321 21:41:38.612399    2292 instance.go:709] API group "internal.apiserver.k8s.io" is not enabled, skipping.
I0321 21:41:38.742802    2292 instance.go:709] API group "resource.k8s.io" is not enabled, skipping.
time="2024-03-21T21:41:38Z" level=info msg="Password verified locally for node server"
time="2024-03-21T21:41:38Z" level=info msg="certificate CN=server signed by CN=k3s-server-ca@1711057297: notBefore=2024-03-21 21:41:37 +0000 UTC notAfter=2025-03-21 21:41:38 +0000 UTC"
I0321 21:41:38.766102    2292 handler.go:275] Adding GroupVersion authentication.k8s.io v1 to ResourceManager
W0321 21:41:38.776787    2292 genericapiserver.go:744] Skipping API authentication.k8s.io/v1beta1 because it has no resources.
W0321 21:41:38.776868    2292 genericapiserver.go:744] Skipping API authentication.k8s.io/v1alpha1 because it has no resources.
I0321 21:41:38.777959    2292 handler.go:275] Adding GroupVersion authorization.k8s.io v1 to ResourceManager
W0321 21:41:38.778013    2292 genericapiserver.go:744] Skipping API authorization.k8s.io/v1beta1 because it has no resources.
I0321 21:41:38.778673    2292 handler.go:275] Adding GroupVersion autoscaling v2 to ResourceManager
I0321 21:41:38.779239    2292 handler.go:275] Adding GroupVersion autoscaling v1 to ResourceManager
W0321 21:41:38.779283    2292 genericapiserver.go:744] Skipping API autoscaling/v2beta1 because it has no resources.
W0321 21:41:38.779288    2292 genericapiserver.go:744] Skipping API autoscaling/v2beta2 because it has no resources.
I0321 21:41:38.781625    2292 handler.go:275] Adding GroupVersion batch v1 to ResourceManager
W0321 21:41:38.781641    2292 genericapiserver.go:744] Skipping API batch/v1beta1 because it has no resources.
I0321 21:41:38.782260    2292 handler.go:275] Adding GroupVersion certificates.k8s.io v1 to ResourceManager
W0321 21:41:38.782273    2292 genericapiserver.go:744] Skipping API certificates.k8s.io/v1beta1 because it has no resources.
W0321 21:41:38.782277    2292 genericapiserver.go:744] Skipping API certificates.k8s.io/v1alpha1 because it has no resources.
I0321 21:41:38.782771    2292 handler.go:275] Adding GroupVersion coordination.k8s.io v1 to ResourceManager
W0321 21:41:38.782785    2292 genericapiserver.go:744] Skipping API coordination.k8s.io/v1beta1 because it has no resources.
W0321 21:41:38.782816    2292 genericapiserver.go:744] Skipping API discovery.k8s.io/v1beta1 because it has no resources.
I0321 21:41:38.783275    2292 handler.go:275] Adding GroupVersion discovery.k8s.io v1 to ResourceManager
I0321 21:41:38.784379    2292 handler.go:275] Adding GroupVersion networking.k8s.io v1 to ResourceManager
W0321 21:41:38.784393    2292 genericapiserver.go:744] Skipping API networking.k8s.io/v1beta1 because it has no resources.
W0321 21:41:38.784398    2292 genericapiserver.go:744] Skipping API networking.k8s.io/v1alpha1 because it has no resources.
I0321 21:41:38.784730    2292 handler.go:275] Adding GroupVersion node.k8s.io v1 to ResourceManager
W0321 21:41:38.784744    2292 genericapiserver.go:744] Skipping API node.k8s.io/v1beta1 because it has no resources.
W0321 21:41:38.784748    2292 genericapiserver.go:744] Skipping API node.k8s.io/v1alpha1 because it has no resources.
I0321 21:41:38.785442    2292 handler.go:275] Adding GroupVersion policy v1 to ResourceManager
W0321 21:41:38.785456    2292 genericapiserver.go:744] Skipping API policy/v1beta1 because it has no resources.
I0321 21:41:38.786624    2292 handler.go:275] Adding GroupVersion rbac.authorization.k8s.io v1 to ResourceManager
W0321 21:41:38.786642    2292 genericapiserver.go:744] Skipping API rbac.authorization.k8s.io/v1beta1 because it has no resources.
W0321 21:41:38.786663    2292 genericapiserver.go:744] Skipping API rbac.authorization.k8s.io/v1alpha1 because it has no resources.
I0321 21:41:38.787130    2292 handler.go:275] Adding GroupVersion scheduling.k8s.io v1 to ResourceManager
W0321 21:41:38.787143    2292 genericapiserver.go:744] Skipping API scheduling.k8s.io/v1beta1 because it has no resources.
W0321 21:41:38.787147    2292 genericapiserver.go:744] Skipping API scheduling.k8s.io/v1alpha1 because it has no resources.
I0321 21:41:38.788792    2292 handler.go:275] Adding GroupVersion storage.k8s.io v1 to ResourceManager
W0321 21:41:38.788806    2292 genericapiserver.go:744] Skipping API storage.k8s.io/v1beta1 because it has no resources.
W0321 21:41:38.788810    2292 genericapiserver.go:744] Skipping API storage.k8s.io/v1alpha1 because it has no resources.
I0321 21:41:38.789655    2292 handler.go:275] Adding GroupVersion flowcontrol.apiserver.k8s.io v1beta3 to ResourceManager
I0321 21:41:38.790387    2292 handler.go:275] Adding GroupVersion flowcontrol.apiserver.k8s.io v1beta2 to ResourceManager
W0321 21:41:38.790400    2292 genericapiserver.go:744] Skipping API flowcontrol.apiserver.k8s.io/v1beta1 because it has no resources.
W0321 21:41:38.790404    2292 genericapiserver.go:744] Skipping API flowcontrol.apiserver.k8s.io/v1alpha1 because it has no resources.
I0321 21:41:38.792788    2292 handler.go:275] Adding GroupVersion apps v1 to ResourceManager
W0321 21:41:38.792800    2292 genericapiserver.go:744] Skipping API apps/v1beta2 because it has no resources.
W0321 21:41:38.792804    2292 genericapiserver.go:744] Skipping API apps/v1beta1 because it has no resources.
I0321 21:41:38.793469    2292 handler.go:275] Adding GroupVersion admissionregistration.k8s.io v1 to ResourceManager
W0321 21:41:38.793482    2292 genericapiserver.go:744] Skipping API admissionregistration.k8s.io/v1beta1 because it has no resources.
W0321 21:41:38.793486    2292 genericapiserver.go:744] Skipping API admissionregistration.k8s.io/v1alpha1 because it has no resources.
I0321 21:41:38.793972    2292 handler.go:275] Adding GroupVersion events.k8s.io v1 to ResourceManager
W0321 21:41:38.793984    2292 genericapiserver.go:744] Skipping API events.k8s.io/v1beta1 because it has no resources.
I0321 21:41:38.797098    2292 handler.go:275] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
W0321 21:41:38.797114    2292 genericapiserver.go:744] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
time="2024-03-21T21:41:39Z" level=info msg="certificate CN=system:node:server,O=system:nodes signed by CN=k3s-client-ca@1711057297: notBefore=2024-03-21 21:41:37 +0000 UTC notAfter=2025-03-21 21:41:39 +0000 UTC"
I0321 21:41:39.293643    2292 secure_serving.go:213] Serving securely on 127.0.0.1:6444
I0321 21:41:39.293866    2292 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/rancher/k3s/server/tls/request-header-ca.crt"
I0321 21:41:39.294097    2292 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.crt::/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.key"
I0321 21:41:39.294273    2292 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0321 21:41:39.301543    2292 system_namespaces_controller.go:67] Starting system namespaces controller
I0321 21:41:39.301729    2292 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/rancher/k3s/server/tls/client-auth-proxy.crt::/var/lib/rancher/k3s/server/tls/client-auth-proxy.key"
I0321 21:41:39.302366    2292 gc_controller.go:78] Starting apiserver lease garbage collector
I0321 21:41:39.302537    2292 aggregator.go:164] waiting for initial CRD sync...
I0321 21:41:39.302608    2292 handler_discovery.go:412] Starting ResourceDiscoveryManager
I0321 21:41:39.302955    2292 gc_controller.go:78] Starting apiserver lease garbage collector
I0321 21:41:39.303239    2292 controller.go:116] Starting legacy_token_tracking_controller
I0321 21:41:39.303308    2292 shared_informer.go:311] Waiting for caches to sync for configmaps
I0321 21:41:39.303336    2292 controller.go:80] Starting OpenAPI V3 AggregationController
I0321 21:41:39.303994    2292 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0321 21:41:39.304059    2292 shared_informer.go:311] Waiting for caches to sync for cluster_authentication_trust_controller
I0321 21:41:39.304425    2292 available_controller.go:423] Starting AvailableConditionController
I0321 21:41:39.304465    2292 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0321 21:41:39.304526    2292 apf_controller.go:374] Starting API Priority and Fairness config controller
I0321 21:41:39.304597    2292 controller.go:78] Starting OpenAPI AggregationController
I0321 21:41:39.304879    2292 customresource_discovery_controller.go:289] Starting DiscoveryController
I0321 21:41:39.305226    2292 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0321 21:41:39.305271    2292 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0321 21:41:39.305386    2292 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/rancher/k3s/server/tls/client-ca.crt"
I0321 21:41:39.309221    2292 crdregistration_controller.go:111] Starting crd-autoregister controller
I0321 21:41:39.309342    2292 shared_informer.go:311] Waiting for caches to sync for crd-autoregister
I0321 21:41:39.309790    2292 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/rancher/k3s/server/tls/client-ca.crt"
I0321 21:41:39.309974    2292 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/rancher/k3s/server/tls/request-header-ca.crt"
I0321 21:41:39.310285    2292 controller.go:134] Starting OpenAPI controller
I0321 21:41:39.310360    2292 controller.go:85] Starting OpenAPI V3 controller
I0321 21:41:39.310405    2292 naming_controller.go:291] Starting NamingConditionController
I0321 21:41:39.310519    2292 establishing_controller.go:76] Starting EstablishingController
I0321 21:41:39.310610    2292 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0321 21:41:39.310651    2292 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0321 21:41:39.310718    2292 crd_finalizer.go:266] Starting CRDFinalizer
time="2024-03-21T21:41:39Z" level=info msg="Module overlay was already loaded"
time="2024-03-21T21:41:39Z" level=info msg="Module br_netfilter was already loaded"
time="2024-03-21T21:41:39Z" level=info msg="Set sysctl 'net/ipv4/conf/all/forwarding' to 1"
time="2024-03-21T21:41:39Z" level=info msg="Set sysctl 'net/netfilter/nf_conntrack_max' to 131072"
time="2024-03-21T21:41:39Z" level=info msg="Set sysctl 'net/netfilter/nf_conntrack_tcp_timeout_established' to 86400"
time="2024-03-21T21:41:39Z" level=info msg="Set sysctl 'net/netfilter/nf_conntrack_tcp_timeout_close_wait' to 3600"
time="2024-03-21T21:41:39Z" level=info msg="Logging containerd to /var/lib/rancher/k3s/agent/containerd/containerd.log"
time="2024-03-21T21:41:39Z" level=info msg="Running containerd -c /var/lib/rancher/k3s/agent/etc/containerd/config.toml -a /run/k3s/containerd/containerd.sock --state /run/k3s/containerd --root /var/lib/rancher/k3s/agent/containerd"
I0321 21:41:39.416795    2292 shared_informer.go:318] Caches are synced for configmaps
I0321 21:41:39.417088    2292 shared_informer.go:318] Caches are synced for cluster_authentication_trust_controller
I0321 21:41:39.417116    2292 cache.go:39] Caches are synced for AvailableConditionController controller
I0321 21:41:39.417151    2292 apf_controller.go:379] Running API Priority and Fairness config worker
I0321 21:41:39.417156    2292 apf_controller.go:382] Running API Priority and Fairness periodic rebalancing process
I0321 21:41:39.417389    2292 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0321 21:41:39.417408    2292 shared_informer.go:318] Caches are synced for crd-autoregister
I0321 21:41:39.417926    2292 aggregator.go:166] initial CRD sync complete...
I0321 21:41:39.417939    2292 autoregister_controller.go:141] Starting autoregister controller
I0321 21:41:39.417943    2292 cache.go:32] Waiting for caches to sync for autoregister controller
I0321 21:41:39.417948    2292 cache.go:39] Caches are synced for autoregister controller
I0321 21:41:39.423989    2292 controller.go:624] quota admission added evaluator for: namespaces
I0321 21:41:39.495021    2292 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.43.0.1"}
I0321 21:41:39.505163    2292 controller.go:624] quota admission added evaluator for: leases.coordination.k8s.io
I0321 21:41:39.505208    2292 shared_informer.go:318] Caches are synced for node_authorizer
W0321 21:41:39.517713    2292 lease.go:263] Resetting endpoints for master service "kubernetes" to [192.168.56.110]
I0321 21:41:39.521006    2292 controller.go:624] quota admission added evaluator for: endpoints
I0321 21:41:39.531026    2292 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
E0321 21:41:39.544779    2292 controller.go:102] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I0321 21:41:40.307313    2292 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0321 21:41:40.320202    2292 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0321 21:41:40.320448    2292 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
time="2024-03-21T21:41:40Z" level=info msg="containerd is now running"
time="2024-03-21T21:41:40Z" level=info msg="Running kubelet --address=0.0.0.0 --allowed-unsafe-sysctls=net.ipv4.ip_forward,net.ipv6.conf.all.forwarding --anonymous-auth=false --authentication-token-webhook=true --authorization-mode=Webhook --cgroup-driver=systemd --client-ca-file=/var/lib/rancher/k3s/agent/client-ca.crt --cloud-provider=external --cluster-dns=10.43.0.10 --cluster-domain=cluster.local --container-runtime-endpoint=unix:///run/k3s/containerd/containerd.sock --containerd=/run/k3s/containerd/containerd.sock --eviction-hard=imagefs.available<5%,nodefs.available<5% --eviction-minimum-reclaim=imagefs.available=10%,nodefs.available=10% --fail-swap-on=false --feature-gates=CloudDualStackNodeIPs=true --healthz-bind-address=127.0.0.1 --hostname-override=server --kubeconfig=/var/lib/rancher/k3s/agent/kubelet.kubeconfig --node-ip=192.168.56.110 --node-labels= --pod-infra-container-image=rancher/mirrored-pause:3.6 --pod-manifest-path=/var/lib/rancher/k3s/agent/pod-manifests --read-only-port=0 --resolv-conf=/run/systemd/resolve/resolv.conf --serialize-image-pulls=false --tls-cert-file=/var/lib/rancher/k3s/agent/serving-kubelet.crt --tls-private-key-file=/var/lib/rancher/k3s/agent/serving-kubelet.key"
time="2024-03-21T21:41:40Z" level=info msg="Connecting to proxy" url="wss://127.0.0.1:6443/v1-k3s/connect"
time="2024-03-21T21:41:40Z" level=info msg="Handling backend connection request [server]"
time="2024-03-21T21:41:40Z" level=info msg="Waiting to retrieve kube-proxy configuration; server is not ready: https://127.0.0.1:6443/v1-k3s/readyz: 500 Internal Server Error"
I0321 21:41:40.631684    2292 controller.go:624] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0321 21:41:40.654024    2292 controller.go:624] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
Flag --cloud-provider has been deprecated, will be removed in 1.25 or later, in favor of removing cloud provider code from Kubelet.
Flag --containerd has been deprecated, This is a cadvisor flag that was mistakenly registered with the Kubelet. Due to legacy concerns, it will follow the standard CLI deprecation timeline before being removed.
Flag --pod-infra-container-image has been deprecated, will be removed in a future release. Image garbage collector will get sandbox image information from CRI.
I0321 21:41:41.458240    2292 server.go:202] "--pod-infra-container-image will not be pruned by the image garbage collector in kubelet and should also be set in the remote runtime"
I0321 21:41:41.461026    2292 server.go:462] "Kubelet version" kubeletVersion="v1.28.7+k3s1"
I0321 21:41:41.461055    2292 server.go:464] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0321 21:41:41.464098    2292 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/rancher/k3s/agent/client-ca.crt"
time="2024-03-21T21:41:41Z" level=info msg="Kube API server is now running"
time="2024-03-21T21:41:41Z" level=info msg="ETCD server is now running"
time="2024-03-21T21:41:41Z" level=info msg="k3s is up and running"
time="2024-03-21T21:41:41Z" level=info msg="Creating k3s-supervisor event broadcaster"
time="2024-03-21T21:41:41Z" level=info msg="Waiting for cloud-controller-manager privileges to become available"
I0321 21:41:41.477722    2292 server.go:720] "--cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /"
I0321 21:41:41.478626    2292 container_manager_linux.go:265] "Container manager verified user specified cgroup-root exists" cgroupRoot=[]
I0321 21:41:41.478910    2292 container_manager_linux.go:270] "Creating Container Manager object based on Node Config" nodeConfig={"RuntimeCgroupsName":"","SystemCgroupsName":"","KubeletCgroupsName":"","KubeletOOMScoreAdj":-999,"ContainerRuntime":"","CgroupsPerQOS":true,"CgroupRoot":"/","CgroupDriver":"systemd","KubeletRootDir":"/var/lib/kubelet","ProtectKernelDefaults":false,"KubeReservedCgroupName":"","SystemReservedCgroupName":"","ReservedSystemCPUs":{},"EnforceNodeAllocatable":{"pods":{}},"KubeReserved":null,"SystemReserved":null,"HardEvictionThresholds":[{"Signal":"imagefs.available","Operator":"LessThan","Value":{"Quantity":null,"Percentage":0.05},"GracePeriod":0,"MinReclaim":null},{"Signal":"nodefs.available","Operator":"LessThan","Value":{"Quantity":null,"Percentage":0.05},"GracePeriod":0,"MinReclaim":null}],"QOSReserved":{},"CPUManagerPolicy":"none","CPUManagerPolicyOptions":null,"TopologyManagerScope":"container","CPUManagerReconcilePeriod":10000000000,"ExperimentalMemoryManagerPolicy":"None","ExperimentalMemoryManagerReservedMemory":null,"PodPidsLimit":-1,"EnforceCPULimits":true,"CPUCFSQuotaPeriod":100000000,"TopologyManagerPolicy":"none","TopologyManagerPolicyOptions":null}
I0321 21:41:41.478953    2292 topology_manager.go:138] "Creating topology manager with none policy"
I0321 21:41:41.478969    2292 container_manager_linux.go:301] "Creating device plugin manager"
I0321 21:41:41.479532    2292 state_mem.go:36] "Initialized new in-memory state store"
I0321 21:41:41.479730    2292 kubelet.go:393] "Attempting to sync node with API server"
I0321 21:41:41.479757    2292 kubelet.go:298] "Adding static pod path" path="/var/lib/rancher/k3s/agent/pod-manifests"
I0321 21:41:41.479789    2292 kubelet.go:309] "Adding apiserver pod source"
I0321 21:41:41.479850    2292 apiserver.go:42] "Waiting for node sync before watching apiserver pods"
I0321 21:41:41.488728    2292 kuberuntime_manager.go:257] "Container runtime initialized" containerRuntime="containerd" version="v1.7.11-k3s2" apiVersion="v1"
W0321 21:41:41.490762    2292 probe.go:268] Flexvolume plugin directory at /usr/libexec/kubernetes/kubelet-plugins/volume/exec/ does not exist. Recreating.
I0321 21:41:41.492083    2292 server.go:1227] "Started kubelet"
I0321 21:41:41.495178    2292 fs_resource_analyzer.go:67] "Starting FS ResourceAnalyzer"
I0321 21:41:41.510122    2292 server.go:162] "Starting to listen" address="0.0.0.0" port=10250
I0321 21:41:41.512033    2292 ratelimit.go:65] "Setting rate limiting for podresources endpoint" qps=100 burstTokens=10
I0321 21:41:41.512229    2292 server.go:233] "Starting to serve the podresources API" endpoint="unix:/var/lib/kubelet/pod-resources/kubelet.sock"
I0321 21:41:41.515300    2292 volume_manager.go:291] "Starting Kubelet Volume Manager"
I0321 21:41:41.515729    2292 desired_state_of_world_populator.go:151] "Desired state populator starts to run"
I0321 21:41:41.516172    2292 reconciler_new.go:29] "Reconciler: start to sync state"
I0321 21:41:41.516941    2292 server.go:462] "Adding debug handlers to kubelet server"
E0321 21:41:41.519137    2292 cri_stats_provider.go:448] "Failed to get the info of the filesystem with mountpoint" err="unable to find data in memory cache" mountpoint="/var/lib/rancher/k3s/agent/containerd/io.containerd.snapshotter.v1.overlayfs"
E0321 21:41:41.519187    2292 kubelet.go:1431] "Image garbage collection failed once. Stats initialization may not have completed yet" err="invalid capacity 0 on image filesystem"
time="2024-03-21T21:41:41Z" level=info msg="Applying CRD addons.k3s.cattle.io"
E0321 21:41:41.583799    2292 nodelease.go:49] "Failed to get node when trying to set owner ref to the node lease" err="nodes \"server\" not found" node="server"
I0321 21:41:41.615333    2292 cpu_manager.go:214] "Starting CPU manager" policy="none"
I0321 21:41:41.615442    2292 cpu_manager.go:215] "Reconciling" reconcilePeriod="10s"
I0321 21:41:41.615460    2292 state_mem.go:36] "Initialized new in-memory state store"
I0321 21:41:41.619777    2292 policy_none.go:49] "None policy: Start"
I0321 21:41:41.620782    2292 kubelet_node_status.go:70] "Attempting to register node" node="server"
I0321 21:41:41.622884    2292 memory_manager.go:169] "Starting memorymanager" policy="None"
I0321 21:41:41.622929    2292 state_mem.go:35] "Initializing new in-memory state store"
time="2024-03-21T21:41:41Z" level=info msg="Applying CRD etcdsnapshotfiles.k3s.cattle.io"
I0321 21:41:41.647977    2292 kubelet_node_status.go:73] "Successfully registered node" node="server"
I0321 21:41:41.650877    2292 kubelet_network_linux.go:50] "Initialized iptables rules." protocol="IPv4"
time="2024-03-21T21:41:41Z" level=info msg="Annotations and labels have been set successfully on node: server"
time="2024-03-21T21:41:41Z" level=info msg="Starting flannel with backend vxlan"
I0321 21:41:41.693551    2292 kubelet_network_linux.go:50] "Initialized iptables rules." protocol="IPv6"
I0321 21:41:41.694081    2292 status_manager.go:217] "Starting to sync pod status with apiserver"
I0321 21:41:41.694110    2292 kubelet.go:2303] "Starting kubelet main sync loop"
E0321 21:41:41.694171    2292 kubelet.go:2327] "Skipping pod synchronization" err="[container runtime status check may not have completed yet, PLEG is not healthy: pleg has yet to be successful]"
I0321 21:41:41.734508    2292 handler.go:275] Adding GroupVersion k3s.cattle.io v1 to ResourceManager
time="2024-03-21T21:41:41Z" level=info msg="Applying CRD helmcharts.helm.cattle.io"
I0321 21:41:41.773416    2292 handler.go:275] Adding GroupVersion k3s.cattle.io v1 to ResourceManager
E0321 21:41:41.800594    2292 kubelet.go:2327] "Skipping pod synchronization" err="container runtime status check may not have completed yet"
I0321 21:41:41.809381    2292 manager.go:471] "Failed to read data from checkpoint" checkpoint="kubelet_internal_checkpoint" err="checkpoint is not found"
W0321 21:41:41.809827    2292 watcher.go:93] Error while processing event ("/sys/fs/cgroup/devices/kubepods.slice/kubepods-besteffort.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/devices/kubepods.slice/kubepods-besteffort.slice: no such file or directory
I0321 21:41:41.812110    2292 plugin_manager.go:118] "Starting Kubelet Plugin Manager"
time="2024-03-21T21:41:41Z" level=info msg="Applying CRD helmchartconfigs.helm.cattle.io"
I0321 21:41:41.841495    2292 kubelet_node_status.go:493] "Fast updating node status as it just became ready"
I0321 21:41:41.851031    2292 handler.go:275] Adding GroupVersion helm.cattle.io v1 to ResourceManager
time="2024-03-21T21:41:41Z" level=info msg="Waiting for CRD helmchartconfigs.helm.cattle.io to become available"
I0321 21:41:41.870073    2292 handler.go:275] Adding GroupVersion helm.cattle.io v1 to ResourceManager
I0321 21:41:42.098503    2292 serving.go:355] Generated self-signed cert in-memory
time="2024-03-21T21:41:42Z" level=info msg="Done waiting for CRD helmchartconfigs.helm.cattle.io to become available"
time="2024-03-21T21:41:42Z" level=info msg="Writing static file: /var/lib/rancher/k3s/server/static/charts/traefik-crd-25.0.2+up25.0.0.tgz"
time="2024-03-21T21:41:42Z" level=info msg="Writing static file: /var/lib/rancher/k3s/server/static/charts/traefik-25.0.2+up25.0.0.tgz"
time="2024-03-21T21:41:42Z" level=info msg="Failed to get existing traefik HelmChart" error="helmcharts.helm.cattle.io \"traefik\" not found"
time="2024-03-21T21:41:42Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/ccm.yaml"
time="2024-03-21T21:41:42Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/auth-delegator.yaml"
time="2024-03-21T21:41:42Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/runtimes.yaml"
time="2024-03-21T21:41:42Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/traefik.yaml"
time="2024-03-21T21:41:42Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/coredns.yaml"
time="2024-03-21T21:41:42Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/auth-reader.yaml"
time="2024-03-21T21:41:42Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-service.yaml"
time="2024-03-21T21:41:42Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/rolebindings.yaml"
time="2024-03-21T21:41:42Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/aggregated-metrics-reader.yaml"
time="2024-03-21T21:41:42Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/local-storage.yaml"
time="2024-03-21T21:41:42Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/metrics-apiservice.yaml"
time="2024-03-21T21:41:42Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-deployment.yaml"
time="2024-03-21T21:41:42Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/resource-reader.yaml"
time="2024-03-21T21:41:42Z" level=info msg="Starting dynamiclistener CN filter node controller"
time="2024-03-21T21:41:42Z" level=info msg="Tunnel server egress proxy mode: agent"
I0321 21:41:42.482263    2292 apiserver.go:52] "Watching apiserver"
time="2024-03-21T21:41:42Z" level=info msg="Starting k3s.cattle.io/v1, Kind=Addon controller"
time="2024-03-21T21:41:42Z" level=info msg="Creating deploy event broadcaster"
I0321 21:41:42.515878    2292 desired_state_of_world_populator.go:159] "Finished populating initial desired state of world"
I0321 21:41:42.602168    2292 controllermanager.go:189] "Starting" version="v1.28.7+k3s1"
I0321 21:41:42.602193    2292 controllermanager.go:191] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0321 21:41:42.604708    2292 secure_serving.go:213] Serving securely on 127.0.0.1:10257
I0321 21:41:42.605115    2292 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I0321 21:41:42.605129    2292 shared_informer.go:311] Waiting for caches to sync for RequestHeaderAuthRequestController
I0321 21:41:42.605140    2292 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0321 21:41:42.605196    2292 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0321 21:41:42.605200    2292 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0321 21:41:42.605208    2292 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0321 21:41:42.605215    2292 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
time="2024-03-21T21:41:42Z" level=info msg="Creating helm-controller event broadcaster"
time="2024-03-21T21:41:42Z" level=info msg="Starting /v1, Kind=Node controller"
time="2024-03-21T21:41:42Z" level=warning msg="Unable to fetch coredns config map: configmaps \"coredns\" not found"
time="2024-03-21T21:41:42Z" level=info msg="Cluster dns configmap has been set successfully"
I0321 21:41:42.631893    2292 shared_informer.go:311] Waiting for caches to sync for tokens
I0321 21:41:42.636694    2292 controller.go:624] quota admission added evaluator for: serviceaccounts
I0321 21:41:42.647106    2292 controllermanager.go:642] "Started controller" controller="disruption-controller"
I0321 21:41:42.647119    2292 disruption.go:433] "Sending events to api server."
I0321 21:41:42.647429    2292 disruption.go:444] "Starting disruption controller"
I0321 21:41:42.647522    2292 shared_informer.go:311] Waiting for caches to sync for disruption
I0321 21:41:42.653980    2292 controllermanager.go:642] "Started controller" controller="statefulset-controller"
I0321 21:41:42.656634    2292 stateful_set.go:161] "Starting stateful set controller"
I0321 21:41:42.656698    2292 shared_informer.go:311] Waiting for caches to sync for stateful set
I0321 21:41:42.659140    2292 controllermanager.go:642] "Started controller" controller="cronjob-controller"
I0321 21:41:42.659258    2292 cronjob_controllerv2.go:139] "Starting cronjob controller v2"
I0321 21:41:42.659265    2292 shared_informer.go:311] Waiting for caches to sync for cronjob
I0321 21:41:42.665178    2292 controllermanager.go:642] "Started controller" controller="certificatesigningrequest-signing-controller"
I0321 21:41:42.665292    2292 certificate_controller.go:115] "Starting certificate controller" name="csrsigning-kubelet-serving"
I0321 21:41:42.665300    2292 shared_informer.go:311] Waiting for caches to sync for certificate-csrsigning-kubelet-serving
I0321 21:41:42.665336    2292 certificate_controller.go:115] "Starting certificate controller" name="csrsigning-kubelet-client"
I0321 21:41:42.665340    2292 shared_informer.go:311] Waiting for caches to sync for certificate-csrsigning-kubelet-client
I0321 21:41:42.665360    2292 certificate_controller.go:115] "Starting certificate controller" name="csrsigning-kube-apiserver-client"
I0321 21:41:42.665363    2292 shared_informer.go:311] Waiting for caches to sync for certificate-csrsigning-kube-apiserver-client
I0321 21:41:42.665382    2292 certificate_controller.go:115] "Starting certificate controller" name="csrsigning-legacy-unknown"
I0321 21:41:42.665386    2292 shared_informer.go:311] Waiting for caches to sync for certificate-csrsigning-legacy-unknown
I0321 21:41:42.665401    2292 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/var/lib/rancher/k3s/server/tls/server-ca.nochain.crt::/var/lib/rancher/k3s/server/tls/server-ca.key"
I0321 21:41:42.665463    2292 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/var/lib/rancher/k3s/server/tls/client-ca.nochain.crt::/var/lib/rancher/k3s/server/tls/client-ca.key"
I0321 21:41:42.665527    2292 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/var/lib/rancher/k3s/server/tls/client-ca.nochain.crt::/var/lib/rancher/k3s/server/tls/client-ca.key"
I0321 21:41:42.665574    2292 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/var/lib/rancher/k3s/server/tls/server-ca.nochain.crt::/var/lib/rancher/k3s/server/tls/server-ca.key"
I0321 21:41:42.672143    2292 controllermanager.go:642] "Started controller" controller="persistentvolume-binder-controller"
I0321 21:41:42.672323    2292 pv_controller_base.go:319] "Starting persistent volume controller"
I0321 21:41:42.672423    2292 shared_informer.go:311] Waiting for caches to sync for persistent volume
I0321 21:41:42.677277    2292 controllermanager.go:642] "Started controller" controller="persistentvolume-attach-detach-controller"
I0321 21:41:42.677460    2292 attach_detach_controller.go:337] "Starting attach detach controller"
I0321 21:41:42.677481    2292 shared_informer.go:311] Waiting for caches to sync for attach detach
I0321 21:41:42.683821    2292 controllermanager.go:642] "Started controller" controller="pod-garbage-collector-controller"
I0321 21:41:42.684108    2292 gc_controller.go:101] "Starting GC controller"
I0321 21:41:42.684151    2292 shared_informer.go:311] Waiting for caches to sync for GC
I0321 21:41:42.695361    2292 controllermanager.go:642] "Started controller" controller="horizontal-pod-autoscaler-controller"
I0321 21:41:42.695455    2292 horizontal.go:200] "Starting HPA controller"
I0321 21:41:42.695462    2292 shared_informer.go:311] Waiting for caches to sync for HPA
I0321 21:41:42.701188    2292 controllermanager.go:642] "Started controller" controller="persistentvolumeclaim-protection-controller"
I0321 21:41:42.701500    2292 pvc_protection_controller.go:102] "Starting PVC protection controller"
I0321 21:41:42.701546    2292 shared_informer.go:311] Waiting for caches to sync for PVC protection
I0321 21:41:42.706082    2292 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0321 21:41:42.706140    2292 shared_informer.go:318] Caches are synced for RequestHeaderAuthRequestController
I0321 21:41:42.706154    2292 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
time="2024-03-21T21:41:42Z" level=info msg="Starting helm.cattle.io/v1, Kind=HelmChart controller"
time="2024-03-21T21:41:42Z" level=info msg="Starting helm.cattle.io/v1, Kind=HelmChartConfig controller"
time="2024-03-21T21:41:42Z" level=info msg="Starting rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding controller"
time="2024-03-21T21:41:42Z" level=info msg="Starting batch/v1, Kind=Job controller"
time="2024-03-21T21:41:42Z" level=info msg="Starting /v1, Kind=ConfigMap controller"
time="2024-03-21T21:41:42Z" level=info msg="Starting /v1, Kind=ServiceAccount controller"
time="2024-03-21T21:41:42Z" level=info msg="Starting /v1, Kind=Secret controller"
I0321 21:41:42.733073    2292 shared_informer.go:318] Caches are synced for tokens
time="2024-03-21T21:41:42Z" level=warning msg="Unable to fetch coredns config map: configmaps \"coredns\" not found"
I0321 21:41:42.846581    2292 controllermanager.go:642] "Started controller" controller="root-ca-certificate-publisher-controller"
I0321 21:41:42.847157    2292 publisher.go:102] "Starting root CA cert publisher controller"
I0321 21:41:42.847328    2292 shared_informer.go:311] Waiting for caches to sync for crt configmap
I0321 21:41:42.987275    2292 controllermanager.go:642] "Started controller" controller="persistentvolume-protection-controller"
I0321 21:41:42.987407    2292 controllermanager.go:607] "Warning: controller is disabled" controller="cloud-node-lifecycle-controller"
I0321 21:41:42.987546    2292 pv_protection_controller.go:78] "Starting PV protection controller"
I0321 21:41:42.987627    2292 shared_informer.go:311] Waiting for caches to sync for PV protection
I0321 21:41:43.037307    2292 controllermanager.go:642] "Started controller" controller="certificatesigningrequest-approving-controller"
I0321 21:41:43.037359    2292 certificate_controller.go:115] "Starting certificate controller" name="csrapproving"
I0321 21:41:43.037368    2292 shared_informer.go:311] Waiting for caches to sync for certificate-csrapproving
I0321 21:41:43.197944    2292 controllermanager.go:642] "Started controller" controller="ttl-controller"
I0321 21:41:43.198597    2292 ttl_controller.go:124] "Starting TTL controller"
I0321 21:41:43.198786    2292 shared_informer.go:311] Waiting for caches to sync for TTL
time="2024-03-21T21:41:43Z" level=info msg="Updating TLS secret for kube-system/k3s-serving (count: 10): map[listener.cattle.io/cn-10.43.0.1:10.43.0.1 listener.cattle.io/cn-127.0.0.1:127.0.0.1 listener.cattle.io/cn-192.168.56.110:192.168.56.110 listener.cattle.io/cn-__1-f16284:::1 listener.cattle.io/cn-kubernetes:kubernetes listener.cattle.io/cn-kubernetes.default:kubernetes.default listener.cattle.io/cn-kubernetes.default.svc:kubernetes.default.svc listener.cattle.io/cn-kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local listener.cattle.io/cn-localhost:localhost listener.cattle.io/cn-server:server listener.cattle.io/fingerprint:SHA1=A9809CD2BE32F035ECEB1CA0D1F006B748463E54]"
I0321 21:41:43.344301    2292 controllermanager.go:642] "Started controller" controller="clusterrole-aggregation-controller"
I0321 21:41:43.344835    2292 clusterroleaggregation_controller.go:189] "Starting ClusterRoleAggregator controller"
I0321 21:41:43.344890    2292 shared_informer.go:311] Waiting for caches to sync for ClusterRoleAggregator
time="2024-03-21T21:41:43Z" level=info msg="Updating TLS secret for kube-system/k3s-serving (count: 10): map[listener.cattle.io/cn-10.43.0.1:10.43.0.1 listener.cattle.io/cn-127.0.0.1:127.0.0.1 listener.cattle.io/cn-192.168.56.110:192.168.56.110 listener.cattle.io/cn-__1-f16284:::1 listener.cattle.io/cn-kubernetes:kubernetes listener.cattle.io/cn-kubernetes.default:kubernetes.default listener.cattle.io/cn-kubernetes.default.svc:kubernetes.default.svc listener.cattle.io/cn-kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local listener.cattle.io/cn-localhost:localhost listener.cattle.io/cn-server:server listener.cattle.io/fingerprint:SHA1=A9809CD2BE32F035ECEB1CA0D1F006B748463E54]"
I0321 21:41:43.495989    2292 controllermanager.go:642] "Started controller" controller="replicationcontroller-controller"
I0321 21:41:43.496105    2292 controllermanager.go:607] "Warning: controller is disabled" controller="node-route-controller"
I0321 21:41:43.496965    2292 replica_set.go:214] "Starting controller" name="replicationcontroller"
I0321 21:41:43.497037    2292 shared_informer.go:311] Waiting for caches to sync for ReplicationController
time="2024-03-21T21:41:43Z" level=info msg="Active TLS secret kube-system/k3s-serving (ver=239) (count 10): map[listener.cattle.io/cn-10.43.0.1:10.43.0.1 listener.cattle.io/cn-127.0.0.1:127.0.0.1 listener.cattle.io/cn-192.168.56.110:192.168.56.110 listener.cattle.io/cn-__1-f16284:::1 listener.cattle.io/cn-kubernetes:kubernetes listener.cattle.io/cn-kubernetes.default:kubernetes.default listener.cattle.io/cn-kubernetes.default.svc:kubernetes.default.svc listener.cattle.io/cn-kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local listener.cattle.io/cn-localhost:localhost listener.cattle.io/cn-server:server listener.cattle.io/fingerprint:SHA1=A9809CD2BE32F035ECEB1CA0D1F006B748463E54]"
I0321 21:41:43.770881    2292 controllermanager.go:642] "Started controller" controller="namespace-controller"
I0321 21:41:43.771001    2292 namespace_controller.go:197] "Starting namespace controller"
I0321 21:41:43.771016    2292 shared_informer.go:311] Waiting for caches to sync for namespace
I0321 21:41:43.997175    2292 controllermanager.go:642] "Started controller" controller="garbage-collector-controller"
I0321 21:41:43.997773    2292 garbagecollector.go:155] "Starting controller" controller="garbagecollector"
I0321 21:41:43.997853    2292 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0321 21:41:43.998365    2292 graph_builder.go:294] "Running" component="GraphBuilder"
I0321 21:41:44.224255    2292 event.go:307] "Event occurred" object="server" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodePasswordValidationComplete" message="Deferred node password secret validation complete"
I0321 21:41:44.241960    2292 controllermanager.go:642] "Started controller" controller="job-controller"
I0321 21:41:44.242310    2292 job_controller.go:226] "Starting job controller"
I0321 21:41:44.242436    2292 shared_informer.go:311] Waiting for caches to sync for job
I0321 21:41:44.389669    2292 controllermanager.go:642] "Started controller" controller="ephemeral-volume-controller"
I0321 21:41:44.389721    2292 controller.go:169] "Starting ephemeral volume controller"
I0321 21:41:44.389729    2292 shared_informer.go:311] Waiting for caches to sync for ephemeral
time="2024-03-21T21:41:44Z" level=info msg="Labels and annotations have been set successfully on node: server"
I0321 21:41:44.509711    2292 controller.go:624] quota admission added evaluator for: addons.k3s.cattle.io
I0321 21:41:44.523912    2292 event.go:307] "Event occurred" object="kube-system/ccm" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/ccm.yaml\""
I0321 21:41:44.549956    2292 controllermanager.go:642] "Started controller" controller="endpointslice-mirroring-controller"
I0321 21:41:44.550147    2292 endpointslicemirroring_controller.go:223] "Starting EndpointSliceMirroring controller"
I0321 21:41:44.550282    2292 shared_informer.go:311] Waiting for caches to sync for endpoint_slice_mirroring
I0321 21:41:44.570031    2292 event.go:307] "Event occurred" object="kube-system/ccm" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/ccm.yaml\""
I0321 21:41:44.580464    2292 event.go:307] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/coredns.yaml\""
time="2024-03-21T21:41:44Z" level=warning msg="Unable to fetch coredns config map: configmaps \"coredns\" not found"
I0321 21:41:44.622195    2292 controller.go:624] quota admission added evaluator for: deployments.apps
I0321 21:41:44.634846    2292 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.43.0.10"}
I0321 21:41:44.635914    2292 event.go:307] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/coredns.yaml\""
I0321 21:41:44.645348    2292 event.go:307] "Event occurred" object="kube-system/local-storage" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/local-storage.yaml\""
I0321 21:41:44.679621    2292 event.go:307] "Event occurred" object="kube-system/local-storage" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/local-storage.yaml\""
I0321 21:41:44.689495    2292 event.go:307] "Event occurred" object="kube-system/aggregated-metrics-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/aggregated-metrics-reader.yaml\""
I0321 21:41:44.697194    2292 event.go:307] "Event occurred" object="kube-system/aggregated-metrics-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/aggregated-metrics-reader.yaml\""
I0321 21:41:44.705927    2292 event.go:307] "Event occurred" object="kube-system/auth-delegator" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/auth-delegator.yaml\""
I0321 21:41:44.711496    2292 event.go:307] "Event occurred" object="kube-system/auth-delegator" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/auth-delegator.yaml\""
I0321 21:41:44.717620    2292 event.go:307] "Event occurred" object="kube-system/auth-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/auth-reader.yaml\""
I0321 21:41:44.722743    2292 event.go:307] "Event occurred" object="kube-system/auth-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/auth-reader.yaml\""
I0321 21:41:44.865788    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="limitranges"
I0321 21:41:44.865888    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="rolebindings.rbac.authorization.k8s.io"
I0321 21:41:44.865922    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="replicasets.apps"
I0321 21:41:44.865951    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="addons.k3s.cattle.io"
I0321 21:41:44.865966    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="deployments.apps"
I0321 21:41:44.866078    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="leases.coordination.k8s.io"
I0321 21:41:44.866170    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="controllerrevisions.apps"
I0321 21:41:44.866219    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingresses.networking.k8s.io"
I0321 21:41:44.866270    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="roles.rbac.authorization.k8s.io"
I0321 21:41:44.866307    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="daemonsets.apps"
I0321 21:41:44.866356    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="jobs.batch"
I0321 21:41:44.866391    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="horizontalpodautoscalers.autoscaling"
I0321 21:41:44.866435    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="helmcharts.helm.cattle.io"
I0321 21:41:44.866451    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="endpointslices.discovery.k8s.io"
I0321 21:41:44.866505    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="helmchartconfigs.helm.cattle.io"
I0321 21:41:44.866518    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="endpoints"
I0321 21:41:44.866556    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="serviceaccounts"
I0321 21:41:44.866579    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="statefulsets.apps"
I0321 21:41:44.866596    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="networkpolicies.networking.k8s.io"
I0321 21:41:44.866617    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="csistoragecapacities.storage.k8s.io"
I0321 21:41:44.866661    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="podtemplates"
I0321 21:41:44.866733    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="cronjobs.batch"
I0321 21:41:44.866750    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="poddisruptionbudgets.policy"
I0321 21:41:44.866770    2292 controllermanager.go:642] "Started controller" controller="resourcequota-controller"
I0321 21:41:44.866778    2292 controllermanager.go:607] "Warning: controller is disabled" controller="bootstrap-signer-controller"
I0321 21:41:44.866870    2292 resource_quota_controller.go:294] "Starting resource quota controller"
I0321 21:41:44.866940    2292 shared_informer.go:311] Waiting for caches to sync for resource quota
I0321 21:41:44.866965    2292 resource_quota_monitor.go:305] "QuotaMonitor running"
I0321 21:41:45.008626    2292 event.go:307] "Event occurred" object="kube-system/metrics-apiservice" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-apiservice.yaml\""
I0321 21:41:45.043553    2292 handler.go:275] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0321 21:41:45.043992    2292 event.go:307] "Event occurred" object="kube-system/metrics-apiservice" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-apiservice.yaml\""
W0321 21:41:45.048738    2292 handler_proxy.go:93] no RequestInfo found in the context
E0321 21:41:45.048793    2292 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
E0321 21:41:45.048836    2292 handler_proxy.go:137] error resolving kube-system/metrics-server: service "metrics-server" not found
I0321 21:41:45.384423    2292 event.go:307] "Event occurred" object="kube-system/metrics-server-deployment" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-deployment.yaml\""
I0321 21:41:45.405438    2292 event.go:307] "Event occurred" object="kube-system/metrics-server-deployment" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-deployment.yaml\""
time="2024-03-21T21:41:45Z" level=info msg="Running kube-proxy --cluster-cidr=10.42.0.0/16 --conntrack-max-per-core=0 --conntrack-tcp-timeout-close-wait=0s --conntrack-tcp-timeout-established=0s --healthz-bind-address=127.0.0.1 --hostname-override=server --kubeconfig=/var/lib/rancher/k3s/agent/kubeproxy.kubeconfig --proxy-mode=iptables"
I0321 21:41:45.804609    2292 event.go:307] "Event occurred" object="kube-system/metrics-server-service" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-service.yaml\""
I0321 21:41:45.814294    2292 node.go:141] Successfully retrieved node IP: 192.168.56.110
I0321 21:41:45.817796    2292 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0321 21:41:45.819274    2292 server_others.go:152] "Using iptables Proxier"
I0321 21:41:45.819292    2292 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0321 21:41:45.819297    2292 server_others.go:438] "Defaulting to no-op detect-local"
I0321 21:41:45.819310    2292 proxier.go:250] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0321 21:41:45.819434    2292 server.go:846] "Version info" version="v1.28.7+k3s1"
I0321 21:41:45.819442    2292 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0321 21:41:45.825183    2292 config.go:188] "Starting service config controller"
I0321 21:41:45.825200    2292 shared_informer.go:311] Waiting for caches to sync for service config
I0321 21:41:45.825213    2292 config.go:97] "Starting endpoint slice config controller"
I0321 21:41:45.825216    2292 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0321 21:41:45.825509    2292 config.go:315] "Starting node config controller"
I0321 21:41:45.825514    2292 shared_informer.go:311] Waiting for caches to sync for node config
I0321 21:41:45.836718    2292 alloc.go:330] "allocated clusterIPs" service="kube-system/metrics-server" clusterIPs={"IPv4":"10.43.7.210"}
I0321 21:41:45.837719    2292 event.go:307] "Event occurred" object="kube-system/metrics-server-service" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-service.yaml\""
W0321 21:41:45.840742    2292 handler_proxy.go:93] no RequestInfo found in the context
E0321 21:41:45.840776    2292 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
E0321 21:41:45.840838    2292 handler_proxy.go:137] error resolving kube-system/metrics-server: endpoints "metrics-server" not found
I0321 21:41:45.897061    2292 serving.go:355] Generated self-signed cert in-memory
I0321 21:41:45.941201    2292 shared_informer.go:318] Caches are synced for service config
I0321 21:41:45.941239    2292 shared_informer.go:318] Caches are synced for endpoint slice config
I0321 21:41:45.943695    2292 shared_informer.go:318] Caches are synced for node config
W0321 21:41:46.046025    2292 handler_proxy.go:93] no RequestInfo found in the context
E0321 21:41:46.048828    2292 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0321 21:41:46.048905    2292 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0321 21:41:46.050376    2292 handler_proxy.go:93] no RequestInfo found in the context
E0321 21:41:46.050394    2292 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I0321 21:41:46.050402    2292 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0321 21:41:46.173509    2292 event.go:307] "Event occurred" object="kube-system/resource-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/resource-reader.yaml\""
I0321 21:41:46.189073    2292 event.go:307] "Event occurred" object="kube-system/resource-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/resource-reader.yaml\""
I0321 21:41:46.325088    2292 serving.go:355] Generated self-signed cert in-memory
I0321 21:41:46.574414    2292 event.go:307] "Event occurred" object="kube-system/rolebindings" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/rolebindings.yaml\""
I0321 21:41:46.592259    2292 event.go:307] "Event occurred" object="kube-system/rolebindings" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/rolebindings.yaml\""
I0321 21:41:46.974308    2292 event.go:307] "Event occurred" object="kube-system/runtimes" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/runtimes.yaml\""
I0321 21:41:47.139748    2292 controllermanager.go:168] Version: v1.28.7+k3s1
I0321 21:41:47.142830    2292 secure_serving.go:213] Serving securely on 127.0.0.1:10258
I0321 21:41:47.143051    2292 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0321 21:41:47.143089    2292 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I0321 21:41:47.143096    2292 shared_informer.go:311] Waiting for caches to sync for RequestHeaderAuthRequestController
I0321 21:41:47.143219    2292 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0321 21:41:47.143273    2292 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0321 21:41:47.143316    2292 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0321 21:41:47.143379    2292 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
E0321 21:41:47.152392    2292 controllermanager.go:524] unable to get all supported resources from server: unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
time="2024-03-21T21:41:47Z" level=info msg="Creating  event broadcaster"
I0321 21:41:47.179097    2292 event.go:307] "Event occurred" object="kube-system/runtimes" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/runtimes.yaml\""
I0321 21:41:47.286764    2292 shared_informer.go:318] Caches are synced for RequestHeaderAuthRequestController
I0321 21:41:47.288079    2292 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0321 21:41:47.288351    2292 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
time="2024-03-21T21:41:47Z" level=info msg="Starting /v1, Kind=Node controller"
time="2024-03-21T21:41:47Z" level=info msg="Starting /v1, Kind=Pod controller"
I0321 21:41:47.319752    2292 controllermanager.go:337] Started "cloud-node-controller"
time="2024-03-21T21:41:47Z" level=info msg="Starting apps/v1, Kind=DaemonSet controller"
time="2024-03-21T21:41:47Z" level=info msg="Starting discovery.k8s.io/v1, Kind=EndpointSlice controller"
I0321 21:41:47.320526    2292 controllermanager.go:337] Started "cloud-node-lifecycle-controller"
I0321 21:41:47.320661    2292 node_lifecycle_controller.go:113] Sending events to api server
I0321 21:41:47.319776    2292 node_controller.go:165] Sending events to api server.
I0321 21:41:47.320887    2292 node_controller.go:174] Waiting for informer caches to sync
I0321 21:41:47.321168    2292 controllermanager.go:337] Started "service-lb-controller"
W0321 21:41:47.321217    2292 controllermanager.go:314] "node-route-controller" is disabled
I0321 21:41:47.321798    2292 controller.go:231] Starting service controller
I0321 21:41:47.321860    2292 shared_informer.go:311] Waiting for caches to sync for service
I0321 21:41:47.375019    2292 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/traefik.yaml\""
I0321 21:41:47.379541    2292 controller.go:624] quota admission added evaluator for: helmcharts.helm.cattle.io
I0321 21:41:47.385849    2292 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/traefik.yaml\""
I0321 21:41:47.400351    2292 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
I0321 21:41:47.403510    2292 event.go:307] "Event occurred" object="kube-system/traefik-crd" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik-crd"
I0321 21:41:47.420963    2292 node_controller.go:431] Initializing node server with cloud provider
I0321 21:41:47.421924    2292 shared_informer.go:318] Caches are synced for service
I0321 21:41:47.425978    2292 controller.go:624] quota admission added evaluator for: jobs.batch
I0321 21:41:47.428276    2292 node_controller.go:502] Successfully initialized node server with cloud provider
I0321 21:41:47.429599    2292 event.go:307] "Event occurred" object="server" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="Synced" message="Node synced successfully"
time="2024-03-21T21:41:47Z" level=info msg="Updated coredns node hosts entry [192.168.56.110 server]"
time="2024-03-21T21:41:47Z" level=error msg="error syncing 'kube-system/traefik-crd': handler helm-controller-chart-registration: helmcharts.helm.cattle.io \"traefik-crd\" not found, requeuing"
I0321 21:41:47.454646    2292 event.go:307] "Event occurred" object="kube-system/traefik-crd" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik-crd"
I0321 21:41:47.478200    2292 event.go:307] "Event occurred" object="kube-system/traefik-crd" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik-crd"
time="2024-03-21T21:41:47Z" level=error msg="error syncing 'kube-system/traefik': handler helm-controller-chart-registration: helmcharts.helm.cattle.io \"traefik\" not found, requeuing"
I0321 21:41:47.496609    2292 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
I0321 21:41:47.509467    2292 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
time="2024-03-21T21:41:47Z" level=info msg="Tunnel authorizer set Kubelet Port 10250"
I0321 21:41:47.805970    2292 serving.go:355] Generated self-signed cert in-memory
I0321 21:41:48.147014    2292 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.7+k3s1"
I0321 21:41:48.147173    2292 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0321 21:41:48.150749    2292 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0321 21:41:48.150804    2292 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I0321 21:41:48.150810    2292 shared_informer.go:311] Waiting for caches to sync for RequestHeaderAuthRequestController
I0321 21:41:48.150818    2292 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0321 21:41:48.151443    2292 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0321 21:41:48.151454    2292 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0321 21:41:48.151464    2292 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0321 21:41:48.151467    2292 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0321 21:41:48.251098    2292 shared_informer.go:318] Caches are synced for RequestHeaderAuthRequestController
I0321 21:41:48.251583    2292 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0321 21:41:48.251709    2292 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
time="2024-03-21T21:41:48Z" level=info msg="Stopped tunnel to 127.0.0.1:6443"
time="2024-03-21T21:41:48Z" level=info msg="Connecting to proxy" url="wss://192.168.56.110:6443/v1-k3s/connect"
time="2024-03-21T21:41:48Z" level=info msg="Proxy done" err="context canceled" url="wss://127.0.0.1:6443/v1-k3s/connect"
time="2024-03-21T21:41:48Z" level=info msg="error in remotedialer server [400]: websocket: close 1006 (abnormal closure): unexpected EOF"
time="2024-03-21T21:41:48Z" level=info msg="Handling backend connection request [server]"
I0321 21:41:55.045471    2292 range_allocator.go:111] "No Secondary Service CIDR provided. Skipping filtering out secondary service addresses"
I0321 21:41:55.045526    2292 controllermanager.go:642] "Started controller" controller="node-ipam-controller"
I0321 21:41:55.045678    2292 node_ipam_controller.go:162] "Starting ipam controller"
I0321 21:41:55.045689    2292 shared_informer.go:311] Waiting for caches to sync for node
I0321 21:41:55.048176    2292 node_lifecycle_controller.go:431] "Controller will reconcile labels"
I0321 21:41:55.048215    2292 controllermanager.go:642] "Started controller" controller="node-lifecycle-controller"
I0321 21:41:55.048330    2292 node_lifecycle_controller.go:465] "Sending events to api server"
I0321 21:41:55.048366    2292 node_lifecycle_controller.go:476] "Starting node controller"
I0321 21:41:55.048372    2292 shared_informer.go:311] Waiting for caches to sync for taint
I0321 21:41:55.053839    2292 controllermanager.go:642] "Started controller" controller="ttl-after-finished-controller"
I0321 21:41:55.053955    2292 ttlafterfinished_controller.go:109] "Starting TTL after finished controller"
I0321 21:41:55.053963    2292 shared_informer.go:311] Waiting for caches to sync for TTL after finished
I0321 21:41:55.060288    2292 controllermanager.go:642] "Started controller" controller="endpoints-controller"
I0321 21:41:55.060444    2292 endpoints_controller.go:174] "Starting endpoint controller"
I0321 21:41:55.060453    2292 shared_informer.go:311] Waiting for caches to sync for endpoint
I0321 21:41:55.066514    2292 controllermanager.go:642] "Started controller" controller="endpointslice-controller"
I0321 21:41:55.066614    2292 controllermanager.go:607] "Warning: controller is disabled" controller="service-lb-controller"
I0321 21:41:55.066729    2292 endpointslice_controller.go:264] "Starting endpoint slice controller"
I0321 21:41:55.066742    2292 shared_informer.go:311] Waiting for caches to sync for endpoint_slice
I0321 21:41:55.072859    2292 controllermanager.go:642] "Started controller" controller="persistentvolume-expander-controller"
I0321 21:41:55.072969    2292 expand_controller.go:328] "Starting expand controller"
I0321 21:41:55.072978    2292 shared_informer.go:311] Waiting for caches to sync for expand
I0321 21:41:55.078787    2292 controllermanager.go:642] "Started controller" controller="deployment-controller"
I0321 21:41:55.078896    2292 deployment_controller.go:168] "Starting controller" controller="deployment"
I0321 21:41:55.078908    2292 shared_informer.go:311] Waiting for caches to sync for deployment
I0321 21:41:55.080876    2292 controllermanager.go:642] "Started controller" controller="certificatesigningrequest-cleaner-controller"
I0321 21:41:55.080938    2292 cleaner.go:83] "Starting CSR cleaner controller"
I0321 21:41:55.086898    2292 controllermanager.go:642] "Started controller" controller="replicaset-controller"
I0321 21:41:55.087287    2292 replica_set.go:214] "Starting controller" name="replicaset"
I0321 21:41:55.087433    2292 shared_informer.go:311] Waiting for caches to sync for ReplicaSet
I0321 21:41:55.093597    2292 controllermanager.go:642] "Started controller" controller="token-cleaner-controller"
I0321 21:41:55.093713    2292 tokencleaner.go:112] "Starting token cleaner controller"
I0321 21:41:55.093721    2292 shared_informer.go:311] Waiting for caches to sync for token_cleaner
I0321 21:41:55.093725    2292 shared_informer.go:318] Caches are synced for token_cleaner
I0321 21:41:55.098862    2292 controllermanager.go:642] "Started controller" controller="serviceaccount-controller"
I0321 21:41:55.099153    2292 serviceaccounts_controller.go:111] "Starting service account controller"
I0321 21:41:55.099167    2292 shared_informer.go:311] Waiting for caches to sync for service account
I0321 21:41:55.104474    2292 controllermanager.go:642] "Started controller" controller="daemonset-controller"
I0321 21:41:55.104742    2292 daemon_controller.go:291] "Starting daemon sets controller"
I0321 21:41:55.104803    2292 shared_informer.go:311] Waiting for caches to sync for daemon sets
I0321 21:41:55.108953    2292 shared_informer.go:311] Waiting for caches to sync for resource quota
I0321 21:41:55.112471    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0321 21:41:55.112509    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0321 21:41:55.121144    2292 actual_state_of_world.go:547] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"server\" does not exist"
I0321 21:41:55.135456    2292 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0321 21:41:55.137431    2292 shared_informer.go:318] Caches are synced for certificate-csrapproving
I0321 21:41:55.142541    2292 shared_informer.go:318] Caches are synced for job
I0321 21:41:55.145890    2292 shared_informer.go:318] Caches are synced for node
I0321 21:41:55.145921    2292 range_allocator.go:174] "Sending events to api server"
I0321 21:41:55.145937    2292 range_allocator.go:178] "Starting range CIDR allocator"
I0321 21:41:55.145941    2292 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I0321 21:41:55.145945    2292 shared_informer.go:318] Caches are synced for cidrallocator
I0321 21:41:55.146000    2292 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I0321 21:41:55.147787    2292 shared_informer.go:318] Caches are synced for disruption
I0321 21:41:55.147818    2292 shared_informer.go:318] Caches are synced for crt configmap
I0321 21:41:55.148605    2292 shared_informer.go:318] Caches are synced for taint
I0321 21:41:55.148653    2292 node_lifecycle_controller.go:1225] "Initializing eviction metric for zone" zone=""
I0321 21:41:55.148711    2292 node_lifecycle_controller.go:877] "Missing timestamp for Node. Assuming now as a timestamp" node="server"
I0321 21:41:55.148742    2292 node_lifecycle_controller.go:1071] "Controller detected that zone is now in new state" zone="" newState="Normal"
I0321 21:41:55.148762    2292 taint_manager.go:205] "Starting NoExecuteTaintManager"
I0321 21:41:55.148778    2292 taint_manager.go:210] "Sending events to api server"
I0321 21:41:55.149166    2292 event.go:307] "Event occurred" object="server" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node server event: Registered Node server in Controller"
I0321 21:41:55.156753    2292 shared_informer.go:318] Caches are synced for TTL after finished
time="2024-03-21T21:41:55Z" level=info msg="Flannel found PodCIDR assigned for node server"
I0321 21:41:55.160819    2292 range_allocator.go:380] "Set node PodCIDR" node="server" podCIDRs=["10.42.0.0/24"]
I0321 21:41:55.160957    2292 shared_informer.go:318] Caches are synced for stateful set
I0321 21:41:55.160976    2292 shared_informer.go:318] Caches are synced for cronjob
time="2024-03-21T21:41:55Z" level=info msg="The interface enp0s3 with ipv4 address 10.0.2.15 will be used by flannel"
I0321 21:41:55.164862    2292 kube.go:139] Waiting 10m0s for node controller to sync
I0321 21:41:55.164889    2292 kube.go:461] Starting kube subnet manager
I0321 21:41:55.170047    2292 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I0321 21:41:55.170136    2292 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I0321 21:41:55.170240    2292 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0321 21:41:55.170347    2292 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I0321 21:41:55.171559    2292 shared_informer.go:318] Caches are synced for namespace
I0321 21:41:55.172659    2292 shared_informer.go:318] Caches are synced for persistent volume
I0321 21:41:55.173084    2292 shared_informer.go:318] Caches are synced for expand
I0321 21:41:55.177508    2292 shared_informer.go:318] Caches are synced for attach detach
I0321 21:41:55.179138    2292 shared_informer.go:318] Caches are synced for deployment
I0321 21:41:55.184692    2292 shared_informer.go:318] Caches are synced for GC
I0321 21:41:55.187479    2292 shared_informer.go:318] Caches are synced for ReplicaSet
I0321 21:41:55.190483    2292 shared_informer.go:318] Caches are synced for ephemeral
I0321 21:41:55.190516    2292 shared_informer.go:318] Caches are synced for PV protection
I0321 21:41:55.193877    2292 event.go:307] "Event occurred" object="kube-system/helm-install-traefik" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: helm-install-traefik-tv7kv"
I0321 21:41:55.195514    2292 shared_informer.go:318] Caches are synced for HPA
I0321 21:41:55.197080    2292 shared_informer.go:318] Caches are synced for ReplicationController
I0321 21:41:55.197975    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0321 21:41:55.200727    2292 shared_informer.go:318] Caches are synced for service account
I0321 21:41:55.201453    2292 shared_informer.go:318] Caches are synced for TTL
I0321 21:41:55.201962    2292 shared_informer.go:318] Caches are synced for PVC protection
I0321 21:41:55.204911    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0321 21:41:55.207017    2292 event.go:307] "Event occurred" object="kube-system/helm-install-traefik-crd" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: helm-install-traefik-crd-clwp2"
I0321 21:41:55.212864    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0321 21:41:55.213857    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0321 21:41:55.214279    2292 shared_informer.go:318] Caches are synced for daemon sets
I0321 21:41:55.217394    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0321 21:41:55.222646    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0321 21:41:55.250773    2292 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I0321 21:41:55.260751    2292 shared_informer.go:318] Caches are synced for endpoint
I0321 21:41:55.267049    2292 shared_informer.go:318] Caches are synced for endpoint_slice
time="2024-03-21T21:41:55Z" level=info msg="Starting network policy controller version v2.0.1, built on 2024-02-29T20:36:21Z, go1.21.7"
I0321 21:41:55.281672    2292 network_policy_controller.go:164] Starting network policy controller
I0321 21:41:55.310333    2292 shared_informer.go:318] Caches are synced for resource quota
I0321 21:41:55.324920    2292 network_policy_controller.go:176] Starting network policy controller full sync goroutine
I0321 21:41:55.367415    2292 shared_informer.go:318] Caches are synced for resource quota
I0321 21:41:55.692699    2292 controller.go:624] quota admission added evaluator for: replicasets.apps
I0321 21:41:55.699940    2292 event.go:307] "Event occurred" object="kube-system/metrics-server" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set metrics-server-67c658944b to 1"
I0321 21:41:55.701662    2292 event.go:307] "Event occurred" object="kube-system/local-path-provisioner" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set local-path-provisioner-6c86858495 to 1"
I0321 21:41:55.706048    2292 event.go:307] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set coredns-6799fbcd5 to 1"
I0321 21:41:55.737253    2292 shared_informer.go:318] Caches are synced for garbage collector
I0321 21:41:55.800837    2292 shared_informer.go:318] Caches are synced for garbage collector
I0321 21:41:55.800870    2292 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
W0321 21:41:55.856442    2292 handler_proxy.go:93] no RequestInfo found in the context
E0321 21:41:55.856558    2292 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
E0321 21:41:55.856700    2292 handler_proxy.go:137] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
I0321 21:41:56.032971    2292 topology_manager.go:215] "Topology Admit Handler" podUID="12134130-fe19-45c4-a6d5-bdd80396cf52" podNamespace="kube-system" podName="metrics-server-67c658944b-mgk2k"
I0321 21:41:56.034481    2292 event.go:307] "Event occurred" object="kube-system/local-path-provisioner-6c86858495" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: local-path-provisioner-6c86858495-xwxcj"
I0321 21:41:56.034507    2292 event.go:307] "Event occurred" object="kube-system/metrics-server-67c658944b" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: metrics-server-67c658944b-mgk2k"
I0321 21:41:56.034519    2292 event.go:307] "Event occurred" object="kube-system/coredns-6799fbcd5" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: coredns-6799fbcd5-g75xc"
I0321 21:41:56.043389    2292 topology_manager.go:215] "Topology Admit Handler" podUID="ba4e048a-d75e-481a-8791-8ae1dcd6350e" podNamespace="kube-system" podName="local-path-provisioner-6c86858495-xwxcj"
I0321 21:41:56.050336    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="351.636337ms"
I0321 21:41:56.056968    2292 topology_manager.go:215] "Topology Admit Handler" podUID="d36c977e-3b64-459b-a4ba-264b53753077" podNamespace="kube-system" podName="coredns-6799fbcd5-g75xc"
I0321 21:41:56.064917    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="359.354057ms"
I0321 21:41:56.084085    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="385.170629ms"
I0321 21:41:56.090288    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="25.326342ms"
I0321 21:41:56.090923    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="47.293s"
I0321 21:41:56.093554    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="43.173874ms"
I0321 21:41:56.093629    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="45.245s"
I0321 21:41:56.100964    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="16.836678ms"
E0321 21:41:56.110775    2292 watcher.go:152] Failed to watch directory "/sys/fs/cgroup/devices/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podd36c977e_3b64_459b_a4ba_264b53753077.slice": readdirent /sys/fs/cgroup/devices/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podd36c977e_3b64_459b_a4ba_264b53753077.slice: no such file or directory
I0321 21:41:56.113922    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="35.446s"
I0321 21:41:56.129154    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="41.676s"
I0321 21:41:56.132486    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="31.484385ms"
I0321 21:41:56.132563    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="43.706s"
I0321 21:41:56.165447    2292 kube.go:146] Node controller sync successful
I0321 21:41:56.165503    2292 vxlan.go:141] VXLAN config: VNI=1 Port=0 GBP=false Learning=false DirectRouting=false
I0321 21:41:56.167869    2292 kube.go:621] List of node(server) annotations: map[string]string{"alpha.kubernetes.io/provided-node-ip":"192.168.56.110", "k3s.io/hostname":"server", "k3s.io/internal-ip":"192.168.56.110", "k3s.io/node-args":"[\"server\",\"--write-kubeconfig-mode\",\"644\",\"--node-ip\",\"192.168.56.110\",\"--log\",\"/vagrant/logs/server.logs\"]", "k3s.io/node-config-hash":"3PZPW4FQCHFROJYSVKXEV2FIFKDUB666MSS6DHLJL37T5VMNZEUA====", "k3s.io/node-env":"{\"K3S_DATA_DIR\":\"/var/lib/rancher/k3s/data/a3b46c0299091b71bfcc617b1e1fec1845c13bdd848584ceb39d2e700e702a4b\"}", "node.alpha.kubernetes.io/ttl":"0", "volumes.kubernetes.io/controller-managed-attach-detach":"true"}
I0321 21:41:56.188073    2292 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-lvr52\" (UniqueName: \"kubernetes.io/projected/ba4e048a-d75e-481a-8791-8ae1dcd6350e-kube-api-access-lvr52\") pod \"local-path-provisioner-6c86858495-xwxcj\" (UID: \"ba4e048a-d75e-481a-8791-8ae1dcd6350e\") " pod="kube-system/local-path-provisioner-6c86858495-xwxcj"
I0321 21:41:56.188112    2292 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/d36c977e-3b64-459b-a4ba-264b53753077-config-volume\") pod \"coredns-6799fbcd5-g75xc\" (UID: \"d36c977e-3b64-459b-a4ba-264b53753077\") " pod="kube-system/coredns-6799fbcd5-g75xc"
I0321 21:41:56.188131    2292 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"custom-config-volume\" (UniqueName: \"kubernetes.io/configmap/d36c977e-3b64-459b-a4ba-264b53753077-custom-config-volume\") pod \"coredns-6799fbcd5-g75xc\" (UID: \"d36c977e-3b64-459b-a4ba-264b53753077\") " pod="kube-system/coredns-6799fbcd5-g75xc"
I0321 21:41:56.188146    2292 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-tz6fw\" (UniqueName: \"kubernetes.io/projected/d36c977e-3b64-459b-a4ba-264b53753077-kube-api-access-tz6fw\") pod \"coredns-6799fbcd5-g75xc\" (UID: \"d36c977e-3b64-459b-a4ba-264b53753077\") " pod="kube-system/coredns-6799fbcd5-g75xc"
I0321 21:41:56.188163    2292 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp-dir\" (UniqueName: \"kubernetes.io/empty-dir/12134130-fe19-45c4-a6d5-bdd80396cf52-tmp-dir\") pod \"metrics-server-67c658944b-mgk2k\" (UID: \"12134130-fe19-45c4-a6d5-bdd80396cf52\") " pod="kube-system/metrics-server-67c658944b-mgk2k"
I0321 21:41:56.191993    2292 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-vz2bz\" (UniqueName: \"kubernetes.io/projected/12134130-fe19-45c4-a6d5-bdd80396cf52-kube-api-access-vz2bz\") pod \"metrics-server-67c658944b-mgk2k\" (UID: \"12134130-fe19-45c4-a6d5-bdd80396cf52\") " pod="kube-system/metrics-server-67c658944b-mgk2k"
I0321 21:41:56.192044    2292 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/ba4e048a-d75e-481a-8791-8ae1dcd6350e-config-volume\") pod \"local-path-provisioner-6c86858495-xwxcj\" (UID: \"ba4e048a-d75e-481a-8791-8ae1dcd6350e\") " pod="kube-system/local-path-provisioner-6c86858495-xwxcj"
I0321 21:41:56.194250    2292 kube.go:482] Creating the node lease for IPv4. This is the n.Spec.PodCIDRs: [10.42.0.0/24]
time="2024-03-21T21:41:56Z" level=info msg="Wrote flannel subnet file to /run/flannel/subnet.env"
time="2024-03-21T21:41:56Z" level=info msg="Running flannel backend."
I0321 21:41:56.210249    2292 vxlan_network.go:65] watching for new subnet leases
I0321 21:41:56.210265    2292 iptables.go:290] generated 3 rules
I0321 21:41:56.212817    2292 iptables.go:290] generated 7 rules
I0321 21:41:56.233316    2292 iptables.go:283] bootstrap done
I0321 21:41:56.238390    2292 iptables.go:283] bootstrap done
I0321 21:41:56.519400    2292 topology_manager.go:215] "Topology Admit Handler" podUID="fd4c637f-8761-4cce-a85c-31d1ed40abed" podNamespace="kube-system" podName="helm-install-traefik-tv7kv"
I0321 21:41:56.520268    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0321 21:41:56.529678    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0321 21:41:56.530973    2292 topology_manager.go:215] "Topology Admit Handler" podUID="92b9020c-170c-42e6-8f5a-26ff87f8902f" podNamespace="kube-system" podName="helm-install-traefik-crd-clwp2"
E0321 21:41:56.558618    2292 watcher.go:152] Failed to watch directory "/sys/fs/cgroup/devices/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod92b9020c_170c_42e6_8f5a_26ff87f8902f.slice": inotify_add_watch /sys/fs/cgroup/devices/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod92b9020c_170c_42e6_8f5a_26ff87f8902f.slice: no such file or directory
I0321 21:41:56.567983    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0321 21:41:56.608987    2292 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/fd4c637f-8761-4cce-a85c-31d1ed40abed-tmp\") pod \"helm-install-traefik-tv7kv\" (UID: \"fd4c637f-8761-4cce-a85c-31d1ed40abed\") " pod="kube-system/helm-install-traefik-tv7kv"
I0321 21:41:56.609029    2292 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"content\" (UniqueName: \"kubernetes.io/configmap/fd4c637f-8761-4cce-a85c-31d1ed40abed-content\") pod \"helm-install-traefik-tv7kv\" (UID: \"fd4c637f-8761-4cce-a85c-31d1ed40abed\") " pod="kube-system/helm-install-traefik-tv7kv"
I0321 21:41:56.609052    2292 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/92b9020c-170c-42e6-8f5a-26ff87f8902f-tmp\") pod \"helm-install-traefik-crd-clwp2\" (UID: \"92b9020c-170c-42e6-8f5a-26ff87f8902f\") " pod="kube-system/helm-install-traefik-crd-clwp2"
I0321 21:41:56.609074    2292 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-44k2p\" (UniqueName: \"kubernetes.io/projected/92b9020c-170c-42e6-8f5a-26ff87f8902f-kube-api-access-44k2p\") pod \"helm-install-traefik-crd-clwp2\" (UID: \"92b9020c-170c-42e6-8f5a-26ff87f8902f\") " pod="kube-system/helm-install-traefik-crd-clwp2"
I0321 21:41:56.609096    2292 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-8n996\" (UniqueName: \"kubernetes.io/projected/fd4c637f-8761-4cce-a85c-31d1ed40abed-kube-api-access-8n996\") pod \"helm-install-traefik-tv7kv\" (UID: \"fd4c637f-8761-4cce-a85c-31d1ed40abed\") " pod="kube-system/helm-install-traefik-tv7kv"
I0321 21:41:56.609112    2292 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/92b9020c-170c-42e6-8f5a-26ff87f8902f-klipper-helm\") pod \"helm-install-traefik-crd-clwp2\" (UID: \"92b9020c-170c-42e6-8f5a-26ff87f8902f\") " pod="kube-system/helm-install-traefik-crd-clwp2"
I0321 21:41:56.609131    2292 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"content\" (UniqueName: \"kubernetes.io/configmap/92b9020c-170c-42e6-8f5a-26ff87f8902f-content\") pod \"helm-install-traefik-crd-clwp2\" (UID: \"92b9020c-170c-42e6-8f5a-26ff87f8902f\") " pod="kube-system/helm-install-traefik-crd-clwp2"
I0321 21:41:56.609150    2292 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/fd4c637f-8761-4cce-a85c-31d1ed40abed-klipper-config\") pod \"helm-install-traefik-tv7kv\" (UID: \"fd4c637f-8761-4cce-a85c-31d1ed40abed\") " pod="kube-system/helm-install-traefik-tv7kv"
I0321 21:41:56.609168    2292 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"values\" (UniqueName: \"kubernetes.io/secret/fd4c637f-8761-4cce-a85c-31d1ed40abed-values\") pod \"helm-install-traefik-tv7kv\" (UID: \"fd4c637f-8761-4cce-a85c-31d1ed40abed\") " pod="kube-system/helm-install-traefik-tv7kv"
I0321 21:41:56.609187    2292 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/92b9020c-170c-42e6-8f5a-26ff87f8902f-klipper-config\") pod \"helm-install-traefik-crd-clwp2\" (UID: \"92b9020c-170c-42e6-8f5a-26ff87f8902f\") " pod="kube-system/helm-install-traefik-crd-clwp2"
I0321 21:41:56.609204    2292 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"values\" (UniqueName: \"kubernetes.io/secret/92b9020c-170c-42e6-8f5a-26ff87f8902f-values\") pod \"helm-install-traefik-crd-clwp2\" (UID: \"92b9020c-170c-42e6-8f5a-26ff87f8902f\") " pod="kube-system/helm-install-traefik-crd-clwp2"
I0321 21:41:56.609226    2292 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/fd4c637f-8761-4cce-a85c-31d1ed40abed-klipper-helm\") pod \"helm-install-traefik-tv7kv\" (UID: \"fd4c637f-8761-4cce-a85c-31d1ed40abed\") " pod="kube-system/helm-install-traefik-tv7kv"
I0321 21:41:56.609245    2292 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/fd4c637f-8761-4cce-a85c-31d1ed40abed-klipper-cache\") pod \"helm-install-traefik-tv7kv\" (UID: \"fd4c637f-8761-4cce-a85c-31d1ed40abed\") " pod="kube-system/helm-install-traefik-tv7kv"
I0321 21:41:56.609263    2292 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/92b9020c-170c-42e6-8f5a-26ff87f8902f-klipper-cache\") pod \"helm-install-traefik-crd-clwp2\" (UID: \"92b9020c-170c-42e6-8f5a-26ff87f8902f\") " pod="kube-system/helm-install-traefik-crd-clwp2"
I0321 21:41:56.609865    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
W0321 21:41:56.857145    2292 handler_proxy.go:93] no RequestInfo found in the context
E0321 21:41:56.857206    2292 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I0321 21:41:56.857222    2292 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0321 21:41:56.857277    2292 handler_proxy.go:93] no RequestInfo found in the context
E0321 21:41:56.857363    2292 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0321 21:41:56.860566    2292 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0321 21:42:02.144577    2292 kuberuntime_manager.go:1528] "Updating runtime config through cri with podcidr" CIDR="10.42.0.0/24"
I0321 21:42:02.146492    2292 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.42.0.0/24"
I0321 21:42:06.168497    2292 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/coredns-6799fbcd5-g75xc" containerName="coredns"
I0321 21:42:07.012763    2292 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/coredns-6799fbcd5-g75xc" podStartSLOduration=5.085526078 podCreationTimestamp="2024-03-21 21:41:56 +0000 UTC" firstStartedPulling="2024-03-21 21:42:00.231597106 +0000 UTC m=+22.685812840" lastFinishedPulling="2024-03-21 21:42:06.153324552 +0000 UTC m=+28.607540290" observedRunningTime="2024-03-21 21:42:07.005385965 +0000 UTC m=+29.459601717" watchObservedRunningTime="2024-03-21 21:42:07.007253528 +0000 UTC m=+29.461469274"
I0321 21:42:07.116403    2292 event.go:307] "Event occurred" object="kube-system/kube-dns" fieldPath="" kind="Endpoints" apiVersion="v1" type="Warning" reason="FailedToUpdateEndpoint" message="Failed to update endpoint kube-system/kube-dns: Operation cannot be fulfilled on endpoints \"kube-dns\": the object has been modified; please apply your changes to the latest version and try again"
I0321 21:42:07.120929    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="98.471931ms"
I0321 21:42:07.121015    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="48.011s"
I0321 21:42:08.756594    2292 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/local-path-provisioner-6c86858495-xwxcj" containerName="local-path-provisioner"
I0321 21:42:09.048866    2292 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/local-path-provisioner-6c86858495-xwxcj" podStartSLOduration=4.5317116760000005 podCreationTimestamp="2024-03-21 21:41:56 +0000 UTC" firstStartedPulling="2024-03-21 21:42:00.236118793 +0000 UTC m=+22.690334529" lastFinishedPulling="2024-03-21 21:42:08.753236509 +0000 UTC m=+31.207452244" observedRunningTime="2024-03-21 21:42:09.038421644 +0000 UTC m=+31.492637408" watchObservedRunningTime="2024-03-21 21:42:09.048829391 +0000 UTC m=+31.503045137"
I0321 21:42:09.054865    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="11.230304ms"
I0321 21:42:09.054995    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="32.524s"
I0321 21:42:11.373227    2292 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/metrics-server-67c658944b-mgk2k" containerName="metrics-server"
I0321 21:42:12.050848    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="182.582s"
I0321 21:42:12.052246    2292 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/metrics-server-67c658944b-mgk2k" podStartSLOduration=4.926035705 podCreationTimestamp="2024-03-21 21:41:56 +0000 UTC" firstStartedPulling="2024-03-21 21:42:00.245520021 +0000 UTC m=+22.699735756" lastFinishedPulling="2024-03-21 21:42:11.371690334 +0000 UTC m=+33.825906074" observedRunningTime="2024-03-21 21:42:12.047469672 +0000 UTC m=+34.501685426" watchObservedRunningTime="2024-03-21 21:42:12.052206023 +0000 UTC m=+34.506421768"
I0321 21:42:17.060257    2292 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/helm-install-traefik-tv7kv" containerName="helm"
I0321 21:42:17.063658    2292 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/helm-install-traefik-crd-clwp2" containerName="helm"
I0321 21:42:18.141822    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0321 21:42:18.183774    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0321 21:42:18.186952    2292 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/helm-install-traefik-tv7kv" podStartSLOduration=6.390477347 podCreationTimestamp="2024-03-21 21:41:55 +0000 UTC" firstStartedPulling="2024-03-21 21:42:00.250530059 +0000 UTC m=+22.704745801" lastFinishedPulling="2024-03-21 21:42:17.044344402 +0000 UTC m=+39.498560163" observedRunningTime="2024-03-21 21:42:18.149654553 +0000 UTC m=+40.603870297" watchObservedRunningTime="2024-03-21 21:42:18.184291709 +0000 UTC m=+40.638507452"
I0321 21:42:18.187201    2292 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/helm-install-traefik-crd-clwp2" podStartSLOduration=6.315980131 podCreationTimestamp="2024-03-21 21:41:55 +0000 UTC" firstStartedPulling="2024-03-21 21:42:00.188693572 +0000 UTC m=+22.642909311" lastFinishedPulling="2024-03-21 21:42:17.059750264 +0000 UTC m=+39.513966012" observedRunningTime="2024-03-21 21:42:18.184184806 +0000 UTC m=+40.638400550" watchObservedRunningTime="2024-03-21 21:42:18.187036832 +0000 UTC m=+40.641252586"
W0321 21:42:18.328271    2292 aggregator.go:166] failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0321 21:42:19.137512    2292 scope.go:117] "RemoveContainer" containerID="a0463a3c9f7b908dabce046d9fe8191aa5ca26dc5c47379367e20307a1919faf"
I0321 21:42:19.160364    2292 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0321 21:42:19.166826    2292 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/helm-install-traefik-tv7kv" containerName="helm"
I0321 21:42:19.208144    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0321 21:42:19.218675    2292 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0321 21:42:19.226359    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0321 21:42:19.255279    2292 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0321 21:42:19.266175    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0321 21:42:19.270818    2292 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0321 21:42:19.290574    2292 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0321 21:42:19.397969    2292 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0321 21:42:19.474130    2292 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0321 21:42:19.495578    2292 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0321 21:42:19.520120    2292 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0321 21:42:19.536116    2292 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0321 21:42:19.662455    2292 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0321 21:42:19.676216    2292 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0321 21:42:19.742900    2292 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0321 21:42:19.773925    2292 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0321 21:42:19.861877    2292 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0321 21:42:19.950912    2292 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0321 21:42:20.003743    2292 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0321 21:42:20.143384    2292 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0321 21:42:20.164826    2292 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0321 21:42:20.227768    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0321 21:42:20.241300    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0321 21:42:20.256928    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
W0321 21:42:20.497215    2292 aggregator.go:166] failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0321 21:42:21.279598    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0321 21:42:21.282754    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0321 21:42:21.565366    2292 alloc.go:330] "allocated clusterIPs" service="kube-system/traefik" clusterIPs={"IPv4":"10.43.136.152"}
I0321 21:42:21.626034    2292 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Service" apiVersion="v1" type="Normal" reason="EnsuringLoadBalancer" message="Ensuring load balancer"
I0321 21:42:21.688024    2292 controller.go:624] quota admission added evaluator for: ingressroutes.traefik.io
I0321 21:42:21.733342    2292 controller.go:624] quota admission added evaluator for: daemonsets.apps
I0321 21:42:21.744853    2292 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set traefik-f4564c4f4 to 1"
I0321 21:42:21.786499    2292 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Service" apiVersion="v1" type="Normal" reason="AppliedDaemonSet" message="Applied LoadBalancer DaemonSet kube-system/svclb-traefik-3f607be5"
I0321 21:42:21.793151    2292 event.go:307] "Event occurred" object="kube-system/traefik-f4564c4f4" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: traefik-f4564c4f4-w7qph"
I0321 21:42:21.848708    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0321 21:42:21.867437    2292 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/92b9020c-170c-42e6-8f5a-26ff87f8902f-klipper-cache\") pod \"92b9020c-170c-42e6-8f5a-26ff87f8902f\" (UID: \"92b9020c-170c-42e6-8f5a-26ff87f8902f\") "
I0321 21:42:21.867847    2292 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/92b9020c-170c-42e6-8f5a-26ff87f8902f-tmp\") pod \"92b9020c-170c-42e6-8f5a-26ff87f8902f\" (UID: \"92b9020c-170c-42e6-8f5a-26ff87f8902f\") "
I0321 21:42:21.867889    2292 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/92b9020c-170c-42e6-8f5a-26ff87f8902f-klipper-config\") pod \"92b9020c-170c-42e6-8f5a-26ff87f8902f\" (UID: \"92b9020c-170c-42e6-8f5a-26ff87f8902f\") "
I0321 21:42:21.867920    2292 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/92b9020c-170c-42e6-8f5a-26ff87f8902f-klipper-helm\") pod \"92b9020c-170c-42e6-8f5a-26ff87f8902f\" (UID: \"92b9020c-170c-42e6-8f5a-26ff87f8902f\") "
I0321 21:42:21.868160    2292 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-44k2p\" (UniqueName: \"kubernetes.io/projected/92b9020c-170c-42e6-8f5a-26ff87f8902f-kube-api-access-44k2p\") pod \"92b9020c-170c-42e6-8f5a-26ff87f8902f\" (UID: \"92b9020c-170c-42e6-8f5a-26ff87f8902f\") "
I0321 21:42:21.868205    2292 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"content\" (UniqueName: \"kubernetes.io/configmap/92b9020c-170c-42e6-8f5a-26ff87f8902f-content\") pod \"92b9020c-170c-42e6-8f5a-26ff87f8902f\" (UID: \"92b9020c-170c-42e6-8f5a-26ff87f8902f\") "
I0321 21:42:21.868230    2292 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"values\" (UniqueName: \"kubernetes.io/secret/92b9020c-170c-42e6-8f5a-26ff87f8902f-values\") pod \"92b9020c-170c-42e6-8f5a-26ff87f8902f\" (UID: \"92b9020c-170c-42e6-8f5a-26ff87f8902f\") "
I0321 21:42:21.906170    2292 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/92b9020c-170c-42e6-8f5a-26ff87f8902f-tmp" (OuterVolumeSpecName: "tmp") pod "92b9020c-170c-42e6-8f5a-26ff87f8902f" (UID: "92b9020c-170c-42e6-8f5a-26ff87f8902f"). InnerVolumeSpecName "tmp". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0321 21:42:21.907970    2292 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/92b9020c-170c-42e6-8f5a-26ff87f8902f-kube-api-access-44k2p" (OuterVolumeSpecName: "kube-api-access-44k2p") pod "92b9020c-170c-42e6-8f5a-26ff87f8902f" (UID: "92b9020c-170c-42e6-8f5a-26ff87f8902f"). InnerVolumeSpecName "kube-api-access-44k2p". PluginName "kubernetes.io/projected", VolumeGidValue ""
I0321 21:42:21.909740    2292 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/92b9020c-170c-42e6-8f5a-26ff87f8902f-klipper-cache" (OuterVolumeSpecName: "klipper-cache") pod "92b9020c-170c-42e6-8f5a-26ff87f8902f" (UID: "92b9020c-170c-42e6-8f5a-26ff87f8902f"). InnerVolumeSpecName "klipper-cache". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0321 21:42:21.911347    2292 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/92b9020c-170c-42e6-8f5a-26ff87f8902f-values" (OuterVolumeSpecName: "values") pod "92b9020c-170c-42e6-8f5a-26ff87f8902f" (UID: "92b9020c-170c-42e6-8f5a-26ff87f8902f"). InnerVolumeSpecName "values". PluginName "kubernetes.io/secret", VolumeGidValue ""
I0321 21:42:21.927133    2292 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/92b9020c-170c-42e6-8f5a-26ff87f8902f-klipper-helm" (OuterVolumeSpecName: "klipper-helm") pod "92b9020c-170c-42e6-8f5a-26ff87f8902f" (UID: "92b9020c-170c-42e6-8f5a-26ff87f8902f"). InnerVolumeSpecName "klipper-helm". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0321 21:42:21.927628    2292 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/92b9020c-170c-42e6-8f5a-26ff87f8902f-klipper-config" (OuterVolumeSpecName: "klipper-config") pod "92b9020c-170c-42e6-8f5a-26ff87f8902f" (UID: "92b9020c-170c-42e6-8f5a-26ff87f8902f"). InnerVolumeSpecName "klipper-config". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0321 21:42:21.932442    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="189.243939ms"
I0321 21:42:21.948000    2292 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/configmap/92b9020c-170c-42e6-8f5a-26ff87f8902f-content" (OuterVolumeSpecName: "content") pod "92b9020c-170c-42e6-8f5a-26ff87f8902f" (UID: "92b9020c-170c-42e6-8f5a-26ff87f8902f"). InnerVolumeSpecName "content". PluginName "kubernetes.io/configmap", VolumeGidValue ""
I0321 21:42:21.950031    2292 controller.go:624] quota admission added evaluator for: controllerrevisions.apps
I0321 21:42:21.968448    2292 reconciler_common.go:300] "Volume detached for volume \"content\" (UniqueName: \"kubernetes.io/configmap/92b9020c-170c-42e6-8f5a-26ff87f8902f-content\") on node \"server\" DevicePath \"\""
I0321 21:42:21.968478    2292 reconciler_common.go:300] "Volume detached for volume \"values\" (UniqueName: \"kubernetes.io/secret/92b9020c-170c-42e6-8f5a-26ff87f8902f-values\") on node \"server\" DevicePath \"\""
I0321 21:42:21.968490    2292 reconciler_common.go:300] "Volume detached for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/92b9020c-170c-42e6-8f5a-26ff87f8902f-klipper-config\") on node \"server\" DevicePath \"\""
I0321 21:42:21.968498    2292 reconciler_common.go:300] "Volume detached for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/92b9020c-170c-42e6-8f5a-26ff87f8902f-klipper-cache\") on node \"server\" DevicePath \"\""
I0321 21:42:21.968507    2292 reconciler_common.go:300] "Volume detached for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/92b9020c-170c-42e6-8f5a-26ff87f8902f-tmp\") on node \"server\" DevicePath \"\""
I0321 21:42:21.968516    2292 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-44k2p\" (UniqueName: \"kubernetes.io/projected/92b9020c-170c-42e6-8f5a-26ff87f8902f-kube-api-access-44k2p\") on node \"server\" DevicePath \"\""
I0321 21:42:21.968522    2292 reconciler_common.go:300] "Volume detached for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/92b9020c-170c-42e6-8f5a-26ff87f8902f-klipper-helm\") on node \"server\" DevicePath \"\""
I0321 21:42:21.989888    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0321 21:42:21.995231    2292 topology_manager.go:215] "Topology Admit Handler" podUID="351d27d0-512e-46c9-85c5-80f61ef2efea" podNamespace="kube-system" podName="traefik-f4564c4f4-w7qph"
E0321 21:42:21.996852    2292 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="92b9020c-170c-42e6-8f5a-26ff87f8902f" containerName="helm"
I0321 21:42:21.997653    2292 memory_manager.go:346] "RemoveStaleState removing state" podUID="92b9020c-170c-42e6-8f5a-26ff87f8902f" containerName="helm"
I0321 21:42:22.005500    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="59.884893ms"
I0321 21:42:22.006699    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="76.791s"
I0321 21:42:22.038286    2292 event.go:307] "Event occurred" object="kube-system/svclb-traefik-3f607be5" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: svclb-traefik-3f607be5-wgmqq"
I0321 21:42:22.066165    2292 topology_manager.go:215] "Topology Admit Handler" podUID="c9a423fa-516d-474f-8c94-a316e31a1305" podNamespace="kube-system" podName="svclb-traefik-3f607be5-wgmqq"
I0321 21:42:22.069768    2292 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"data\" (UniqueName: \"kubernetes.io/empty-dir/351d27d0-512e-46c9-85c5-80f61ef2efea-data\") pod \"traefik-f4564c4f4-w7qph\" (UID: \"351d27d0-512e-46c9-85c5-80f61ef2efea\") " pod="kube-system/traefik-f4564c4f4-w7qph"
I0321 21:42:22.069805    2292 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-mf7m9\" (UniqueName: \"kubernetes.io/projected/351d27d0-512e-46c9-85c5-80f61ef2efea-kube-api-access-mf7m9\") pod \"traefik-f4564c4f4-w7qph\" (UID: \"351d27d0-512e-46c9-85c5-80f61ef2efea\") " pod="kube-system/traefik-f4564c4f4-w7qph"
I0321 21:42:22.069824    2292 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/351d27d0-512e-46c9-85c5-80f61ef2efea-tmp\") pod \"traefik-f4564c4f4-w7qph\" (UID: \"351d27d0-512e-46c9-85c5-80f61ef2efea\") " pod="kube-system/traefik-f4564c4f4-w7qph"
I0321 21:42:22.072233    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="57.357s"
I0321 21:42:22.286235    2292 scope.go:117] "RemoveContainer" containerID="a0463a3c9f7b908dabce046d9fe8191aa5ca26dc5c47379367e20307a1919faf"
I0321 21:42:22.308324    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0321 21:42:22.321261    2292 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="f624526388d01b2762e1de1bb53ea5f9034a44a3591455c4b6531b0d6fe35e68"
I0321 21:42:22.328294    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0321 21:42:22.338883    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0321 21:42:22.353273    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0321 21:42:22.362152    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0321 21:42:22.362856    2292 event.go:307] "Event occurred" object="kube-system/helm-install-traefik-crd" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="Completed" message="Job completed"
I0321 21:42:23.359261    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0321 21:42:23.399924    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0321 21:42:23.645317    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0321 21:42:23.717221    2292 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/fd4c637f-8761-4cce-a85c-31d1ed40abed-klipper-config\") pod \"fd4c637f-8761-4cce-a85c-31d1ed40abed\" (UID: \"fd4c637f-8761-4cce-a85c-31d1ed40abed\") "
I0321 21:42:23.717544    2292 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/fd4c637f-8761-4cce-a85c-31d1ed40abed-klipper-helm\") pod \"fd4c637f-8761-4cce-a85c-31d1ed40abed\" (UID: \"fd4c637f-8761-4cce-a85c-31d1ed40abed\") "
I0321 21:42:23.717998    2292 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-8n996\" (UniqueName: \"kubernetes.io/projected/fd4c637f-8761-4cce-a85c-31d1ed40abed-kube-api-access-8n996\") pod \"fd4c637f-8761-4cce-a85c-31d1ed40abed\" (UID: \"fd4c637f-8761-4cce-a85c-31d1ed40abed\") "
I0321 21:42:23.718411    2292 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"content\" (UniqueName: \"kubernetes.io/configmap/fd4c637f-8761-4cce-a85c-31d1ed40abed-content\") pod \"fd4c637f-8761-4cce-a85c-31d1ed40abed\" (UID: \"fd4c637f-8761-4cce-a85c-31d1ed40abed\") "
I0321 21:42:23.718755    2292 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"values\" (UniqueName: \"kubernetes.io/secret/fd4c637f-8761-4cce-a85c-31d1ed40abed-values\") pod \"fd4c637f-8761-4cce-a85c-31d1ed40abed\" (UID: \"fd4c637f-8761-4cce-a85c-31d1ed40abed\") "
I0321 21:42:23.719178    2292 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/fd4c637f-8761-4cce-a85c-31d1ed40abed-tmp\") pod \"fd4c637f-8761-4cce-a85c-31d1ed40abed\" (UID: \"fd4c637f-8761-4cce-a85c-31d1ed40abed\") "
I0321 21:42:23.719510    2292 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/fd4c637f-8761-4cce-a85c-31d1ed40abed-klipper-cache\") pod \"fd4c637f-8761-4cce-a85c-31d1ed40abed\" (UID: \"fd4c637f-8761-4cce-a85c-31d1ed40abed\") "
I0321 21:42:23.731490    2292 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/configmap/fd4c637f-8761-4cce-a85c-31d1ed40abed-content" (OuterVolumeSpecName: "content") pod "fd4c637f-8761-4cce-a85c-31d1ed40abed" (UID: "fd4c637f-8761-4cce-a85c-31d1ed40abed"). InnerVolumeSpecName "content". PluginName "kubernetes.io/configmap", VolumeGidValue ""
I0321 21:42:23.732792    2292 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/fd4c637f-8761-4cce-a85c-31d1ed40abed-klipper-config" (OuterVolumeSpecName: "klipper-config") pod "fd4c637f-8761-4cce-a85c-31d1ed40abed" (UID: "fd4c637f-8761-4cce-a85c-31d1ed40abed"). InnerVolumeSpecName "klipper-config". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0321 21:42:23.745098    2292 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/fd4c637f-8761-4cce-a85c-31d1ed40abed-values" (OuterVolumeSpecName: "values") pod "fd4c637f-8761-4cce-a85c-31d1ed40abed" (UID: "fd4c637f-8761-4cce-a85c-31d1ed40abed"). InnerVolumeSpecName "values". PluginName "kubernetes.io/secret", VolumeGidValue ""
I0321 21:42:23.752150    2292 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/fd4c637f-8761-4cce-a85c-31d1ed40abed-kube-api-access-8n996" (OuterVolumeSpecName: "kube-api-access-8n996") pod "fd4c637f-8761-4cce-a85c-31d1ed40abed" (UID: "fd4c637f-8761-4cce-a85c-31d1ed40abed"). InnerVolumeSpecName "kube-api-access-8n996". PluginName "kubernetes.io/projected", VolumeGidValue ""
I0321 21:42:23.753647    2292 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/fd4c637f-8761-4cce-a85c-31d1ed40abed-klipper-helm" (OuterVolumeSpecName: "klipper-helm") pod "fd4c637f-8761-4cce-a85c-31d1ed40abed" (UID: "fd4c637f-8761-4cce-a85c-31d1ed40abed"). InnerVolumeSpecName "klipper-helm". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0321 21:42:23.754524    2292 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/fd4c637f-8761-4cce-a85c-31d1ed40abed-tmp" (OuterVolumeSpecName: "tmp") pod "fd4c637f-8761-4cce-a85c-31d1ed40abed" (UID: "fd4c637f-8761-4cce-a85c-31d1ed40abed"). InnerVolumeSpecName "tmp". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0321 21:42:23.757929    2292 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/fd4c637f-8761-4cce-a85c-31d1ed40abed-klipper-cache" (OuterVolumeSpecName: "klipper-cache") pod "fd4c637f-8761-4cce-a85c-31d1ed40abed" (UID: "fd4c637f-8761-4cce-a85c-31d1ed40abed"). InnerVolumeSpecName "klipper-cache". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0321 21:42:23.824013    2292 reconciler_common.go:300] "Volume detached for volume \"content\" (UniqueName: \"kubernetes.io/configmap/fd4c637f-8761-4cce-a85c-31d1ed40abed-content\") on node \"server\" DevicePath \"\""
I0321 21:42:23.824094    2292 reconciler_common.go:300] "Volume detached for volume \"values\" (UniqueName: \"kubernetes.io/secret/fd4c637f-8761-4cce-a85c-31d1ed40abed-values\") on node \"server\" DevicePath \"\""
I0321 21:42:23.824164    2292 reconciler_common.go:300] "Volume detached for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/fd4c637f-8761-4cce-a85c-31d1ed40abed-tmp\") on node \"server\" DevicePath \"\""
I0321 21:42:23.824186    2292 reconciler_common.go:300] "Volume detached for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/fd4c637f-8761-4cce-a85c-31d1ed40abed-klipper-cache\") on node \"server\" DevicePath \"\""
I0321 21:42:23.824214    2292 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-8n996\" (UniqueName: \"kubernetes.io/projected/fd4c637f-8761-4cce-a85c-31d1ed40abed-kube-api-access-8n996\") on node \"server\" DevicePath \"\""
I0321 21:42:23.824249    2292 reconciler_common.go:300] "Volume detached for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/fd4c637f-8761-4cce-a85c-31d1ed40abed-klipper-config\") on node \"server\" DevicePath \"\""
I0321 21:42:23.824274    2292 reconciler_common.go:300] "Volume detached for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/fd4c637f-8761-4cce-a85c-31d1ed40abed-klipper-helm\") on node \"server\" DevicePath \"\""
I0321 21:42:24.344083    2292 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="84fcaf4c83b6cc38ff2c36aeeb3ce62c6e6b04fa06c0a1a8b2fcea3dd00d4d80"
I0321 21:42:24.371275    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0321 21:42:24.379138    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0321 21:42:24.386599    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0321 21:42:24.398029    2292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0321 21:42:24.398599    2292 event.go:307] "Event occurred" object="kube-system/helm-install-traefik" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="Completed" message="Job completed"
E0321 21:42:25.377680    2292 resource_quota_controller.go:440] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I0321 21:42:25.390201    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressroutes.traefik.io"
I0321 21:42:25.390469    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressrouteudps.traefik.io"
I0321 21:42:25.390616    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="tlsstores.traefik.containo.us"
I0321 21:42:25.390716    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="middlewares.traefik.io"
I0321 21:42:25.390868    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="tlsoptions.traefik.containo.us"
I0321 21:42:25.390975    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressroutetcps.traefik.io"
I0321 21:42:25.391165    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="traefikservices.traefik.containo.us"
I0321 21:42:25.391245    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressroutes.traefik.containo.us"
I0321 21:42:25.391315    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="tlsoptions.traefik.io"
I0321 21:42:25.391397    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="middlewaretcps.traefik.containo.us"
I0321 21:42:25.391469    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="traefikservices.traefik.io"
I0321 21:42:25.391688    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="tlsstores.traefik.io"
I0321 21:42:25.391862    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="serverstransports.traefik.io"
I0321 21:42:25.391919    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressroutetcps.traefik.containo.us"
I0321 21:42:25.391987    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="middlewares.traefik.containo.us"
I0321 21:42:25.392145    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="serverstransporttcps.traefik.io"
I0321 21:42:25.392202    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressrouteudps.traefik.containo.us"
I0321 21:42:25.392217    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="middlewaretcps.traefik.io"
I0321 21:42:25.392344    2292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="serverstransports.traefik.containo.us"
I0321 21:42:25.393916    2292 shared_informer.go:311] Waiting for caches to sync for resource quota
I0321 21:42:25.696200    2292 shared_informer.go:318] Caches are synced for resource quota
I0321 21:42:25.760550    2292 garbagecollector.go:816] "failed to discover some groups" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"
I0321 21:42:25.776390    2292 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0321 21:42:25.776764    2292 shared_informer.go:318] Caches are synced for garbage collector
I0321 21:42:26.545634    2292 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/svclb-traefik-3f607be5-wgmqq" containerName="lb-tcp-80"
I0321 21:42:26.756162    2292 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/svclb-traefik-3f607be5-wgmqq" containerName="lb-tcp-443"
I0321 21:42:27.447248    2292 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/svclb-traefik-3f607be5-wgmqq" podStartSLOduration=2.679524984 podCreationTimestamp="2024-03-21 21:42:21 +0000 UTC" firstStartedPulling="2024-03-21 21:42:22.774666326 +0000 UTC m=+45.228882062" lastFinishedPulling="2024-03-21 21:42:26.540151437 +0000 UTC m=+48.994367184" observedRunningTime="2024-03-21 21:42:27.422758161 +0000 UTC m=+49.876973915" watchObservedRunningTime="2024-03-21 21:42:27.445010106 +0000 UTC m=+49.899225852"
I0321 21:42:27.509875    2292 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Service" apiVersion="v1" type="Normal" reason="UpdatedLoadBalancer" message="Updated LoadBalancer with new IPs: [] -> [192.168.56.110]"
I0321 21:42:28.460510    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="28.053542ms"
I0321 21:42:28.465607    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="64.893s"
I0321 21:42:28.634616    2292 handler.go:275] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0321 21:42:30.998367    2292 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/traefik-f4564c4f4-w7qph" containerName="traefik"
I0321 21:42:31.561813    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="5.347568ms"
I0321 21:42:33.508315    2292 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/traefik-f4564c4f4-w7qph" podStartSLOduration=4.213200658 podCreationTimestamp="2024-03-21 21:42:21 +0000 UTC" firstStartedPulling="2024-03-21 21:42:22.667162586 +0000 UTC m=+45.121378326" lastFinishedPulling="2024-03-21 21:42:30.959465208 +0000 UTC m=+53.413680950" observedRunningTime="2024-03-21 21:42:31.547054676 +0000 UTC m=+54.001270429" watchObservedRunningTime="2024-03-21 21:42:33.505503282 +0000 UTC m=+55.959719033"
I0321 21:42:33.566049    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="67.696835ms"
I0321 21:42:33.566415    2292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="131.113s"
time="2024-03-21T21:46:37Z" level=info msg="COMPACT revision 0 has already been compacted"
time="2024-03-21T21:51:37Z" level=info msg="COMPACT revision 0 has already been compacted"
time="2024-03-21T21:56:37Z" level=info msg="COMPACT revision 0 has already been compacted"
time="2024-03-21T22:01:37Z" level=info msg="COMPACT compactRev=0 targetCompactRev=20 currentRev=1020"
time="2024-03-21T22:01:37Z" level=info msg="COMPACT deleted 0 rows from 20 revisions in 10.112721ms - compacted to 20/1020"
time="2024-03-21T22:06:37Z" level=info msg="COMPACT compactRev=20 targetCompactRev=109 currentRev=1109"
time="2024-03-21T22:06:37Z" level=info msg="COMPACT deleted 17 rows from 89 revisions in 10.32892ms - compacted to 109/1109"
E0321 22:07:13.426883    2292 kubelet_node_status.go:701] "Failed to set some node status fields" err="failed to validate nodeIP: node IP: \"192.168.56.110\" not found in the host's network interfaces" node="server"
E0321 22:43:00.811274    2292 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0321 22:43:00.811527    2292 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0321 22:43:00.816639    2292 resource_quota_controller.go:440] failed to discover resources: the server has asked for the client to provide credentials
I0321 22:43:00.820432    2292 garbagecollector.go:818] "failed to discover preferred resources" error="the server has asked for the client to provide credentials"
time="2024-03-21T22:44:11Z" level=info msg="COMPACT compactRev=109 targetCompactRev=198 currentRev=1198"
time="2024-03-21T22:44:11Z" level=info msg="COMPACT deleted 0 rows from 89 revisions in 9.107666ms - compacted to 198/1198"
time="2024-03-21T22:49:11Z" level=info msg="COMPACT compactRev=198 targetCompactRev=289 currentRev=1289"
time="2024-03-21T22:49:11Z" level=info msg="COMPACT deleted 21 rows from 91 revisions in 41.125011ms - compacted to 289/1289"
time="2024-03-22T03:12:21Z" level=info msg="Starting k3s v1.28.7+k3s1 (051b14b2)"
time="2024-03-22T03:12:21Z" level=info msg="Configuring sqlite3 database connection pooling: maxIdleConns=2, maxOpenConns=0, connMaxLifetime=0s"
time="2024-03-22T03:12:21Z" level=info msg="Configuring database table schema and indexes, this may take a moment..."
time="2024-03-22T03:12:21Z" level=info msg="Database tables and indexes are up to date"
time="2024-03-22T03:12:21Z" level=info msg="Kine available at unix://kine.sock"
time="2024-03-22T03:12:21Z" level=info msg="generated self-signed CA certificate CN=k3s-client-ca@1711077141: notBefore=2024-03-22 03:12:21.793451716 +0000 UTC notAfter=2034-03-20 03:12:21.793451716 +0000 UTC"
time="2024-03-22T03:12:21Z" level=info msg="certificate CN=system:admin,O=system:masters signed by CN=k3s-client-ca@1711077141: notBefore=2024-03-22 03:12:21 +0000 UTC notAfter=2025-03-22 03:12:21 +0000 UTC"
time="2024-03-22T03:12:21Z" level=info msg="certificate CN=system:k3s-supervisor,O=system:masters signed by CN=k3s-client-ca@1711077141: notBefore=2024-03-22 03:12:21 +0000 UTC notAfter=2025-03-22 03:12:21 +0000 UTC"
time="2024-03-22T03:12:21Z" level=info msg="certificate CN=system:kube-controller-manager signed by CN=k3s-client-ca@1711077141: notBefore=2024-03-22 03:12:21 +0000 UTC notAfter=2025-03-22 03:12:21 +0000 UTC"
time="2024-03-22T03:12:21Z" level=info msg="certificate CN=system:kube-scheduler signed by CN=k3s-client-ca@1711077141: notBefore=2024-03-22 03:12:21 +0000 UTC notAfter=2025-03-22 03:12:21 +0000 UTC"
time="2024-03-22T03:12:21Z" level=info msg="certificate CN=system:apiserver,O=system:masters signed by CN=k3s-client-ca@1711077141: notBefore=2024-03-22 03:12:21 +0000 UTC notAfter=2025-03-22 03:12:21 +0000 UTC"
time="2024-03-22T03:12:21Z" level=info msg="certificate CN=system:kube-proxy signed by CN=k3s-client-ca@1711077141: notBefore=2024-03-22 03:12:21 +0000 UTC notAfter=2025-03-22 03:12:21 +0000 UTC"
time="2024-03-22T03:12:21Z" level=info msg="certificate CN=system:k3s-controller signed by CN=k3s-client-ca@1711077141: notBefore=2024-03-22 03:12:21 +0000 UTC notAfter=2025-03-22 03:12:21 +0000 UTC"
time="2024-03-22T03:12:21Z" level=info msg="certificate CN=k3s-cloud-controller-manager signed by CN=k3s-client-ca@1711077141: notBefore=2024-03-22 03:12:21 +0000 UTC notAfter=2025-03-22 03:12:21 +0000 UTC"
time="2024-03-22T03:12:21Z" level=info msg="generated self-signed CA certificate CN=k3s-server-ca@1711077141: notBefore=2024-03-22 03:12:21.798723124 +0000 UTC notAfter=2034-03-20 03:12:21.798723124 +0000 UTC"
time="2024-03-22T03:12:21Z" level=info msg="certificate CN=kube-apiserver signed by CN=k3s-server-ca@1711077141: notBefore=2024-03-22 03:12:21 +0000 UTC notAfter=2025-03-22 03:12:21 +0000 UTC"
time="2024-03-22T03:12:21Z" level=info msg="generated self-signed CA certificate CN=k3s-request-header-ca@1711077141: notBefore=2024-03-22 03:12:21.799885902 +0000 UTC notAfter=2034-03-20 03:12:21.799885902 +0000 UTC"
time="2024-03-22T03:12:21Z" level=info msg="certificate CN=system:auth-proxy signed by CN=k3s-request-header-ca@1711077141: notBefore=2024-03-22 03:12:21 +0000 UTC notAfter=2025-03-22 03:12:21 +0000 UTC"
time="2024-03-22T03:12:21Z" level=info msg="generated self-signed CA certificate CN=etcd-server-ca@1711077141: notBefore=2024-03-22 03:12:21.800847641 +0000 UTC notAfter=2034-03-20 03:12:21.800847641 +0000 UTC"
time="2024-03-22T03:12:21Z" level=info msg="certificate CN=etcd-client signed by CN=etcd-server-ca@1711077141: notBefore=2024-03-22 03:12:21 +0000 UTC notAfter=2025-03-22 03:12:21 +0000 UTC"
time="2024-03-22T03:12:21Z" level=info msg="generated self-signed CA certificate CN=etcd-peer-ca@1711077141: notBefore=2024-03-22 03:12:21.801761479 +0000 UTC notAfter=2034-03-20 03:12:21.801761479 +0000 UTC"
time="2024-03-22T03:12:21Z" level=info msg="certificate CN=etcd-peer signed by CN=etcd-peer-ca@1711077141: notBefore=2024-03-22 03:12:21 +0000 UTC notAfter=2025-03-22 03:12:21 +0000 UTC"
time="2024-03-22T03:12:21Z" level=info msg="certificate CN=etcd-server signed by CN=etcd-server-ca@1711077141: notBefore=2024-03-22 03:12:21 +0000 UTC notAfter=2025-03-22 03:12:21 +0000 UTC"
time="2024-03-22T03:12:22Z" level=info msg="Saving cluster bootstrap data to datastore"
time="2024-03-22T03:12:22Z" level=info msg="certificate CN=k3s,O=k3s signed by CN=k3s-server-ca@1711077141: notBefore=2024-03-22 03:12:21 +0000 UTC notAfter=2025-03-22 03:12:22 +0000 UTC"
time="2024-03-22T03:12:22Z" level=warning msg="dynamiclistener [::]:6443: no cached certificate available for preload - deferring certificate load until storage initialization or first client request"
time="2024-03-22T03:12:22Z" level=info msg="Active TLS secret / (ver=) (count 10): map[listener.cattle.io/cn-10.43.0.1:10.43.0.1 listener.cattle.io/cn-127.0.0.1:127.0.0.1 listener.cattle.io/cn-192.168.56.110:192.168.56.110 listener.cattle.io/cn-__1-f16284:::1 listener.cattle.io/cn-kubernetes:kubernetes listener.cattle.io/cn-kubernetes.default:kubernetes.default listener.cattle.io/cn-kubernetes.default.svc:kubernetes.default.svc listener.cattle.io/cn-kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local listener.cattle.io/cn-localhost:localhost listener.cattle.io/cn-server:server listener.cattle.io/fingerprint:SHA1=37D0B95FFB08322E62B0632F2C7F6044EF1070A8]"
time="2024-03-22T03:12:22Z" level=info msg="Running kube-apiserver --advertise-address=192.168.56.110 --advertise-port=6443 --allow-privileged=true --anonymous-auth=false --api-audiences=https://kubernetes.default.svc.cluster.local,k3s --authorization-mode=Node,RBAC --bind-address=127.0.0.1 --cert-dir=/var/lib/rancher/k3s/server/tls/temporary-certs --client-ca-file=/var/lib/rancher/k3s/server/tls/client-ca.crt --egress-selector-config-file=/var/lib/rancher/k3s/server/etc/egress-selector-config.yaml --enable-admission-plugins=NodeRestriction --enable-aggregator-routing=true --enable-bootstrap-token-auth=true --etcd-servers=unix://kine.sock --kubelet-certificate-authority=/var/lib/rancher/k3s/server/tls/server-ca.crt --kubelet-client-certificate=/var/lib/rancher/k3s/server/tls/client-kube-apiserver.crt --kubelet-client-key=/var/lib/rancher/k3s/server/tls/client-kube-apiserver.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --profiling=false --proxy-client-cert-file=/var/lib/rancher/k3s/server/tls/client-auth-proxy.crt --proxy-client-key-file=/var/lib/rancher/k3s/server/tls/client-auth-proxy.key --requestheader-allowed-names=system:auth-proxy --requestheader-client-ca-file=/var/lib/rancher/k3s/server/tls/request-header-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6444 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/var/lib/rancher/k3s/server/tls/service.key --service-account-signing-key-file=/var/lib/rancher/k3s/server/tls/service.current.key --service-cluster-ip-range=10.43.0.0/16 --service-node-port-range=30000-32767 --storage-backend=etcd3 --tls-cert-file=/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.crt --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 --tls-private-key-file=/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.key"
time="2024-03-22T03:12:22Z" level=info msg="Running kube-scheduler --authentication-kubeconfig=/var/lib/rancher/k3s/server/cred/scheduler.kubeconfig --authorization-kubeconfig=/var/lib/rancher/k3s/server/cred/scheduler.kubeconfig --bind-address=127.0.0.1 --kubeconfig=/var/lib/rancher/k3s/server/cred/scheduler.kubeconfig --leader-elect=false --profiling=false --secure-port=10259"
I0322 03:12:22.319550    2293 options.go:220] external host was not specified, using 192.168.56.110
time="2024-03-22T03:12:22Z" level=info msg="Running kube-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/var/lib/rancher/k3s/server/cred/controller.kubeconfig --authorization-kubeconfig=/var/lib/rancher/k3s/server/cred/controller.kubeconfig --bind-address=127.0.0.1 --cluster-cidr=10.42.0.0/16 --cluster-signing-kube-apiserver-client-cert-file=/var/lib/rancher/k3s/server/tls/client-ca.nochain.crt --cluster-signing-kube-apiserver-client-key-file=/var/lib/rancher/k3s/server/tls/client-ca.key --cluster-signing-kubelet-client-cert-file=/var/lib/rancher/k3s/server/tls/client-ca.nochain.crt --cluster-signing-kubelet-client-key-file=/var/lib/rancher/k3s/server/tls/client-ca.key --cluster-signing-kubelet-serving-cert-file=/var/lib/rancher/k3s/server/tls/server-ca.nochain.crt --cluster-signing-kubelet-serving-key-file=/var/lib/rancher/k3s/server/tls/server-ca.key --cluster-signing-legacy-unknown-cert-file=/var/lib/rancher/k3s/server/tls/server-ca.nochain.crt --cluster-signing-legacy-unknown-key-file=/var/lib/rancher/k3s/server/tls/server-ca.key --configure-cloud-routes=false --controllers=*,tokencleaner,-service,-route,-cloud-node-lifecycle --kubeconfig=/var/lib/rancher/k3s/server/cred/controller.kubeconfig --leader-elect=false --profiling=false --root-ca-file=/var/lib/rancher/k3s/server/tls/server-ca.crt --secure-port=10257 --service-account-private-key-file=/var/lib/rancher/k3s/server/tls/service.current.key --service-cluster-ip-range=10.43.0.0/16 --use-service-account-credentials=true"
time="2024-03-22T03:12:22Z" level=info msg="Running cloud-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/var/lib/rancher/k3s/server/cred/cloud-controller.kubeconfig --authorization-kubeconfig=/var/lib/rancher/k3s/server/cred/cloud-controller.kubeconfig --bind-address=127.0.0.1 --cloud-config=/var/lib/rancher/k3s/server/etc/cloud-config.yaml --cloud-provider=k3s --cluster-cidr=10.42.0.0/16 --configure-cloud-routes=false --controllers=*,-route --feature-gates=CloudDualStackNodeIPs=true --kubeconfig=/var/lib/rancher/k3s/server/cred/cloud-controller.kubeconfig --leader-elect=false --leader-elect-resource-name=k3s-cloud-controller-manager --node-status-update-frequency=1m0s --profiling=false"
I0322 03:12:22.320699    2293 server.go:156] Version: v1.28.7+k3s1
I0322 03:12:22.320739    2293 server.go:158] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
time="2024-03-22T03:12:22Z" level=info msg="Server node token is available at /var/lib/rancher/k3s/server/token"
time="2024-03-22T03:12:22Z" level=info msg="To join server node to cluster: k3s server -s https://10.0.2.15:6443 -t ${SERVER_NODE_TOKEN}"
time="2024-03-22T03:12:22Z" level=info msg="Agent node token is available at /var/lib/rancher/k3s/server/agent-token"
time="2024-03-22T03:12:22Z" level=info msg="To join agent node to cluster: k3s agent -s https://10.0.2.15:6443 -t ${AGENT_NODE_TOKEN}"
time="2024-03-22T03:12:22Z" level=info msg="Wrote kubeconfig /etc/rancher/k3s/k3s.yaml"
time="2024-03-22T03:12:22Z" level=info msg="Run: k3s kubectl"
time="2024-03-22T03:12:22Z" level=info msg="Waiting for API server to become available"
time="2024-03-22T03:12:22Z" level=info msg="Password verified locally for node server"
time="2024-03-22T03:12:22Z" level=info msg="certificate CN=server signed by CN=k3s-server-ca@1711077141: notBefore=2024-03-22 03:12:21 +0000 UTC notAfter=2025-03-22 03:12:22 +0000 UTC"
time="2024-03-22T03:12:23Z" level=info msg="certificate CN=system:node:server,O=system:nodes signed by CN=k3s-client-ca@1711077141: notBefore=2024-03-22 03:12:21 +0000 UTC notAfter=2025-03-22 03:12:23 +0000 UTC"
I0322 03:12:23.292717    2293 shared_informer.go:311] Waiting for caches to sync for node_authorizer
I0322 03:12:23.293636    2293 plugins.go:158] Loaded 12 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,TaintNodesByCondition,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,MutatingAdmissionWebhook.
I0322 03:12:23.293651    2293 plugins.go:161] Loaded 13 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,ClusterTrustBundleAttest,CertificateSubjectRestriction,ValidatingAdmissionPolicy,ValidatingAdmissionWebhook,ResourceQuota.
I0322 03:12:23.294172    2293 instance.go:298] Using reconciler: lease
I0322 03:12:23.300487    2293 handler.go:275] Adding GroupVersion apiextensions.k8s.io v1 to ResourceManager
W0322 03:12:23.300508    2293 genericapiserver.go:744] Skipping API apiextensions.k8s.io/v1beta1 because it has no resources.
time="2024-03-22T03:12:23Z" level=info msg="Module overlay was already loaded"
time="2024-03-22T03:12:23Z" level=info msg="Module br_netfilter was already loaded"
time="2024-03-22T03:12:23Z" level=info msg="Set sysctl 'net/ipv4/conf/all/forwarding' to 1"
time="2024-03-22T03:12:23Z" level=info msg="Set sysctl 'net/netfilter/nf_conntrack_max' to 131072"
time="2024-03-22T03:12:23Z" level=info msg="Set sysctl 'net/netfilter/nf_conntrack_tcp_timeout_established' to 86400"
time="2024-03-22T03:12:23Z" level=info msg="Set sysctl 'net/netfilter/nf_conntrack_tcp_timeout_close_wait' to 3600"
I0322 03:12:23.386750    2293 handler.go:275] Adding GroupVersion  v1 to ResourceManager
I0322 03:12:23.386990    2293 instance.go:709] API group "internal.apiserver.k8s.io" is not enabled, skipping.
time="2024-03-22T03:12:23Z" level=info msg="Logging containerd to /var/lib/rancher/k3s/agent/containerd/containerd.log"
time="2024-03-22T03:12:23Z" level=info msg="Running containerd -c /var/lib/rancher/k3s/agent/etc/containerd/config.toml -a /run/k3s/containerd/containerd.sock --state /run/k3s/containerd --root /var/lib/rancher/k3s/agent/containerd"
I0322 03:12:23.496384    2293 instance.go:709] API group "resource.k8s.io" is not enabled, skipping.
I0322 03:12:23.536426    2293 handler.go:275] Adding GroupVersion authentication.k8s.io v1 to ResourceManager
W0322 03:12:23.536849    2293 genericapiserver.go:744] Skipping API authentication.k8s.io/v1beta1 because it has no resources.
W0322 03:12:23.537353    2293 genericapiserver.go:744] Skipping API authentication.k8s.io/v1alpha1 because it has no resources.
I0322 03:12:23.537934    2293 handler.go:275] Adding GroupVersion authorization.k8s.io v1 to ResourceManager
W0322 03:12:23.537988    2293 genericapiserver.go:744] Skipping API authorization.k8s.io/v1beta1 because it has no resources.
I0322 03:12:23.539046    2293 handler.go:275] Adding GroupVersion autoscaling v2 to ResourceManager
I0322 03:12:23.540708    2293 handler.go:275] Adding GroupVersion autoscaling v1 to ResourceManager
W0322 03:12:23.540759    2293 genericapiserver.go:744] Skipping API autoscaling/v2beta1 because it has no resources.
W0322 03:12:23.540765    2293 genericapiserver.go:744] Skipping API autoscaling/v2beta2 because it has no resources.
I0322 03:12:23.541871    2293 handler.go:275] Adding GroupVersion batch v1 to ResourceManager
W0322 03:12:23.541925    2293 genericapiserver.go:744] Skipping API batch/v1beta1 because it has no resources.
I0322 03:12:23.542762    2293 handler.go:275] Adding GroupVersion certificates.k8s.io v1 to ResourceManager
W0322 03:12:23.542807    2293 genericapiserver.go:744] Skipping API certificates.k8s.io/v1beta1 because it has no resources.
W0322 03:12:23.542812    2293 genericapiserver.go:744] Skipping API certificates.k8s.io/v1alpha1 because it has no resources.
I0322 03:12:23.543347    2293 handler.go:275] Adding GroupVersion coordination.k8s.io v1 to ResourceManager
W0322 03:12:23.543394    2293 genericapiserver.go:744] Skipping API coordination.k8s.io/v1beta1 because it has no resources.
W0322 03:12:23.543564    2293 genericapiserver.go:744] Skipping API discovery.k8s.io/v1beta1 because it has no resources.
I0322 03:12:23.544148    2293 handler.go:275] Adding GroupVersion discovery.k8s.io v1 to ResourceManager
I0322 03:12:23.545295    2293 handler.go:275] Adding GroupVersion networking.k8s.io v1 to ResourceManager
W0322 03:12:23.545361    2293 genericapiserver.go:744] Skipping API networking.k8s.io/v1beta1 because it has no resources.
W0322 03:12:23.545367    2293 genericapiserver.go:744] Skipping API networking.k8s.io/v1alpha1 because it has no resources.
I0322 03:12:23.545857    2293 handler.go:275] Adding GroupVersion node.k8s.io v1 to ResourceManager
W0322 03:12:23.545919    2293 genericapiserver.go:744] Skipping API node.k8s.io/v1beta1 because it has no resources.
W0322 03:12:23.545995    2293 genericapiserver.go:744] Skipping API node.k8s.io/v1alpha1 because it has no resources.
I0322 03:12:23.546732    2293 handler.go:275] Adding GroupVersion policy v1 to ResourceManager
W0322 03:12:23.546843    2293 genericapiserver.go:744] Skipping API policy/v1beta1 because it has no resources.
I0322 03:12:23.549021    2293 handler.go:275] Adding GroupVersion rbac.authorization.k8s.io v1 to ResourceManager
W0322 03:12:23.549671    2293 genericapiserver.go:744] Skipping API rbac.authorization.k8s.io/v1beta1 because it has no resources.
W0322 03:12:23.549728    2293 genericapiserver.go:744] Skipping API rbac.authorization.k8s.io/v1alpha1 because it has no resources.
I0322 03:12:23.550298    2293 handler.go:275] Adding GroupVersion scheduling.k8s.io v1 to ResourceManager
W0322 03:12:23.550578    2293 genericapiserver.go:744] Skipping API scheduling.k8s.io/v1beta1 because it has no resources.
W0322 03:12:23.550626    2293 genericapiserver.go:744] Skipping API scheduling.k8s.io/v1alpha1 because it has no resources.
I0322 03:12:23.552372    2293 handler.go:275] Adding GroupVersion storage.k8s.io v1 to ResourceManager
W0322 03:12:23.552427    2293 genericapiserver.go:744] Skipping API storage.k8s.io/v1beta1 because it has no resources.
W0322 03:12:23.552433    2293 genericapiserver.go:744] Skipping API storage.k8s.io/v1alpha1 because it has no resources.
I0322 03:12:23.553268    2293 handler.go:275] Adding GroupVersion flowcontrol.apiserver.k8s.io v1beta3 to ResourceManager
I0322 03:12:23.554122    2293 handler.go:275] Adding GroupVersion flowcontrol.apiserver.k8s.io v1beta2 to ResourceManager
W0322 03:12:23.554190    2293 genericapiserver.go:744] Skipping API flowcontrol.apiserver.k8s.io/v1beta1 because it has no resources.
W0322 03:12:23.554196    2293 genericapiserver.go:744] Skipping API flowcontrol.apiserver.k8s.io/v1alpha1 because it has no resources.
I0322 03:12:23.558887    2293 handler.go:275] Adding GroupVersion apps v1 to ResourceManager
W0322 03:12:23.558965    2293 genericapiserver.go:744] Skipping API apps/v1beta2 because it has no resources.
W0322 03:12:23.558972    2293 genericapiserver.go:744] Skipping API apps/v1beta1 because it has no resources.
I0322 03:12:23.559984    2293 handler.go:275] Adding GroupVersion admissionregistration.k8s.io v1 to ResourceManager
W0322 03:12:23.560035    2293 genericapiserver.go:744] Skipping API admissionregistration.k8s.io/v1beta1 because it has no resources.
W0322 03:12:23.560043    2293 genericapiserver.go:744] Skipping API admissionregistration.k8s.io/v1alpha1 because it has no resources.
I0322 03:12:23.560669    2293 handler.go:275] Adding GroupVersion events.k8s.io v1 to ResourceManager
W0322 03:12:23.560825    2293 genericapiserver.go:744] Skipping API events.k8s.io/v1beta1 because it has no resources.
I0322 03:12:23.563728    2293 handler.go:275] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
W0322 03:12:23.563844    2293 genericapiserver.go:744] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I0322 03:12:24.029924    2293 secure_serving.go:213] Serving securely on 127.0.0.1:6444
I0322 03:12:24.030339    2293 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/rancher/k3s/server/tls/request-header-ca.crt"
I0322 03:12:24.030575    2293 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.crt::/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.key"
I0322 03:12:24.030807    2293 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0322 03:12:24.031397    2293 apf_controller.go:374] Starting API Priority and Fairness config controller
I0322 03:12:24.031512    2293 controller.go:80] Starting OpenAPI V3 AggregationController
I0322 03:12:24.042118    2293 gc_controller.go:78] Starting apiserver lease garbage collector
I0322 03:12:24.042162    2293 system_namespaces_controller.go:67] Starting system namespaces controller
I0322 03:12:24.042882    2293 controller.go:116] Starting legacy_token_tracking_controller
I0322 03:12:24.042896    2293 shared_informer.go:311] Waiting for caches to sync for configmaps
I0322 03:12:24.043120    2293 customresource_discovery_controller.go:289] Starting DiscoveryController
I0322 03:12:24.043153    2293 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0322 03:12:24.043160    2293 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0322 03:12:24.043446    2293 gc_controller.go:78] Starting apiserver lease garbage collector
I0322 03:12:24.043744    2293 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0322 03:12:24.043791    2293 shared_informer.go:311] Waiting for caches to sync for cluster_authentication_trust_controller
I0322 03:12:24.043808    2293 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/rancher/k3s/server/tls/client-auth-proxy.crt::/var/lib/rancher/k3s/server/tls/client-auth-proxy.key"
I0322 03:12:24.043871    2293 handler_discovery.go:412] Starting ResourceDiscoveryManager
I0322 03:12:24.044114    2293 aggregator.go:164] waiting for initial CRD sync...
I0322 03:12:24.044242    2293 controller.go:78] Starting OpenAPI AggregationController
I0322 03:12:24.044649    2293 available_controller.go:423] Starting AvailableConditionController
I0322 03:12:24.044722    2293 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0322 03:12:24.044985    2293 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/rancher/k3s/server/tls/client-ca.crt"
I0322 03:12:24.045548    2293 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/rancher/k3s/server/tls/client-ca.crt"
I0322 03:12:24.045763    2293 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/rancher/k3s/server/tls/request-header-ca.crt"
I0322 03:12:24.052351    2293 controller.go:134] Starting OpenAPI controller
I0322 03:12:24.052381    2293 controller.go:85] Starting OpenAPI V3 controller
I0322 03:12:24.052393    2293 naming_controller.go:291] Starting NamingConditionController
I0322 03:12:24.052428    2293 establishing_controller.go:76] Starting EstablishingController
I0322 03:12:24.052438    2293 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0322 03:12:24.052446    2293 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0322 03:12:24.052457    2293 crd_finalizer.go:266] Starting CRDFinalizer
I0322 03:12:24.052842    2293 crdregistration_controller.go:111] Starting crd-autoregister controller
I0322 03:12:24.052864    2293 shared_informer.go:311] Waiting for caches to sync for crd-autoregister
I0322 03:12:24.131634    2293 apf_controller.go:379] Running API Priority and Fairness config worker
I0322 03:12:24.131651    2293 apf_controller.go:382] Running API Priority and Fairness periodic rebalancing process
I0322 03:12:24.149370    2293 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0322 03:12:24.150246    2293 shared_informer.go:318] Caches are synced for configmaps
I0322 03:12:24.151029    2293 shared_informer.go:318] Caches are synced for cluster_authentication_trust_controller
I0322 03:12:24.151829    2293 cache.go:39] Caches are synced for AvailableConditionController controller
I0322 03:12:24.154305    2293 shared_informer.go:318] Caches are synced for crd-autoregister
I0322 03:12:24.155112    2293 aggregator.go:166] initial CRD sync complete...
I0322 03:12:24.155163    2293 autoregister_controller.go:141] Starting autoregister controller
I0322 03:12:24.155178    2293 cache.go:32] Waiting for caches to sync for autoregister controller
I0322 03:12:24.155195    2293 cache.go:39] Caches are synced for autoregister controller
I0322 03:12:24.161276    2293 controller.go:624] quota admission added evaluator for: namespaces
E0322 03:12:24.188792    2293 controller.go:146] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
I0322 03:12:24.196059    2293 shared_informer.go:318] Caches are synced for node_authorizer
E0322 03:12:24.216751    2293 controller.go:95] Unable to perform initial Kubernetes service initialization: namespaces "default" not found
I0322 03:12:24.450277    2293 controller.go:624] quota admission added evaluator for: leases.coordination.k8s.io
time="2024-03-22T03:12:24Z" level=info msg="containerd is now running"
time="2024-03-22T03:12:24Z" level=info msg="Running kubelet --address=0.0.0.0 --allowed-unsafe-sysctls=net.ipv4.ip_forward,net.ipv6.conf.all.forwarding --anonymous-auth=false --authentication-token-webhook=true --authorization-mode=Webhook --cgroup-driver=systemd --client-ca-file=/var/lib/rancher/k3s/agent/client-ca.crt --cloud-provider=external --cluster-dns=10.43.0.10 --cluster-domain=cluster.local --container-runtime-endpoint=unix:///run/k3s/containerd/containerd.sock --containerd=/run/k3s/containerd/containerd.sock --eviction-hard=imagefs.available<5%,nodefs.available<5% --eviction-minimum-reclaim=imagefs.available=10%,nodefs.available=10% --fail-swap-on=false --feature-gates=CloudDualStackNodeIPs=true --healthz-bind-address=127.0.0.1 --hostname-override=server --kubeconfig=/var/lib/rancher/k3s/agent/kubelet.kubeconfig --node-ip=192.168.56.110 --node-labels= --pod-infra-container-image=rancher/mirrored-pause:3.6 --pod-manifest-path=/var/lib/rancher/k3s/agent/pod-manifests --read-only-port=0 --resolv-conf=/run/systemd/resolve/resolv.conf --serialize-image-pulls=false --tls-cert-file=/var/lib/rancher/k3s/agent/serving-kubelet.crt --tls-private-key-file=/var/lib/rancher/k3s/agent/serving-kubelet.key"
time="2024-03-22T03:12:24Z" level=info msg="Connecting to proxy" url="wss://127.0.0.1:6443/v1-k3s/connect"
time="2024-03-22T03:12:24Z" level=info msg="Handling backend connection request [server]"
time="2024-03-22T03:12:24Z" level=info msg="Waiting to retrieve kube-proxy configuration; server is not ready: https://127.0.0.1:6443/v1-k3s/readyz: 500 Internal Server Error"
I0322 03:12:25.100312    2293 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0322 03:12:25.105133    2293 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0322 03:12:25.105154    2293 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0322 03:12:25.556469    2293 controller.go:624] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0322 03:12:25.584003    2293 controller.go:624] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0322 03:12:25.630107    2293 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.43.0.1"}
W0322 03:12:25.634486    2293 lease.go:263] Resetting endpoints for master service "kubernetes" to [192.168.56.110]
I0322 03:12:25.635267    2293 controller.go:624] quota admission added evaluator for: endpoints
I0322 03:12:25.638144    2293 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
time="2024-03-22T03:12:26Z" level=info msg="Kube API server is now running"
time="2024-03-22T03:12:26Z" level=info msg="ETCD server is now running"
time="2024-03-22T03:12:26Z" level=info msg="k3s is up and running"
time="2024-03-22T03:12:26Z" level=info msg="Creating k3s-supervisor event broadcaster"
time="2024-03-22T03:12:26Z" level=info msg="Applying CRD addons.k3s.cattle.io"
time="2024-03-22T03:12:26Z" level=info msg="Waiting for cloud-controller-manager privileges to become available"
time="2024-03-22T03:12:26Z" level=info msg="Applying CRD etcdsnapshotfiles.k3s.cattle.io"
I0322 03:12:26.093428    2293 handler.go:275] Adding GroupVersion k3s.cattle.io v1 to ResourceManager
time="2024-03-22T03:12:26Z" level=info msg="Applying CRD helmcharts.helm.cattle.io"
I0322 03:12:26.117155    2293 handler.go:275] Adding GroupVersion k3s.cattle.io v1 to ResourceManager
time="2024-03-22T03:12:26Z" level=info msg="Applying CRD helmchartconfigs.helm.cattle.io"
I0322 03:12:26.140846    2293 handler.go:275] Adding GroupVersion helm.cattle.io v1 to ResourceManager
time="2024-03-22T03:12:26Z" level=info msg="Waiting for CRD helmchartconfigs.helm.cattle.io to become available"
I0322 03:12:26.161819    2293 handler.go:275] Adding GroupVersion helm.cattle.io v1 to ResourceManager
Flag --cloud-provider has been deprecated, will be removed in 1.25 or later, in favor of removing cloud provider code from Kubelet.
Flag --containerd has been deprecated, This is a cadvisor flag that was mistakenly registered with the Kubelet. Due to legacy concerns, it will follow the standard CLI deprecation timeline before being removed.
Flag --pod-infra-container-image has been deprecated, will be removed in a future release. Image garbage collector will get sandbox image information from CRI.
I0322 03:12:26.473724    2293 server.go:202] "--pod-infra-container-image will not be pruned by the image garbage collector in kubelet and should also be set in the remote runtime"
I0322 03:12:26.475589    2293 server.go:462] "Kubelet version" kubeletVersion="v1.28.7+k3s1"
I0322 03:12:26.475606    2293 server.go:464] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0322 03:12:26.480254    2293 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/rancher/k3s/agent/client-ca.crt"
I0322 03:12:26.495194    2293 server.go:720] "--cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /"
I0322 03:12:26.496477    2293 container_manager_linux.go:265] "Container manager verified user specified cgroup-root exists" cgroupRoot=[]
I0322 03:12:26.496847    2293 container_manager_linux.go:270] "Creating Container Manager object based on Node Config" nodeConfig={"RuntimeCgroupsName":"","SystemCgroupsName":"","KubeletCgroupsName":"","KubeletOOMScoreAdj":-999,"ContainerRuntime":"","CgroupsPerQOS":true,"CgroupRoot":"/","CgroupDriver":"systemd","KubeletRootDir":"/var/lib/kubelet","ProtectKernelDefaults":false,"KubeReservedCgroupName":"","SystemReservedCgroupName":"","ReservedSystemCPUs":{},"EnforceNodeAllocatable":{"pods":{}},"KubeReserved":null,"SystemReserved":null,"HardEvictionThresholds":[{"Signal":"imagefs.available","Operator":"LessThan","Value":{"Quantity":null,"Percentage":0.05},"GracePeriod":0,"MinReclaim":null},{"Signal":"nodefs.available","Operator":"LessThan","Value":{"Quantity":null,"Percentage":0.05},"GracePeriod":0,"MinReclaim":null}],"QOSReserved":{},"CPUManagerPolicy":"none","CPUManagerPolicyOptions":null,"TopologyManagerScope":"container","CPUManagerReconcilePeriod":10000000000,"ExperimentalMemoryManagerPolicy":"None","ExperimentalMemoryManagerReservedMemory":null,"PodPidsLimit":-1,"EnforceCPULimits":true,"CPUCFSQuotaPeriod":100000000,"TopologyManagerPolicy":"none","TopologyManagerPolicyOptions":null}
I0322 03:12:26.496874    2293 topology_manager.go:138] "Creating topology manager with none policy"
I0322 03:12:26.496885    2293 container_manager_linux.go:301] "Creating device plugin manager"
I0322 03:12:26.497343    2293 state_mem.go:36] "Initialized new in-memory state store"
I0322 03:12:26.497475    2293 kubelet.go:393] "Attempting to sync node with API server"
I0322 03:12:26.497494    2293 kubelet.go:298] "Adding static pod path" path="/var/lib/rancher/k3s/agent/pod-manifests"
I0322 03:12:26.497532    2293 kubelet.go:309] "Adding apiserver pod source"
I0322 03:12:26.497551    2293 apiserver.go:42] "Waiting for node sync before watching apiserver pods"
I0322 03:12:26.502895    2293 kuberuntime_manager.go:257] "Container runtime initialized" containerRuntime="containerd" version="v1.7.11-k3s2" apiVersion="v1"
W0322 03:12:26.504601    2293 probe.go:268] Flexvolume plugin directory at /usr/libexec/kubernetes/kubelet-plugins/volume/exec/ does not exist. Recreating.
I0322 03:12:26.506030    2293 server.go:1227] "Started kubelet"
I0322 03:12:26.512618    2293 fs_resource_analyzer.go:67] "Starting FS ResourceAnalyzer"
I0322 03:12:26.521886    2293 server.go:162] "Starting to listen" address="0.0.0.0" port=10250
I0322 03:12:26.523476    2293 server.go:462] "Adding debug handlers to kubelet server"
I0322 03:12:26.524556    2293 ratelimit.go:65] "Setting rate limiting for podresources endpoint" qps=100 burstTokens=10
I0322 03:12:26.524732    2293 server.go:233] "Starting to serve the podresources API" endpoint="unix:/var/lib/kubelet/pod-resources/kubelet.sock"
I0322 03:12:26.527974    2293 volume_manager.go:291] "Starting Kubelet Volume Manager"
I0322 03:12:26.529805    2293 desired_state_of_world_populator.go:151] "Desired state populator starts to run"
I0322 03:12:26.530157    2293 reconciler_new.go:29] "Reconciler: start to sync state"
E0322 03:12:26.571704    2293 cri_stats_provider.go:448] "Failed to get the info of the filesystem with mountpoint" err="unable to find data in memory cache" mountpoint="/var/lib/rancher/k3s/agent/containerd/io.containerd.snapshotter.v1.overlayfs"
E0322 03:12:26.571753    2293 kubelet.go:1431] "Image garbage collection failed once. Stats initialization may not have completed yet" err="invalid capacity 0 on image filesystem"
I0322 03:12:26.663951    2293 kubelet_node_status.go:70] "Attempting to register node" node="server"
E0322 03:12:26.665531    2293 nodelease.go:49] "Failed to get node when trying to set owner ref to the node lease" err="nodes \"server\" not found" node="server"
time="2024-03-22T03:12:26Z" level=info msg="Done waiting for CRD helmchartconfigs.helm.cattle.io to become available"
time="2024-03-22T03:12:26Z" level=info msg="Writing static file: /var/lib/rancher/k3s/server/static/charts/traefik-25.0.2+up25.0.0.tgz"
time="2024-03-22T03:12:26Z" level=info msg="Writing static file: /var/lib/rancher/k3s/server/static/charts/traefik-crd-25.0.2+up25.0.0.tgz"
I0322 03:12:26.672917    2293 cpu_manager.go:214] "Starting CPU manager" policy="none"
I0322 03:12:26.672926    2293 cpu_manager.go:215] "Reconciling" reconcilePeriod="10s"
I0322 03:12:26.672939    2293 state_mem.go:36] "Initialized new in-memory state store"
I0322 03:12:26.680900    2293 policy_none.go:49] "None policy: Start"
I0322 03:12:26.687378    2293 kubelet_node_status.go:73] "Successfully registered node" node="server"
I0322 03:12:26.698016    2293 memory_manager.go:169] "Starting memorymanager" policy="None"
I0322 03:12:26.698251    2293 state_mem.go:35] "Initializing new in-memory state store"
I0322 03:12:26.702146    2293 kubelet_network_linux.go:50] "Initialized iptables rules." protocol="IPv4"
time="2024-03-22T03:12:26Z" level=info msg="Failed to get existing traefik HelmChart" error="helmcharts.helm.cattle.io \"traefik\" not found"
time="2024-03-22T03:12:26Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/ccm.yaml"
time="2024-03-22T03:12:26Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/aggregated-metrics-reader.yaml"
time="2024-03-22T03:12:26Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/resource-reader.yaml"
time="2024-03-22T03:12:26Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/metrics-apiservice.yaml"
time="2024-03-22T03:12:26Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-service.yaml"
time="2024-03-22T03:12:26Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/runtimes.yaml"
time="2024-03-22T03:12:26Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/coredns.yaml"
time="2024-03-22T03:12:26Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/local-storage.yaml"
time="2024-03-22T03:12:26Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/auth-delegator.yaml"
time="2024-03-22T03:12:26Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/auth-reader.yaml"
time="2024-03-22T03:12:26Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/rolebindings.yaml"
time="2024-03-22T03:12:26Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/traefik.yaml"
time="2024-03-22T03:12:26Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-deployment.yaml"
time="2024-03-22T03:12:26Z" level=info msg="Starting dynamiclistener CN filter node controller"
time="2024-03-22T03:12:26Z" level=info msg="Tunnel server egress proxy mode: agent"
time="2024-03-22T03:12:26Z" level=info msg="Annotations and labels have been set successfully on node: server"
time="2024-03-22T03:12:26Z" level=info msg="Starting flannel with backend vxlan"
I0322 03:12:26.717628    2293 kubelet_network_linux.go:50] "Initialized iptables rules." protocol="IPv6"
I0322 03:12:26.718154    2293 status_manager.go:217] "Starting to sync pod status with apiserver"
I0322 03:12:26.718220    2293 kubelet.go:2303] "Starting kubelet main sync loop"
E0322 03:12:26.718279    2293 kubelet.go:2327] "Skipping pod synchronization" err="[container runtime status check may not have completed yet, PLEG is not healthy: pleg has yet to be successful]"
I0322 03:12:26.794370    2293 manager.go:471] "Failed to read data from checkpoint" checkpoint="kubelet_internal_checkpoint" err="checkpoint is not found"
I0322 03:12:26.796580    2293 plugin_manager.go:118] "Starting Kubelet Plugin Manager"
I0322 03:12:26.808371    2293 kubelet_node_status.go:493] "Fast updating node status as it just became ready"
I0322 03:12:26.825910    2293 serving.go:355] Generated self-signed cert in-memory
time="2024-03-22T03:12:26Z" level=info msg="Starting k3s.cattle.io/v1, Kind=Addon controller"
time="2024-03-22T03:12:26Z" level=info msg="Creating deploy event broadcaster"
time="2024-03-22T03:12:26Z" level=info msg="Creating helm-controller event broadcaster"
time="2024-03-22T03:12:26Z" level=info msg="Starting /v1, Kind=Node controller"
time="2024-03-22T03:12:26Z" level=warning msg="Unable to fetch coredns config map: configmaps \"coredns\" not found"
time="2024-03-22T03:12:26Z" level=info msg="Cluster dns configmap has been set successfully"
time="2024-03-22T03:12:26Z" level=info msg="Starting helm.cattle.io/v1, Kind=HelmChartConfig controller"
time="2024-03-22T03:12:26Z" level=info msg="Starting helm.cattle.io/v1, Kind=HelmChart controller"
time="2024-03-22T03:12:26Z" level=info msg="Starting rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding controller"
time="2024-03-22T03:12:26Z" level=info msg="Starting batch/v1, Kind=Job controller"
time="2024-03-22T03:12:26Z" level=info msg="Starting /v1, Kind=ConfigMap controller"
time="2024-03-22T03:12:26Z" level=info msg="Starting /v1, Kind=ServiceAccount controller"
time="2024-03-22T03:12:26Z" level=info msg="Starting /v1, Kind=Secret controller"
time="2024-03-22T03:12:27Z" level=warning msg="Unable to fetch coredns config map: configmaps \"coredns\" not found"
I0322 03:12:27.447476    2293 controllermanager.go:189] "Starting" version="v1.28.7+k3s1"
I0322 03:12:27.447538    2293 controllermanager.go:191] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0322 03:12:27.450423    2293 secure_serving.go:213] Serving securely on 127.0.0.1:10257
I0322 03:12:27.450476    2293 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0322 03:12:27.450518    2293 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I0322 03:12:27.450528    2293 shared_informer.go:311] Waiting for caches to sync for RequestHeaderAuthRequestController
I0322 03:12:27.450547    2293 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0322 03:12:27.450553    2293 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0322 03:12:27.450564    2293 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0322 03:12:27.450570    2293 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0322 03:12:27.458045    2293 shared_informer.go:311] Waiting for caches to sync for tokens
I0322 03:12:27.462799    2293 controller.go:624] quota admission added evaluator for: serviceaccounts
I0322 03:12:27.464952    2293 controllermanager.go:642] "Started controller" controller="endpointslice-controller"
I0322 03:12:27.465304    2293 endpointslice_controller.go:264] "Starting endpoint slice controller"
I0322 03:12:27.465360    2293 shared_informer.go:311] Waiting for caches to sync for endpoint_slice
I0322 03:12:27.470733    2293 controllermanager.go:642] "Started controller" controller="job-controller"
I0322 03:12:27.470759    2293 controllermanager.go:607] "Warning: controller is disabled" controller="service-lb-controller"
I0322 03:12:27.470768    2293 controllermanager.go:607] "Warning: controller is disabled" controller="cloud-node-lifecycle-controller"
I0322 03:12:27.471010    2293 job_controller.go:226] "Starting job controller"
I0322 03:12:27.471027    2293 shared_informer.go:311] Waiting for caches to sync for job
I0322 03:12:27.476327    2293 controllermanager.go:642] "Started controller" controller="ttl-after-finished-controller"
I0322 03:12:27.476427    2293 ttlafterfinished_controller.go:109] "Starting TTL after finished controller"
I0322 03:12:27.476439    2293 shared_informer.go:311] Waiting for caches to sync for TTL after finished
I0322 03:12:27.482222    2293 controllermanager.go:642] "Started controller" controller="serviceaccount-controller"
I0322 03:12:27.482255    2293 controllermanager.go:607] "Warning: controller is disabled" controller="bootstrap-signer-controller"
I0322 03:12:27.482441    2293 serviceaccounts_controller.go:111] "Starting service account controller"
I0322 03:12:27.482453    2293 shared_informer.go:311] Waiting for caches to sync for service account
I0322 03:12:27.487748    2293 controllermanager.go:642] "Started controller" controller="clusterrole-aggregation-controller"
I0322 03:12:27.487861    2293 clusterroleaggregation_controller.go:189] "Starting ClusterRoleAggregator controller"
I0322 03:12:27.488089    2293 shared_informer.go:311] Waiting for caches to sync for ClusterRoleAggregator
I0322 03:12:27.493487    2293 controllermanager.go:642] "Started controller" controller="persistentvolume-protection-controller"
I0322 03:12:27.493748    2293 pv_protection_controller.go:78] "Starting PV protection controller"
I0322 03:12:27.493809    2293 shared_informer.go:311] Waiting for caches to sync for PV protection
I0322 03:12:27.498689    2293 apiserver.go:52] "Watching apiserver"
I0322 03:12:27.499374    2293 controllermanager.go:642] "Started controller" controller="ephemeral-volume-controller"
I0322 03:12:27.499560    2293 controller.go:169] "Starting ephemeral volume controller"
I0322 03:12:27.499577    2293 shared_informer.go:311] Waiting for caches to sync for ephemeral
I0322 03:12:27.505889    2293 controllermanager.go:642] "Started controller" controller="endpoints-controller"
I0322 03:12:27.506140    2293 endpoints_controller.go:174] "Starting endpoint controller"
I0322 03:12:27.506160    2293 shared_informer.go:311] Waiting for caches to sync for endpoint
I0322 03:12:27.519219    2293 controllermanager.go:642] "Started controller" controller="namespace-controller"
time="2024-03-22T03:12:27Z" level=info msg="Updating TLS secret for kube-system/k3s-serving (count: 10): map[listener.cattle.io/cn-10.43.0.1:10.43.0.1 listener.cattle.io/cn-127.0.0.1:127.0.0.1 listener.cattle.io/cn-192.168.56.110:192.168.56.110 listener.cattle.io/cn-__1-f16284:::1 listener.cattle.io/cn-kubernetes:kubernetes listener.cattle.io/cn-kubernetes.default:kubernetes.default listener.cattle.io/cn-kubernetes.default.svc:kubernetes.default.svc listener.cattle.io/cn-kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local listener.cattle.io/cn-localhost:localhost listener.cattle.io/cn-server:server listener.cattle.io/fingerprint:SHA1=37D0B95FFB08322E62B0632F2C7F6044EF1070A8]"
I0322 03:12:27.519488    2293 namespace_controller.go:197] "Starting namespace controller"
I0322 03:12:27.521396    2293 shared_informer.go:311] Waiting for caches to sync for namespace
I0322 03:12:27.531794    2293 desired_state_of_world_populator.go:159] "Finished populating initial desired state of world"
I0322 03:12:27.550704    2293 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0322 03:12:27.550719    2293 shared_informer.go:318] Caches are synced for RequestHeaderAuthRequestController
I0322 03:12:27.550734    2293 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0322 03:12:27.562518    2293 shared_informer.go:318] Caches are synced for tokens
I0322 03:12:27.563049    2293 controllermanager.go:642] "Started controller" controller="deployment-controller"
I0322 03:12:27.563197    2293 deployment_controller.go:168] "Starting controller" controller="deployment"
I0322 03:12:27.563287    2293 shared_informer.go:311] Waiting for caches to sync for deployment
time="2024-03-22T03:12:27Z" level=info msg="Updating TLS secret for kube-system/k3s-serving (count: 10): map[listener.cattle.io/cn-10.43.0.1:10.43.0.1 listener.cattle.io/cn-127.0.0.1:127.0.0.1 listener.cattle.io/cn-192.168.56.110:192.168.56.110 listener.cattle.io/cn-__1-f16284:::1 listener.cattle.io/cn-kubernetes:kubernetes listener.cattle.io/cn-kubernetes.default:kubernetes.default listener.cattle.io/cn-kubernetes.default.svc:kubernetes.default.svc listener.cattle.io/cn-kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local listener.cattle.io/cn-localhost:localhost listener.cattle.io/cn-server:server listener.cattle.io/fingerprint:SHA1=37D0B95FFB08322E62B0632F2C7F6044EF1070A8]"
time="2024-03-22T03:12:27Z" level=info msg="Active TLS secret kube-system/k3s-serving (ver=226) (count 10): map[listener.cattle.io/cn-10.43.0.1:10.43.0.1 listener.cattle.io/cn-127.0.0.1:127.0.0.1 listener.cattle.io/cn-192.168.56.110:192.168.56.110 listener.cattle.io/cn-__1-f16284:::1 listener.cattle.io/cn-kubernetes:kubernetes listener.cattle.io/cn-kubernetes.default:kubernetes.default listener.cattle.io/cn-kubernetes.default.svc:kubernetes.default.svc listener.cattle.io/cn-kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local listener.cattle.io/cn-localhost:localhost listener.cattle.io/cn-server:server listener.cattle.io/fingerprint:SHA1=37D0B95FFB08322E62B0632F2C7F6044EF1070A8]"
I0322 03:12:28.577756    2293 event.go:307] "Event occurred" object="server" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodePasswordValidationComplete" message="Deferred node password secret validation complete"
time="2024-03-22T03:12:28Z" level=info msg="Labels and annotations have been set successfully on node: server"
I0322 03:12:28.847026    2293 controller.go:624] quota admission added evaluator for: addons.k3s.cattle.io
I0322 03:12:28.863623    2293 event.go:307] "Event occurred" object="kube-system/ccm" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/ccm.yaml\""
I0322 03:12:28.903362    2293 event.go:307] "Event occurred" object="kube-system/ccm" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/ccm.yaml\""
time="2024-03-22T03:12:28Z" level=warning msg="Unable to fetch coredns config map: configmaps \"coredns\" not found"
I0322 03:12:28.914907    2293 event.go:307] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/coredns.yaml\""
I0322 03:12:28.947465    2293 controller.go:624] quota admission added evaluator for: deployments.apps
I0322 03:12:28.958930    2293 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.43.0.10"}
I0322 03:12:28.959813    2293 event.go:307] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/coredns.yaml\""
I0322 03:12:28.968460    2293 event.go:307] "Event occurred" object="kube-system/local-storage" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/local-storage.yaml\""
I0322 03:12:29.003278    2293 event.go:307] "Event occurred" object="kube-system/local-storage" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/local-storage.yaml\""
I0322 03:12:29.009731    2293 event.go:307] "Event occurred" object="kube-system/aggregated-metrics-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/aggregated-metrics-reader.yaml\""
I0322 03:12:29.018448    2293 event.go:307] "Event occurred" object="kube-system/aggregated-metrics-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/aggregated-metrics-reader.yaml\""
I0322 03:12:29.024715    2293 event.go:307] "Event occurred" object="kube-system/auth-delegator" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/auth-delegator.yaml\""
I0322 03:12:29.032876    2293 event.go:307] "Event occurred" object="kube-system/auth-delegator" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/auth-delegator.yaml\""
I0322 03:12:29.040002    2293 event.go:307] "Event occurred" object="kube-system/auth-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/auth-reader.yaml\""
I0322 03:12:29.043778    2293 event.go:307] "Event occurred" object="kube-system/auth-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/auth-reader.yaml\""
I0322 03:12:29.311472    2293 event.go:307] "Event occurred" object="kube-system/metrics-apiservice" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-apiservice.yaml\""
I0322 03:12:29.319124    2293 event.go:307] "Event occurred" object="kube-system/metrics-apiservice" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-apiservice.yaml\""
I0322 03:12:29.319985    2293 handler.go:275] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
W0322 03:12:29.322604    2293 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:12:29.322765    2293 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
E0322 03:12:29.322845    2293 handler_proxy.go:137] error resolving kube-system/metrics-server: service "metrics-server" not found
I0322 03:12:29.382890    2293 serving.go:355] Generated self-signed cert in-memory
I0322 03:12:29.735169    2293 event.go:307] "Event occurred" object="kube-system/metrics-server-deployment" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-deployment.yaml\""
time="2024-03-22T03:12:29Z" level=info msg="Running kube-proxy --cluster-cidr=10.42.0.0/16 --conntrack-max-per-core=0 --conntrack-tcp-timeout-close-wait=0s --conntrack-tcp-timeout-established=0s --healthz-bind-address=127.0.0.1 --hostname-override=server --kubeconfig=/var/lib/rancher/k3s/agent/kubeproxy.kubeconfig --proxy-mode=iptables"
I0322 03:12:29.856675    2293 node.go:141] Successfully retrieved node IP: 192.168.56.110
I0322 03:12:29.865765    2293 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0322 03:12:29.867220    2293 server_others.go:152] "Using iptables Proxier"
I0322 03:12:29.867240    2293 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0322 03:12:29.867245    2293 server_others.go:438] "Defaulting to no-op detect-local"
I0322 03:12:29.867259    2293 proxier.go:250] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0322 03:12:29.867407    2293 server.go:846] "Version info" version="v1.28.7+k3s1"
I0322 03:12:29.867414    2293 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0322 03:12:29.877278    2293 config.go:188] "Starting service config controller"
I0322 03:12:29.877300    2293 shared_informer.go:311] Waiting for caches to sync for service config
I0322 03:12:29.877319    2293 config.go:97] "Starting endpoint slice config controller"
I0322 03:12:29.877323    2293 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0322 03:12:29.877640    2293 config.go:315] "Starting node config controller"
I0322 03:12:29.877648    2293 shared_informer.go:311] Waiting for caches to sync for node config
I0322 03:12:29.884810    2293 event.go:307] "Event occurred" object="kube-system/metrics-server-deployment" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-deployment.yaml\""
I0322 03:12:29.893380    2293 serving.go:355] Generated self-signed cert in-memory
I0322 03:12:29.977901    2293 shared_informer.go:318] Caches are synced for node config
I0322 03:12:29.978013    2293 shared_informer.go:318] Caches are synced for service config
I0322 03:12:29.978168    2293 shared_informer.go:318] Caches are synced for endpoint slice config
I0322 03:12:30.119303    2293 controllermanager.go:168] Version: v1.28.7+k3s1
I0322 03:12:30.122172    2293 secure_serving.go:213] Serving securely on 127.0.0.1:10258
I0322 03:12:30.122517    2293 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I0322 03:12:30.122526    2293 shared_informer.go:311] Waiting for caches to sync for RequestHeaderAuthRequestController
I0322 03:12:30.122539    2293 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0322 03:12:30.122579    2293 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0322 03:12:30.122584    2293 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0322 03:12:30.122593    2293 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0322 03:12:30.122596    2293 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0322 03:12:30.126351    2293 event.go:307] "Event occurred" object="kube-system/metrics-server-service" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-service.yaml\""
I0322 03:12:30.133001    2293 alloc.go:330] "allocated clusterIPs" service="kube-system/metrics-server" clusterIPs={"IPv4":"10.43.145.214"}
E0322 03:12:30.133291    2293 controllermanager.go:524] unable to get all supported resources from server: unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
time="2024-03-22T03:12:30Z" level=info msg="Creating  event broadcaster"
W0322 03:12:30.137155    2293 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:12:30.137209    2293 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
E0322 03:12:30.137287    2293 handler_proxy.go:137] error resolving kube-system/metrics-server: endpoints "metrics-server" not found
I0322 03:12:30.137673    2293 event.go:307] "Event occurred" object="kube-system/metrics-server-service" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-service.yaml\""
I0322 03:12:30.222767    2293 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0322 03:12:30.222863    2293 shared_informer.go:318] Caches are synced for RequestHeaderAuthRequestController
I0322 03:12:30.222875    2293 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
time="2024-03-22T03:12:30Z" level=info msg="Starting /v1, Kind=Node controller"
time="2024-03-22T03:12:30Z" level=info msg="Starting /v1, Kind=Pod controller"
time="2024-03-22T03:12:30Z" level=info msg="Starting apps/v1, Kind=DaemonSet controller"
I0322 03:12:30.283702    2293 controllermanager.go:337] Started "cloud-node-lifecycle-controller"
time="2024-03-22T03:12:30Z" level=info msg="Starting discovery.k8s.io/v1, Kind=EndpointSlice controller"
I0322 03:12:30.284973    2293 controllermanager.go:337] Started "service-lb-controller"
W0322 03:12:30.285075    2293 controllermanager.go:314] "node-route-controller" is disabled
I0322 03:12:30.284139    2293 node_lifecycle_controller.go:113] Sending events to api server
I0322 03:12:30.285782    2293 controllermanager.go:337] Started "cloud-node-controller"
I0322 03:12:30.286899    2293 controller.go:231] Starting service controller
I0322 03:12:30.287004    2293 shared_informer.go:311] Waiting for caches to sync for service
I0322 03:12:30.287191    2293 node_controller.go:165] Sending events to api server.
I0322 03:12:30.287463    2293 node_controller.go:174] Waiting for informer caches to sync
W0322 03:12:30.319964    2293 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:12:30.319988    2293 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I0322 03:12:30.319995    2293 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0322 03:12:30.320021    2293 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:12:30.320058    2293 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 03:12:30.321329    2293 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0322 03:12:30.387230    2293 shared_informer.go:318] Caches are synced for service
I0322 03:12:30.387928    2293 node_controller.go:431] Initializing node server with cloud provider
time="2024-03-22T03:12:30Z" level=info msg="Updated coredns node hosts entry [192.168.56.110 server]"
I0322 03:12:30.401281    2293 node_controller.go:502] Successfully initialized node server with cloud provider
I0322 03:12:30.404293    2293 event.go:307] "Event occurred" object="server" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="Synced" message="Node synced successfully"
I0322 03:12:30.507881    2293 event.go:307] "Event occurred" object="kube-system/resource-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/resource-reader.yaml\""
I0322 03:12:30.520669    2293 event.go:307] "Event occurred" object="kube-system/resource-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/resource-reader.yaml\""
I0322 03:12:30.907254    2293 event.go:307] "Event occurred" object="kube-system/rolebindings" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/rolebindings.yaml\""
I0322 03:12:30.927843    2293 event.go:307] "Event occurred" object="kube-system/rolebindings" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/rolebindings.yaml\""
I0322 03:12:30.955833    2293 serving.go:355] Generated self-signed cert in-memory
I0322 03:12:31.254387    2293 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.7+k3s1"
I0322 03:12:31.254410    2293 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0322 03:12:31.257300    2293 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0322 03:12:31.257343    2293 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I0322 03:12:31.257349    2293 shared_informer.go:311] Waiting for caches to sync for RequestHeaderAuthRequestController
I0322 03:12:31.257358    2293 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0322 03:12:31.258522    2293 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0322 03:12:31.258529    2293 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0322 03:12:31.258539    2293 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0322 03:12:31.258542    2293 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0322 03:12:31.308716    2293 event.go:307] "Event occurred" object="kube-system/runtimes" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/runtimes.yaml\""
I0322 03:12:31.357526    2293 shared_informer.go:318] Caches are synced for RequestHeaderAuthRequestController
I0322 03:12:31.358755    2293 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0322 03:12:31.358975    2293 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0322 03:12:31.520752    2293 event.go:307] "Event occurred" object="kube-system/runtimes" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/runtimes.yaml\""
time="2024-03-22T03:12:31Z" level=info msg="Tunnel authorizer set Kubelet Port 10250"
I0322 03:12:31.718412    2293 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/traefik.yaml\""
I0322 03:12:31.736027    2293 controller.go:624] quota admission added evaluator for: helmcharts.helm.cattle.io
I0322 03:12:31.754472    2293 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/traefik.yaml\""
I0322 03:12:31.770465    2293 event.go:307] "Event occurred" object="kube-system/traefik-crd" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik-crd"
I0322 03:12:31.770488    2293 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
I0322 03:12:31.789881    2293 controller.go:624] quota admission added evaluator for: jobs.batch
time="2024-03-22T03:12:31Z" level=error msg="error syncing 'kube-system/traefik': handler helm-controller-chart-registration: helmcharts.helm.cattle.io \"traefik\" not found, requeuing"
I0322 03:12:31.801451    2293 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
time="2024-03-22T03:12:31Z" level=error msg="error syncing 'kube-system/traefik-crd': handler helm-controller-chart-registration: helmcharts.helm.cattle.io \"traefik-crd\" not found, requeuing"
I0322 03:12:31.814655    2293 event.go:307] "Event occurred" object="kube-system/traefik-crd" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik-crd"
I0322 03:12:31.818172    2293 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
I0322 03:12:31.824397    2293 event.go:307] "Event occurred" object="kube-system/traefik-crd" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik-crd"
I0322 03:12:31.843076    2293 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
time="2024-03-22T03:12:32Z" level=info msg="Stopped tunnel to 127.0.0.1:6443"
time="2024-03-22T03:12:32Z" level=info msg="Connecting to proxy" url="wss://192.168.56.110:6443/v1-k3s/connect"
time="2024-03-22T03:12:32Z" level=info msg="Proxy done" err="context canceled" url="wss://127.0.0.1:6443/v1-k3s/connect"
time="2024-03-22T03:12:32Z" level=info msg="error in remotedialer server [400]: websocket: close 1006 (abnormal closure): unexpected EOF"
time="2024-03-22T03:12:32Z" level=info msg="Handling backend connection request [server]"
I0322 03:12:37.760371    2293 range_allocator.go:111] "No Secondary Service CIDR provided. Skipping filtering out secondary service addresses"
I0322 03:12:37.760526    2293 controllermanager.go:642] "Started controller" controller="node-ipam-controller"
I0322 03:12:37.761291    2293 node_ipam_controller.go:162] "Starting ipam controller"
I0322 03:12:37.761463    2293 shared_informer.go:311] Waiting for caches to sync for node
I0322 03:12:37.782593    2293 controllermanager.go:642] "Started controller" controller="persistentvolume-binder-controller"
I0322 03:12:37.783016    2293 pv_controller_base.go:319] "Starting persistent volume controller"
I0322 03:12:37.783050    2293 shared_informer.go:311] Waiting for caches to sync for persistent volume
E0322 03:12:37.816325    2293 resource_quota_controller.go:169] initial discovery check failure, continuing and counting on future sync update: unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I0322 03:12:37.817305    2293 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="roles.rbac.authorization.k8s.io"
I0322 03:12:37.817446    2293 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="helmcharts.helm.cattle.io"
I0322 03:12:37.817474    2293 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="endpoints"
I0322 03:12:37.817513    2293 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="horizontalpodautoscalers.autoscaling"
I0322 03:12:37.817540    2293 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="cronjobs.batch"
I0322 03:12:37.817617    2293 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="deployments.apps"
I0322 03:12:37.817662    2293 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="rolebindings.rbac.authorization.k8s.io"
I0322 03:12:37.817932    2293 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="leases.coordination.k8s.io"
I0322 03:12:37.817974    2293 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="podtemplates"
I0322 03:12:37.817996    2293 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="replicasets.apps"
I0322 03:12:37.818030    2293 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="networkpolicies.networking.k8s.io"
I0322 03:12:37.818160    2293 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingresses.networking.k8s.io"
I0322 03:12:37.818198    2293 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="endpointslices.discovery.k8s.io"
I0322 03:12:37.818235    2293 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="addons.k3s.cattle.io"
I0322 03:12:37.818322    2293 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="serviceaccounts"
I0322 03:12:37.818349    2293 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="daemonsets.apps"
I0322 03:12:37.818372    2293 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="controllerrevisions.apps"
I0322 03:12:37.818396    2293 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="jobs.batch"
I0322 03:12:37.818421    2293 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="limitranges"
I0322 03:12:37.818551    2293 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="statefulsets.apps"
I0322 03:12:37.818585    2293 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="poddisruptionbudgets.policy"
I0322 03:12:37.818609    2293 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="csistoragecapacities.storage.k8s.io"
I0322 03:12:37.818632    2293 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="helmchartconfigs.helm.cattle.io"
I0322 03:12:37.818700    2293 controllermanager.go:642] "Started controller" controller="resourcequota-controller"
I0322 03:12:37.818806    2293 resource_quota_controller.go:294] "Starting resource quota controller"
I0322 03:12:37.818890    2293 shared_informer.go:311] Waiting for caches to sync for resource quota
I0322 03:12:37.819029    2293 resource_quota_monitor.go:305] "QuotaMonitor running"
E0322 03:12:37.823308    2293 resource_quota_controller.go:440] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I0322 03:12:37.832454    2293 controllermanager.go:642] "Started controller" controller="horizontal-pod-autoscaler-controller"
I0322 03:12:37.832744    2293 horizontal.go:200] "Starting HPA controller"
I0322 03:12:37.832769    2293 shared_informer.go:311] Waiting for caches to sync for HPA
I0322 03:12:37.838032    2293 controllermanager.go:642] "Started controller" controller="token-cleaner-controller"
I0322 03:12:37.838316    2293 tokencleaner.go:112] "Starting token cleaner controller"
I0322 03:12:37.841641    2293 shared_informer.go:311] Waiting for caches to sync for token_cleaner
I0322 03:12:37.841661    2293 shared_informer.go:318] Caches are synced for token_cleaner
I0322 03:12:37.843758    2293 controllermanager.go:642] "Started controller" controller="persistentvolume-expander-controller"
I0322 03:12:37.844016    2293 expand_controller.go:328] "Starting expand controller"
I0322 03:12:37.844077    2293 shared_informer.go:311] Waiting for caches to sync for expand
I0322 03:12:37.849325    2293 controllermanager.go:642] "Started controller" controller="root-ca-certificate-publisher-controller"
I0322 03:12:37.849683    2293 publisher.go:102] "Starting root CA cert publisher controller"
I0322 03:12:37.849733    2293 shared_informer.go:311] Waiting for caches to sync for crt configmap
I0322 03:12:37.856115    2293 controllermanager.go:642] "Started controller" controller="certificatesigningrequest-signing-controller"
I0322 03:12:37.856414    2293 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/var/lib/rancher/k3s/server/tls/client-ca.nochain.crt::/var/lib/rancher/k3s/server/tls/client-ca.key"
I0322 03:12:37.856272    2293 certificate_controller.go:115] "Starting certificate controller" name="csrsigning-kubelet-serving"
I0322 03:12:37.856652    2293 shared_informer.go:311] Waiting for caches to sync for certificate-csrsigning-kubelet-serving
I0322 03:12:37.856289    2293 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/var/lib/rancher/k3s/server/tls/server-ca.nochain.crt::/var/lib/rancher/k3s/server/tls/server-ca.key"
I0322 03:12:37.856345    2293 certificate_controller.go:115] "Starting certificate controller" name="csrsigning-legacy-unknown"
I0322 03:12:37.863451    2293 shared_informer.go:311] Waiting for caches to sync for certificate-csrsigning-legacy-unknown
I0322 03:12:37.856360    2293 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/var/lib/rancher/k3s/server/tls/server-ca.nochain.crt::/var/lib/rancher/k3s/server/tls/server-ca.key"
I0322 03:12:37.856359    2293 certificate_controller.go:115] "Starting certificate controller" name="csrsigning-kubelet-client"
I0322 03:12:37.864091    2293 shared_informer.go:311] Waiting for caches to sync for certificate-csrsigning-kubelet-client
I0322 03:12:37.856371    2293 certificate_controller.go:115] "Starting certificate controller" name="csrsigning-kube-apiserver-client"
I0322 03:12:37.864406    2293 shared_informer.go:311] Waiting for caches to sync for certificate-csrsigning-kube-apiserver-client
I0322 03:12:37.856385    2293 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/var/lib/rancher/k3s/server/tls/client-ca.nochain.crt::/var/lib/rancher/k3s/server/tls/client-ca.key"
I0322 03:12:37.865290    2293 controllermanager.go:642] "Started controller" controller="endpointslice-mirroring-controller"
I0322 03:12:37.865439    2293 endpointslicemirroring_controller.go:223] "Starting EndpointSliceMirroring controller"
I0322 03:12:37.865452    2293 shared_informer.go:311] Waiting for caches to sync for endpoint_slice_mirroring
I0322 03:12:37.991594    2293 controllermanager.go:642] "Started controller" controller="pod-garbage-collector-controller"
I0322 03:12:37.992176    2293 gc_controller.go:101] "Starting GC controller"
I0322 03:12:37.992649    2293 shared_informer.go:311] Waiting for caches to sync for GC
I0322 03:12:38.148073    2293 controllermanager.go:642] "Started controller" controller="daemonset-controller"
I0322 03:12:38.148506    2293 daemon_controller.go:291] "Starting daemon sets controller"
I0322 03:12:38.148941    2293 shared_informer.go:311] Waiting for caches to sync for daemon sets
I0322 03:12:38.290548    2293 controllermanager.go:642] "Started controller" controller="statefulset-controller"
I0322 03:12:38.291493    2293 stateful_set.go:161] "Starting stateful set controller"
I0322 03:12:38.291597    2293 shared_informer.go:311] Waiting for caches to sync for stateful set
I0322 03:12:38.437315    2293 controllermanager.go:642] "Started controller" controller="cronjob-controller"
I0322 03:12:38.437600    2293 cronjob_controllerv2.go:139] "Starting cronjob controller v2"
I0322 03:12:38.437858    2293 shared_informer.go:311] Waiting for caches to sync for cronjob
I0322 03:12:38.590501    2293 controllermanager.go:642] "Started controller" controller="replicationcontroller-controller"
I0322 03:12:38.590831    2293 replica_set.go:214] "Starting controller" name="replicationcontroller"
I0322 03:12:38.591101    2293 shared_informer.go:311] Waiting for caches to sync for ReplicationController
I0322 03:12:38.789324    2293 controllermanager.go:642] "Started controller" controller="disruption-controller"
I0322 03:12:38.789419    2293 controllermanager.go:607] "Warning: controller is disabled" controller="node-route-controller"
I0322 03:12:38.789647    2293 disruption.go:433] "Sending events to api server."
I0322 03:12:38.789711    2293 disruption.go:444] "Starting disruption controller"
I0322 03:12:38.789721    2293 shared_informer.go:311] Waiting for caches to sync for disruption
I0322 03:12:38.941534    2293 controllermanager.go:642] "Started controller" controller="persistentvolume-attach-detach-controller"
I0322 03:12:38.942625    2293 attach_detach_controller.go:337] "Starting attach detach controller"
I0322 03:12:38.942687    2293 shared_informer.go:311] Waiting for caches to sync for attach detach
I0322 03:12:38.988552    2293 controllermanager.go:642] "Started controller" controller="certificatesigningrequest-approving-controller"
I0322 03:12:38.988706    2293 certificate_controller.go:115] "Starting certificate controller" name="csrapproving"
I0322 03:12:38.988935    2293 shared_informer.go:311] Waiting for caches to sync for certificate-csrapproving
I0322 03:12:39.141963    2293 controllermanager.go:642] "Started controller" controller="ttl-controller"
I0322 03:12:39.142108    2293 ttl_controller.go:124] "Starting TTL controller"
I0322 03:12:39.142129    2293 shared_informer.go:311] Waiting for caches to sync for TTL
I0322 03:12:39.293447    2293 controllermanager.go:642] "Started controller" controller="persistentvolumeclaim-protection-controller"
I0322 03:12:39.293599    2293 pvc_protection_controller.go:102] "Starting PVC protection controller"
I0322 03:12:39.294326    2293 shared_informer.go:311] Waiting for caches to sync for PVC protection
I0322 03:12:39.546037    2293 controllermanager.go:642] "Started controller" controller="garbage-collector-controller"
I0322 03:12:39.546793    2293 garbagecollector.go:155] "Starting controller" controller="garbagecollector"
I0322 03:12:39.547008    2293 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0322 03:12:39.547093    2293 graph_builder.go:294] "Running" component="GraphBuilder"
I0322 03:12:39.770431    2293 garbagecollector.go:816] "failed to discover some groups" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"
I0322 03:12:39.790376    2293 controllermanager.go:642] "Started controller" controller="replicaset-controller"
I0322 03:12:39.790747    2293 replica_set.go:214] "Starting controller" name="replicaset"
I0322 03:12:39.790781    2293 shared_informer.go:311] Waiting for caches to sync for ReplicaSet
I0322 03:12:39.832745    2293 controllermanager.go:642] "Started controller" controller="certificatesigningrequest-cleaner-controller"
I0322 03:12:39.832836    2293 cleaner.go:83] "Starting CSR cleaner controller"
I0322 03:12:39.883599    2293 node_lifecycle_controller.go:431] "Controller will reconcile labels"
I0322 03:12:39.883643    2293 controllermanager.go:642] "Started controller" controller="node-lifecycle-controller"
I0322 03:12:39.884410    2293 node_lifecycle_controller.go:465] "Sending events to api server"
I0322 03:12:39.884436    2293 node_lifecycle_controller.go:476] "Starting node controller"
I0322 03:12:39.884442    2293 shared_informer.go:311] Waiting for caches to sync for taint
I0322 03:12:39.889412    2293 shared_informer.go:311] Waiting for caches to sync for resource quota
I0322 03:12:39.903819    2293 actual_state_of_world.go:547] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"server\" does not exist"
I0322 03:12:39.904278    2293 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:12:39.904334    2293 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:12:39.906191    2293 shared_informer.go:318] Caches are synced for endpoint
I0322 03:12:39.913611    2293 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0322 03:12:39.921842    2293 shared_informer.go:318] Caches are synced for namespace
I0322 03:12:39.933457    2293 shared_informer.go:318] Caches are synced for HPA
I0322 03:12:39.939789    2293 shared_informer.go:318] Caches are synced for cronjob
I0322 03:12:39.942267    2293 shared_informer.go:318] Caches are synced for TTL
I0322 03:12:39.942852    2293 shared_informer.go:318] Caches are synced for attach detach
I0322 03:12:39.944710    2293 shared_informer.go:318] Caches are synced for expand
I0322 03:12:39.949323    2293 shared_informer.go:318] Caches are synced for daemon sets
I0322 03:12:39.950736    2293 shared_informer.go:318] Caches are synced for crt configmap
I0322 03:12:39.957833    2293 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I0322 03:12:39.962595    2293 shared_informer.go:318] Caches are synced for node
I0322 03:12:39.962657    2293 range_allocator.go:174] "Sending events to api server"
I0322 03:12:39.962676    2293 range_allocator.go:178] "Starting range CIDR allocator"
I0322 03:12:39.962680    2293 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I0322 03:12:39.962684    2293 shared_informer.go:318] Caches are synced for cidrallocator
I0322 03:12:39.963551    2293 shared_informer.go:318] Caches are synced for deployment
I0322 03:12:39.963612    2293 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I0322 03:12:39.965144    2293 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0322 03:12:39.965211    2293 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I0322 03:12:39.965411    2293 shared_informer.go:318] Caches are synced for endpoint_slice
I0322 03:12:39.965707    2293 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I0322 03:12:39.967394    2293 range_allocator.go:380] "Set node PodCIDR" node="server" podCIDRs=["10.42.0.0/24"]
time="2024-03-22T03:12:39Z" level=info msg="Flannel found PodCIDR assigned for node server"
time="2024-03-22T03:12:39Z" level=info msg="The interface enp0s3 with ipv4 address 10.0.2.15 will be used by flannel"
I0322 03:12:39.974682    2293 kube.go:139] Waiting 10m0s for node controller to sync
I0322 03:12:39.974725    2293 shared_informer.go:318] Caches are synced for job
I0322 03:12:39.974740    2293 kube.go:461] Starting kube subnet manager
I0322 03:12:39.978874    2293 shared_informer.go:318] Caches are synced for TTL after finished
I0322 03:12:39.982487    2293 shared_informer.go:318] Caches are synced for service account
I0322 03:12:39.983394    2293 shared_informer.go:318] Caches are synced for persistent volume
I0322 03:12:39.988380    2293 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I0322 03:12:39.989064    2293 shared_informer.go:318] Caches are synced for certificate-csrapproving
I0322 03:12:39.990853    2293 shared_informer.go:318] Caches are synced for ReplicaSet
I0322 03:12:39.991145    2293 shared_informer.go:318] Caches are synced for ReplicationController
I0322 03:12:39.991897    2293 shared_informer.go:318] Caches are synced for stateful set
I0322 03:12:39.992791    2293 shared_informer.go:318] Caches are synced for GC
I0322 03:12:39.993880    2293 shared_informer.go:318] Caches are synced for PV protection
I0322 03:12:39.995219    2293 shared_informer.go:318] Caches are synced for PVC protection
W0322 03:12:39.997915    2293 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:12:39.997990    2293 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
E0322 03:12:39.998098    2293 handler_proxy.go:137] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
I0322 03:12:40.000430    2293 shared_informer.go:318] Caches are synced for ephemeral
I0322 03:12:40.084824    2293 shared_informer.go:318] Caches are synced for taint
I0322 03:12:40.085499    2293 node_lifecycle_controller.go:1225] "Initializing eviction metric for zone" zone=""
I0322 03:12:40.086186    2293 node_lifecycle_controller.go:877] "Missing timestamp for Node. Assuming now as a timestamp" node="server"
I0322 03:12:40.086705    2293 node_lifecycle_controller.go:1071] "Controller detected that zone is now in new state" zone="" newState="Normal"
I0322 03:12:40.087151    2293 taint_manager.go:205] "Starting NoExecuteTaintManager"
I0322 03:12:40.087627    2293 taint_manager.go:210] "Sending events to api server"
I0322 03:12:40.089452    2293 event.go:307] "Event occurred" object="server" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node server event: Registered Node server in Controller"
time="2024-03-22T03:12:40Z" level=info msg="Starting network policy controller version v2.0.1, built on 2024-02-29T20:36:21Z, go1.21.7"
I0322 03:12:40.116569    2293 network_policy_controller.go:164] Starting network policy controller
I0322 03:12:40.121057    2293 shared_informer.go:318] Caches are synced for resource quota
I0322 03:12:40.159000    2293 network_policy_controller.go:176] Starting network policy controller full sync goroutine
I0322 03:12:40.189651    2293 shared_informer.go:318] Caches are synced for resource quota
I0322 03:12:40.189871    2293 shared_informer.go:318] Caches are synced for disruption
I0322 03:12:40.488680    2293 controller.go:624] quota admission added evaluator for: replicasets.apps
I0322 03:12:40.494202    2293 event.go:307] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set coredns-6799fbcd5 to 1"
I0322 03:12:40.494242    2293 event.go:307] "Event occurred" object="kube-system/metrics-server" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set metrics-server-67c658944b to 1"
I0322 03:12:40.500718    2293 event.go:307] "Event occurred" object="kube-system/local-path-provisioner" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set local-path-provisioner-6c86858495 to 1"
I0322 03:12:40.513842    2293 shared_informer.go:318] Caches are synced for garbage collector
I0322 03:12:40.547197    2293 shared_informer.go:318] Caches are synced for garbage collector
I0322 03:12:40.547222    2293 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I0322 03:12:40.597120    2293 event.go:307] "Event occurred" object="kube-system/helm-install-traefik" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: helm-install-traefik-2tzqc"
I0322 03:12:40.597145    2293 event.go:307] "Event occurred" object="kube-system/helm-install-traefik-crd" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: helm-install-traefik-crd-4tv2v"
I0322 03:12:40.598232    2293 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:12:40.599032    2293 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:12:40.607972    2293 topology_manager.go:215] "Topology Admit Handler" podUID="f09ce148-b293-48d9-90da-43e367f14e51" podNamespace="kube-system" podName="helm-install-traefik-2tzqc"
I0322 03:12:40.609352    2293 topology_manager.go:215] "Topology Admit Handler" podUID="04bbb547-3b89-4f9d-b436-ff0016510cc5" podNamespace="kube-system" podName="helm-install-traefik-crd-4tv2v"
I0322 03:12:40.616156    2293 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:12:40.616524    2293 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:12:40.616541    2293 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:12:40.616690    2293 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:12:40.644930    2293 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/f09ce148-b293-48d9-90da-43e367f14e51-tmp\") pod \"helm-install-traefik-2tzqc\" (UID: \"f09ce148-b293-48d9-90da-43e367f14e51\") " pod="kube-system/helm-install-traefik-2tzqc"
I0322 03:12:40.644975    2293 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-mc7x8\" (UniqueName: \"kubernetes.io/projected/f09ce148-b293-48d9-90da-43e367f14e51-kube-api-access-mc7x8\") pod \"helm-install-traefik-2tzqc\" (UID: \"f09ce148-b293-48d9-90da-43e367f14e51\") " pod="kube-system/helm-install-traefik-2tzqc"
I0322 03:12:40.644995    2293 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/f09ce148-b293-48d9-90da-43e367f14e51-klipper-cache\") pod \"helm-install-traefik-2tzqc\" (UID: \"f09ce148-b293-48d9-90da-43e367f14e51\") " pod="kube-system/helm-install-traefik-2tzqc"
I0322 03:12:40.645009    2293 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/f09ce148-b293-48d9-90da-43e367f14e51-klipper-config\") pod \"helm-install-traefik-2tzqc\" (UID: \"f09ce148-b293-48d9-90da-43e367f14e51\") " pod="kube-system/helm-install-traefik-2tzqc"
I0322 03:12:40.645024    2293 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"values\" (UniqueName: \"kubernetes.io/secret/04bbb547-3b89-4f9d-b436-ff0016510cc5-values\") pod \"helm-install-traefik-crd-4tv2v\" (UID: \"04bbb547-3b89-4f9d-b436-ff0016510cc5\") " pod="kube-system/helm-install-traefik-crd-4tv2v"
I0322 03:12:40.645038    2293 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"values\" (UniqueName: \"kubernetes.io/secret/f09ce148-b293-48d9-90da-43e367f14e51-values\") pod \"helm-install-traefik-2tzqc\" (UID: \"f09ce148-b293-48d9-90da-43e367f14e51\") " pod="kube-system/helm-install-traefik-2tzqc"
I0322 03:12:40.645056    2293 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/04bbb547-3b89-4f9d-b436-ff0016510cc5-tmp\") pod \"helm-install-traefik-crd-4tv2v\" (UID: \"04bbb547-3b89-4f9d-b436-ff0016510cc5\") " pod="kube-system/helm-install-traefik-crd-4tv2v"
I0322 03:12:40.645074    2293 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-w7x9k\" (UniqueName: \"kubernetes.io/projected/04bbb547-3b89-4f9d-b436-ff0016510cc5-kube-api-access-w7x9k\") pod \"helm-install-traefik-crd-4tv2v\" (UID: \"04bbb547-3b89-4f9d-b436-ff0016510cc5\") " pod="kube-system/helm-install-traefik-crd-4tv2v"
I0322 03:12:40.645091    2293 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/04bbb547-3b89-4f9d-b436-ff0016510cc5-klipper-helm\") pod \"helm-install-traefik-crd-4tv2v\" (UID: \"04bbb547-3b89-4f9d-b436-ff0016510cc5\") " pod="kube-system/helm-install-traefik-crd-4tv2v"
I0322 03:12:40.645108    2293 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"content\" (UniqueName: \"kubernetes.io/configmap/04bbb547-3b89-4f9d-b436-ff0016510cc5-content\") pod \"helm-install-traefik-crd-4tv2v\" (UID: \"04bbb547-3b89-4f9d-b436-ff0016510cc5\") " pod="kube-system/helm-install-traefik-crd-4tv2v"
I0322 03:12:40.645127    2293 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/04bbb547-3b89-4f9d-b436-ff0016510cc5-klipper-cache\") pod \"helm-install-traefik-crd-4tv2v\" (UID: \"04bbb547-3b89-4f9d-b436-ff0016510cc5\") " pod="kube-system/helm-install-traefik-crd-4tv2v"
I0322 03:12:40.645141    2293 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/04bbb547-3b89-4f9d-b436-ff0016510cc5-klipper-config\") pod \"helm-install-traefik-crd-4tv2v\" (UID: \"04bbb547-3b89-4f9d-b436-ff0016510cc5\") " pod="kube-system/helm-install-traefik-crd-4tv2v"
I0322 03:12:40.645159    2293 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/f09ce148-b293-48d9-90da-43e367f14e51-klipper-helm\") pod \"helm-install-traefik-2tzqc\" (UID: \"f09ce148-b293-48d9-90da-43e367f14e51\") " pod="kube-system/helm-install-traefik-2tzqc"
I0322 03:12:40.645172    2293 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"content\" (UniqueName: \"kubernetes.io/configmap/f09ce148-b293-48d9-90da-43e367f14e51-content\") pod \"helm-install-traefik-2tzqc\" (UID: \"f09ce148-b293-48d9-90da-43e367f14e51\") " pod="kube-system/helm-install-traefik-2tzqc"
I0322 03:12:40.657095    2293 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:12:40.676045    2293 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:12:40.791276    2293 event.go:307] "Event occurred" object="kube-system/local-path-provisioner-6c86858495" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: local-path-provisioner-6c86858495-nwtpb"
I0322 03:12:40.797325    2293 topology_manager.go:215] "Topology Admit Handler" podUID="358a9637-3fd0-43dc-b47d-36d91ac868a6" podNamespace="kube-system" podName="local-path-provisioner-6c86858495-nwtpb"
I0322 03:12:40.805437    2293 event.go:307] "Event occurred" object="kube-system/metrics-server-67c658944b" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: metrics-server-67c658944b-mf927"
I0322 03:12:40.806091    2293 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="304.158372ms"
I0322 03:12:40.806569    2293 event.go:307] "Event occurred" object="kube-system/coredns-6799fbcd5" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: coredns-6799fbcd5-s85j7"
I0322 03:12:40.846811    2293 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-kbg2n\" (UniqueName: \"kubernetes.io/projected/358a9637-3fd0-43dc-b47d-36d91ac868a6-kube-api-access-kbg2n\") pod \"local-path-provisioner-6c86858495-nwtpb\" (UID: \"358a9637-3fd0-43dc-b47d-36d91ac868a6\") " pod="kube-system/local-path-provisioner-6c86858495-nwtpb"
I0322 03:12:40.846855    2293 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/358a9637-3fd0-43dc-b47d-36d91ac868a6-config-volume\") pod \"local-path-provisioner-6c86858495-nwtpb\" (UID: \"358a9637-3fd0-43dc-b47d-36d91ac868a6\") " pod="kube-system/local-path-provisioner-6c86858495-nwtpb"
I0322 03:12:40.848003    2293 topology_manager.go:215] "Topology Admit Handler" podUID="99632039-0e05-49ff-8871-e8c6287196cd" podNamespace="kube-system" podName="metrics-server-67c658944b-mf927"
I0322 03:12:40.848193    2293 topology_manager.go:215] "Topology Admit Handler" podUID="fdd956cd-d322-4938-a779-d81b49f29a6e" podNamespace="kube-system" podName="coredns-6799fbcd5-s85j7"
I0322 03:12:40.849917    2293 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="350.162669ms"
I0322 03:12:40.857837    2293 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="356.081515ms"
I0322 03:12:40.905767    2293 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="47.888702ms"
I0322 03:12:40.905860    2293 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="62.838s"
I0322 03:12:40.944086    2293 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="137.92271ms"
I0322 03:12:40.944170    2293 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="52.689s"
I0322 03:12:40.944887    2293 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="37.763s"
I0322 03:12:40.947482    2293 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp-dir\" (UniqueName: \"kubernetes.io/empty-dir/99632039-0e05-49ff-8871-e8c6287196cd-tmp-dir\") pod \"metrics-server-67c658944b-mf927\" (UID: \"99632039-0e05-49ff-8871-e8c6287196cd\") " pod="kube-system/metrics-server-67c658944b-mf927"
I0322 03:12:40.947508    2293 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-87cqb\" (UniqueName: \"kubernetes.io/projected/99632039-0e05-49ff-8871-e8c6287196cd-kube-api-access-87cqb\") pod \"metrics-server-67c658944b-mf927\" (UID: \"99632039-0e05-49ff-8871-e8c6287196cd\") " pod="kube-system/metrics-server-67c658944b-mf927"
I0322 03:12:40.947533    2293 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-x6kjf\" (UniqueName: \"kubernetes.io/projected/fdd956cd-d322-4938-a779-d81b49f29a6e-kube-api-access-x6kjf\") pod \"coredns-6799fbcd5-s85j7\" (UID: \"fdd956cd-d322-4938-a779-d81b49f29a6e\") " pod="kube-system/coredns-6799fbcd5-s85j7"
I0322 03:12:40.947553    2293 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/fdd956cd-d322-4938-a779-d81b49f29a6e-config-volume\") pod \"coredns-6799fbcd5-s85j7\" (UID: \"fdd956cd-d322-4938-a779-d81b49f29a6e\") " pod="kube-system/coredns-6799fbcd5-s85j7"
I0322 03:12:40.947569    2293 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"custom-config-volume\" (UniqueName: \"kubernetes.io/configmap/fdd956cd-d322-4938-a779-d81b49f29a6e-custom-config-volume\") pod \"coredns-6799fbcd5-s85j7\" (UID: \"fdd956cd-d322-4938-a779-d81b49f29a6e\") " pod="kube-system/coredns-6799fbcd5-s85j7"
I0322 03:12:40.948631    2293 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="98.635239ms"
I0322 03:12:40.948755    2293 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="95.825s"
W0322 03:12:40.955525    2293 watcher.go:93] Error while processing event ("/sys/fs/cgroup/devices/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod99632039_0e05_49ff_8871_e8c6287196cd.slice": 0x40000100 == IN_CREATE|IN_ISDIR): open /sys/fs/cgroup/devices/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod99632039_0e05_49ff_8871_e8c6287196cd.slice: no such file or directory
I0322 03:12:40.974981    2293 kube.go:146] Node controller sync successful
I0322 03:12:40.975217    2293 vxlan.go:141] VXLAN config: VNI=1 Port=0 GBP=false Learning=false DirectRouting=false
I0322 03:12:40.982173    2293 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="67.395s"
I0322 03:12:40.989439    2293 kube.go:621] List of node(server) annotations: map[string]string{"alpha.kubernetes.io/provided-node-ip":"192.168.56.110", "k3s.io/hostname":"server", "k3s.io/internal-ip":"192.168.56.110", "k3s.io/node-args":"[\"server\",\"--node-ip\",\"192.168.56.110\",\"--write-kubeconfig-mode\",\"644\",\"--log\",\"/vagrant/logs/server.logs\"]", "k3s.io/node-config-hash":"OEDLXPLCFN65I5K4IWF2M6JMEKD34L47WZJH7YZKLHVRXJTJG6TQ====", "k3s.io/node-env":"{\"K3S_DATA_DIR\":\"/var/lib/rancher/k3s/data/a3b46c0299091b71bfcc617b1e1fec1845c13bdd848584ceb39d2e700e702a4b\"}", "node.alpha.kubernetes.io/ttl":"0", "volumes.kubernetes.io/controller-managed-attach-detach":"true"}
W0322 03:12:40.998813    2293 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:12:40.998880    2293 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 03:12:40.998887    2293 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0322 03:12:40.998921    2293 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:12:40.998933    2293 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I0322 03:12:41.000091    2293 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0322 03:12:41.022145    2293 kube.go:482] Creating the node lease for IPv4. This is the n.Spec.PodCIDRs: [10.42.0.0/24]
time="2024-03-22T03:12:41Z" level=info msg="Wrote flannel subnet file to /run/flannel/subnet.env"
time="2024-03-22T03:12:41Z" level=info msg="Running flannel backend."
I0322 03:12:41.027955    2293 vxlan_network.go:65] watching for new subnet leases
I0322 03:12:41.028441    2293 iptables.go:290] generated 3 rules
I0322 03:12:41.031804    2293 iptables.go:290] generated 7 rules
I0322 03:12:41.051236    2293 iptables.go:283] bootstrap done
I0322 03:12:41.081922    2293 iptables.go:283] bootstrap done
I0322 03:12:47.127244    2293 kuberuntime_manager.go:1528] "Updating runtime config through cri with podcidr" CIDR="10.42.0.0/24"
I0322 03:12:47.132776    2293 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.42.0.0/24"
I0322 03:13:01.966686    2293 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/local-path-provisioner-6c86858495-nwtpb" containerName="local-path-provisioner"
I0322 03:13:02.497860    2293 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/coredns-6799fbcd5-s85j7" containerName="coredns"
I0322 03:13:03.194489    2293 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/coredns-6799fbcd5-s85j7" podStartSLOduration=5.140810904 podCreationTimestamp="2024-03-22 03:12:40 +0000 UTC" firstStartedPulling="2024-03-22 03:12:44.443353566 +0000 UTC m=+22.752090069" lastFinishedPulling="2024-03-22 03:13:02.495993449 +0000 UTC m=+40.804729952" observedRunningTime="2024-03-22 03:13:03.188039514 +0000 UTC m=+41.496776022" watchObservedRunningTime="2024-03-22 03:13:03.193450787 +0000 UTC m=+41.502187310"
I0322 03:13:03.210591    2293 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="64.265s"
I0322 03:13:03.257800    2293 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="39.173696ms"
I0322 03:13:03.257935    2293 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="61.952s"
I0322 03:13:03.284889    2293 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/local-path-provisioner-6c86858495-nwtpb" podStartSLOduration=5.699980203 podCreationTimestamp="2024-03-22 03:12:40 +0000 UTC" firstStartedPulling="2024-03-22 03:12:44.364321483 +0000 UTC m=+22.673057987" lastFinishedPulling="2024-03-22 03:13:01.949173307 +0000 UTC m=+40.257909815" observedRunningTime="2024-03-22 03:13:03.219888313 +0000 UTC m=+41.528624827" watchObservedRunningTime="2024-03-22 03:13:03.284832031 +0000 UTC m=+41.593568539"
I0322 03:13:03.298850    2293 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="13.518634ms"
I0322 03:13:03.298976    2293 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="35.356s"
I0322 03:13:07.667742    2293 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/metrics-server-67c658944b-mf927" containerName="metrics-server"
I0322 03:13:08.214373    2293 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="208.465s"
I0322 03:13:08.222865    2293 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/metrics-server-67c658944b-mf927" podStartSLOduration=4.987742189 podCreationTimestamp="2024-03-22 03:12:40 +0000 UTC" firstStartedPulling="2024-03-22 03:12:44.429577128 +0000 UTC m=+22.738313642" lastFinishedPulling="2024-03-22 03:13:07.664653982 +0000 UTC m=+45.973390499" observedRunningTime="2024-03-22 03:13:08.208323154 +0000 UTC m=+46.517059666" watchObservedRunningTime="2024-03-22 03:13:08.222819046 +0000 UTC m=+46.531555556"
E0322 03:13:10.237231    2293 resource_quota_controller.go:440] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I0322 03:13:10.536983    2293 garbagecollector.go:816] "failed to discover some groups" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"
I0322 03:13:13.168499    2293 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/helm-install-traefik-2tzqc" containerName="helm"
I0322 03:13:13.176077    2293 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/helm-install-traefik-crd-4tv2v" containerName="helm"
I0322 03:13:14.254050    2293 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:13:14.291211    2293 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:13:14.294786    2293 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/helm-install-traefik-crd-4tv2v" podStartSLOduration=5.558273497 podCreationTimestamp="2024-03-22 03:12:40 +0000 UTC" firstStartedPulling="2024-03-22 03:12:44.435906641 +0000 UTC m=+22.744643144" lastFinishedPulling="2024-03-22 03:13:13.169321699 +0000 UTC m=+51.478058197" observedRunningTime="2024-03-22 03:13:14.263177411 +0000 UTC m=+52.571913926" watchObservedRunningTime="2024-03-22 03:13:14.29168855 +0000 UTC m=+52.600425056"
W0322 03:13:14.606456    2293 aggregator.go:166] failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 03:13:15.372385    2293 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:13:15.410733    2293 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:13:15.430183    2293 scope.go:117] "RemoveContainer" containerID="5c21e692fbd243abbb71a28002b9576989757292e4c1bc90b3950acdce39b995"
I0322 03:13:15.437681    2293 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/helm-install-traefik-2tzqc" containerName="helm"
I0322 03:13:15.502205    2293 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:13:15.519237    2293 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0322 03:13:15.670339    2293 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0322 03:13:15.694342    2293 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0322 03:13:15.716257    2293 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0322 03:13:15.759735    2293 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0322 03:13:15.795881    2293 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0322 03:13:15.815456    2293 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0322 03:13:15.837767    2293 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0322 03:13:15.869759    2293 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0322 03:13:15.919624    2293 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0322 03:13:15.933723    2293 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0322 03:13:15.963628    2293 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0322 03:13:15.972293    2293 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0322 03:13:16.030257    2293 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0322 03:13:16.033320    2293 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0322 03:13:16.055461    2293 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0322 03:13:16.070313    2293 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0322 03:13:16.102536    2293 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0322 03:13:16.158751    2293 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0322 03:13:16.419102    2293 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:13:16.448644    2293 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:13:16.449153    2293 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/helm-install-traefik-2tzqc" podStartSLOduration=7.725133292 podCreationTimestamp="2024-03-22 03:12:40 +0000 UTC" firstStartedPulling="2024-03-22 03:12:44.429578063 +0000 UTC m=+22.738314565" lastFinishedPulling="2024-03-22 03:13:13.153562162 +0000 UTC m=+51.462298681" observedRunningTime="2024-03-22 03:13:14.295319535 +0000 UTC m=+52.604056033" watchObservedRunningTime="2024-03-22 03:13:16.449117408 +0000 UTC m=+54.757853925"
I0322 03:13:16.462033    2293 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
W0322 03:13:16.689713    2293 aggregator.go:166] failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 03:13:17.457502    2293 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:13:17.482753    2293 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:13:17.688820    2293 alloc.go:330] "allocated clusterIPs" service="kube-system/traefik" clusterIPs={"IPv4":"10.43.71.231"}
I0322 03:13:17.726927    2293 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Service" apiVersion="v1" type="Normal" reason="EnsuringLoadBalancer" message="Ensuring load balancer"
I0322 03:13:17.812923    2293 controller.go:624] quota admission added evaluator for: ingressroutes.traefik.io
I0322 03:13:17.827221    2293 controller.go:624] quota admission added evaluator for: daemonsets.apps
I0322 03:13:17.846347    2293 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Service" apiVersion="v1" type="Normal" reason="AppliedDaemonSet" message="Applied LoadBalancer DaemonSet kube-system/svclb-traefik-7713e47b"
I0322 03:13:17.866059    2293 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set traefik-f4564c4f4 to 1"
I0322 03:13:17.937984    2293 controller.go:624] quota admission added evaluator for: controllerrevisions.apps
I0322 03:13:17.967483    2293 event.go:307] "Event occurred" object="kube-system/traefik-f4564c4f4" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: traefik-f4564c4f4-z2rtj"
I0322 03:13:18.024135    2293 topology_manager.go:215] "Topology Admit Handler" podUID="629d25b4-5e6d-4826-aa04-631a72672d8c" podNamespace="kube-system" podName="traefik-f4564c4f4-z2rtj"
E0322 03:13:18.028100    2293 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="04bbb547-3b89-4f9d-b436-ff0016510cc5" containerName="helm"
I0322 03:13:18.030274    2293 memory_manager.go:346] "RemoveStaleState removing state" podUID="04bbb547-3b89-4f9d-b436-ff0016510cc5" containerName="helm"
I0322 03:13:18.032135    2293 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="188.922584ms"
I0322 03:13:18.034306    2293 event.go:307] "Event occurred" object="kube-system/svclb-traefik-7713e47b" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: svclb-traefik-7713e47b-7gtl9"
I0322 03:13:18.122317    2293 topology_manager.go:215] "Topology Admit Handler" podUID="e7d8b127-4fea-426b-80db-59ec137c5f12" podNamespace="kube-system" podName="svclb-traefik-7713e47b-7gtl9"
I0322 03:13:18.127472    2293 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:13:18.144397    2293 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="112.224455ms"
I0322 03:13:18.144495    2293 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="65.034s"
I0322 03:13:18.164433    2293 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="63.164s"
I0322 03:13:18.198344    2293 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"values\" (UniqueName: \"kubernetes.io/secret/04bbb547-3b89-4f9d-b436-ff0016510cc5-values\") pod \"04bbb547-3b89-4f9d-b436-ff0016510cc5\" (UID: \"04bbb547-3b89-4f9d-b436-ff0016510cc5\") "
I0322 03:13:18.198391    2293 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/04bbb547-3b89-4f9d-b436-ff0016510cc5-klipper-cache\") pod \"04bbb547-3b89-4f9d-b436-ff0016510cc5\" (UID: \"04bbb547-3b89-4f9d-b436-ff0016510cc5\") "
I0322 03:13:18.198418    2293 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-w7x9k\" (UniqueName: \"kubernetes.io/projected/04bbb547-3b89-4f9d-b436-ff0016510cc5-kube-api-access-w7x9k\") pod \"04bbb547-3b89-4f9d-b436-ff0016510cc5\" (UID: \"04bbb547-3b89-4f9d-b436-ff0016510cc5\") "
I0322 03:13:18.198439    2293 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/04bbb547-3b89-4f9d-b436-ff0016510cc5-tmp\") pod \"04bbb547-3b89-4f9d-b436-ff0016510cc5\" (UID: \"04bbb547-3b89-4f9d-b436-ff0016510cc5\") "
I0322 03:13:18.198460    2293 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/04bbb547-3b89-4f9d-b436-ff0016510cc5-klipper-helm\") pod \"04bbb547-3b89-4f9d-b436-ff0016510cc5\" (UID: \"04bbb547-3b89-4f9d-b436-ff0016510cc5\") "
I0322 03:13:18.198478    2293 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"content\" (UniqueName: \"kubernetes.io/configmap/04bbb547-3b89-4f9d-b436-ff0016510cc5-content\") pod \"04bbb547-3b89-4f9d-b436-ff0016510cc5\" (UID: \"04bbb547-3b89-4f9d-b436-ff0016510cc5\") "
I0322 03:13:18.198498    2293 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/04bbb547-3b89-4f9d-b436-ff0016510cc5-klipper-config\") pod \"04bbb547-3b89-4f9d-b436-ff0016510cc5\" (UID: \"04bbb547-3b89-4f9d-b436-ff0016510cc5\") "
I0322 03:13:18.198822    2293 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-bnp84\" (UniqueName: \"kubernetes.io/projected/629d25b4-5e6d-4826-aa04-631a72672d8c-kube-api-access-bnp84\") pod \"traefik-f4564c4f4-z2rtj\" (UID: \"629d25b4-5e6d-4826-aa04-631a72672d8c\") " pod="kube-system/traefik-f4564c4f4-z2rtj"
I0322 03:13:18.198850    2293 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"data\" (UniqueName: \"kubernetes.io/empty-dir/629d25b4-5e6d-4826-aa04-631a72672d8c-data\") pod \"traefik-f4564c4f4-z2rtj\" (UID: \"629d25b4-5e6d-4826-aa04-631a72672d8c\") " pod="kube-system/traefik-f4564c4f4-z2rtj"
I0322 03:13:18.198865    2293 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/629d25b4-5e6d-4826-aa04-631a72672d8c-tmp\") pod \"traefik-f4564c4f4-z2rtj\" (UID: \"629d25b4-5e6d-4826-aa04-631a72672d8c\") " pod="kube-system/traefik-f4564c4f4-z2rtj"
I0322 03:13:18.220534    2293 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/04bbb547-3b89-4f9d-b436-ff0016510cc5-tmp" (OuterVolumeSpecName: "tmp") pod "04bbb547-3b89-4f9d-b436-ff0016510cc5" (UID: "04bbb547-3b89-4f9d-b436-ff0016510cc5"). InnerVolumeSpecName "tmp". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0322 03:13:18.224643    2293 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/04bbb547-3b89-4f9d-b436-ff0016510cc5-klipper-cache" (OuterVolumeSpecName: "klipper-cache") pod "04bbb547-3b89-4f9d-b436-ff0016510cc5" (UID: "04bbb547-3b89-4f9d-b436-ff0016510cc5"). InnerVolumeSpecName "klipper-cache". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0322 03:13:18.224799    2293 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/04bbb547-3b89-4f9d-b436-ff0016510cc5-kube-api-access-w7x9k" (OuterVolumeSpecName: "kube-api-access-w7x9k") pod "04bbb547-3b89-4f9d-b436-ff0016510cc5" (UID: "04bbb547-3b89-4f9d-b436-ff0016510cc5"). InnerVolumeSpecName "kube-api-access-w7x9k". PluginName "kubernetes.io/projected", VolumeGidValue ""
I0322 03:13:18.225804    2293 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/configmap/04bbb547-3b89-4f9d-b436-ff0016510cc5-content" (OuterVolumeSpecName: "content") pod "04bbb547-3b89-4f9d-b436-ff0016510cc5" (UID: "04bbb547-3b89-4f9d-b436-ff0016510cc5"). InnerVolumeSpecName "content". PluginName "kubernetes.io/configmap", VolumeGidValue ""
I0322 03:13:18.226072    2293 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/04bbb547-3b89-4f9d-b436-ff0016510cc5-klipper-helm" (OuterVolumeSpecName: "klipper-helm") pod "04bbb547-3b89-4f9d-b436-ff0016510cc5" (UID: "04bbb547-3b89-4f9d-b436-ff0016510cc5"). InnerVolumeSpecName "klipper-helm". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0322 03:13:18.226924    2293 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/04bbb547-3b89-4f9d-b436-ff0016510cc5-values" (OuterVolumeSpecName: "values") pod "04bbb547-3b89-4f9d-b436-ff0016510cc5" (UID: "04bbb547-3b89-4f9d-b436-ff0016510cc5"). InnerVolumeSpecName "values". PluginName "kubernetes.io/secret", VolumeGidValue ""
I0322 03:13:18.230286    2293 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/04bbb547-3b89-4f9d-b436-ff0016510cc5-klipper-config" (OuterVolumeSpecName: "klipper-config") pod "04bbb547-3b89-4f9d-b436-ff0016510cc5" (UID: "04bbb547-3b89-4f9d-b436-ff0016510cc5"). InnerVolumeSpecName "klipper-config". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0322 03:13:18.299682    2293 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-w7x9k\" (UniqueName: \"kubernetes.io/projected/04bbb547-3b89-4f9d-b436-ff0016510cc5-kube-api-access-w7x9k\") on node \"server\" DevicePath \"\""
I0322 03:13:18.299746    2293 reconciler_common.go:300] "Volume detached for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/04bbb547-3b89-4f9d-b436-ff0016510cc5-tmp\") on node \"server\" DevicePath \"\""
I0322 03:13:18.299756    2293 reconciler_common.go:300] "Volume detached for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/04bbb547-3b89-4f9d-b436-ff0016510cc5-klipper-helm\") on node \"server\" DevicePath \"\""
I0322 03:13:18.299764    2293 reconciler_common.go:300] "Volume detached for volume \"content\" (UniqueName: \"kubernetes.io/configmap/04bbb547-3b89-4f9d-b436-ff0016510cc5-content\") on node \"server\" DevicePath \"\""
I0322 03:13:18.299773    2293 reconciler_common.go:300] "Volume detached for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/04bbb547-3b89-4f9d-b436-ff0016510cc5-klipper-config\") on node \"server\" DevicePath \"\""
I0322 03:13:18.299781    2293 reconciler_common.go:300] "Volume detached for volume \"values\" (UniqueName: \"kubernetes.io/secret/04bbb547-3b89-4f9d-b436-ff0016510cc5-values\") on node \"server\" DevicePath \"\""
I0322 03:13:18.299788    2293 reconciler_common.go:300] "Volume detached for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/04bbb547-3b89-4f9d-b436-ff0016510cc5-klipper-cache\") on node \"server\" DevicePath \"\""
I0322 03:13:18.470085    2293 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="34d1163db84d96555af61caf0dd727e11519c10a3ae44bf7bf7cfab69acbe942"
I0322 03:13:18.592049    2293 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:13:18.614568    2293 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:13:18.616936    2293 scope.go:117] "RemoveContainer" containerID="5c21e692fbd243abbb71a28002b9576989757292e4c1bc90b3950acdce39b995"
I0322 03:13:18.629048    2293 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:13:18.682988    2293 event.go:307] "Event occurred" object="kube-system/helm-install-traefik-crd" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="Completed" message="Job completed"
I0322 03:13:18.718159    2293 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:13:18.776927    2293 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:13:19.662692    2293 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:13:19.789761    2293 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:13:19.866100    2293 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:13:19.961976    2293 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/f09ce148-b293-48d9-90da-43e367f14e51-klipper-cache\") pod \"f09ce148-b293-48d9-90da-43e367f14e51\" (UID: \"f09ce148-b293-48d9-90da-43e367f14e51\") "
I0322 03:13:19.962116    2293 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"values\" (UniqueName: \"kubernetes.io/secret/f09ce148-b293-48d9-90da-43e367f14e51-values\") pod \"f09ce148-b293-48d9-90da-43e367f14e51\" (UID: \"f09ce148-b293-48d9-90da-43e367f14e51\") "
I0322 03:13:19.962184    2293 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/f09ce148-b293-48d9-90da-43e367f14e51-klipper-helm\") pod \"f09ce148-b293-48d9-90da-43e367f14e51\" (UID: \"f09ce148-b293-48d9-90da-43e367f14e51\") "
I0322 03:13:19.962249    2293 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"content\" (UniqueName: \"kubernetes.io/configmap/f09ce148-b293-48d9-90da-43e367f14e51-content\") pod \"f09ce148-b293-48d9-90da-43e367f14e51\" (UID: \"f09ce148-b293-48d9-90da-43e367f14e51\") "
I0322 03:13:19.962315    2293 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/f09ce148-b293-48d9-90da-43e367f14e51-tmp\") pod \"f09ce148-b293-48d9-90da-43e367f14e51\" (UID: \"f09ce148-b293-48d9-90da-43e367f14e51\") "
I0322 03:13:19.962370    2293 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/f09ce148-b293-48d9-90da-43e367f14e51-klipper-config\") pod \"f09ce148-b293-48d9-90da-43e367f14e51\" (UID: \"f09ce148-b293-48d9-90da-43e367f14e51\") "
I0322 03:13:19.962433    2293 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-mc7x8\" (UniqueName: \"kubernetes.io/projected/f09ce148-b293-48d9-90da-43e367f14e51-kube-api-access-mc7x8\") pod \"f09ce148-b293-48d9-90da-43e367f14e51\" (UID: \"f09ce148-b293-48d9-90da-43e367f14e51\") "
I0322 03:13:19.986054    2293 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/configmap/f09ce148-b293-48d9-90da-43e367f14e51-content" (OuterVolumeSpecName: "content") pod "f09ce148-b293-48d9-90da-43e367f14e51" (UID: "f09ce148-b293-48d9-90da-43e367f14e51"). InnerVolumeSpecName "content". PluginName "kubernetes.io/configmap", VolumeGidValue ""
I0322 03:13:19.995039    2293 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/f09ce148-b293-48d9-90da-43e367f14e51-values" (OuterVolumeSpecName: "values") pod "f09ce148-b293-48d9-90da-43e367f14e51" (UID: "f09ce148-b293-48d9-90da-43e367f14e51"). InnerVolumeSpecName "values". PluginName "kubernetes.io/secret", VolumeGidValue ""
I0322 03:13:19.997093    2293 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/f09ce148-b293-48d9-90da-43e367f14e51-klipper-helm" (OuterVolumeSpecName: "klipper-helm") pod "f09ce148-b293-48d9-90da-43e367f14e51" (UID: "f09ce148-b293-48d9-90da-43e367f14e51"). InnerVolumeSpecName "klipper-helm". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0322 03:13:19.998242    2293 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/f09ce148-b293-48d9-90da-43e367f14e51-klipper-cache" (OuterVolumeSpecName: "klipper-cache") pod "f09ce148-b293-48d9-90da-43e367f14e51" (UID: "f09ce148-b293-48d9-90da-43e367f14e51"). InnerVolumeSpecName "klipper-cache". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0322 03:13:19.999249    2293 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/f09ce148-b293-48d9-90da-43e367f14e51-kube-api-access-mc7x8" (OuterVolumeSpecName: "kube-api-access-mc7x8") pod "f09ce148-b293-48d9-90da-43e367f14e51" (UID: "f09ce148-b293-48d9-90da-43e367f14e51"). InnerVolumeSpecName "kube-api-access-mc7x8". PluginName "kubernetes.io/projected", VolumeGidValue ""
I0322 03:13:20.003232    2293 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/f09ce148-b293-48d9-90da-43e367f14e51-klipper-config" (OuterVolumeSpecName: "klipper-config") pod "f09ce148-b293-48d9-90da-43e367f14e51" (UID: "f09ce148-b293-48d9-90da-43e367f14e51"). InnerVolumeSpecName "klipper-config". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0322 03:13:20.003630    2293 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/f09ce148-b293-48d9-90da-43e367f14e51-tmp" (OuterVolumeSpecName: "tmp") pod "f09ce148-b293-48d9-90da-43e367f14e51" (UID: "f09ce148-b293-48d9-90da-43e367f14e51"). InnerVolumeSpecName "tmp". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0322 03:13:20.071402    2293 reconciler_common.go:300] "Volume detached for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/f09ce148-b293-48d9-90da-43e367f14e51-klipper-helm\") on node \"server\" DevicePath \"\""
I0322 03:13:20.071438    2293 reconciler_common.go:300] "Volume detached for volume \"content\" (UniqueName: \"kubernetes.io/configmap/f09ce148-b293-48d9-90da-43e367f14e51-content\") on node \"server\" DevicePath \"\""
I0322 03:13:20.071448    2293 reconciler_common.go:300] "Volume detached for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/f09ce148-b293-48d9-90da-43e367f14e51-tmp\") on node \"server\" DevicePath \"\""
I0322 03:13:20.071461    2293 reconciler_common.go:300] "Volume detached for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/f09ce148-b293-48d9-90da-43e367f14e51-klipper-cache\") on node \"server\" DevicePath \"\""
I0322 03:13:20.071468    2293 reconciler_common.go:300] "Volume detached for volume \"values\" (UniqueName: \"kubernetes.io/secret/f09ce148-b293-48d9-90da-43e367f14e51-values\") on node \"server\" DevicePath \"\""
I0322 03:13:20.071481    2293 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-mc7x8\" (UniqueName: \"kubernetes.io/projected/f09ce148-b293-48d9-90da-43e367f14e51-kube-api-access-mc7x8\") on node \"server\" DevicePath \"\""
I0322 03:13:20.071488    2293 reconciler_common.go:300] "Volume detached for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/f09ce148-b293-48d9-90da-43e367f14e51-klipper-config\") on node \"server\" DevicePath \"\""
I0322 03:13:20.654081    2293 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="2e346f001ae78d180c6c21a949c8c52562cc8c10a2903f3116489406f1fd093e"
I0322 03:13:20.690062    2293 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:13:20.821186    2293 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:13:20.845344    2293 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:13:20.852731    2293 event.go:307] "Event occurred" object="kube-system/helm-install-traefik" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="Completed" message="Job completed"
I0322 03:13:20.853888    2293 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
E0322 03:13:24.057001    2293 handler_proxy.go:137] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
I0322 03:13:25.342565    2293 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="35.241179ms"
I0322 03:13:25.342698    2293 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="95.545s"
I0322 03:13:25.529578    2293 handler.go:275] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0322 03:13:26.246093    2293 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/svclb-traefik-7713e47b-7gtl9" containerName="lb-tcp-80"
I0322 03:13:26.411832    2293 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/svclb-traefik-7713e47b-7gtl9" containerName="lb-tcp-443"
I0322 03:13:26.756889    2293 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/svclb-traefik-7713e47b-7gtl9" podStartSLOduration=2.267138626 podCreationTimestamp="2024-03-22 03:13:17 +0000 UTC" firstStartedPulling="2024-03-22 03:13:18.75098174 +0000 UTC m=+57.059718253" lastFinishedPulling="2024-03-22 03:13:26.238559188 +0000 UTC m=+64.547295692" observedRunningTime="2024-03-22 03:13:26.739724643 +0000 UTC m=+65.048461156" watchObservedRunningTime="2024-03-22 03:13:26.754716065 +0000 UTC m=+65.063452572"
I0322 03:13:26.793877    2293 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Service" apiVersion="v1" type="Normal" reason="UpdatedLoadBalancer" message="Updated LoadBalancer with new IPs: [] -> [192.168.56.110]"
I0322 03:13:29.829006    2293 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/traefik-f4564c4f4-z2rtj" containerName="traefik"
I0322 03:13:30.865732    2293 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="15.331828ms"
I0322 03:13:38.487332    2293 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/traefik-f4564c4f4-z2rtj" podStartSLOduration=10.47680959 podCreationTimestamp="2024-03-22 03:13:17 +0000 UTC" firstStartedPulling="2024-03-22 03:13:18.812318441 +0000 UTC m=+57.121054939" lastFinishedPulling="2024-03-22 03:13:29.819378834 +0000 UTC m=+68.128115345" observedRunningTime="2024-03-22 03:13:30.87181316 +0000 UTC m=+69.180549673" watchObservedRunningTime="2024-03-22 03:13:38.483869996 +0000 UTC m=+76.792606503"
I0322 03:13:38.504468    2293 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="17.013874ms"
I0322 03:13:38.510902    2293 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="754.093s"
I0322 03:13:40.280034    2293 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="middlewares.traefik.io"
I0322 03:13:40.280095    2293 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressroutes.traefik.containo.us"
I0322 03:13:40.280118    2293 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressroutetcps.traefik.io"
I0322 03:13:40.280167    2293 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="serverstransporttcps.traefik.io"
I0322 03:13:40.280191    2293 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="serverstransports.traefik.containo.us"
I0322 03:13:40.280214    2293 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="tlsoptions.traefik.containo.us"
I0322 03:13:40.280291    2293 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="tlsoptions.traefik.io"
I0322 03:13:40.280316    2293 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="tlsstores.traefik.containo.us"
I0322 03:13:40.280360    2293 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="tlsstores.traefik.io"
I0322 03:13:40.280380    2293 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressrouteudps.traefik.io"
I0322 03:13:40.280398    2293 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="serverstransports.traefik.io"
I0322 03:13:40.280417    2293 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="middlewaretcps.traefik.containo.us"
I0322 03:13:40.280469    2293 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="traefikservices.traefik.io"
I0322 03:13:40.280501    2293 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="middlewaretcps.traefik.io"
I0322 03:13:40.280528    2293 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressrouteudps.traefik.containo.us"
I0322 03:13:40.280592    2293 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressroutes.traefik.io"
I0322 03:13:40.280627    2293 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="traefikservices.traefik.containo.us"
I0322 03:13:40.280646    2293 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressroutetcps.traefik.containo.us"
I0322 03:13:40.280709    2293 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="middlewares.traefik.containo.us"
I0322 03:13:40.282555    2293 shared_informer.go:311] Waiting for caches to sync for resource quota
I0322 03:13:40.418741    2293 shared_informer.go:318] Caches are synced for resource quota
I0322 03:13:40.578921    2293 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0322 03:13:40.579316    2293 shared_informer.go:318] Caches are synced for garbage collector
time="2024-03-22T03:14:49Z" level=info msg="certificate CN=serverworker signed by CN=k3s-server-ca@1711077141: notBefore=2024-03-22 03:12:21 +0000 UTC notAfter=2025-03-22 03:14:49 +0000 UTC"
time="2024-03-22T03:14:49Z" level=info msg="certificate CN=system:node:serverworker,O=system:nodes signed by CN=k3s-client-ca@1711077141: notBefore=2024-03-22 03:12:21 +0000 UTC notAfter=2025-03-22 03:14:49 +0000 UTC"
E0322 03:15:11.420038    2293 health_controller.go:162] Metrics Controller heartbeat missed
time="2024-03-22T03:17:24Z" level=info msg="Starting k3s v1.28.7+k3s1 (051b14b2)"
time="2024-03-22T03:17:24Z" level=info msg="Configuring sqlite3 database connection pooling: maxIdleConns=2, maxOpenConns=0, connMaxLifetime=0s"
time="2024-03-22T03:17:24Z" level=info msg="Configuring database table schema and indexes, this may take a moment..."
time="2024-03-22T03:17:24Z" level=info msg="Database tables and indexes are up to date"
time="2024-03-22T03:17:24Z" level=info msg="Kine available at unix://kine.sock"
time="2024-03-22T03:17:24Z" level=info msg="generated self-signed CA certificate CN=k3s-client-ca@1711077444: notBefore=2024-03-22 03:17:24.491673453 +0000 UTC notAfter=2034-03-20 03:17:24.491673453 +0000 UTC"
time="2024-03-22T03:17:24Z" level=info msg="certificate CN=system:admin,O=system:masters signed by CN=k3s-client-ca@1711077444: notBefore=2024-03-22 03:17:24 +0000 UTC notAfter=2025-03-22 03:17:24 +0000 UTC"
time="2024-03-22T03:17:24Z" level=info msg="certificate CN=system:k3s-supervisor,O=system:masters signed by CN=k3s-client-ca@1711077444: notBefore=2024-03-22 03:17:24 +0000 UTC notAfter=2025-03-22 03:17:24 +0000 UTC"
time="2024-03-22T03:17:24Z" level=info msg="certificate CN=system:kube-controller-manager signed by CN=k3s-client-ca@1711077444: notBefore=2024-03-22 03:17:24 +0000 UTC notAfter=2025-03-22 03:17:24 +0000 UTC"
time="2024-03-22T03:17:24Z" level=info msg="certificate CN=system:kube-scheduler signed by CN=k3s-client-ca@1711077444: notBefore=2024-03-22 03:17:24 +0000 UTC notAfter=2025-03-22 03:17:24 +0000 UTC"
time="2024-03-22T03:17:24Z" level=info msg="certificate CN=system:apiserver,O=system:masters signed by CN=k3s-client-ca@1711077444: notBefore=2024-03-22 03:17:24 +0000 UTC notAfter=2025-03-22 03:17:24 +0000 UTC"
time="2024-03-22T03:17:24Z" level=info msg="certificate CN=system:kube-proxy signed by CN=k3s-client-ca@1711077444: notBefore=2024-03-22 03:17:24 +0000 UTC notAfter=2025-03-22 03:17:24 +0000 UTC"
time="2024-03-22T03:17:24Z" level=info msg="certificate CN=system:k3s-controller signed by CN=k3s-client-ca@1711077444: notBefore=2024-03-22 03:17:24 +0000 UTC notAfter=2025-03-22 03:17:24 +0000 UTC"
time="2024-03-22T03:17:24Z" level=info msg="certificate CN=k3s-cloud-controller-manager signed by CN=k3s-client-ca@1711077444: notBefore=2024-03-22 03:17:24 +0000 UTC notAfter=2025-03-22 03:17:24 +0000 UTC"
time="2024-03-22T03:17:24Z" level=info msg="generated self-signed CA certificate CN=k3s-server-ca@1711077444: notBefore=2024-03-22 03:17:24.50421608 +0000 UTC notAfter=2034-03-20 03:17:24.50421608 +0000 UTC"
time="2024-03-22T03:17:24Z" level=info msg="certificate CN=kube-apiserver signed by CN=k3s-server-ca@1711077444: notBefore=2024-03-22 03:17:24 +0000 UTC notAfter=2025-03-22 03:17:24 +0000 UTC"
time="2024-03-22T03:17:24Z" level=info msg="generated self-signed CA certificate CN=k3s-request-header-ca@1711077444: notBefore=2024-03-22 03:17:24.506070024 +0000 UTC notAfter=2034-03-20 03:17:24.506070024 +0000 UTC"
time="2024-03-22T03:17:24Z" level=info msg="certificate CN=system:auth-proxy signed by CN=k3s-request-header-ca@1711077444: notBefore=2024-03-22 03:17:24 +0000 UTC notAfter=2025-03-22 03:17:24 +0000 UTC"
time="2024-03-22T03:17:24Z" level=info msg="generated self-signed CA certificate CN=etcd-server-ca@1711077444: notBefore=2024-03-22 03:17:24.507373304 +0000 UTC notAfter=2034-03-20 03:17:24.507373304 +0000 UTC"
time="2024-03-22T03:17:24Z" level=info msg="certificate CN=etcd-client signed by CN=etcd-server-ca@1711077444: notBefore=2024-03-22 03:17:24 +0000 UTC notAfter=2025-03-22 03:17:24 +0000 UTC"
time="2024-03-22T03:17:24Z" level=info msg="generated self-signed CA certificate CN=etcd-peer-ca@1711077444: notBefore=2024-03-22 03:17:24.508824093 +0000 UTC notAfter=2034-03-20 03:17:24.508824093 +0000 UTC"
time="2024-03-22T03:17:24Z" level=info msg="certificate CN=etcd-peer signed by CN=etcd-peer-ca@1711077444: notBefore=2024-03-22 03:17:24 +0000 UTC notAfter=2025-03-22 03:17:24 +0000 UTC"
time="2024-03-22T03:17:24Z" level=info msg="certificate CN=etcd-server signed by CN=etcd-server-ca@1711077444: notBefore=2024-03-22 03:17:24 +0000 UTC notAfter=2025-03-22 03:17:24 +0000 UTC"
time="2024-03-22T03:17:24Z" level=info msg="Saving cluster bootstrap data to datastore"
time="2024-03-22T03:17:24Z" level=info msg="certificate CN=k3s,O=k3s signed by CN=k3s-server-ca@1711077444: notBefore=2024-03-22 03:17:24 +0000 UTC notAfter=2025-03-22 03:17:24 +0000 UTC"
time="2024-03-22T03:17:24Z" level=warning msg="dynamiclistener [::]:6443: no cached certificate available for preload - deferring certificate load until storage initialization or first client request"
time="2024-03-22T03:17:24Z" level=info msg="Active TLS secret / (ver=) (count 10): map[listener.cattle.io/cn-10.43.0.1:10.43.0.1 listener.cattle.io/cn-127.0.0.1:127.0.0.1 listener.cattle.io/cn-192.168.56.110:192.168.56.110 listener.cattle.io/cn-__1-f16284:::1 listener.cattle.io/cn-kubernetes:kubernetes listener.cattle.io/cn-kubernetes.default:kubernetes.default listener.cattle.io/cn-kubernetes.default.svc:kubernetes.default.svc listener.cattle.io/cn-kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local listener.cattle.io/cn-localhost:localhost listener.cattle.io/cn-server:server listener.cattle.io/fingerprint:SHA1=296AFC6A252966C067401FF9072578D9CE0A72C7]"
time="2024-03-22T03:17:24Z" level=info msg="Running kube-apiserver --advertise-address=192.168.56.110 --advertise-port=6443 --allow-privileged=true --anonymous-auth=false --api-audiences=https://kubernetes.default.svc.cluster.local,k3s --authorization-mode=Node,RBAC --bind-address=127.0.0.1 --cert-dir=/var/lib/rancher/k3s/server/tls/temporary-certs --client-ca-file=/var/lib/rancher/k3s/server/tls/client-ca.crt --egress-selector-config-file=/var/lib/rancher/k3s/server/etc/egress-selector-config.yaml --enable-admission-plugins=NodeRestriction --enable-aggregator-routing=true --enable-bootstrap-token-auth=true --etcd-servers=unix://kine.sock --kubelet-certificate-authority=/var/lib/rancher/k3s/server/tls/server-ca.crt --kubelet-client-certificate=/var/lib/rancher/k3s/server/tls/client-kube-apiserver.crt --kubelet-client-key=/var/lib/rancher/k3s/server/tls/client-kube-apiserver.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --profiling=false --proxy-client-cert-file=/var/lib/rancher/k3s/server/tls/client-auth-proxy.crt --proxy-client-key-file=/var/lib/rancher/k3s/server/tls/client-auth-proxy.key --requestheader-allowed-names=system:auth-proxy --requestheader-client-ca-file=/var/lib/rancher/k3s/server/tls/request-header-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6444 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/var/lib/rancher/k3s/server/tls/service.key --service-account-signing-key-file=/var/lib/rancher/k3s/server/tls/service.current.key --service-cluster-ip-range=10.43.0.0/16 --service-node-port-range=30000-32767 --storage-backend=etcd3 --tls-cert-file=/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.crt --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 --tls-private-key-file=/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.key"
time="2024-03-22T03:17:24Z" level=info msg="Running kube-scheduler --authentication-kubeconfig=/var/lib/rancher/k3s/server/cred/scheduler.kubeconfig --authorization-kubeconfig=/var/lib/rancher/k3s/server/cred/scheduler.kubeconfig --bind-address=127.0.0.1 --kubeconfig=/var/lib/rancher/k3s/server/cred/scheduler.kubeconfig --leader-elect=false --profiling=false --secure-port=10259"
time="2024-03-22T03:17:24Z" level=info msg="Running kube-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/var/lib/rancher/k3s/server/cred/controller.kubeconfig --authorization-kubeconfig=/var/lib/rancher/k3s/server/cred/controller.kubeconfig --bind-address=127.0.0.1 --cluster-cidr=10.42.0.0/16 --cluster-signing-kube-apiserver-client-cert-file=/var/lib/rancher/k3s/server/tls/client-ca.nochain.crt --cluster-signing-kube-apiserver-client-key-file=/var/lib/rancher/k3s/server/tls/client-ca.key --cluster-signing-kubelet-client-cert-file=/var/lib/rancher/k3s/server/tls/client-ca.nochain.crt --cluster-signing-kubelet-client-key-file=/var/lib/rancher/k3s/server/tls/client-ca.key --cluster-signing-kubelet-serving-cert-file=/var/lib/rancher/k3s/server/tls/server-ca.nochain.crt --cluster-signing-kubelet-serving-key-file=/var/lib/rancher/k3s/server/tls/server-ca.key --cluster-signing-legacy-unknown-cert-file=/var/lib/rancher/k3s/server/tls/server-ca.nochain.crt --cluster-signing-legacy-unknown-key-file=/var/lib/rancher/k3s/server/tls/server-ca.key --configure-cloud-routes=false --controllers=*,tokencleaner,-service,-route,-cloud-node-lifecycle --kubeconfig=/var/lib/rancher/k3s/server/cred/controller.kubeconfig --leader-elect=false --profiling=false --root-ca-file=/var/lib/rancher/k3s/server/tls/server-ca.crt --secure-port=10257 --service-account-private-key-file=/var/lib/rancher/k3s/server/tls/service.current.key --service-cluster-ip-range=10.43.0.0/16 --use-service-account-credentials=true"
I0322 03:17:24.851815    2308 options.go:220] external host was not specified, using 192.168.56.110
time="2024-03-22T03:17:24Z" level=info msg="Running cloud-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/var/lib/rancher/k3s/server/cred/cloud-controller.kubeconfig --authorization-kubeconfig=/var/lib/rancher/k3s/server/cred/cloud-controller.kubeconfig --bind-address=127.0.0.1 --cloud-config=/var/lib/rancher/k3s/server/etc/cloud-config.yaml --cloud-provider=k3s --cluster-cidr=10.42.0.0/16 --configure-cloud-routes=false --controllers=*,-route --feature-gates=CloudDualStackNodeIPs=true --kubeconfig=/var/lib/rancher/k3s/server/cred/cloud-controller.kubeconfig --leader-elect=false --leader-elect-resource-name=k3s-cloud-controller-manager --node-status-update-frequency=1m0s --profiling=false"
time="2024-03-22T03:17:24Z" level=info msg="Server node token is available at /var/lib/rancher/k3s/server/token"
I0322 03:17:24.852944    2308 server.go:156] Version: v1.28.7+k3s1
I0322 03:17:24.853054    2308 server.go:158] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
time="2024-03-22T03:17:24Z" level=info msg="To join server node to cluster: k3s server -s https://10.0.2.15:6443 -t ${SERVER_NODE_TOKEN}"
time="2024-03-22T03:17:24Z" level=info msg="Agent node token is available at /var/lib/rancher/k3s/server/agent-token"
time="2024-03-22T03:17:24Z" level=info msg="To join agent node to cluster: k3s agent -s https://10.0.2.15:6443 -t ${AGENT_NODE_TOKEN}"
time="2024-03-22T03:17:24Z" level=info msg="Wrote kubeconfig /etc/rancher/k3s/k3s.yaml"
time="2024-03-22T03:17:24Z" level=info msg="Run: k3s kubectl"
time="2024-03-22T03:17:24Z" level=info msg="Waiting for API server to become available"
I0322 03:17:25.214792    2308 plugins.go:158] Loaded 12 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,TaintNodesByCondition,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,MutatingAdmissionWebhook.
I0322 03:17:25.214891    2308 plugins.go:161] Loaded 13 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,ClusterTrustBundleAttest,CertificateSubjectRestriction,ValidatingAdmissionPolicy,ValidatingAdmissionWebhook,ResourceQuota.
I0322 03:17:25.215476    2308 instance.go:298] Using reconciler: lease
I0322 03:17:25.215786    2308 shared_informer.go:311] Waiting for caches to sync for node_authorizer
I0322 03:17:25.223451    2308 handler.go:275] Adding GroupVersion apiextensions.k8s.io v1 to ResourceManager
W0322 03:17:25.223559    2308 genericapiserver.go:744] Skipping API apiextensions.k8s.io/v1beta1 because it has no resources.
I0322 03:17:25.318238    2308 handler.go:275] Adding GroupVersion  v1 to ResourceManager
I0322 03:17:25.318699    2308 instance.go:709] API group "internal.apiserver.k8s.io" is not enabled, skipping.
I0322 03:17:25.460174    2308 instance.go:709] API group "resource.k8s.io" is not enabled, skipping.
time="2024-03-22T03:17:25Z" level=info msg="Password verified locally for node server"
time="2024-03-22T03:17:25Z" level=info msg="certificate CN=server signed by CN=k3s-server-ca@1711077444: notBefore=2024-03-22 03:17:24 +0000 UTC notAfter=2025-03-22 03:17:25 +0000 UTC"
I0322 03:17:25.466708    2308 handler.go:275] Adding GroupVersion authentication.k8s.io v1 to ResourceManager
W0322 03:17:25.466809    2308 genericapiserver.go:744] Skipping API authentication.k8s.io/v1beta1 because it has no resources.
W0322 03:17:25.466816    2308 genericapiserver.go:744] Skipping API authentication.k8s.io/v1alpha1 because it has no resources.
I0322 03:17:25.467247    2308 handler.go:275] Adding GroupVersion authorization.k8s.io v1 to ResourceManager
W0322 03:17:25.467342    2308 genericapiserver.go:744] Skipping API authorization.k8s.io/v1beta1 because it has no resources.
I0322 03:17:25.468023    2308 handler.go:275] Adding GroupVersion autoscaling v2 to ResourceManager
I0322 03:17:25.468590    2308 handler.go:275] Adding GroupVersion autoscaling v1 to ResourceManager
W0322 03:17:25.468686    2308 genericapiserver.go:744] Skipping API autoscaling/v2beta1 because it has no resources.
W0322 03:17:25.468751    2308 genericapiserver.go:744] Skipping API autoscaling/v2beta2 because it has no resources.
I0322 03:17:25.469786    2308 handler.go:275] Adding GroupVersion batch v1 to ResourceManager
W0322 03:17:25.469837    2308 genericapiserver.go:744] Skipping API batch/v1beta1 because it has no resources.
I0322 03:17:25.470679    2308 handler.go:275] Adding GroupVersion certificates.k8s.io v1 to ResourceManager
W0322 03:17:25.470731    2308 genericapiserver.go:744] Skipping API certificates.k8s.io/v1beta1 because it has no resources.
W0322 03:17:25.470844    2308 genericapiserver.go:744] Skipping API certificates.k8s.io/v1alpha1 because it has no resources.
I0322 03:17:25.477881    2308 handler.go:275] Adding GroupVersion coordination.k8s.io v1 to ResourceManager
W0322 03:17:25.477987    2308 genericapiserver.go:744] Skipping API coordination.k8s.io/v1beta1 because it has no resources.
W0322 03:17:25.484405    2308 genericapiserver.go:744] Skipping API discovery.k8s.io/v1beta1 because it has no resources.
I0322 03:17:25.484997    2308 handler.go:275] Adding GroupVersion discovery.k8s.io v1 to ResourceManager
I0322 03:17:25.486039    2308 handler.go:275] Adding GroupVersion networking.k8s.io v1 to ResourceManager
W0322 03:17:25.486165    2308 genericapiserver.go:744] Skipping API networking.k8s.io/v1beta1 because it has no resources.
W0322 03:17:25.486177    2308 genericapiserver.go:744] Skipping API networking.k8s.io/v1alpha1 because it has no resources.
I0322 03:17:25.486620    2308 handler.go:275] Adding GroupVersion node.k8s.io v1 to ResourceManager
W0322 03:17:25.486632    2308 genericapiserver.go:744] Skipping API node.k8s.io/v1beta1 because it has no resources.
W0322 03:17:25.486636    2308 genericapiserver.go:744] Skipping API node.k8s.io/v1alpha1 because it has no resources.
I0322 03:17:25.487235    2308 handler.go:275] Adding GroupVersion policy v1 to ResourceManager
W0322 03:17:25.487247    2308 genericapiserver.go:744] Skipping API policy/v1beta1 because it has no resources.
I0322 03:17:25.488703    2308 handler.go:275] Adding GroupVersion rbac.authorization.k8s.io v1 to ResourceManager
W0322 03:17:25.488718    2308 genericapiserver.go:744] Skipping API rbac.authorization.k8s.io/v1beta1 because it has no resources.
W0322 03:17:25.488722    2308 genericapiserver.go:744] Skipping API rbac.authorization.k8s.io/v1alpha1 because it has no resources.
I0322 03:17:25.489126    2308 handler.go:275] Adding GroupVersion scheduling.k8s.io v1 to ResourceManager
W0322 03:17:25.489139    2308 genericapiserver.go:744] Skipping API scheduling.k8s.io/v1beta1 because it has no resources.
W0322 03:17:25.489143    2308 genericapiserver.go:744] Skipping API scheduling.k8s.io/v1alpha1 because it has no resources.
I0322 03:17:25.490479    2308 handler.go:275] Adding GroupVersion storage.k8s.io v1 to ResourceManager
W0322 03:17:25.490493    2308 genericapiserver.go:744] Skipping API storage.k8s.io/v1beta1 because it has no resources.
W0322 03:17:25.490497    2308 genericapiserver.go:744] Skipping API storage.k8s.io/v1alpha1 because it has no resources.
I0322 03:17:25.491368    2308 handler.go:275] Adding GroupVersion flowcontrol.apiserver.k8s.io v1beta3 to ResourceManager
I0322 03:17:25.492091    2308 handler.go:275] Adding GroupVersion flowcontrol.apiserver.k8s.io v1beta2 to ResourceManager
W0322 03:17:25.492104    2308 genericapiserver.go:744] Skipping API flowcontrol.apiserver.k8s.io/v1beta1 because it has no resources.
W0322 03:17:25.492109    2308 genericapiserver.go:744] Skipping API flowcontrol.apiserver.k8s.io/v1alpha1 because it has no resources.
I0322 03:17:25.495297    2308 handler.go:275] Adding GroupVersion apps v1 to ResourceManager
W0322 03:17:25.495310    2308 genericapiserver.go:744] Skipping API apps/v1beta2 because it has no resources.
W0322 03:17:25.495314    2308 genericapiserver.go:744] Skipping API apps/v1beta1 because it has no resources.
I0322 03:17:25.497884    2308 handler.go:275] Adding GroupVersion admissionregistration.k8s.io v1 to ResourceManager
W0322 03:17:25.497898    2308 genericapiserver.go:744] Skipping API admissionregistration.k8s.io/v1beta1 because it has no resources.
W0322 03:17:25.497902    2308 genericapiserver.go:744] Skipping API admissionregistration.k8s.io/v1alpha1 because it has no resources.
I0322 03:17:25.499713    2308 handler.go:275] Adding GroupVersion events.k8s.io v1 to ResourceManager
W0322 03:17:25.499727    2308 genericapiserver.go:744] Skipping API events.k8s.io/v1beta1 because it has no resources.
I0322 03:17:25.508720    2308 handler.go:275] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
W0322 03:17:25.508878    2308 genericapiserver.go:744] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
time="2024-03-22T03:17:25Z" level=info msg="certificate CN=system:node:server,O=system:nodes signed by CN=k3s-client-ca@1711077444: notBefore=2024-03-22 03:17:24 +0000 UTC notAfter=2025-03-22 03:17:25 +0000 UTC"
I0322 03:17:25.996955    2308 secure_serving.go:213] Serving securely on 127.0.0.1:6444
I0322 03:17:25.997217    2308 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/rancher/k3s/server/tls/request-header-ca.crt"
I0322 03:17:25.997348    2308 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.crt::/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.key"
I0322 03:17:25.997467    2308 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0322 03:17:25.998040    2308 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/rancher/k3s/server/tls/client-auth-proxy.crt::/var/lib/rancher/k3s/server/tls/client-auth-proxy.key"
I0322 03:17:25.998176    2308 handler_discovery.go:412] Starting ResourceDiscoveryManager
I0322 03:17:25.998402    2308 customresource_discovery_controller.go:289] Starting DiscoveryController
I0322 03:17:25.999083    2308 gc_controller.go:78] Starting apiserver lease garbage collector
I0322 03:17:25.999136    2308 available_controller.go:423] Starting AvailableConditionController
I0322 03:17:25.999146    2308 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0322 03:17:25.999184    2308 controller.go:78] Starting OpenAPI AggregationController
I0322 03:17:25.999315    2308 system_namespaces_controller.go:67] Starting system namespaces controller
I0322 03:17:25.999554    2308 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0322 03:17:25.999566    2308 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0322 03:17:25.999617    2308 apf_controller.go:374] Starting API Priority and Fairness config controller
I0322 03:17:25.999900    2308 gc_controller.go:78] Starting apiserver lease garbage collector
I0322 03:17:26.000261    2308 controller.go:80] Starting OpenAPI V3 AggregationController
I0322 03:17:26.000560    2308 controller.go:116] Starting legacy_token_tracking_controller
I0322 03:17:26.000620    2308 shared_informer.go:311] Waiting for caches to sync for configmaps
I0322 03:17:26.001184    2308 aggregator.go:164] waiting for initial CRD sync...
I0322 03:17:26.001930    2308 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0322 03:17:26.001944    2308 shared_informer.go:311] Waiting for caches to sync for cluster_authentication_trust_controller
I0322 03:17:26.002023    2308 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/rancher/k3s/server/tls/client-ca.crt"
I0322 03:17:26.003001    2308 controller.go:134] Starting OpenAPI controller
I0322 03:17:26.003061    2308 controller.go:85] Starting OpenAPI V3 controller
I0322 03:17:26.003097    2308 naming_controller.go:291] Starting NamingConditionController
I0322 03:17:26.003147    2308 establishing_controller.go:76] Starting EstablishingController
I0322 03:17:26.003210    2308 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0322 03:17:26.003258    2308 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0322 03:17:26.003341    2308 crd_finalizer.go:266] Starting CRDFinalizer
I0322 03:17:26.006214    2308 crdregistration_controller.go:111] Starting crd-autoregister controller
I0322 03:17:26.006230    2308 shared_informer.go:311] Waiting for caches to sync for crd-autoregister
I0322 03:17:26.006272    2308 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/rancher/k3s/server/tls/client-ca.crt"
I0322 03:17:26.006327    2308 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/rancher/k3s/server/tls/request-header-ca.crt"
I0322 03:17:26.100044    2308 cache.go:39] Caches are synced for AvailableConditionController controller
I0322 03:17:26.100486    2308 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0322 03:17:26.100604    2308 apf_controller.go:379] Running API Priority and Fairness config worker
I0322 03:17:26.100659    2308 apf_controller.go:382] Running API Priority and Fairness periodic rebalancing process
I0322 03:17:26.103660    2308 controller.go:624] quota admission added evaluator for: namespaces
I0322 03:17:26.104219    2308 shared_informer.go:318] Caches are synced for configmaps
I0322 03:17:26.104482    2308 shared_informer.go:318] Caches are synced for cluster_authentication_trust_controller
time="2024-03-22T03:17:26Z" level=info msg="Module overlay was already loaded"
I0322 03:17:26.129109    2308 shared_informer.go:318] Caches are synced for node_authorizer
I0322 03:17:26.129138    2308 shared_informer.go:318] Caches are synced for crd-autoregister
I0322 03:17:26.129158    2308 aggregator.go:166] initial CRD sync complete...
I0322 03:17:26.129165    2308 autoregister_controller.go:141] Starting autoregister controller
I0322 03:17:26.129170    2308 cache.go:32] Waiting for caches to sync for autoregister controller
I0322 03:17:26.129175    2308 cache.go:39] Caches are synced for autoregister controller
I0322 03:17:26.141894    2308 controller.go:624] quota admission added evaluator for: leases.coordination.k8s.io
time="2024-03-22T03:17:26Z" level=info msg="Module br_netfilter was already loaded"
time="2024-03-22T03:17:26Z" level=info msg="Set sysctl 'net/netfilter/nf_conntrack_max' to 131072"
time="2024-03-22T03:17:26Z" level=info msg="Set sysctl 'net/netfilter/nf_conntrack_tcp_timeout_established' to 86400"
time="2024-03-22T03:17:26Z" level=info msg="Set sysctl 'net/netfilter/nf_conntrack_tcp_timeout_close_wait' to 3600"
time="2024-03-22T03:17:26Z" level=info msg="Set sysctl 'net/ipv4/conf/all/forwarding' to 1"
E0322 03:17:26.172260    2308 controller.go:95] Unable to perform initial Kubernetes service initialization: namespaces "default" not found
time="2024-03-22T03:17:26Z" level=info msg="Logging containerd to /var/lib/rancher/k3s/agent/containerd/containerd.log"
time="2024-03-22T03:17:26Z" level=info msg="Running containerd -c /var/lib/rancher/k3s/agent/etc/containerd/config.toml -a /run/k3s/containerd/containerd.sock --state /run/k3s/containerd --root /var/lib/rancher/k3s/agent/containerd"
I0322 03:17:27.007317    2308 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0322 03:17:27.011446    2308 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0322 03:17:27.011558    2308 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
time="2024-03-22T03:17:27Z" level=info msg="containerd is now running"
time="2024-03-22T03:17:27Z" level=info msg="Running kubelet --address=0.0.0.0 --allowed-unsafe-sysctls=net.ipv4.ip_forward,net.ipv6.conf.all.forwarding --anonymous-auth=false --authentication-token-webhook=true --authorization-mode=Webhook --cgroup-driver=systemd --client-ca-file=/var/lib/rancher/k3s/agent/client-ca.crt --cloud-provider=external --cluster-dns=10.43.0.10 --cluster-domain=cluster.local --container-runtime-endpoint=unix:///run/k3s/containerd/containerd.sock --containerd=/run/k3s/containerd/containerd.sock --eviction-hard=imagefs.available<5%,nodefs.available<5% --eviction-minimum-reclaim=imagefs.available=10%,nodefs.available=10% --fail-swap-on=false --feature-gates=CloudDualStackNodeIPs=true --healthz-bind-address=127.0.0.1 --hostname-override=server --kubeconfig=/var/lib/rancher/k3s/agent/kubelet.kubeconfig --node-ip=192.168.56.110 --node-labels= --pod-infra-container-image=rancher/mirrored-pause:3.6 --pod-manifest-path=/var/lib/rancher/k3s/agent/pod-manifests --read-only-port=0 --resolv-conf=/run/systemd/resolve/resolv.conf --serialize-image-pulls=false --tls-cert-file=/var/lib/rancher/k3s/agent/serving-kubelet.crt --tls-private-key-file=/var/lib/rancher/k3s/agent/serving-kubelet.key"
time="2024-03-22T03:17:27Z" level=info msg="Connecting to proxy" url="wss://127.0.0.1:6443/v1-k3s/connect"
time="2024-03-22T03:17:27Z" level=info msg="Handling backend connection request [server]"
time="2024-03-22T03:17:27Z" level=info msg="Waiting to retrieve kube-proxy configuration; server is not ready: https://127.0.0.1:6443/v1-k3s/readyz: 500 Internal Server Error"
I0322 03:17:27.373480    2308 controller.go:624] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0322 03:17:27.400506    2308 controller.go:624] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0322 03:17:27.493585    2308 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.43.0.1"}
W0322 03:17:27.498004    2308 lease.go:263] Resetting endpoints for master service "kubernetes" to [192.168.56.110]
I0322 03:17:27.498953    2308 controller.go:624] quota admission added evaluator for: endpoints
I0322 03:17:27.502464    2308 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
time="2024-03-22T03:17:28Z" level=info msg="Kube API server is now running"
time="2024-03-22T03:17:28Z" level=info msg="ETCD server is now running"
time="2024-03-22T03:17:28Z" level=info msg="k3s is up and running"
time="2024-03-22T03:17:28Z" level=info msg="Creating k3s-supervisor event broadcaster"
time="2024-03-22T03:17:28Z" level=info msg="Waiting for cloud-controller-manager privileges to become available"
time="2024-03-22T03:17:28Z" level=info msg="Applying CRD addons.k3s.cattle.io"
time="2024-03-22T03:17:28Z" level=info msg="Applying CRD etcdsnapshotfiles.k3s.cattle.io"
I0322 03:17:28.141670    2308 handler.go:275] Adding GroupVersion k3s.cattle.io v1 to ResourceManager
time="2024-03-22T03:17:28Z" level=info msg="Applying CRD helmcharts.helm.cattle.io"
I0322 03:17:28.196487    2308 handler.go:275] Adding GroupVersion k3s.cattle.io v1 to ResourceManager
time="2024-03-22T03:17:28Z" level=info msg="Applying CRD helmchartconfigs.helm.cattle.io"
time="2024-03-22T03:17:28Z" level=info msg="Waiting for CRD helmcharts.helm.cattle.io to become available"
I0322 03:17:28.254079    2308 handler.go:275] Adding GroupVersion helm.cattle.io v1 to ResourceManager
Flag --cloud-provider has been deprecated, will be removed in 1.25 or later, in favor of removing cloud provider code from Kubelet.
Flag --containerd has been deprecated, This is a cadvisor flag that was mistakenly registered with the Kubelet. Due to legacy concerns, it will follow the standard CLI deprecation timeline before being removed.
Flag --pod-infra-container-image has been deprecated, will be removed in a future release. Image garbage collector will get sandbox image information from CRI.
I0322 03:17:28.266270    2308 server.go:202] "--pod-infra-container-image will not be pruned by the image garbage collector in kubelet and should also be set in the remote runtime"
I0322 03:17:28.268763    2308 server.go:462] "Kubelet version" kubeletVersion="v1.28.7+k3s1"
I0322 03:17:28.268779    2308 server.go:464] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0322 03:17:28.270891    2308 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/rancher/k3s/agent/client-ca.crt"
I0322 03:17:28.272488    2308 handler.go:275] Adding GroupVersion helm.cattle.io v1 to ResourceManager
I0322 03:17:28.284849    2308 server.go:720] "--cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /"
I0322 03:17:28.287621    2308 container_manager_linux.go:265] "Container manager verified user specified cgroup-root exists" cgroupRoot=[]
I0322 03:17:28.287984    2308 container_manager_linux.go:270] "Creating Container Manager object based on Node Config" nodeConfig={"RuntimeCgroupsName":"","SystemCgroupsName":"","KubeletCgroupsName":"","KubeletOOMScoreAdj":-999,"ContainerRuntime":"","CgroupsPerQOS":true,"CgroupRoot":"/","CgroupDriver":"systemd","KubeletRootDir":"/var/lib/kubelet","ProtectKernelDefaults":false,"KubeReservedCgroupName":"","SystemReservedCgroupName":"","ReservedSystemCPUs":{},"EnforceNodeAllocatable":{"pods":{}},"KubeReserved":null,"SystemReserved":null,"HardEvictionThresholds":[{"Signal":"imagefs.available","Operator":"LessThan","Value":{"Quantity":null,"Percentage":0.05},"GracePeriod":0,"MinReclaim":null},{"Signal":"nodefs.available","Operator":"LessThan","Value":{"Quantity":null,"Percentage":0.05},"GracePeriod":0,"MinReclaim":null}],"QOSReserved":{},"CPUManagerPolicy":"none","CPUManagerPolicyOptions":null,"TopologyManagerScope":"container","CPUManagerReconcilePeriod":10000000000,"ExperimentalMemoryManagerPolicy":"None","ExperimentalMemoryManagerReservedMemory":null,"PodPidsLimit":-1,"EnforceCPULimits":true,"CPUCFSQuotaPeriod":100000000,"TopologyManagerPolicy":"none","TopologyManagerPolicyOptions":null}
I0322 03:17:28.288570    2308 topology_manager.go:138] "Creating topology manager with none policy"
I0322 03:17:28.288612    2308 container_manager_linux.go:301] "Creating device plugin manager"
I0322 03:17:28.289185    2308 state_mem.go:36] "Initialized new in-memory state store"
I0322 03:17:28.290044    2308 kubelet.go:393] "Attempting to sync node with API server"
I0322 03:17:28.290109    2308 kubelet.go:298] "Adding static pod path" path="/var/lib/rancher/k3s/agent/pod-manifests"
I0322 03:17:28.290145    2308 kubelet.go:309] "Adding apiserver pod source"
I0322 03:17:28.290161    2308 apiserver.go:42] "Waiting for node sync before watching apiserver pods"
I0322 03:17:28.292660    2308 kuberuntime_manager.go:257] "Container runtime initialized" containerRuntime="containerd" version="v1.7.11-k3s2" apiVersion="v1"
W0322 03:17:28.295622    2308 probe.go:268] Flexvolume plugin directory at /usr/libexec/kubernetes/kubelet-plugins/volume/exec/ does not exist. Recreating.
I0322 03:17:28.297504    2308 server.go:1227] "Started kubelet"
I0322 03:17:28.300649    2308 fs_resource_analyzer.go:67] "Starting FS ResourceAnalyzer"
I0322 03:17:28.317835    2308 server.go:162] "Starting to listen" address="0.0.0.0" port=10250
I0322 03:17:28.319498    2308 server.go:462] "Adding debug handlers to kubelet server"
I0322 03:17:28.320168    2308 ratelimit.go:65] "Setting rate limiting for podresources endpoint" qps=100 burstTokens=10
I0322 03:17:28.320500    2308 server.go:233] "Starting to serve the podresources API" endpoint="unix:/var/lib/kubelet/pod-resources/kubelet.sock"
I0322 03:17:28.325214    2308 volume_manager.go:291] "Starting Kubelet Volume Manager"
I0322 03:17:28.327683    2308 desired_state_of_world_populator.go:151] "Desired state populator starts to run"
I0322 03:17:28.328201    2308 reconciler_new.go:29] "Reconciler: start to sync state"
I0322 03:17:28.341172    2308 kubelet_network_linux.go:50] "Initialized iptables rules." protocol="IPv4"
E0322 03:17:28.347846    2308 cri_stats_provider.go:448] "Failed to get the info of the filesystem with mountpoint" err="unable to find data in memory cache" mountpoint="/var/lib/rancher/k3s/agent/containerd/io.containerd.snapshotter.v1.overlayfs"
E0322 03:17:28.348067    2308 kubelet.go:1431] "Image garbage collection failed once. Stats initialization may not have completed yet" err="invalid capacity 0 on image filesystem"
I0322 03:17:28.355711    2308 kubelet_network_linux.go:50] "Initialized iptables rules." protocol="IPv6"
I0322 03:17:28.356621    2308 status_manager.go:217] "Starting to sync pod status with apiserver"
I0322 03:17:28.356662    2308 kubelet.go:2303] "Starting kubelet main sync loop"
E0322 03:17:28.356731    2308 kubelet.go:2327] "Skipping pod synchronization" err="[container runtime status check may not have completed yet, PLEG is not healthy: pleg has yet to be successful]"
I0322 03:17:28.454325    2308 kubelet_node_status.go:70] "Attempting to register node" node="server"
E0322 03:17:28.456777    2308 kubelet.go:2327] "Skipping pod synchronization" err="container runtime status check may not have completed yet"
I0322 03:17:28.459038    2308 cpu_manager.go:214] "Starting CPU manager" policy="none"
I0322 03:17:28.459059    2308 cpu_manager.go:215] "Reconciling" reconcilePeriod="10s"
I0322 03:17:28.459143    2308 state_mem.go:36] "Initialized new in-memory state store"
I0322 03:17:28.462981    2308 kubelet_node_status.go:73] "Successfully registered node" node="server"
I0322 03:17:28.471657    2308 policy_none.go:49] "None policy: Start"
I0322 03:17:28.478527    2308 memory_manager.go:169] "Starting memorymanager" policy="None"
I0322 03:17:28.478557    2308 state_mem.go:35] "Initializing new in-memory state store"
time="2024-03-22T03:17:28Z" level=info msg="Annotations and labels have been set successfully on node: server"
time="2024-03-22T03:17:28Z" level=info msg="Starting flannel with backend vxlan"
I0322 03:17:28.530880    2308 manager.go:471] "Failed to read data from checkpoint" checkpoint="kubelet_internal_checkpoint" err="checkpoint is not found"
I0322 03:17:28.534200    2308 plugin_manager.go:118] "Starting Kubelet Plugin Manager"
I0322 03:17:28.541692    2308 kubelet_node_status.go:493] "Fast updating node status as it just became ready"
time="2024-03-22T03:17:28Z" level=info msg="Done waiting for CRD helmcharts.helm.cattle.io to become available"
time="2024-03-22T03:17:28Z" level=info msg="Waiting for CRD helmchartconfigs.helm.cattle.io to become available"
I0322 03:17:28.827163    2308 serving.go:355] Generated self-signed cert in-memory
I0322 03:17:29.082358    2308 controllermanager.go:189] "Starting" version="v1.28.7+k3s1"
I0322 03:17:29.082693    2308 controllermanager.go:191] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0322 03:17:29.088486    2308 secure_serving.go:213] Serving securely on 127.0.0.1:10257
I0322 03:17:29.088650    2308 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I0322 03:17:29.088877    2308 shared_informer.go:311] Waiting for caches to sync for RequestHeaderAuthRequestController
I0322 03:17:29.088668    2308 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0322 03:17:29.088899    2308 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0322 03:17:29.088679    2308 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0322 03:17:29.089137    2308 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0322 03:17:29.088737    2308 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0322 03:17:29.099785    2308 shared_informer.go:311] Waiting for caches to sync for tokens
I0322 03:17:29.106323    2308 controller.go:624] quota admission added evaluator for: serviceaccounts
I0322 03:17:29.108628    2308 controllermanager.go:642] "Started controller" controller="endpointslice-mirroring-controller"
I0322 03:17:29.108764    2308 endpointslicemirroring_controller.go:223] "Starting EndpointSliceMirroring controller"
I0322 03:17:29.108772    2308 shared_informer.go:311] Waiting for caches to sync for endpoint_slice_mirroring
I0322 03:17:29.132000    2308 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="podtemplates"
I0322 03:17:29.132263    2308 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="leases.coordination.k8s.io"
I0322 03:17:29.132424    2308 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="rolebindings.rbac.authorization.k8s.io"
I0322 03:17:29.132987    2308 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="replicasets.apps"
I0322 03:17:29.133141    2308 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="horizontalpodautoscalers.autoscaling"
I0322 03:17:29.133293    2308 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="jobs.batch"
I0322 03:17:29.133451    2308 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="roles.rbac.authorization.k8s.io"
I0322 03:17:29.133601    2308 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="helmchartconfigs.helm.cattle.io"
I0322 03:17:29.133740    2308 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="deployments.apps"
I0322 03:17:29.133859    2308 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="statefulsets.apps"
I0322 03:17:29.133978    2308 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="limitranges"
I0322 03:17:29.134096    2308 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="endpoints"
I0322 03:17:29.134216    2308 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="endpointslices.discovery.k8s.io"
I0322 03:17:29.134371    2308 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="helmcharts.helm.cattle.io"
I0322 03:17:29.134514    2308 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="addons.k3s.cattle.io"
W0322 03:17:29.134623    2308 shared_informer.go:593] resyncPeriod 15h56m26.677665696s is smaller than resyncCheckPeriod 22h33m41.834556738s and the informer has already started. Changing it to 22h33m41.834556738s
I0322 03:17:29.134796    2308 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="daemonsets.apps"
I0322 03:17:29.134953    2308 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingresses.networking.k8s.io"
I0322 03:17:29.135095    2308 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="poddisruptionbudgets.policy"
I0322 03:17:29.135257    2308 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="csistoragecapacities.storage.k8s.io"
I0322 03:17:29.135408    2308 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="networkpolicies.networking.k8s.io"
W0322 03:17:29.135558    2308 shared_informer.go:593] resyncPeriod 13h29m12.118894935s is smaller than resyncCheckPeriod 22h33m41.834556738s and the informer has already started. Changing it to 22h33m41.834556738s
I0322 03:17:29.135706    2308 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="serviceaccounts"
I0322 03:17:29.135880    2308 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="controllerrevisions.apps"
I0322 03:17:29.136046    2308 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="cronjobs.batch"
I0322 03:17:29.136208    2308 resource_quota_controller.go:294] "Starting resource quota controller"
I0322 03:17:29.136225    2308 shared_informer.go:311] Waiting for caches to sync for resource quota
I0322 03:17:29.136245    2308 resource_quota_monitor.go:305] "QuotaMonitor running"
I0322 03:17:29.136407    2308 controllermanager.go:642] "Started controller" controller="resourcequota-controller"
I0322 03:17:29.143968    2308 controllermanager.go:642] "Started controller" controller="token-cleaner-controller"
I0322 03:17:29.143992    2308 controllermanager.go:607] "Warning: controller is disabled" controller="node-route-controller"
I0322 03:17:29.144867    2308 tokencleaner.go:112] "Starting token cleaner controller"
I0322 03:17:29.144884    2308 shared_informer.go:311] Waiting for caches to sync for token_cleaner
I0322 03:17:29.144893    2308 shared_informer.go:318] Caches are synced for token_cleaner
I0322 03:17:29.152043    2308 controllermanager.go:642] "Started controller" controller="root-ca-certificate-publisher-controller"
I0322 03:17:29.159799    2308 controllermanager.go:642] "Started controller" controller="endpoints-controller"
I0322 03:17:29.160072    2308 publisher.go:102] "Starting root CA cert publisher controller"
I0322 03:17:29.160458    2308 shared_informer.go:311] Waiting for caches to sync for crt configmap
I0322 03:17:29.160649    2308 endpoints_controller.go:174] "Starting endpoint controller"
I0322 03:17:29.160659    2308 shared_informer.go:311] Waiting for caches to sync for endpoint
I0322 03:17:29.167265    2308 controllermanager.go:642] "Started controller" controller="replicationcontroller-controller"
I0322 03:17:29.167564    2308 replica_set.go:214] "Starting controller" name="replicationcontroller"
I0322 03:17:29.167656    2308 shared_informer.go:311] Waiting for caches to sync for ReplicationController
I0322 03:17:29.173680    2308 controllermanager.go:642] "Started controller" controller="serviceaccount-controller"
I0322 03:17:29.173818    2308 serviceaccounts_controller.go:111] "Starting service account controller"
I0322 03:17:29.173827    2308 shared_informer.go:311] Waiting for caches to sync for service account
I0322 03:17:29.179666    2308 controllermanager.go:642] "Started controller" controller="replicaset-controller"
I0322 03:17:29.179809    2308 replica_set.go:214] "Starting controller" name="replicaset"
I0322 03:17:29.179836    2308 shared_informer.go:311] Waiting for caches to sync for ReplicaSet
I0322 03:17:29.186608    2308 controllermanager.go:642] "Started controller" controller="cronjob-controller"
I0322 03:17:29.186724    2308 cronjob_controllerv2.go:139] "Starting cronjob controller v2"
I0322 03:17:29.187812    2308 shared_informer.go:311] Waiting for caches to sync for cronjob
I0322 03:17:29.188923    2308 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0322 03:17:29.188963    2308 shared_informer.go:318] Caches are synced for RequestHeaderAuthRequestController
I0322 03:17:29.189939    2308 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0322 03:17:29.199990    2308 shared_informer.go:318] Caches are synced for tokens
time="2024-03-22T03:17:29Z" level=info msg="Done waiting for CRD helmchartconfigs.helm.cattle.io to become available"
time="2024-03-22T03:17:29Z" level=info msg="Writing static file: /var/lib/rancher/k3s/server/static/charts/traefik-25.0.2+up25.0.0.tgz"
time="2024-03-22T03:17:29Z" level=info msg="Writing static file: /var/lib/rancher/k3s/server/static/charts/traefik-crd-25.0.2+up25.0.0.tgz"
time="2024-03-22T03:17:29Z" level=info msg="Failed to get existing traefik HelmChart" error="helmcharts.helm.cattle.io \"traefik\" not found"
time="2024-03-22T03:17:29Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/coredns.yaml"
time="2024-03-22T03:17:29Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/auth-delegator.yaml"
time="2024-03-22T03:17:29Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/metrics-apiservice.yaml"
time="2024-03-22T03:17:29Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-deployment.yaml"
time="2024-03-22T03:17:29Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/rolebindings.yaml"
time="2024-03-22T03:17:29Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/traefik.yaml"
time="2024-03-22T03:17:29Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/runtimes.yaml"
time="2024-03-22T03:17:29Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/ccm.yaml"
time="2024-03-22T03:17:29Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/local-storage.yaml"
time="2024-03-22T03:17:29Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/aggregated-metrics-reader.yaml"
time="2024-03-22T03:17:29Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/auth-reader.yaml"
time="2024-03-22T03:17:29Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-service.yaml"
time="2024-03-22T03:17:29Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/resource-reader.yaml"
time="2024-03-22T03:17:29Z" level=info msg="Starting dynamiclistener CN filter node controller"
time="2024-03-22T03:17:29Z" level=info msg="Tunnel server egress proxy mode: agent"
I0322 03:17:29.260239    2308 controllermanager.go:642] "Started controller" controller="persistentvolume-protection-controller"
I0322 03:17:29.264433    2308 pv_protection_controller.go:78] "Starting PV protection controller"
I0322 03:17:29.264622    2308 shared_informer.go:311] Waiting for caches to sync for PV protection
I0322 03:17:29.293158    2308 apiserver.go:52] "Watching apiserver"
I0322 03:17:29.334744    2308 desired_state_of_world_populator.go:159] "Finished populating initial desired state of world"
time="2024-03-22T03:17:29Z" level=info msg="Starting k3s.cattle.io/v1, Kind=Addon controller"
time="2024-03-22T03:17:29Z" level=info msg="Creating helm-controller event broadcaster"
time="2024-03-22T03:17:29Z" level=info msg="Creating deploy event broadcaster"
time="2024-03-22T03:17:29Z" level=info msg="Starting /v1, Kind=Node controller"
time="2024-03-22T03:17:29Z" level=warning msg="Unable to fetch coredns config map: configmaps \"coredns\" not found"
time="2024-03-22T03:17:29Z" level=info msg="Labels and annotations have been set successfully on node: server"
I0322 03:17:29.427110    2308 controllermanager.go:642] "Started controller" controller="pod-garbage-collector-controller"
I0322 03:17:29.427165    2308 gc_controller.go:101] "Starting GC controller"
I0322 03:17:29.427173    2308 shared_informer.go:311] Waiting for caches to sync for GC
time="2024-03-22T03:17:29Z" level=info msg="Cluster dns configmap has been set successfully"
time="2024-03-22T03:17:29Z" level=info msg="Starting helm.cattle.io/v1, Kind=HelmChart controller"
time="2024-03-22T03:17:29Z" level=info msg="Starting helm.cattle.io/v1, Kind=HelmChartConfig controller"
time="2024-03-22T03:17:29Z" level=info msg="Starting rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding controller"
time="2024-03-22T03:17:29Z" level=info msg="Starting /v1, Kind=Secret controller"
time="2024-03-22T03:17:29Z" level=info msg="Starting /v1, Kind=ConfigMap controller"
time="2024-03-22T03:17:29Z" level=info msg="Starting /v1, Kind=ServiceAccount controller"
time="2024-03-22T03:17:29Z" level=info msg="Starting batch/v1, Kind=Job controller"
I0322 03:17:29.553796    2308 controllermanager.go:642] "Started controller" controller="daemonset-controller"
I0322 03:17:29.554103    2308 daemon_controller.go:291] "Starting daemon sets controller"
I0322 03:17:29.554164    2308 shared_informer.go:311] Waiting for caches to sync for daemon sets
time="2024-03-22T03:17:29Z" level=warning msg="Unable to fetch coredns config map: configmaps \"coredns\" not found"
I0322 03:17:29.853223    2308 controllermanager.go:642] "Started controller" controller="horizontal-pod-autoscaler-controller"
I0322 03:17:29.853406    2308 horizontal.go:200] "Starting HPA controller"
I0322 03:17:29.853419    2308 shared_informer.go:311] Waiting for caches to sync for HPA
time="2024-03-22T03:17:29Z" level=info msg="Active TLS secret kube-system/k3s-serving (ver=239) (count 10): map[listener.cattle.io/cn-10.43.0.1:10.43.0.1 listener.cattle.io/cn-127.0.0.1:127.0.0.1 listener.cattle.io/cn-192.168.56.110:192.168.56.110 listener.cattle.io/cn-__1-f16284:::1 listener.cattle.io/cn-kubernetes:kubernetes listener.cattle.io/cn-kubernetes.default:kubernetes.default listener.cattle.io/cn-kubernetes.default.svc:kubernetes.default.svc listener.cattle.io/cn-kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local listener.cattle.io/cn-localhost:localhost listener.cattle.io/cn-server:server listener.cattle.io/fingerprint:SHA1=296AFC6A252966C067401FF9072578D9CE0A72C7]"
I0322 03:17:30.053032    2308 controllermanager.go:642] "Started controller" controller="disruption-controller"
I0322 03:17:30.053327    2308 disruption.go:433] "Sending events to api server."
I0322 03:17:30.053457    2308 disruption.go:444] "Starting disruption controller"
I0322 03:17:30.053466    2308 shared_informer.go:311] Waiting for caches to sync for disruption
time="2024-03-22T03:17:30Z" level=info msg="Updating TLS secret for kube-system/k3s-serving (count: 10): map[listener.cattle.io/cn-10.43.0.1:10.43.0.1 listener.cattle.io/cn-127.0.0.1:127.0.0.1 listener.cattle.io/cn-192.168.56.110:192.168.56.110 listener.cattle.io/cn-__1-f16284:::1 listener.cattle.io/cn-kubernetes:kubernetes listener.cattle.io/cn-kubernetes.default:kubernetes.default listener.cattle.io/cn-kubernetes.default.svc:kubernetes.default.svc listener.cattle.io/cn-kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local listener.cattle.io/cn-localhost:localhost listener.cattle.io/cn-server:server listener.cattle.io/fingerprint:SHA1=296AFC6A252966C067401FF9072578D9CE0A72C7]"
I0322 03:17:30.306107    2308 controllermanager.go:642] "Started controller" controller="garbage-collector-controller"
I0322 03:17:30.306298    2308 garbagecollector.go:155] "Starting controller" controller="garbagecollector"
I0322 03:17:30.306313    2308 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0322 03:17:30.306360    2308 graph_builder.go:294] "Running" component="GraphBuilder"
time="2024-03-22T03:17:30Z" level=warning msg="Unable to fetch coredns config map: configmaps \"coredns\" not found"
I0322 03:17:30.560395    2308 controllermanager.go:642] "Started controller" controller="certificatesigningrequest-signing-controller"
I0322 03:17:30.560787    2308 certificate_controller.go:115] "Starting certificate controller" name="csrsigning-kubelet-serving"
I0322 03:17:30.560856    2308 shared_informer.go:311] Waiting for caches to sync for certificate-csrsigning-kubelet-serving
I0322 03:17:30.561019    2308 certificate_controller.go:115] "Starting certificate controller" name="csrsigning-kubelet-client"
I0322 03:17:30.561027    2308 shared_informer.go:311] Waiting for caches to sync for certificate-csrsigning-kubelet-client
I0322 03:17:30.561187    2308 certificate_controller.go:115] "Starting certificate controller" name="csrsigning-kube-apiserver-client"
I0322 03:17:30.561197    2308 shared_informer.go:311] Waiting for caches to sync for certificate-csrsigning-kube-apiserver-client
I0322 03:17:30.561434    2308 certificate_controller.go:115] "Starting certificate controller" name="csrsigning-legacy-unknown"
I0322 03:17:30.561576    2308 shared_informer.go:311] Waiting for caches to sync for certificate-csrsigning-legacy-unknown
I0322 03:17:30.561724    2308 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/var/lib/rancher/k3s/server/tls/server-ca.nochain.crt::/var/lib/rancher/k3s/server/tls/server-ca.key"
I0322 03:17:30.561906    2308 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/var/lib/rancher/k3s/server/tls/client-ca.nochain.crt::/var/lib/rancher/k3s/server/tls/client-ca.key"
I0322 03:17:30.562073    2308 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/var/lib/rancher/k3s/server/tls/client-ca.nochain.crt::/var/lib/rancher/k3s/server/tls/client-ca.key"
I0322 03:17:30.562229    2308 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/var/lib/rancher/k3s/server/tls/server-ca.nochain.crt::/var/lib/rancher/k3s/server/tls/server-ca.key"
I0322 03:17:30.603392    2308 controllermanager.go:642] "Started controller" controller="certificatesigningrequest-approving-controller"
I0322 03:17:30.603553    2308 controllermanager.go:607] "Warning: controller is disabled" controller="service-lb-controller"
I0322 03:17:30.603711    2308 certificate_controller.go:115] "Starting certificate controller" name="csrapproving"
I0322 03:17:30.603743    2308 shared_informer.go:311] Waiting for caches to sync for certificate-csrapproving
I0322 03:17:30.754284    2308 controllermanager.go:642] "Started controller" controller="endpointslice-controller"
I0322 03:17:30.754377    2308 endpointslice_controller.go:264] "Starting endpoint slice controller"
I0322 03:17:30.754385    2308 shared_informer.go:311] Waiting for caches to sync for endpoint_slice
I0322 03:17:31.072296    2308 event.go:307] "Event occurred" object="server" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodePasswordValidationComplete" message="Deferred node password secret validation complete"
time="2024-03-22T03:17:31Z" level=warning msg="Unable to fetch coredns config map: configmaps \"coredns\" not found"
I0322 03:17:31.374824    2308 controller.go:624] quota admission added evaluator for: addons.k3s.cattle.io
I0322 03:17:31.379651    2308 event.go:307] "Event occurred" object="kube-system/ccm" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/ccm.yaml\""
I0322 03:17:31.405893    2308 event.go:307] "Event occurred" object="kube-system/ccm" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/ccm.yaml\""
I0322 03:17:31.419970    2308 event.go:307] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/coredns.yaml\""
I0322 03:17:31.457981    2308 controller.go:624] quota admission added evaluator for: deployments.apps
I0322 03:17:31.470365    2308 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.43.0.10"}
I0322 03:17:31.471410    2308 event.go:307] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/coredns.yaml\""
I0322 03:17:31.481395    2308 event.go:307] "Event occurred" object="kube-system/local-storage" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/local-storage.yaml\""
I0322 03:17:31.519196    2308 event.go:307] "Event occurred" object="kube-system/local-storage" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/local-storage.yaml\""
I0322 03:17:31.526800    2308 event.go:307] "Event occurred" object="kube-system/aggregated-metrics-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/aggregated-metrics-reader.yaml\""
I0322 03:17:31.570176    2308 event.go:307] "Event occurred" object="kube-system/aggregated-metrics-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/aggregated-metrics-reader.yaml\""
I0322 03:17:31.585119    2308 event.go:307] "Event occurred" object="kube-system/auth-delegator" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/auth-delegator.yaml\""
I0322 03:17:31.594965    2308 event.go:307] "Event occurred" object="kube-system/auth-delegator" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/auth-delegator.yaml\""
I0322 03:17:31.603028    2308 event.go:307] "Event occurred" object="kube-system/auth-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/auth-reader.yaml\""
I0322 03:17:31.608000    2308 event.go:307] "Event occurred" object="kube-system/auth-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/auth-reader.yaml\""
time="2024-03-22T03:17:31Z" level=info msg="Updated coredns node hosts entry [192.168.56.110 server]"
I0322 03:17:31.811104    2308 event.go:307] "Event occurred" object="kube-system/metrics-apiservice" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-apiservice.yaml\""
I0322 03:17:31.818188    2308 event.go:307] "Event occurred" object="kube-system/metrics-apiservice" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-apiservice.yaml\""
I0322 03:17:31.819318    2308 handler.go:275] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
W0322 03:17:31.822127    2308 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:17:31.822592    2308 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
E0322 03:17:31.822801    2308 handler_proxy.go:137] error resolving kube-system/metrics-server: service "metrics-server" not found
I0322 03:17:32.258345    2308 event.go:307] "Event occurred" object="kube-system/metrics-server-deployment" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-deployment.yaml\""
I0322 03:17:32.271425    2308 event.go:307] "Event occurred" object="kube-system/metrics-server-deployment" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-deployment.yaml\""
I0322 03:17:32.558701    2308 serving.go:355] Generated self-signed cert in-memory
I0322 03:17:32.682615    2308 event.go:307] "Event occurred" object="kube-system/metrics-server-service" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-service.yaml\""
I0322 03:17:32.689557    2308 alloc.go:330] "allocated clusterIPs" service="kube-system/metrics-server" clusterIPs={"IPv4":"10.43.17.14"}
I0322 03:17:32.690579    2308 event.go:307] "Event occurred" object="kube-system/metrics-server-service" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-service.yaml\""
time="2024-03-22T03:17:32Z" level=info msg="Running kube-proxy --cluster-cidr=10.42.0.0/16 --conntrack-max-per-core=0 --conntrack-tcp-timeout-close-wait=0s --conntrack-tcp-timeout-established=0s --healthz-bind-address=127.0.0.1 --hostname-override=server --kubeconfig=/var/lib/rancher/k3s/agent/kubeproxy.kubeconfig --proxy-mode=iptables"
W0322 03:17:32.698689    2308 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:17:32.698733    2308 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
E0322 03:17:32.698784    2308 handler_proxy.go:137] error resolving kube-system/metrics-server: endpoints "metrics-server" not found
I0322 03:17:32.701790    2308 node.go:141] Successfully retrieved node IP: 192.168.56.110
I0322 03:17:32.705916    2308 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0322 03:17:32.707533    2308 server_others.go:152] "Using iptables Proxier"
I0322 03:17:32.707581    2308 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0322 03:17:32.707592    2308 server_others.go:438] "Defaulting to no-op detect-local"
I0322 03:17:32.707631    2308 proxier.go:250] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0322 03:17:32.707913    2308 server.go:846] "Version info" version="v1.28.7+k3s1"
I0322 03:17:32.707942    2308 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0322 03:17:32.714342    2308 config.go:188] "Starting service config controller"
I0322 03:17:32.714366    2308 shared_informer.go:311] Waiting for caches to sync for service config
I0322 03:17:32.714382    2308 config.go:97] "Starting endpoint slice config controller"
I0322 03:17:32.714385    2308 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0322 03:17:32.714740    2308 config.go:315] "Starting node config controller"
I0322 03:17:32.714752    2308 shared_informer.go:311] Waiting for caches to sync for node config
I0322 03:17:32.815612    2308 shared_informer.go:318] Caches are synced for node config
I0322 03:17:32.815641    2308 shared_informer.go:318] Caches are synced for service config
I0322 03:17:32.815663    2308 shared_informer.go:318] Caches are synced for endpoint slice config
W0322 03:17:32.820072    2308 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:17:32.820110    2308 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I0322 03:17:32.820132    2308 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0322 03:17:32.822487    2308 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:17:32.822556    2308 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 03:17:32.822566    2308 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0322 03:17:32.923225    2308 serving.go:355] Generated self-signed cert in-memory
I0322 03:17:33.029539    2308 event.go:307] "Event occurred" object="kube-system/resource-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/resource-reader.yaml\""
I0322 03:17:33.041790    2308 event.go:307] "Event occurred" object="kube-system/resource-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/resource-reader.yaml\""
I0322 03:17:33.315594    2308 controllermanager.go:168] Version: v1.28.7+k3s1
I0322 03:17:33.319600    2308 secure_serving.go:213] Serving securely on 127.0.0.1:10258
I0322 03:17:33.319807    2308 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I0322 03:17:33.320016    2308 shared_informer.go:311] Waiting for caches to sync for RequestHeaderAuthRequestController
I0322 03:17:33.319829    2308 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0322 03:17:33.320043    2308 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0322 03:17:33.319839    2308 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0322 03:17:33.320347    2308 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0322 03:17:33.319940    2308 tlsconfig.go:240] "Starting DynamicServingCertificateController"
E0322 03:17:33.334855    2308 controllermanager.go:524] unable to get all supported resources from server: unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
time="2024-03-22T03:17:33Z" level=info msg="Creating  event broadcaster"
I0322 03:17:33.461704    2308 shared_informer.go:318] Caches are synced for RequestHeaderAuthRequestController
I0322 03:17:33.462003    2308 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0322 03:17:33.462139    2308 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0322 03:17:33.466336    2308 event.go:307] "Event occurred" object="kube-system/rolebindings" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/rolebindings.yaml\""
time="2024-03-22T03:17:33Z" level=info msg="Starting /v1, Kind=Node controller"
time="2024-03-22T03:17:33Z" level=info msg="Starting /v1, Kind=Pod controller"
time="2024-03-22T03:17:33Z" level=info msg="Starting apps/v1, Kind=DaemonSet controller"
W0322 03:17:33.486861    2308 controllermanager.go:314] "node-route-controller" is disabled
I0322 03:17:33.487250    2308 controllermanager.go:337] Started "cloud-node-controller"
I0322 03:17:33.487506    2308 controllermanager.go:337] Started "cloud-node-lifecycle-controller"
I0322 03:17:33.487894    2308 controllermanager.go:337] Started "service-lb-controller"
time="2024-03-22T03:17:33Z" level=info msg="Starting discovery.k8s.io/v1, Kind=EndpointSlice controller"
I0322 03:17:33.488749    2308 node_controller.go:165] Sending events to api server.
I0322 03:17:33.488814    2308 node_controller.go:174] Waiting for informer caches to sync
I0322 03:17:33.488841    2308 node_lifecycle_controller.go:113] Sending events to api server
I0322 03:17:33.488922    2308 controller.go:231] Starting service controller
I0322 03:17:33.488942    2308 shared_informer.go:311] Waiting for caches to sync for service
I0322 03:17:33.494864    2308 event.go:307] "Event occurred" object="kube-system/rolebindings" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/rolebindings.yaml\""
I0322 03:17:33.651727    2308 node_controller.go:431] Initializing node server with cloud provider
I0322 03:17:33.652130    2308 shared_informer.go:318] Caches are synced for service
I0322 03:17:33.667850    2308 node_controller.go:502] Successfully initialized node server with cloud provider
I0322 03:17:33.672706    2308 event.go:307] "Event occurred" object="server" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="Synced" message="Node synced successfully"
I0322 03:17:33.811734    2308 event.go:307] "Event occurred" object="kube-system/runtimes" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/runtimes.yaml\""
I0322 03:17:34.017323    2308 event.go:307] "Event occurred" object="kube-system/runtimes" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/runtimes.yaml\""
I0322 03:17:34.204514    2308 serving.go:355] Generated self-signed cert in-memory
I0322 03:17:34.212014    2308 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/traefik.yaml\""
I0322 03:17:34.217717    2308 controller.go:624] quota admission added evaluator for: helmcharts.helm.cattle.io
I0322 03:17:34.230777    2308 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/traefik.yaml\""
I0322 03:17:34.251426    2308 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
I0322 03:17:34.251530    2308 event.go:307] "Event occurred" object="kube-system/traefik-crd" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik-crd"
I0322 03:17:34.309072    2308 controller.go:624] quota admission added evaluator for: jobs.batch
time="2024-03-22T03:17:34Z" level=info msg="Tunnel authorizer set Kubelet Port 10250"
time="2024-03-22T03:17:34Z" level=error msg="error syncing 'kube-system/traefik-crd': handler helm-controller-chart-registration: helmcharts.helm.cattle.io \"traefik-crd\" not found, requeuing"
I0322 03:17:34.342312    2308 event.go:307] "Event occurred" object="kube-system/traefik-crd" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik-crd"
I0322 03:17:34.351827    2308 event.go:307] "Event occurred" object="kube-system/traefik-crd" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik-crd"
time="2024-03-22T03:17:34Z" level=error msg="error syncing 'kube-system/traefik': handler helm-controller-chart-registration: helmcharts.helm.cattle.io \"traefik\" not found, requeuing"
I0322 03:17:34.360814    2308 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
I0322 03:17:34.377188    2308 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
I0322 03:17:34.403827    2308 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
I0322 03:17:34.662668    2308 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.7+k3s1"
I0322 03:17:34.662701    2308 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0322 03:17:34.667126    2308 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0322 03:17:34.667221    2308 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I0322 03:17:34.667234    2308 shared_informer.go:311] Waiting for caches to sync for RequestHeaderAuthRequestController
I0322 03:17:34.667249    2308 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0322 03:17:34.668951    2308 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0322 03:17:34.668975    2308 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0322 03:17:34.668995    2308 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0322 03:17:34.669004    2308 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0322 03:17:34.767316    2308 shared_informer.go:318] Caches are synced for RequestHeaderAuthRequestController
I0322 03:17:34.769898    2308 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0322 03:17:34.769913    2308 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
time="2024-03-22T03:17:35Z" level=info msg="Stopped tunnel to 127.0.0.1:6443"
time="2024-03-22T03:17:35Z" level=info msg="Connecting to proxy" url="wss://192.168.56.110:6443/v1-k3s/connect"
time="2024-03-22T03:17:35Z" level=info msg="Proxy done" err="context canceled" url="wss://127.0.0.1:6443/v1-k3s/connect"
time="2024-03-22T03:17:35Z" level=info msg="error in remotedialer server [400]: websocket: close 1006 (abnormal closure): unexpected EOF"
time="2024-03-22T03:17:35Z" level=info msg="Handling backend connection request [server]"
I0322 03:17:40.982898    2308 range_allocator.go:111] "No Secondary Service CIDR provided. Skipping filtering out secondary service addresses"
I0322 03:17:40.983573    2308 controllermanager.go:642] "Started controller" controller="node-ipam-controller"
I0322 03:17:40.983914    2308 node_ipam_controller.go:162] "Starting ipam controller"
I0322 03:17:40.984055    2308 shared_informer.go:311] Waiting for caches to sync for node
I0322 03:17:40.986939    2308 node_lifecycle_controller.go:431] "Controller will reconcile labels"
I0322 03:17:40.987127    2308 controllermanager.go:642] "Started controller" controller="node-lifecycle-controller"
I0322 03:17:40.987301    2308 node_lifecycle_controller.go:465] "Sending events to api server"
I0322 03:17:40.987376    2308 node_lifecycle_controller.go:476] "Starting node controller"
I0322 03:17:40.987389    2308 shared_informer.go:311] Waiting for caches to sync for taint
I0322 03:17:41.000590    2308 controllermanager.go:642] "Started controller" controller="persistentvolume-binder-controller"
I0322 03:17:41.000861    2308 pv_controller_base.go:319] "Starting persistent volume controller"
I0322 03:17:41.003494    2308 shared_informer.go:311] Waiting for caches to sync for persistent volume
I0322 03:17:41.008728    2308 controllermanager.go:642] "Started controller" controller="persistentvolume-expander-controller"
I0322 03:17:41.009020    2308 expand_controller.go:328] "Starting expand controller"
I0322 03:17:41.009111    2308 shared_informer.go:311] Waiting for caches to sync for expand
I0322 03:17:41.016959    2308 controllermanager.go:642] "Started controller" controller="ttl-after-finished-controller"
I0322 03:17:41.017146    2308 ttlafterfinished_controller.go:109] "Starting TTL after finished controller"
I0322 03:17:41.017168    2308 shared_informer.go:311] Waiting for caches to sync for TTL after finished
I0322 03:17:41.024524    2308 controllermanager.go:642] "Started controller" controller="deployment-controller"
I0322 03:17:41.024844    2308 deployment_controller.go:168] "Starting controller" controller="deployment"
I0322 03:17:41.024933    2308 shared_informer.go:311] Waiting for caches to sync for deployment
I0322 03:17:41.027114    2308 controllermanager.go:642] "Started controller" controller="certificatesigningrequest-cleaner-controller"
I0322 03:17:41.027290    2308 cleaner.go:83] "Starting CSR cleaner controller"
I0322 03:17:41.033070    2308 controllermanager.go:642] "Started controller" controller="ttl-controller"
I0322 03:17:41.033189    2308 controllermanager.go:607] "Warning: controller is disabled" controller="bootstrap-signer-controller"
I0322 03:17:41.033374    2308 controllermanager.go:607] "Warning: controller is disabled" controller="cloud-node-lifecycle-controller"
I0322 03:17:41.033097    2308 ttl_controller.go:124] "Starting TTL controller"
I0322 03:17:41.036572    2308 shared_informer.go:311] Waiting for caches to sync for TTL
I0322 03:17:41.040732    2308 controllermanager.go:642] "Started controller" controller="persistentvolume-attach-detach-controller"
I0322 03:17:41.040957    2308 attach_detach_controller.go:337] "Starting attach detach controller"
I0322 03:17:41.046416    2308 shared_informer.go:311] Waiting for caches to sync for attach detach
I0322 03:17:41.046288    2308 controllermanager.go:642] "Started controller" controller="persistentvolumeclaim-protection-controller"
I0322 03:17:41.046300    2308 pvc_protection_controller.go:102] "Starting PVC protection controller"
I0322 03:17:41.046771    2308 shared_informer.go:311] Waiting for caches to sync for PVC protection
I0322 03:17:41.052907    2308 controllermanager.go:642] "Started controller" controller="ephemeral-volume-controller"
I0322 03:17:41.053188    2308 controller.go:169] "Starting ephemeral volume controller"
I0322 03:17:41.053247    2308 shared_informer.go:311] Waiting for caches to sync for ephemeral
I0322 03:17:41.058970    2308 controllermanager.go:642] "Started controller" controller="job-controller"
I0322 03:17:41.059203    2308 job_controller.go:226] "Starting job controller"
I0322 03:17:41.059220    2308 shared_informer.go:311] Waiting for caches to sync for job
I0322 03:17:41.183658    2308 controllermanager.go:642] "Started controller" controller="statefulset-controller"
I0322 03:17:41.185863    2308 stateful_set.go:161] "Starting stateful set controller"
I0322 03:17:41.185896    2308 shared_informer.go:311] Waiting for caches to sync for stateful set
E0322 03:17:41.442071    2308 namespaced_resources_deleter.go:162] unable to get all supported resources from server: unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I0322 03:17:41.442188    2308 controllermanager.go:642] "Started controller" controller="namespace-controller"
I0322 03:17:41.442310    2308 namespace_controller.go:197] "Starting namespace controller"
I0322 03:17:41.442323    2308 shared_informer.go:311] Waiting for caches to sync for namespace
I0322 03:17:41.585706    2308 controllermanager.go:642] "Started controller" controller="clusterrole-aggregation-controller"
I0322 03:17:41.586231    2308 clusterroleaggregation_controller.go:189] "Starting ClusterRoleAggregator controller"
I0322 03:17:41.586293    2308 shared_informer.go:311] Waiting for caches to sync for ClusterRoleAggregator
I0322 03:17:41.605026    2308 shared_informer.go:311] Waiting for caches to sync for resource quota
I0322 03:17:41.631468    2308 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:17:41.632311    2308 actual_state_of_world.go:547] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"server\" does not exist"
I0322 03:17:41.632943    2308 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:17:41.634851    2308 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0322 03:17:41.636617    2308 shared_informer.go:318] Caches are synced for TTL
I0322 03:17:41.642379    2308 shared_informer.go:318] Caches are synced for namespace
I0322 03:17:41.669812    2308 shared_informer.go:318] Caches are synced for PV protection
I0322 03:17:41.669967    2308 shared_informer.go:318] Caches are synced for crt configmap
I0322 03:17:41.670082    2308 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I0322 03:17:41.670172    2308 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I0322 03:17:41.670266    2308 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I0322 03:17:41.670359    2308 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0322 03:17:41.680001    2308 shared_informer.go:318] Caches are synced for service account
I0322 03:17:41.684179    2308 shared_informer.go:318] Caches are synced for node
I0322 03:17:41.684216    2308 range_allocator.go:174] "Sending events to api server"
I0322 03:17:41.684232    2308 range_allocator.go:178] "Starting range CIDR allocator"
I0322 03:17:41.684235    2308 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I0322 03:17:41.684239    2308 shared_informer.go:318] Caches are synced for cidrallocator
I0322 03:17:41.686430    2308 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I0322 03:17:41.690322    2308 shared_informer.go:318] Caches are synced for cronjob
time="2024-03-22T03:17:41Z" level=info msg="Flannel found PodCIDR assigned for node server"
I0322 03:17:41.694246    2308 range_allocator.go:380] "Set node PodCIDR" node="server" podCIDRs=["10.42.0.0/24"]
time="2024-03-22T03:17:41Z" level=info msg="The interface enp0s3 with ipv4 address 10.0.2.15 will be used by flannel"
I0322 03:17:41.703995    2308 kube.go:139] Waiting 10m0s for node controller to sync
I0322 03:17:41.704158    2308 kube.go:461] Starting kube subnet manager
I0322 03:17:41.708923    2308 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I0322 03:17:41.710552    2308 shared_informer.go:318] Caches are synced for expand
I0322 03:17:41.710599    2308 shared_informer.go:318] Caches are synced for certificate-csrapproving
I0322 03:17:41.718266    2308 shared_informer.go:318] Caches are synced for TTL after finished
I0322 03:17:41.727579    2308 shared_informer.go:318] Caches are synced for GC
I0322 03:17:41.765431    2308 shared_informer.go:318] Caches are synced for endpoint
I0322 03:17:41.765596    2308 shared_informer.go:318] Caches are synced for endpoint_slice
I0322 03:17:41.765735    2308 shared_informer.go:318] Caches are synced for attach detach
I0322 03:17:41.765948    2308 shared_informer.go:318] Caches are synced for PVC protection
I0322 03:17:41.766150    2308 shared_informer.go:318] Caches are synced for ephemeral
I0322 03:17:41.766255    2308 shared_informer.go:318] Caches are synced for HPA
I0322 03:17:41.766393    2308 shared_informer.go:318] Caches are synced for daemon sets
I0322 03:17:41.765957    2308 shared_informer.go:318] Caches are synced for job
I0322 03:17:41.767771    2308 shared_informer.go:318] Caches are synced for ReplicationController
I0322 03:17:41.800081    2308 shared_informer.go:318] Caches are synced for taint
I0322 03:17:41.800332    2308 node_lifecycle_controller.go:1225] "Initializing eviction metric for zone" zone=""
I0322 03:17:41.800144    2308 shared_informer.go:318] Caches are synced for ReplicaSet
I0322 03:17:41.800501    2308 taint_manager.go:205] "Starting NoExecuteTaintManager"
I0322 03:17:41.800526    2308 taint_manager.go:210] "Sending events to api server"
I0322 03:17:41.800538    2308 node_lifecycle_controller.go:877] "Missing timestamp for Node. Assuming now as a timestamp" node="server"
I0322 03:17:41.800750    2308 node_lifecycle_controller.go:1071] "Controller detected that zone is now in new state" zone="" newState="Normal"
I0322 03:17:41.801158    2308 event.go:307] "Event occurred" object="server" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node server event: Registered Node server in Controller"
I0322 03:17:41.803540    2308 shared_informer.go:318] Caches are synced for persistent volume
I0322 03:17:41.806411    2308 shared_informer.go:318] Caches are synced for resource quota
time="2024-03-22T03:17:41Z" level=info msg="Starting network policy controller version v2.0.1, built on 2024-02-29T20:36:21Z, go1.21.7"
I0322 03:17:41.818386    2308 network_policy_controller.go:164] Starting network policy controller
I0322 03:17:41.825249    2308 shared_informer.go:318] Caches are synced for deployment
I0322 03:17:41.836461    2308 shared_informer.go:318] Caches are synced for resource quota
I0322 03:17:41.853743    2308 shared_informer.go:318] Caches are synced for disruption
I0322 03:17:41.855294    2308 network_policy_controller.go:176] Starting network policy controller full sync goroutine
I0322 03:17:41.887703    2308 shared_informer.go:318] Caches are synced for stateful set
I0322 03:17:42.287306    2308 shared_informer.go:318] Caches are synced for garbage collector
I0322 03:17:42.287498    2308 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
W0322 03:17:42.317447    2308 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:17:42.317517    2308 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
E0322 03:17:42.317577    2308 handler_proxy.go:137] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
I0322 03:17:42.335182    2308 shared_informer.go:318] Caches are synced for garbage collector
I0322 03:17:42.341404    2308 event.go:307] "Event occurred" object="kube-system/helm-install-traefik-crd" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: helm-install-traefik-crd-m54x6"
I0322 03:17:42.341522    2308 event.go:307] "Event occurred" object="kube-system/helm-install-traefik" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: helm-install-traefik-t9wf9"
I0322 03:17:42.349094    2308 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:17:42.357422    2308 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:17:42.357445    2308 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:17:42.351294    2308 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:17:42.357456    2308 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:17:42.354507    2308 topology_manager.go:215] "Topology Admit Handler" podUID="416389e0-ea56-4446-afa6-333902153fd1" podNamespace="kube-system" podName="helm-install-traefik-crd-m54x6"
I0322 03:17:42.365241    2308 topology_manager.go:215] "Topology Admit Handler" podUID="598e1d6b-dd9f-40b6-8abc-7ed9692a3a85" podNamespace="kube-system" podName="helm-install-traefik-t9wf9"
I0322 03:17:42.365835    2308 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:17:42.391665    2308 controller.go:624] quota admission added evaluator for: replicasets.apps
W0322 03:17:42.404429    2308 watcher.go:93] Error while processing event ("/sys/fs/cgroup/devices/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod598e1d6b_dd9f_40b6_8abc_7ed9692a3a85.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/devices/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod598e1d6b_dd9f_40b6_8abc_7ed9692a3a85.slice: no such file or directory
I0322 03:17:42.407521    2308 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:17:42.408247    2308 event.go:307] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set coredns-6799fbcd5 to 1"
I0322 03:17:42.408264    2308 event.go:307] "Event occurred" object="kube-system/local-path-provisioner" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set local-path-provisioner-6c86858495 to 1"
I0322 03:17:42.408271    2308 event.go:307] "Event occurred" object="kube-system/metrics-server" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set metrics-server-67c658944b to 1"
I0322 03:17:42.430644    2308 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:17:42.486940    2308 event.go:307] "Event occurred" object="kube-system/metrics-server-67c658944b" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: metrics-server-67c658944b-8mmfz"
I0322 03:17:42.486972    2308 event.go:307] "Event occurred" object="kube-system/coredns-6799fbcd5" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: coredns-6799fbcd5-tqlcn"
I0322 03:17:42.486979    2308 event.go:307] "Event occurred" object="kube-system/local-path-provisioner-6c86858495" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: local-path-provisioner-6c86858495-mpn68"
I0322 03:17:42.491897    2308 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-kxvt2\" (UniqueName: \"kubernetes.io/projected/416389e0-ea56-4446-afa6-333902153fd1-kube-api-access-kxvt2\") pod \"helm-install-traefik-crd-m54x6\" (UID: \"416389e0-ea56-4446-afa6-333902153fd1\") " pod="kube-system/helm-install-traefik-crd-m54x6"
I0322 03:17:42.508103    2308 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/598e1d6b-dd9f-40b6-8abc-7ed9692a3a85-klipper-cache\") pod \"helm-install-traefik-t9wf9\" (UID: \"598e1d6b-dd9f-40b6-8abc-7ed9692a3a85\") " pod="kube-system/helm-install-traefik-t9wf9"
I0322 03:17:42.501580    2308 topology_manager.go:215] "Topology Admit Handler" podUID="2a800b99-aa73-4f14-b7d3-23efaffcaa4b" podNamespace="kube-system" podName="local-path-provisioner-6c86858495-mpn68"
I0322 03:17:42.503771    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="95.799223ms"
I0322 03:17:42.508768    2308 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/416389e0-ea56-4446-afa6-333902153fd1-tmp\") pod \"helm-install-traefik-crd-m54x6\" (UID: \"416389e0-ea56-4446-afa6-333902153fd1\") " pod="kube-system/helm-install-traefik-crd-m54x6"
I0322 03:17:42.508823    2308 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"content\" (UniqueName: \"kubernetes.io/configmap/416389e0-ea56-4446-afa6-333902153fd1-content\") pod \"helm-install-traefik-crd-m54x6\" (UID: \"416389e0-ea56-4446-afa6-333902153fd1\") " pod="kube-system/helm-install-traefik-crd-m54x6"
I0322 03:17:42.508843    2308 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"values\" (UniqueName: \"kubernetes.io/secret/598e1d6b-dd9f-40b6-8abc-7ed9692a3a85-values\") pod \"helm-install-traefik-t9wf9\" (UID: \"598e1d6b-dd9f-40b6-8abc-7ed9692a3a85\") " pod="kube-system/helm-install-traefik-t9wf9"
I0322 03:17:42.508860    2308 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-w58d6\" (UniqueName: \"kubernetes.io/projected/598e1d6b-dd9f-40b6-8abc-7ed9692a3a85-kube-api-access-w58d6\") pod \"helm-install-traefik-t9wf9\" (UID: \"598e1d6b-dd9f-40b6-8abc-7ed9692a3a85\") " pod="kube-system/helm-install-traefik-t9wf9"
I0322 03:17:42.508878    2308 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"values\" (UniqueName: \"kubernetes.io/secret/416389e0-ea56-4446-afa6-333902153fd1-values\") pod \"helm-install-traefik-crd-m54x6\" (UID: \"416389e0-ea56-4446-afa6-333902153fd1\") " pod="kube-system/helm-install-traefik-crd-m54x6"
I0322 03:17:42.508894    2308 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/598e1d6b-dd9f-40b6-8abc-7ed9692a3a85-klipper-config\") pod \"helm-install-traefik-t9wf9\" (UID: \"598e1d6b-dd9f-40b6-8abc-7ed9692a3a85\") " pod="kube-system/helm-install-traefik-t9wf9"
I0322 03:17:42.508910    2308 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/598e1d6b-dd9f-40b6-8abc-7ed9692a3a85-tmp\") pod \"helm-install-traefik-t9wf9\" (UID: \"598e1d6b-dd9f-40b6-8abc-7ed9692a3a85\") " pod="kube-system/helm-install-traefik-t9wf9"
I0322 03:17:42.508924    2308 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"content\" (UniqueName: \"kubernetes.io/configmap/598e1d6b-dd9f-40b6-8abc-7ed9692a3a85-content\") pod \"helm-install-traefik-t9wf9\" (UID: \"598e1d6b-dd9f-40b6-8abc-7ed9692a3a85\") " pod="kube-system/helm-install-traefik-t9wf9"
I0322 03:17:42.508941    2308 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/416389e0-ea56-4446-afa6-333902153fd1-klipper-cache\") pod \"helm-install-traefik-crd-m54x6\" (UID: \"416389e0-ea56-4446-afa6-333902153fd1\") " pod="kube-system/helm-install-traefik-crd-m54x6"
I0322 03:17:42.508960    2308 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/416389e0-ea56-4446-afa6-333902153fd1-klipper-config\") pod \"helm-install-traefik-crd-m54x6\" (UID: \"416389e0-ea56-4446-afa6-333902153fd1\") " pod="kube-system/helm-install-traefik-crd-m54x6"
I0322 03:17:42.508990    2308 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/598e1d6b-dd9f-40b6-8abc-7ed9692a3a85-klipper-helm\") pod \"helm-install-traefik-t9wf9\" (UID: \"598e1d6b-dd9f-40b6-8abc-7ed9692a3a85\") " pod="kube-system/helm-install-traefik-t9wf9"
I0322 03:17:42.509005    2308 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/416389e0-ea56-4446-afa6-333902153fd1-klipper-helm\") pod \"helm-install-traefik-crd-m54x6\" (UID: \"416389e0-ea56-4446-afa6-333902153fd1\") " pod="kube-system/helm-install-traefik-crd-m54x6"
I0322 03:17:42.509133    2308 topology_manager.go:215] "Topology Admit Handler" podUID="66fee466-3482-40be-9504-885ffe98a974" podNamespace="kube-system" podName="metrics-server-67c658944b-8mmfz"
I0322 03:17:42.509184    2308 topology_manager.go:215] "Topology Admit Handler" podUID="24c650bf-5c3c-460f-96f8-53aaa5b4f5a7" podNamespace="kube-system" podName="coredns-6799fbcd5-tqlcn"
I0322 03:17:42.532363    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="123.981397ms"
I0322 03:17:42.532772    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="124.59373ms"
I0322 03:17:42.532945    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="24.517608ms"
I0322 03:17:42.534278    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="29.591s"
I0322 03:17:42.616222    2308 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/24c650bf-5c3c-460f-96f8-53aaa5b4f5a7-config-volume\") pod \"coredns-6799fbcd5-tqlcn\" (UID: \"24c650bf-5c3c-460f-96f8-53aaa5b4f5a7\") " pod="kube-system/coredns-6799fbcd5-tqlcn"
I0322 03:17:42.616367    2308 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-m4xgk\" (UniqueName: \"kubernetes.io/projected/2a800b99-aa73-4f14-b7d3-23efaffcaa4b-kube-api-access-m4xgk\") pod \"local-path-provisioner-6c86858495-mpn68\" (UID: \"2a800b99-aa73-4f14-b7d3-23efaffcaa4b\") " pod="kube-system/local-path-provisioner-6c86858495-mpn68"
I0322 03:17:42.616590    2308 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/2a800b99-aa73-4f14-b7d3-23efaffcaa4b-config-volume\") pod \"local-path-provisioner-6c86858495-mpn68\" (UID: \"2a800b99-aa73-4f14-b7d3-23efaffcaa4b\") " pod="kube-system/local-path-provisioner-6c86858495-mpn68"
I0322 03:17:42.616695    2308 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp-dir\" (UniqueName: \"kubernetes.io/empty-dir/66fee466-3482-40be-9504-885ffe98a974-tmp-dir\") pod \"metrics-server-67c658944b-8mmfz\" (UID: \"66fee466-3482-40be-9504-885ffe98a974\") " pod="kube-system/metrics-server-67c658944b-8mmfz"
I0322 03:17:42.616759    2308 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-lzk7g\" (UniqueName: \"kubernetes.io/projected/66fee466-3482-40be-9504-885ffe98a974-kube-api-access-lzk7g\") pod \"metrics-server-67c658944b-8mmfz\" (UID: \"66fee466-3482-40be-9504-885ffe98a974\") " pod="kube-system/metrics-server-67c658944b-8mmfz"
I0322 03:17:42.616916    2308 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"custom-config-volume\" (UniqueName: \"kubernetes.io/configmap/24c650bf-5c3c-460f-96f8-53aaa5b4f5a7-custom-config-volume\") pod \"coredns-6799fbcd5-tqlcn\" (UID: \"24c650bf-5c3c-460f-96f8-53aaa5b4f5a7\") " pod="kube-system/coredns-6799fbcd5-tqlcn"
I0322 03:17:42.616981    2308 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-6nnd9\" (UniqueName: \"kubernetes.io/projected/24c650bf-5c3c-460f-96f8-53aaa5b4f5a7-kube-api-access-6nnd9\") pod \"coredns-6799fbcd5-tqlcn\" (UID: \"24c650bf-5c3c-460f-96f8-53aaa5b4f5a7\") " pod="kube-system/coredns-6799fbcd5-tqlcn"
I0322 03:17:42.647231    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="114.825803ms"
I0322 03:17:42.647836    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="115.043064ms"
I0322 03:17:42.708355    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="125.004s"
I0322 03:17:42.728237    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="80.941783ms"
I0322 03:17:42.728603    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="197.773s"
I0322 03:17:42.728816    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="80.922325ms"
I0322 03:17:42.729030    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="142.477s"
I0322 03:17:42.761653    2308 kube.go:146] Node controller sync successful
I0322 03:17:42.761725    2308 vxlan.go:141] VXLAN config: VNI=1 Port=0 GBP=false Learning=false DirectRouting=false
I0322 03:17:42.769204    2308 kube.go:621] List of node(server) annotations: map[string]string{"alpha.kubernetes.io/provided-node-ip":"192.168.56.110", "k3s.io/hostname":"server", "k3s.io/internal-ip":"192.168.56.110", "k3s.io/node-args":"[\"server\",\"--node-ip\",\"192.168.56.110\",\"--write-kubeconfig-mode\",\"644\",\"--log\",\"/vagrant/logs/server.logs\"]", "k3s.io/node-config-hash":"OEDLXPLCFN65I5K4IWF2M6JMEKD34L47WZJH7YZKLHVRXJTJG6TQ====", "k3s.io/node-env":"{\"K3S_DATA_DIR\":\"/var/lib/rancher/k3s/data/a3b46c0299091b71bfcc617b1e1fec1845c13bdd848584ceb39d2e700e702a4b\"}", "node.alpha.kubernetes.io/ttl":"0", "volumes.kubernetes.io/controller-managed-attach-detach":"true"}
I0322 03:17:42.818538    2308 kube.go:482] Creating the node lease for IPv4. This is the n.Spec.PodCIDRs: [10.42.0.0/24]
time="2024-03-22T03:17:42Z" level=info msg="Wrote flannel subnet file to /run/flannel/subnet.env"
time="2024-03-22T03:17:42Z" level=info msg="Running flannel backend."
I0322 03:17:42.842110    2308 vxlan_network.go:65] watching for new subnet leases
I0322 03:17:42.842317    2308 iptables.go:290] generated 3 rules
I0322 03:17:42.848143    2308 iptables.go:290] generated 7 rules
I0322 03:17:42.866877    2308 iptables.go:283] bootstrap done
I0322 03:17:42.892423    2308 iptables.go:283] bootstrap done
W0322 03:17:43.323326    2308 handler_proxy.go:93] no RequestInfo found in the context
W0322 03:17:43.323337    2308 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:17:43.323728    2308 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I0322 03:17:43.323738    2308 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E0322 03:17:43.323757    2308 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 03:17:43.324982    2308 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0322 03:17:48.893671    2308 kuberuntime_manager.go:1528] "Updating runtime config through cri with podcidr" CIDR="10.42.0.0/24"
I0322 03:17:48.894574    2308 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.42.0.0/24"
I0322 03:17:53.241367    2308 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/local-path-provisioner-6c86858495-mpn68" containerName="local-path-provisioner"
I0322 03:17:53.778340    2308 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/local-path-provisioner-6c86858495-mpn68" podStartSLOduration=4.782023505 podCreationTimestamp="2024-03-22 03:17:42 +0000 UTC" firstStartedPulling="2024-03-22 03:17:46.22756481 +0000 UTC m=+21.922378005" lastFinishedPulling="2024-03-22 03:17:53.223369539 +0000 UTC m=+28.918182737" observedRunningTime="2024-03-22 03:17:53.776672284 +0000 UTC m=+29.471485494" watchObservedRunningTime="2024-03-22 03:17:53.777828237 +0000 UTC m=+29.472641447"
I0322 03:17:53.792962    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="12.24795ms"
I0322 03:17:53.793081    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="61.176s"
I0322 03:18:02.349919    2308 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/metrics-server-67c658944b-8mmfz" containerName="metrics-server"
I0322 03:18:02.873364    2308 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/metrics-server-67c658944b-8mmfz" podStartSLOduration=4.728216669 podCreationTimestamp="2024-03-22 03:17:42 +0000 UTC" firstStartedPulling="2024-03-22 03:17:46.20380568 +0000 UTC m=+21.898618870" lastFinishedPulling="2024-03-22 03:18:02.346280976 +0000 UTC m=+38.041094174" observedRunningTime="2024-03-22 03:18:02.869893324 +0000 UTC m=+38.564706522" watchObservedRunningTime="2024-03-22 03:18:02.870691973 +0000 UTC m=+38.565505172"
I0322 03:18:02.885377    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="2.083618ms"
I0322 03:18:03.833513    2308 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/coredns-6799fbcd5-tqlcn" containerName="coredns"
I0322 03:18:04.915226    2308 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/coredns-6799fbcd5-tqlcn" podStartSLOduration=5.29751816 podCreationTimestamp="2024-03-22 03:17:42 +0000 UTC" firstStartedPulling="2024-03-22 03:17:46.212244117 +0000 UTC m=+21.907057312" lastFinishedPulling="2024-03-22 03:18:03.829306599 +0000 UTC m=+39.524119791" observedRunningTime="2024-03-22 03:18:04.897452976 +0000 UTC m=+40.592266180" watchObservedRunningTime="2024-03-22 03:18:04.914580639 +0000 UTC m=+40.609393837"
I0322 03:18:04.934402    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="90.282s"
I0322 03:18:04.982945    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="48.496031ms"
I0322 03:18:05.000694    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="76.686s"
I0322 03:18:05.052452    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="61.426225ms"
I0322 03:18:05.056874    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="72.727s"
I0322 03:18:05.145372    2308 handler.go:275] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0322 03:18:23.894774    2308 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/helm-install-traefik-t9wf9" containerName="helm"
I0322 03:18:23.895082    2308 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/helm-install-traefik-crd-m54x6" containerName="helm"
I0322 03:18:25.106845    2308 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:18:25.137355    2308 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:18:25.140926    2308 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/helm-install-traefik-crd-m54x6" podStartSLOduration=5.503967202 podCreationTimestamp="2024-03-22 03:17:42 +0000 UTC" firstStartedPulling="2024-03-22 03:17:46.244965047 +0000 UTC m=+21.939778242" lastFinishedPulling="2024-03-22 03:18:23.880128541 +0000 UTC m=+59.574941738" observedRunningTime="2024-03-22 03:18:25.109062301 +0000 UTC m=+60.803875504" watchObservedRunningTime="2024-03-22 03:18:25.139130698 +0000 UTC m=+60.833943892"
I0322 03:18:26.111646    2308 scope.go:117] "RemoveContainer" containerID="cc95254329b570eb8b224b74723c7eaab983af585a148e5540fdb01d7e053bab"
I0322 03:18:26.137455    2308 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/helm-install-traefik-t9wf9" containerName="helm"
I0322 03:18:26.209705    2308 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:18:26.222346    2308 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:18:26.226610    2308 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:18:27.116687    2308 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0322 03:18:27.150806    2308 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0322 03:18:27.161297    2308 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0322 03:18:27.208866    2308 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:18:27.226085    2308 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/helm-install-traefik-t9wf9" podStartSLOduration=7.572216039 podCreationTimestamp="2024-03-22 03:17:42 +0000 UTC" firstStartedPulling="2024-03-22 03:17:46.238449409 +0000 UTC m=+21.933262604" lastFinishedPulling="2024-03-22 03:18:23.883162315 +0000 UTC m=+59.577975512" observedRunningTime="2024-03-22 03:18:25.141495117 +0000 UTC m=+60.836308316" watchObservedRunningTime="2024-03-22 03:18:27.216928947 +0000 UTC m=+62.911742155"
I0322 03:18:27.270310    2308 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0322 03:18:27.413177    2308 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0322 03:18:27.426695    2308 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0322 03:18:27.584463    2308 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0322 03:18:27.809454    2308 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0322 03:18:27.824027    2308 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0322 03:18:27.978634    2308 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0322 03:18:28.033650    2308 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0322 03:18:28.033717    2308 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0322 03:18:28.051246    2308 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0322 03:18:28.065034    2308 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0322 03:18:28.082664    2308 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0322 03:18:28.096696    2308 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0322 03:18:28.111862    2308 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0322 03:18:28.165173    2308 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0322 03:18:28.202139    2308 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:18:28.430692    2308 scope.go:117] "RemoveContainer" containerID="cc95254329b570eb8b224b74723c7eaab983af585a148e5540fdb01d7e053bab"
I0322 03:18:29.212797    2308 scope.go:117] "RemoveContainer" containerID="99f14d58a0ef229261a3734019bc51448bd530d4cf41690d804f452d725e4532"
E0322 03:18:29.268703    2308 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"helm\" with CrashLoopBackOff: \"back-off 10s restarting failed container=helm pod=helm-install-traefik-t9wf9_kube-system(598e1d6b-dd9f-40b6-8abc-7ed9692a3a85)\"" pod="kube-system/helm-install-traefik-t9wf9" podUID="598e1d6b-dd9f-40b6-8abc-7ed9692a3a85"
I0322 03:18:30.818027    2308 trace.go:236] Trace[1885439838]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:e34a3485-06b3-46e0-b656-278a1d438e58,client:127.0.0.1,protocol:HTTP/2.0,resource:jobs,scope:resource,url:/apis/batch/v1/namespaces/kube-system/jobs/helm-install-traefik-crd/status,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:job-controller,verb:PUT (22-Mar-2024 03:18:29.341) (total time: 1275ms):
Trace[1885439838]: ["GuaranteedUpdate etcd3" audit-id:e34a3485-06b3-46e0-b656-278a1d438e58,key:/jobs/kube-system/helm-install-traefik-crd,type:*batch.Job,resource:jobs.batch 1460ms (03:18:29.353)
Trace[1885439838]:  ---"About to Encode" 252ms (03:18:29.606)
Trace[1885439838]:  ---"Txn call completed" 250ms (03:18:29.866)
Trace[1885439838]:  ---"decode succeeded" len:6660 326ms (03:18:30.193)]
Trace[1885439838]: ---"Write to database call succeeded" len:6663 123ms (03:18:30.317)
Trace[1885439838]: ---"Writing http response done" 296ms (03:18:30.614)
Trace[1885439838]: [1.275787139s] [1.275787139s] END
I0322 03:18:30.901351    2308 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:18:31.173299    2308 trace.go:236] Trace[1348919224]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:42b29eeb-5557-4cf0-b1d3-b9335b42fa16,client:127.0.0.1,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/kube-system/pods/helm-install-traefik-t9wf9,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:18:29.470) (total time: 1643ms):
Trace[1348919224]: ---"About to write a response" 981ms (03:18:30.452)
Trace[1348919224]: ---"Writing http response done" 661ms (03:18:31.113)
Trace[1348919224]: [1.643587878s] [1.643587878s] END
E0322 03:18:31.502677    2308 kubelet.go:2477] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.131s"
I0322 03:18:31.630411    2308 trace.go:236] Trace[2027214987]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:77ce1ddf-4775-4f98-b7d3-c827812e8ade,client:127.0.0.1,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:POST (22-Mar-2024 03:18:29.470) (total time: 2153ms):
Trace[2027214987]: ["Create etcd3" audit-id:77ce1ddf-4775-4f98-b7d3-c827812e8ade,key:/events/kube-system/helm-install-traefik-t9wf9.17bef7edd2f699bd,type:*core.Event,resource:events 2022ms (03:18:29.602)
Trace[2027214987]:  ---"TransformToStorage succeeded" 268ms (03:18:29.909)
Trace[2027214987]:  ---"Txn call succeeded" 1270ms (03:18:31.179)
Trace[2027214987]:  ---"decode succeeded" len:728 92ms (03:18:31.272)]
Trace[2027214987]: ---"Write to database call succeeded" len:421 33ms (03:18:31.307)
Trace[2027214987]: ---"Writing http response done" 314ms (03:18:31.622)
Trace[2027214987]: [2.153694489s] [2.153694489s] END
I0322 03:18:31.939053    2308 trace.go:236] Trace[924086074]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:a5f95d3c-74e3-49c1-8e0b-6092faace4cf,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/server,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PUT (22-Mar-2024 03:18:29.520) (total time: 2377ms):
Trace[924086074]: ---"limitedReadBody succeeded" len:465 36ms (03:18:29.556)
Trace[924086074]: ---"About to store object in database" 64ms (03:18:29.629)
Trace[924086074]: ["GuaranteedUpdate etcd3" audit-id:a5f95d3c-74e3-49c1-8e0b-6092faace4cf,key:/leases/kube-node-lease/server,type:*coordination.Lease,resource:leases.coordination.k8s.io 2256ms (03:18:29.642)
Trace[924086074]:  ---"About to Encode" 2006ms (03:18:31.653)
Trace[924086074]:  ---"Txn call completed" 219ms (03:18:31.877)]
Trace[924086074]: [2.377454555s] [2.377454555s] END
I0322 03:18:31.980262    2308 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="0f69a54664ce7366d7ddf37b43766454e20df747128023ad7e35c3a35b278af9"
I0322 03:18:32.503376    2308 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-kxvt2\" (UniqueName: \"kubernetes.io/projected/416389e0-ea56-4446-afa6-333902153fd1-kube-api-access-kxvt2\") pod \"416389e0-ea56-4446-afa6-333902153fd1\" (UID: \"416389e0-ea56-4446-afa6-333902153fd1\") "
I0322 03:18:32.504821    2308 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/416389e0-ea56-4446-afa6-333902153fd1-klipper-cache\") pod \"416389e0-ea56-4446-afa6-333902153fd1\" (UID: \"416389e0-ea56-4446-afa6-333902153fd1\") "
I0322 03:18:32.504862    2308 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/416389e0-ea56-4446-afa6-333902153fd1-tmp\") pod \"416389e0-ea56-4446-afa6-333902153fd1\" (UID: \"416389e0-ea56-4446-afa6-333902153fd1\") "
I0322 03:18:32.504883    2308 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"content\" (UniqueName: \"kubernetes.io/configmap/416389e0-ea56-4446-afa6-333902153fd1-content\") pod \"416389e0-ea56-4446-afa6-333902153fd1\" (UID: \"416389e0-ea56-4446-afa6-333902153fd1\") "
I0322 03:18:32.504902    2308 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"values\" (UniqueName: \"kubernetes.io/secret/416389e0-ea56-4446-afa6-333902153fd1-values\") pod \"416389e0-ea56-4446-afa6-333902153fd1\" (UID: \"416389e0-ea56-4446-afa6-333902153fd1\") "
I0322 03:18:32.504918    2308 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/416389e0-ea56-4446-afa6-333902153fd1-klipper-config\") pod \"416389e0-ea56-4446-afa6-333902153fd1\" (UID: \"416389e0-ea56-4446-afa6-333902153fd1\") "
I0322 03:18:32.504938    2308 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/416389e0-ea56-4446-afa6-333902153fd1-klipper-helm\") pod \"416389e0-ea56-4446-afa6-333902153fd1\" (UID: \"416389e0-ea56-4446-afa6-333902153fd1\") "
I0322 03:18:32.662541    2308 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:18:32.706681    2308 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/416389e0-ea56-4446-afa6-333902153fd1-klipper-cache" (OuterVolumeSpecName: "klipper-cache") pod "416389e0-ea56-4446-afa6-333902153fd1" (UID: "416389e0-ea56-4446-afa6-333902153fd1"). InnerVolumeSpecName "klipper-cache". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0322 03:18:32.731660    2308 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/configmap/416389e0-ea56-4446-afa6-333902153fd1-content" (OuterVolumeSpecName: "content") pod "416389e0-ea56-4446-afa6-333902153fd1" (UID: "416389e0-ea56-4446-afa6-333902153fd1"). InnerVolumeSpecName "content". PluginName "kubernetes.io/configmap", VolumeGidValue ""
I0322 03:18:32.733848    2308 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/416389e0-ea56-4446-afa6-333902153fd1-klipper-config" (OuterVolumeSpecName: "klipper-config") pod "416389e0-ea56-4446-afa6-333902153fd1" (UID: "416389e0-ea56-4446-afa6-333902153fd1"). InnerVolumeSpecName "klipper-config". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0322 03:18:32.748699    2308 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/416389e0-ea56-4446-afa6-333902153fd1-klipper-helm" (OuterVolumeSpecName: "klipper-helm") pod "416389e0-ea56-4446-afa6-333902153fd1" (UID: "416389e0-ea56-4446-afa6-333902153fd1"). InnerVolumeSpecName "klipper-helm". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0322 03:18:32.839675    2308 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/416389e0-ea56-4446-afa6-333902153fd1-kube-api-access-kxvt2" (OuterVolumeSpecName: "kube-api-access-kxvt2") pod "416389e0-ea56-4446-afa6-333902153fd1" (UID: "416389e0-ea56-4446-afa6-333902153fd1"). InnerVolumeSpecName "kube-api-access-kxvt2". PluginName "kubernetes.io/projected", VolumeGidValue ""
I0322 03:18:32.848246    2308 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/416389e0-ea56-4446-afa6-333902153fd1-tmp" (OuterVolumeSpecName: "tmp") pod "416389e0-ea56-4446-afa6-333902153fd1" (UID: "416389e0-ea56-4446-afa6-333902153fd1"). InnerVolumeSpecName "tmp". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0322 03:18:32.848980    2308 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/416389e0-ea56-4446-afa6-333902153fd1-values" (OuterVolumeSpecName: "values") pod "416389e0-ea56-4446-afa6-333902153fd1" (UID: "416389e0-ea56-4446-afa6-333902153fd1"). InnerVolumeSpecName "values". PluginName "kubernetes.io/secret", VolumeGidValue ""
I0322 03:18:32.853111    2308 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-kxvt2\" (UniqueName: \"kubernetes.io/projected/416389e0-ea56-4446-afa6-333902153fd1-kube-api-access-kxvt2\") pod \"416389e0-ea56-4446-afa6-333902153fd1\" (UID: \"416389e0-ea56-4446-afa6-333902153fd1\") "
I0322 03:18:32.855815    2308 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/416389e0-ea56-4446-afa6-333902153fd1-tmp\") pod \"416389e0-ea56-4446-afa6-333902153fd1\" (UID: \"416389e0-ea56-4446-afa6-333902153fd1\") "
I0322 03:18:32.855930    2308 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"content\" (UniqueName: \"kubernetes.io/configmap/416389e0-ea56-4446-afa6-333902153fd1-content\") pod \"416389e0-ea56-4446-afa6-333902153fd1\" (UID: \"416389e0-ea56-4446-afa6-333902153fd1\") "
I0322 03:18:32.855955    2308 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"values\" (UniqueName: \"kubernetes.io/secret/416389e0-ea56-4446-afa6-333902153fd1-values\") pod \"416389e0-ea56-4446-afa6-333902153fd1\" (UID: \"416389e0-ea56-4446-afa6-333902153fd1\") "
I0322 03:18:32.855980    2308 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/416389e0-ea56-4446-afa6-333902153fd1-klipper-config\") pod \"416389e0-ea56-4446-afa6-333902153fd1\" (UID: \"416389e0-ea56-4446-afa6-333902153fd1\") "
I0322 03:18:32.856000    2308 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/416389e0-ea56-4446-afa6-333902153fd1-klipper-helm\") pod \"416389e0-ea56-4446-afa6-333902153fd1\" (UID: \"416389e0-ea56-4446-afa6-333902153fd1\") "
I0322 03:18:32.856751    2308 reconciler_common.go:300] "Volume detached for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/416389e0-ea56-4446-afa6-333902153fd1-klipper-cache\") on node \"server\" DevicePath \"\""
W0322 03:18:32.866239    2308 empty_dir.go:499] Warning: Unmount skipped because path does not exist: /var/lib/kubelet/pods/416389e0-ea56-4446-afa6-333902153fd1/volumes/kubernetes.io~projected/kube-api-access-kxvt2
I0322 03:18:32.866739    2308 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/416389e0-ea56-4446-afa6-333902153fd1-kube-api-access-kxvt2" (OuterVolumeSpecName: "kube-api-access-kxvt2") pod "416389e0-ea56-4446-afa6-333902153fd1" (UID: "416389e0-ea56-4446-afa6-333902153fd1"). InnerVolumeSpecName "kube-api-access-kxvt2". PluginName "kubernetes.io/projected", VolumeGidValue ""
W0322 03:18:32.871419    2308 empty_dir.go:499] Warning: Unmount skipped because path does not exist: /var/lib/kubelet/pods/416389e0-ea56-4446-afa6-333902153fd1/volumes/kubernetes.io~empty-dir/tmp
I0322 03:18:32.871620    2308 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/416389e0-ea56-4446-afa6-333902153fd1-tmp" (OuterVolumeSpecName: "tmp") pod "416389e0-ea56-4446-afa6-333902153fd1" (UID: "416389e0-ea56-4446-afa6-333902153fd1"). InnerVolumeSpecName "tmp". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
W0322 03:18:32.871837    2308 empty_dir.go:499] Warning: Unmount skipped because path does not exist: /var/lib/kubelet/pods/416389e0-ea56-4446-afa6-333902153fd1/volumes/kubernetes.io~configmap/content
I0322 03:18:32.872071    2308 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/configmap/416389e0-ea56-4446-afa6-333902153fd1-content" (OuterVolumeSpecName: "content") pod "416389e0-ea56-4446-afa6-333902153fd1" (UID: "416389e0-ea56-4446-afa6-333902153fd1"). InnerVolumeSpecName "content". PluginName "kubernetes.io/configmap", VolumeGidValue ""
W0322 03:18:32.872354    2308 empty_dir.go:499] Warning: Unmount skipped because path does not exist: /var/lib/kubelet/pods/416389e0-ea56-4446-afa6-333902153fd1/volumes/kubernetes.io~secret/values
I0322 03:18:32.872449    2308 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/416389e0-ea56-4446-afa6-333902153fd1-values" (OuterVolumeSpecName: "values") pod "416389e0-ea56-4446-afa6-333902153fd1" (UID: "416389e0-ea56-4446-afa6-333902153fd1"). InnerVolumeSpecName "values". PluginName "kubernetes.io/secret", VolumeGidValue ""
W0322 03:18:32.872621    2308 empty_dir.go:499] Warning: Unmount skipped because path does not exist: /var/lib/kubelet/pods/416389e0-ea56-4446-afa6-333902153fd1/volumes/kubernetes.io~empty-dir/klipper-config
I0322 03:18:32.872789    2308 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/416389e0-ea56-4446-afa6-333902153fd1-klipper-config" (OuterVolumeSpecName: "klipper-config") pod "416389e0-ea56-4446-afa6-333902153fd1" (UID: "416389e0-ea56-4446-afa6-333902153fd1"). InnerVolumeSpecName "klipper-config". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
W0322 03:18:32.872920    2308 empty_dir.go:499] Warning: Unmount skipped because path does not exist: /var/lib/kubelet/pods/416389e0-ea56-4446-afa6-333902153fd1/volumes/kubernetes.io~empty-dir/klipper-helm
I0322 03:18:32.873044    2308 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/416389e0-ea56-4446-afa6-333902153fd1-klipper-helm" (OuterVolumeSpecName: "klipper-helm") pod "416389e0-ea56-4446-afa6-333902153fd1" (UID: "416389e0-ea56-4446-afa6-333902153fd1"). InnerVolumeSpecName "klipper-helm". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0322 03:18:33.030983    2308 reconciler_common.go:300] "Volume detached for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/416389e0-ea56-4446-afa6-333902153fd1-klipper-helm\") on node \"server\" DevicePath \"\""
I0322 03:18:33.033093    2308 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-kxvt2\" (UniqueName: \"kubernetes.io/projected/416389e0-ea56-4446-afa6-333902153fd1-kube-api-access-kxvt2\") on node \"server\" DevicePath \"\""
I0322 03:18:33.035049    2308 reconciler_common.go:300] "Volume detached for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/416389e0-ea56-4446-afa6-333902153fd1-tmp\") on node \"server\" DevicePath \"\""
I0322 03:18:33.035905    2308 reconciler_common.go:300] "Volume detached for volume \"content\" (UniqueName: \"kubernetes.io/configmap/416389e0-ea56-4446-afa6-333902153fd1-content\") on node \"server\" DevicePath \"\""
I0322 03:18:33.035985    2308 reconciler_common.go:300] "Volume detached for volume \"values\" (UniqueName: \"kubernetes.io/secret/416389e0-ea56-4446-afa6-333902153fd1-values\") on node \"server\" DevicePath \"\""
I0322 03:18:33.036400    2308 reconciler_common.go:300] "Volume detached for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/416389e0-ea56-4446-afa6-333902153fd1-klipper-config\") on node \"server\" DevicePath \"\""
time="2024-03-22T03:18:33Z" level=info msg="certificate CN=serverworker signed by CN=k3s-server-ca@1711077444: notBefore=2024-03-22 03:17:24 +0000 UTC notAfter=2025-03-22 03:18:32 +0000 UTC"
I0322 03:18:33.605713    2308 trace.go:236] Trace[525042787]: "Get" accept:application/json, */*,audit-id:bb3b755a-a113-4c02-9f4d-2d8ee674de63,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps/coredns,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:GET (22-Mar-2024 03:18:33.090) (total time: 508ms):
Trace[525042787]: ---"About to write a response" 391ms (03:18:33.484)
Trace[525042787]: [508.473612ms] [508.473612ms] END
E0322 03:19:05.040311    2308 health_controller.go:162] Metrics Controller heartbeat missed
E0322 03:19:05.047347    2308 health_controller.go:162] Metrics Controller heartbeat missed
E0322 03:19:05.048126    2308 health_controller.go:162] Metrics Controller heartbeat missed
time="2024-03-22T03:19:08Z" level=info msg="Slow SQL (started: 2024-03-22 03:18:39.486722233 +0000 UTC m=+75.181535442) (total time: 20.181298045s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC LIMIT 500 : [[% 594]]"
I0322 03:19:13.855021    2308 request.go:697] Waited for 8.732360881s due to client-side throttling, not priority and fairness, request: GET:https://127.0.0.1:6444/api
E0322 03:19:21.628150    2308 health_controller.go:162] Metrics Controller heartbeat missed
W0322 03:19:56.865472    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Namespace ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:21.561572    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Secret ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:21.562598    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.RoleBinding ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:35.194187    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Namespace ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:46.955360    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:47.227702    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CSIStorageCapacity ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:35.404964    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:47.618029    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PersistentVolume ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:49.984494    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.StorageClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:05.033274    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:19:28.039712    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:37.105568    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PodDisruptionBudget ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:40.688268    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PodDisruptionBudget ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:40.689245    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ClusterRoleBinding ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:40.689284    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CSINode ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:40.689491    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:40.689309    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.NetworkPolicy ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:40.689569    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Job ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:40.689592    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PodTemplate ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:40.689609    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PersistentVolumeClaim ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:40.690127    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Secret ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:40.690159    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1beta3.FlowSchema ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:40.690186    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.RuntimeClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:40.690227    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:40.690265    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.IngressClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:40.690284    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:40.690304    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CSIStorageCapacity ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:40.690417    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ValidatingWebhookConfiguration ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:40.690459    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1beta3.PriorityLevelConfiguration ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:40.781523    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CertificateSigningRequest ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:40.781664    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:40.781778    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.RoleBinding ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:40.781875    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:40.782248    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:40.782272    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.StorageClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:40.782673    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Endpoints ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:40.782696    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ControllerRevision ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:40.782726    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.EndpointSlice ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:40.782749    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ReplicationController ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:40.782837    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.StatefulSet ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:40.782859    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PriorityClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:40.782880    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ReplicaSet ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:40.782905    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Deployment ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:40.782921    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.MutatingWebhookConfiguration ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:40.783587    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Namespace ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:40.783606    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.VolumeAttachment ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:40.783724    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ResourceQuota ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:40.783785    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v2.HorizontalPodAutoscaler ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:40.783808    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:40.783893    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Lease ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:40.788365    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:40.799915    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Ingress ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:40.800671    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:40.801084    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.DaemonSet ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:40.801144    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:40.801175    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ServiceAccount ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:40.801738    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:35.099688    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:20:52.274156    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
time="2024-03-22T03:20:40Z" level=info msg="Slow SQL (started: 2024-03-22 03:18:34.315949055 +0000 UTC m=+70.010762270) (total time: 25.024222052s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/controllerrevisions/% false]]"
W0322 03:20:57.312418    2308 transport.go:301] Unable to cancel request for *otelhttp.Transport
W0322 03:20:58.647281    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Lease ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:20:45.011924    2308 health_controller.go:162] Metrics Controller heartbeat missed
W0322 03:20:47.009741    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ReplicationController ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:21:12.700281    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Endpoints ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:21:15.855371    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ReplicaSet ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
time="2024-03-22T03:22:31Z" level=error msg="Error writing ping" error="write tcp 192.168.56.110:39688->192.168.56.110:6443: i/o timeout"
time="2024-03-22T03:22:31Z" level=error msg="Error writing ping" error="write tcp 192.168.56.110:39688->192.168.56.110:6443: i/o timeout"
W0322 03:21:52.252255    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:22:31.636562    2308 remote_image.go:128] "ListImages with filter from image service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" filter="nil"
E0322 03:22:31.640086    2308 kuberuntime_image.go:103] "Failed to list images" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded"
I0322 03:22:31.641024    2308 image_gc_manager.go:210] "Failed to update image list" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded"
W0322 03:22:31.702130    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
time="2024-03-22T03:22:31Z" level=info msg="Slow SQL (started: 2024-03-22 03:19:22.098197207 +0000 UTC m=+117.793010404) (total time: 2m32.37140576s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC LIMIT 500 : [[% 594]]"
E0322 03:22:31.968248    2308 server.go:310] "Unable to authenticate the request due to an error" err="context canceled"
E0322 03:22:32.300798    2308 watch.go:287] unable to encode watch object *v1.WatchEvent: client disconnected (&streaming.encoderWithAllocator{writer:(*framer.lengthDelimitedFrameWriter)(0xc0065e8468), encoder:(*versioning.codec)(0xc00795c780), memAllocator:(*runtime.Allocator)(0xc0065e8480)})
W0322 03:22:32.367967    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.HelmChart ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:01.104601    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:32.539079    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:21:07.110491    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.LimitRange ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:21:09.042553    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.NetworkPolicy ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:21:12.773406    2308 reflector.go:458] object-"kube-system"/"chart-values-traefik": watch of *v1.Secret ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:14.029637    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.StatefulSet ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:21.577634    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:21.583225    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PersistentVolume ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I0322 03:22:30.739832    2308 handler.go:275] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
W0322 03:22:30.852258    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ServiceAccount ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I0322 03:22:30.911065    2308 trace.go:236] Trace[60592371]: "iptables ChainExists" (22-Mar-2024 03:19:02.798) (total time: 154223ms):
Trace[60592371]: [2m34.223103361s] [2m34.223103361s] END
E0322 03:22:30.919706    2308 watch.go:287] unable to encode watch object *v1.WatchEvent: client disconnected (&streaming.encoderWithAllocator{writer:(*framer.lengthDelimitedFrameWriter)(0xc00537f830), encoder:(*versioning.codec)(0xc002572640), memAllocator:(*runtime.Allocator)(0xc00537f848)})
W0322 03:21:42.350400    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CronJob ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:22:30.919805    2308 watch.go:287] unable to encode watch object *v1.WatchEvent: client disconnected (&streaming.encoderWithAllocator{writer:(*framer.lengthDelimitedFrameWriter)(0xc003103a28), encoder:(*versioning.codec)(0xc0030215e0), memAllocator:(*runtime.Allocator)(0xc003103a40)})
I0322 03:22:30.921175    2308 trace.go:236] Trace[1499915473]: "Calculate volume metrics of custom-config-volume for pod kube-system/coredns-6799fbcd5-tqlcn" (22-Mar-2024 03:21:37.369) (total time: 53551ms):
Trace[1499915473]: [53.551502354s] [53.551502354s] END
W0322 03:22:31.076246    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PersistentVolume ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:31.140176    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Role ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:21:45.066553    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CSIDriver ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:31.187744    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:31.202441    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I0322 03:22:31.250741    2308 log.go:245] http: TLS handshake error from 10.42.0.5:55314: write tcp 192.168.56.110:10250->10.42.0.5:55314: write: broken pipe
I0322 03:22:32.962534    2308 log.go:245] http: TLS handshake error from 10.42.0.5:43388: write tcp 192.168.56.110:10250->10.42.0.5:43388: write: broken pipe
I0322 03:22:32.964949    2308 log.go:245] http: TLS handshake error from 10.42.0.5:49996: write tcp 192.168.56.110:10250->10.42.0.5:49996: write: broken pipe
W0322 03:21:49.055554    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I0322 03:22:33.001634    2308 log.go:245] http: TLS handshake error from 10.42.0.5:37022: write tcp 192.168.56.110:10250->10.42.0.5:37022: write: broken pipe
I0322 03:22:33.002406    2308 log.go:245] http: TLS handshake error from 10.42.0.5:40962: write tcp 192.168.56.110:10250->10.42.0.5:40962: write: broken pipe
W0322 03:22:31.263076    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:22:31.314746    2308 remote_runtime.go:294] "ListPodSandbox with filter from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" filter="nil"
W0322 03:22:31.315248    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ClusterRoleBinding ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I0322 03:22:31.420289    2308 trace.go:236] Trace[2092247094]: "DeltaFIFO Pop Process" ID:v1.k3s.cattle.io,Depth:25,Reason:slow event handlers blocking the queue (22-Mar-2024 03:20:40.559) (total time: 70390ms):
Trace[2092247094]: [1m10.390490427s] [1m10.390490427s] END
W0322 03:22:31.566981    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.EndpointSlice ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:33.041036    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:22:33.413666    2308 health_controller.go:162] Metrics Controller heartbeat missed
E0322 03:22:33.416309    2308 health_controller.go:162] Metrics Controller heartbeat missed
W0322 03:22:33.631936    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CSIDriver ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:33.719538    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:33.824632    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ClusterRole ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:34.201403    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.RuntimeClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:34.265263    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PersistentVolumeClaim ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:12.183736    2308 reflector.go:458] object-"kube-system"/"local-path-config": watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:34.811775    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.RuntimeClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I0322 03:22:11.164745    2308 trace.go:236] Trace[624975771]: "iptables ChainExists" (22-Mar-2024 03:19:04.459) (total time: 102458ms):
Trace[624975771]: [1m42.458082088s] [1m42.458082088s] END
W0322 03:22:34.947823    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Namespace ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:34.959717    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CSINode ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:14.745072    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:21:15.456886    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:35.212295    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CSIDriver ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
time="2024-03-22T03:22:35Z" level=error msg="Error writing ping" error="write tcp 192.168.56.110:39688->192.168.56.110:6443: i/o timeout"
W0322 03:22:35.417124    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:35.437062    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.EndpointSlice ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:18.401032    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.VolumeAttachment ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:18.401137    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.HelmChartConfig ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:35.591882    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ValidatingWebhookConfiguration ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:35.820709    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:36.335736    2308 reflector.go:458] object-"kube-system"/"coredns-custom": watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:36.338808    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Endpoints ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:36.344900    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ServiceAccount ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:36.345341    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PriorityClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:36.347375    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:36.365311    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:36.397424    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Job ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:36.504406    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.MutatingWebhookConfiguration ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:36.557522    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1beta3.FlowSchema ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:36.557983    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.StorageClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:36.559696    2308 reflector.go:458] object-"kube-system"/"coredns": watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:36.579942    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:36.676411    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CustomResourceDefinition ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:36.694425    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.APIService ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:36.755463    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:37.130549    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1beta3.PriorityLevelConfiguration ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:37.132721    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ResourceQuota ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:37.248052    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Role ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:37.278389    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Lease ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:37.465961    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:37.468453    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.LimitRange ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:37.495288    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.IngressClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I0322 03:22:37.581363    2308 trace.go:236] Trace[1915557883]: "iptables ChainExists" (22-Mar-2024 03:19:36.382) (total time: 181191ms):
Trace[1915557883]: [3m1.191055211s] [3m1.191055211s] END
E0322 03:21:32.398291    2308 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: request timed out, Header: map[]
W0322 03:22:37.788162    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Addon ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:37.798224    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Secret ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:37.798829    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ClusterRoleBinding ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:37.951122    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I0322 03:22:38.245965    2308 trace.go:236] Trace[923373736]: "iptables ChainExists" (22-Mar-2024 03:19:36.484) (total time: 181756ms):
Trace[923373736]: [3m1.756384994s] [3m1.756384994s] END
I0322 03:22:39.017403    2308 log.go:245] http: TLS handshake error from 10.42.0.5:35932: write tcp 192.168.56.110:10250->10.42.0.5:35932: write: broken pipe
I0322 03:22:41.997668    2308 log.go:245] http: TLS handshake error from 10.42.0.5:56640: write tcp 192.168.56.110:10250->10.42.0.5:56640: write: broken pipe
I0322 03:22:42.102255    2308 trace.go:236] Trace[623199122]: "Calculate volume metrics of klipper-helm for pod kube-system/helm-install-traefik-t9wf9" (22-Mar-2024 03:22:31.805) (total time: 8058ms):
Trace[623199122]: [8.058943894s] [8.058943894s] END
E0322 03:22:42.322618    2308 watch.go:287] unable to encode watch object *v1.WatchEvent: client disconnected (&streaming.encoderWithAllocator{writer:(*framer.lengthDelimitedFrameWriter)(0xc001e5f830), encoder:(*versioning.codec)(0xc00ade7ae0), memAllocator:(*runtime.Allocator)(0xc001e5f848)})
time="2024-03-22T03:22:38Z" level=error msg="error while range on /registry/controllerrevisions/ /registry/controllerrevisions/: context canceled"
W0322 03:22:39.982354    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I0322 03:22:42.464270    2308 log.go:245] http: TLS handshake error from 10.42.0.5:41956: EOF
I0322 03:22:42.465668    2308 log.go:245] http: TLS handshake error from 10.42.0.5:52762: EOF
I0322 03:22:42.465832    2308 log.go:245] http: TLS handshake error from 10.42.0.5:36176: EOF
I0322 03:22:42.465929    2308 log.go:245] http: TLS handshake error from 10.42.0.5:34414: EOF
I0322 03:22:42.466106    2308 log.go:245] http: TLS handshake error from 10.42.0.5:57374: write tcp 192.168.56.110:10250->10.42.0.5:57374: write: broken pipe
I0322 03:22:42.466201    2308 log.go:245] http: TLS handshake error from 10.42.0.5:43170: write tcp 192.168.56.110:10250->10.42.0.5:43170: write: broken pipe
I0322 03:22:42.466293    2308 log.go:245] http: TLS handshake error from 10.42.0.5:58362: write tcp 192.168.56.110:10250->10.42.0.5:58362: write: broken pipe
E0322 03:22:42.477644    2308 compact.go:124] etcd: endpoint ([unix://kine.sock]) compact failed: rpc error: code = Unavailable desc = keepalive ping failed to receive ACK within timeout
I0322 03:22:42.613774    2308 trace.go:236] Trace[782124349]: "DeltaFIFO Pop Process" ID:middlewares.traefik.containo.us,Depth:21,Reason:slow event handlers blocking the queue (22-Mar-2024 03:22:41.801) (total time: 803ms):
Trace[782124349]: [803.42149ms] [803.42149ms] END
time="2024-03-22T03:22:41Z" level=error msg="Error writing ping" error="write tcp 192.168.56.110:39688->192.168.56.110:6443: i/o timeout"
E0322 03:22:40.126427    2308 watch.go:287] unable to encode watch object *v1.WatchEvent: client disconnected (&streaming.encoderWithAllocator{writer:(*framer.lengthDelimitedFrameWriter)(0xc0078440d8), encoder:(*versioning.codec)(0xc00820b4a0), memAllocator:(*runtime.Allocator)(0xc0078440f0)})
E0322 03:22:32.083290    2308 watch.go:287] unable to encode watch object *v1.WatchEvent: client disconnected (&streaming.encoderWithAllocator{writer:(*framer.lengthDelimitedFrameWriter)(0xc003103308), encoder:(*versioning.codec)(0xc0030210e0), memAllocator:(*runtime.Allocator)(0xc003103320)})
W0322 03:22:34.498363    2308 reflector.go:458] object-"kube-system"/"chart-content-traefik": watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:22:40.307810    2308 remote_runtime.go:633] "Status from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded"
E0322 03:22:42.763464    2308 kubelet.go:2840] "Container runtime sanity check failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded"
I0322 03:22:40.598007    2308 status_manager.go:877] "Failed to update status for pod" pod="kube-system/helm-install-traefik-crd-m54x6" err="failed to patch status \"{\\\"metadata\\\":{\\\"uid\\\":\\\"416389e0-ea56-4446-afa6-333902153fd1\\\"},\\\"status\\\":{\\\"phase\\\":\\\"Succeeded\\\",\\\"podIP\\\":null,\\\"podIPs\\\":null}}\" for pod \"kube-system\"/\"helm-install-traefik-crd-m54x6\": Patch \"https://127.0.0.1:6443/api/v1/namespaces/kube-system/pods/helm-install-traefik-crd-m54x6/status\": http2: client connection lost"
W0322 03:22:40.639177    2308 reflector.go:458] object-"kube-system"/"kube-root-ca.crt": watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:22:35.574160    2308 kubelet_node_status.go:540] "Error updating node status, will retry" err="error getting node \"server\": Get \"https://127.0.0.1:6443/api/v1/nodes/server?resourceVersion=0&timeout=10s\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)"
E0322 03:22:41.006608    2308 kuberuntime_sandbox.go:297] "Failed to list pod sandboxes" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded"
E0322 03:22:42.806126    2308 generic.go:238] "GenericPLEG: Unable to retrieve pods" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded"
E0322 03:22:41.069288    2308 job_controller.go:595] syncing job: tracking status: adding uncounted pods to status: Put "https://127.0.0.1:6444/apis/batch/v1/namespaces/kube-system/jobs/helm-install-traefik/status": http2: client connection lost
W0322 03:22:41.191739    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.DaemonSet ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:22:41.676221    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ClusterRole ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:22:41.695123    2308 watch.go:287] unable to encode watch object *v1.WatchEvent: client disconnected (&streaming.encoderWithAllocator{writer:(*framer.lengthDelimitedFrameWriter)(0xc00754dc80), encoder:(*versioning.codec)(0xc004916000), memAllocator:(*runtime.Allocator)(0xc00754dc98)})
W0322 03:22:41.702628    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:22:41.769413    2308 resource_quota_controller.go:440] failed to discover resources: Get "https://127.0.0.1:6444/api": dial tcp 127.0.0.1:6444: i/o timeout
I0322 03:22:41.772229    2308 garbagecollector.go:818] "failed to discover preferred resources" error="Get \"https://127.0.0.1:6444/api\": dial tcp 127.0.0.1:6444: i/o timeout"
I0322 03:22:41.775381    2308 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E0322 03:22:42.829798    2308 watch.go:287] unable to encode watch object *v1.WatchEvent: client disconnected (&streaming.encoderWithAllocator{writer:(*framer.lengthDelimitedFrameWriter)(0xc007ad2858), encoder:(*versioning.codec)(0xc00381afa0), memAllocator:(*runtime.Allocator)(0xc007ad2870)})
E0322 03:22:43.060634    2308 controller.go:193] "Failed to update lease" err="Put \"https://127.0.0.1:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/server?timeout=10s\": context deadline exceeded"
E0322 03:22:43.109983    2308 server.go:310] "Unable to authenticate the request due to an error" err="context canceled"
E0322 03:22:43.127555    2308 remote_runtime.go:407] "ListContainers with filter from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" filter="&ContainerFilter{Id:,State:nil,PodSandboxId:,LabelSelector:map[string]string{},}"
E0322 03:22:43.134695    2308 kuberuntime_container.go:477] "ListContainers failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded"
I0322 03:22:43.379012    2308 trace.go:236] Trace[1917840899]: "DeltaFIFO Pop Process" ID:tlsoptions.traefik.io,Depth:20,Reason:slow event handlers blocking the queue (22-Mar-2024 03:22:42.613) (total time: 754ms):
Trace[1917840899]: [754.61092ms] [754.61092ms] END
time="2024-03-22T03:22:42Z" level=error msg="Compact failed: failed to begin transaction: context deadline exceeded"
time="2024-03-22T03:22:42Z" level=error msg="error while range on /registry/secrets/ /registry/secrets/: context canceled"
I0322 03:22:43.692471    2308 trace.go:236] Trace[1483908663]: "DeltaFIFO Pop Process" ID:ingressrouteudps.traefik.io,Depth:19,Reason:slow event handlers blocking the queue (22-Mar-2024 03:22:43.381) (total time: 307ms):
Trace[1483908663]: [307.728033ms] [307.728033ms] END
I0322 03:22:43.694188    2308 scope.go:117] "RemoveContainer" containerID="99f14d58a0ef229261a3734019bc51448bd530d4cf41690d804f452d725e4532"
time="2024-03-22T03:22:42Z" level=info msg="certificate CN=system:node:serverworker,O=system:nodes signed by CN=k3s-client-ca@1711077444: notBefore=2024-03-22 03:17:24 +0000 UTC notAfter=2025-03-22 03:22:30 +0000 UTC"
time="2024-03-22T03:22:42Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:40.720368034 +0000 UTC m=+316.415181238) (total time: 1.352026754s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC LIMIT 500 : [[% 594]]"
time="2024-03-22T03:22:42Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:30.924403453 +0000 UTC m=+306.619216653) (total time: 11.166030633s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/ingress/% false]]"
time="2024-03-22T03:22:43Z" level=error msg="error while range on /registry/ingress/ /registry/ingress/: context canceled"
I0322 03:22:44.038758    2308 trace.go:236] Trace[1253320504]: "iptables ChainExists" (22-Mar-2024 03:22:35.481) (total time: 8551ms):
Trace[1253320504]: [8.551706507s] [8.551706507s] END
E0322 03:22:44.122117    2308 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, context deadline exceeded]"
time="2024-03-22T03:22:45Z" level=info msg="Slow SQL (started: 2024-03-22 03:18:59.81494735 +0000 UTC m=+95.509760564) (total time: 3m42.009818494s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC LIMIT 1 : [[/registry/masterleases/192.168.56.110 true]]"
W0322 03:22:45.186177    2308 controller.go:181] slow openapi aggregation of "tlsoptions.traefik.io": 1.589649757s
time="2024-03-22T03:22:45Z" level=error msg="Error writing ping" error="write tcp 192.168.56.110:39688->192.168.56.110:6443: i/o timeout"
time="2024-03-22T03:22:45Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:42.991108437 +0000 UTC m=+318.685921645) (total time: 1.1081265s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC  : [[/registry/masterleases/192.168.56.110 false]]"
time="2024-03-22T03:22:46Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:44.513729708 +0000 UTC m=+320.208542920) (total time: 2.216066976s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/k3s.cattle.io/addons/% false]]"
time="2024-03-22T03:22:46Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:44.561075036 +0000 UTC m=+320.255888246) (total time: 2.217644221s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/jobs/% false]]"
time="2024-03-22T03:22:46Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:45.583921687 +0000 UTC m=+321.278734887) (total time: 1.284610723s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/runtimeclasses/% false]]"
time="2024-03-22T03:22:46Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:45.5749039 +0000 UTC m=+321.269717110) (total time: 1.296106088s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/apiregistration.k8s.io/apiservices/% false]]"
time="2024-03-22T03:22:46Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:45.582530421 +0000 UTC m=+321.277343630) (total time: 1.292464543s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC LIMIT 500 : [[% 594]]"
time="2024-03-22T03:22:46Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:45.584858876 +0000 UTC m=+321.279672085) (total time: 1.329168031s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/serviceaccounts/% false]]"
time="2024-03-22T03:22:46Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:45.595513795 +0000 UTC m=+321.290327004) (total time: 1.330402772s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/runtimeclasses/% 594]]"
E0322 03:22:47.112678    2308 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
I0322 03:22:47.560950    2308 trace.go:236] Trace[2046410264]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:d2c353f5-2442-4811-bbf2-2ae1aad2f0c1,client:127.0.0.1,protocol:HTTP/2.0,resource:priorityclasses,scope:cluster,url:/apis/scheduling.k8s.io/v1/priorityclasses,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:22:45.113) (total time: 2440ms):
Trace[2046410264]: ---"Writing http response done" count:2 2438ms (03:22:47.554)
Trace[2046410264]: [2.440847899s] [2.440847899s] END
I0322 03:22:47.599539    2308 trace.go:236] Trace[870954093]: "List" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json,audit-id:23278ef2-f008-4b56-94f3-b8b6a40bc30b,client:127.0.0.1,protocol:HTTP/2.0,resource:helmchartconfigs,scope:cluster,url:/apis/helm.cattle.io/v1/helmchartconfigs,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/metadata-informers,verb:LIST (22-Mar-2024 03:22:45.162) (total time: 2437ms):
Trace[870954093]: ---"About to List from storage" 2430ms (03:22:47.592)
Trace[870954093]: [2.437010577s] [2.437010577s] END
time="2024-03-22T03:22:47Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:43.675481404 +0000 UTC m=+319.370294616) (total time: 1.581688368s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC  : [[/registry/ranges/serviceips false]]"
I0322 03:22:48.214704    2308 trace.go:236] Trace[652533443]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:b14bd95b-f2b5-4e91-85ed-72b6560c9539,client:127.0.0.1,protocol:HTTP/2.0,resource:rolebindings,scope:cluster,url:/apis/rbac.authorization.k8s.io/v1/rolebindings,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:22:44.733) (total time: 3476ms):
Trace[652533443]: ---"Writing http response done" count:9 3473ms (03:22:48.209)
Trace[652533443]: [3.476228622s] [3.476228622s] END
I0322 03:22:48.216530    2308 trace.go:236] Trace[959735264]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:b2385847-2608-4184-a6ff-f9e89b4ebfe9,client:127.0.0.1,protocol:HTTP/2.0,resource:replicasets,scope:cluster,url:/apis/apps/v1/replicasets,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:22:44.766) (total time: 3449ms):
Trace[959735264]: ---"Writing http response done" count:3 3447ms (03:22:48.216)
Trace[959735264]: [3.449796935s] [3.449796935s] END
I0322 03:22:48.216690    2308 trace.go:236] Trace[1020832452]: "List" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json,audit-id:3fe1f434-d5da-42bd-bd52-09dbb04ae1c8,client:127.0.0.1,protocol:HTTP/2.0,resource:apiservices,scope:cluster,url:/apis/apiregistration.k8s.io/v1/apiservices,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/metadata-informers,verb:LIST (22-Mar-2024 03:22:44.783) (total time: 3433ms):
Trace[1020832452]: ---"Writing http response done" count:26 3431ms (03:22:48.216)
Trace[1020832452]: [3.433548671s] [3.433548671s] END
I0322 03:22:48.217285    2308 trace.go:236] Trace[764645144]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:38bcbb1d-e78d-4a97-9761-aa24f2125d84,client:127.0.0.1,protocol:HTTP/2.0,resource:deployments,scope:cluster,url:/apis/apps/v1/deployments,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:22:44.820) (total time: 3396ms):
Trace[764645144]: ---"Writing http response done" count:3 3393ms (03:22:48.217)
Trace[764645144]: [3.396863932s] [3.396863932s] END
I0322 03:22:48.217403    2308 trace.go:236] Trace[86063352]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:fa103a77-d673-49e4-bc3f-40bc470b2f57,client:127.0.0.1,protocol:HTTP/2.0,resource:prioritylevelconfigurations,scope:cluster,url:/apis/flowcontrol.apiserver.k8s.io/v1beta3/prioritylevelconfigurations,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:22:44.871) (total time: 3345ms):
Trace[86063352]: ---"Writing http response done" count:8 3343ms (03:22:48.217)
Trace[86063352]: [3.345721429s] [3.345721429s] END
I0322 03:22:48.217822    2308 trace.go:236] Trace[433039941]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:8820c8a7-3d8f-4d76-96b6-f9421f7e49d6,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:cluster,url:/api/v1/nodes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:22:44.877) (total time: 3340ms):
Trace[433039941]: ---"Writing http response done" count:1 3338ms (03:22:48.217)
Trace[433039941]: [3.340284183s] [3.340284183s] END
I0322 03:22:48.217931    2308 trace.go:236] Trace[1859517346]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:5adcf235-c341-4479-8db1-5effbf6d83f6,client:127.0.0.1,protocol:HTTP/2.0,resource:pods,scope:cluster,url:/api/v1/pods,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:22:44.945) (total time: 3272ms):
Trace[1859517346]: ---"Writing http response done" count:5 3269ms (03:22:48.217)
Trace[1859517346]: [3.272309603s] [3.272309603s] END
I0322 03:22:48.218798    2308 trace.go:236] Trace[1435956143]: "List" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json,audit-id:74ed356d-8e70-493b-b63f-d056952c73ac,client:127.0.0.1,protocol:HTTP/2.0,resource:customresourcedefinitions,scope:cluster,url:/apis/apiextensions.k8s.io/v1/customresourcedefinitions,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/metadata-informers,verb:LIST (22-Mar-2024 03:22:45.070) (total time: 3147ms):
Trace[1435956143]: ---"Writing http response done" count:23 3138ms (03:22:48.218)
Trace[1435956143]: [3.147981845s] [3.147981845s] END
time="2024-03-22T03:22:49Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:46.207649245 +0000 UTC m=+321.902462464) (total time: 2.810175084s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/mutatingwebhookconfigurations/% false]]"
E0322 03:22:49.046999    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
time="2024-03-22T03:22:49Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:46.252159159 +0000 UTC m=+321.946972358) (total time: 2.823954658s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/resourcequotas/% false]]"
I0322 03:22:49.119805    2308 trace.go:236] Trace[334797960]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:a7dea7b7-c199-4bf5-a284-f3902bca4acc,client:127.0.0.1,protocol:HTTP/2.0,resource:ingressclasses,scope:cluster,url:/apis/networking.k8s.io/v1/ingressclasses,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:22:44.880) (total time: 2329ms):
Trace[334797960]: ---"Writing http response done" count:0 2327ms (03:22:47.210)
Trace[334797960]: [2.329653778s] [2.329653778s] END
time="2024-03-22T03:22:49Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:47.199184934 +0000 UTC m=+322.893998142) (total time: 1.929003816s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/replicasets/% false]]"
time="2024-03-22T03:22:49Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:47.315520834 +0000 UTC m=+323.010334033) (total time: 1.931174961s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/volumeattachments/% false]]"
I0322 03:22:49.288718    2308 trace.go:236] Trace[229236808]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:7f1b7ead-3014-407c-b395-9e3d9e3872e4,client:127.0.0.1,protocol:HTTP/2.0,resource:networkpolicies,scope:cluster,url:/apis/networking.k8s.io/v1/networkpolicies,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:22:44.978) (total time: 4303ms):
Trace[229236808]: ---"Writing http response done" count:0 4300ms (03:22:49.281)
Trace[229236808]: [4.303043432s] [4.303043432s] END
I0322 03:22:49.828675    2308 trace.go:236] Trace[1627718527]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:ea93f167-c6ef-4472-adb9-5755cfa5c43f,client:127.0.0.1,protocol:HTTP/2.0,resource:persistentvolumes,scope:cluster,url:/api/v1/persistentvolumes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:22:45.244) (total time: 4577ms):
Trace[1627718527]: ---"Writing http response done" count:0 4576ms (03:22:49.822)
Trace[1627718527]: [4.577737917s] [4.577737917s] END
time="2024-03-22T03:22:50Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:44.997655094 +0000 UTC m=+320.692468307) (total time: 5.101123959s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/namespaces/% 594]]"
I0322 03:22:50.198557    2308 trace.go:236] Trace[1962391515]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:21:00.764) (total time: 109428ms):
Trace[1962391515]: ---"Objects listed" error:<nil> 109426ms (03:22:50.190)
Trace[1962391515]: [1m49.428705582s] [1m49.428705582s] END
I0322 03:22:50.204581    2308 trace.go:236] Trace[1651357729]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:21:01.218) (total time: 108985ms):
Trace[1651357729]: ---"Objects listed" error:<nil> 108983ms (03:22:50.202)
Trace[1651357729]: [1m48.985997668s] [1m48.985997668s] END
I0322 03:22:50.225372    2308 trace.go:236] Trace[1903989898]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:20:52.320) (total time: 117904ms):
Trace[1903989898]: ---"Objects listed" error:<nil> 117904ms (03:22:50.224)
Trace[1903989898]: [1m57.904105082s] [1m57.904105082s] END
I0322 03:22:50.232836    2308 trace.go:236] Trace[782070767]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:20:52.558) (total time: 117674ms):
Trace[782070767]: ---"Objects listed" error:<nil> 117673ms (03:22:50.232)
Trace[782070767]: [1m57.674085878s] [1m57.674085878s] END
I0322 03:22:50.234105    2308 trace.go:236] Trace[1493835568]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:21:01.702) (total time: 108531ms):
Trace[1493835568]: ---"Objects listed" error:<nil> 108531ms (03:22:50.234)
Trace[1493835568]: [1m48.531202294s] [1m48.531202294s] END
I0322 03:22:50.234626    2308 trace.go:236] Trace[598174899]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:21:01.991) (total time: 108243ms):
Trace[598174899]: ---"Objects listed" error:<nil> 108243ms (03:22:50.234)
Trace[598174899]: [1m48.243455929s] [1m48.243455929s] END
I0322 03:22:50.236382    2308 trace.go:236] Trace[554573370]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:21:02.023) (total time: 108212ms):
Trace[554573370]: ---"Objects listed" error:<nil> 108212ms (03:22:50.235)
Trace[554573370]: [1m48.212742683s] [1m48.212742683s] END
I0322 03:22:50.240160    2308 trace.go:236] Trace[35814095]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:21:02.614) (total time: 107625ms):
Trace[35814095]: ---"Objects listed" error:<nil> 107625ms (03:22:50.240)
Trace[35814095]: [1m47.625519321s] [1m47.625519321s] END
I0322 03:22:50.242957    2308 trace.go:236] Trace[1018567596]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:21:02.662) (total time: 107580ms):
Trace[1018567596]: ---"Objects listed" error:<nil> 107580ms (03:22:50.242)
Trace[1018567596]: [1m47.580810917s] [1m47.580810917s] END
I0322 03:22:50.245120    2308 trace.go:236] Trace[441334194]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:20:59.858) (total time: 110386ms):
Trace[441334194]: ---"Objects listed" error:<nil> 110386ms (03:22:50.245)
Trace[441334194]: [1m50.386418571s] [1m50.386418571s] END
I0322 03:22:50.246417    2308 trace.go:236] Trace[472160952]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:21:33.979) (total time: 76266ms):
Trace[472160952]: ---"Objects listed" error:<nil> 76266ms (03:22:50.246)
Trace[472160952]: [1m16.266806659s] [1m16.266806659s] END
I0322 03:22:50.249546    2308 trace.go:236] Trace[803412339]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:21:00.880) (total time: 109368ms):
Trace[803412339]: ---"Objects listed" error:<nil> 109368ms (03:22:50.249)
Trace[803412339]: [1m49.368811528s] [1m49.368811528s] END
I0322 03:22:50.284306    2308 trace.go:236] Trace[1031904258]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:20:52.108) (total time: 118175ms):
Trace[1031904258]: ---"Objects listed" error:<nil> 118174ms (03:22:50.283)
Trace[1031904258]: [1m58.175139211s] [1m58.175139211s] END
I0322 03:22:50.287741    2308 trace.go:236] Trace[2053408340]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:20:52.111) (total time: 118176ms):
Trace[2053408340]: ---"Objects listed" error:<nil> 118176ms (03:22:50.287)
Trace[2053408340]: [1m58.17625047s] [1m58.17625047s] END
I0322 03:22:50.294984    2308 trace.go:236] Trace[1445084677]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:34.022) (total time: 16269ms):
Trace[1445084677]: ---"Objects listed" error:<nil> 16269ms (03:22:50.291)
Trace[1445084677]: [16.269014287s] [16.269014287s] END
time="2024-03-22T03:22:50Z" level=info msg="error in remotedialer server [400]: read tcp 192.168.56.110:6443->192.168.56.110:39688: i/o timeout"
I0322 03:22:50.496014    2308 trace.go:236] Trace[2119159496]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:a89fe8d6-b4ba-4c0e-9074-eb9e2a941d95,client:127.0.0.1,protocol:HTTP/2.0,resource:jobs,scope:cluster,url:/apis/batch/v1/jobs,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:22:45.001) (total time: 5494ms):
Trace[2119159496]: ---"Writing http response done" count:2 5484ms (03:22:50.495)
Trace[2119159496]: [5.494367835s] [5.494367835s] END
I0322 03:22:50.569468    2308 trace.go:236] Trace[627896228]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:21:00.569) (total time: 109998ms):
Trace[627896228]: ---"Objects listed" error:<nil> 109992ms (03:22:50.561)
Trace[627896228]: [1m49.998347201s] [1m49.998347201s] END
I0322 03:22:50.577813    2308 trace.go:236] Trace[1337041809]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:21:00.636) (total time: 109940ms):
Trace[1337041809]: ---"Objects listed" error:<nil> 109938ms (03:22:50.574)
Trace[1337041809]: [1m49.940923664s] [1m49.940923664s] END
I0322 03:22:50.596559    2308 trace.go:236] Trace[1400112382]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:21:00.709) (total time: 109886ms):
Trace[1400112382]: ---"Objects listed" error:<nil> 109879ms (03:22:50.589)
Trace[1400112382]: [1m49.886010766s] [1m49.886010766s] END
I0322 03:22:50.612254    2308 trace.go:236] Trace[1533266241]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:35.230) (total time: 15381ms):
Trace[1533266241]: ---"Objects listed" error:<nil> 15380ms (03:22:50.611)
Trace[1533266241]: [15.381321324s] [15.381321324s] END
I0322 03:22:50.614658    2308 trace.go:236] Trace[1255241542]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:34.399) (total time: 16215ms):
Trace[1255241542]: ---"Objects listed" error:<nil> 16215ms (03:22:50.614)
Trace[1255241542]: [16.215582203s] [16.215582203s] END
time="2024-03-22T03:22:50Z" level=error msg="Error writing ping" error="write tcp 192.168.56.110:39688->192.168.56.110:6443: i/o timeout"
I0322 03:22:50.658838    2308 trace.go:236] Trace[947482161]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:f2ea7820-c8b9-4abf-a9fa-72de578408d9,client:127.0.0.1,protocol:HTTP/2.0,resource:services,scope:cluster,url:/api/v1/services,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:22:46.240) (total time: 4411ms):
Trace[947482161]: ---"Writing http response done" count:3 4409ms (03:22:50.651)
Trace[947482161]: [4.411054829s] [4.411054829s] END
time="2024-03-22T03:22:50Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:47.703085619 +0000 UTC m=+323.397898829) (total time: 2.961098385s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/horizontalpodautoscalers/% false]]"
I0322 03:22:50.679469    2308 trace.go:236] Trace[1569385279]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:46f2c28b-3031-4ea2-badb-dad5529c1641,client:127.0.0.1,protocol:HTTP/2.0,resource:apiservices,scope:cluster,url:/apis/apiregistration.k8s.io/v1/apiservices,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:22:46.301) (total time: 4369ms):
Trace[1569385279]: ---"Writing http response done" count:26 4363ms (03:22:50.670)
Trace[1569385279]: [4.369443251s] [4.369443251s] END
I0322 03:22:50.682350    2308 trace.go:236] Trace[2111494665]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:be4bbd00-89f0-44c3-80e6-2d9d58adb869,client:127.0.0.1,protocol:HTTP/2.0,resource:serviceaccounts,scope:cluster,url:/api/v1/serviceaccounts,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:22:46.554) (total time: 4127ms):
Trace[2111494665]: ---"Writing http response done" count:40 4123ms (03:22:50.682)
Trace[2111494665]: [4.127663129s] [4.127663129s] END
I0322 03:22:50.682992    2308 trace.go:236] Trace[1914012143]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:d1e16653-d898-42ea-ba75-7b3bef54e37f,client:127.0.0.1,protocol:HTTP/2.0,resource:rolebindings,scope:cluster,url:/apis/rbac.authorization.k8s.io/v1/rolebindings,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:22:46.599) (total time: 4083ms):
Trace[1914012143]: ---"Writing http response done" count:9 4078ms (03:22:50.682)
Trace[1914012143]: [4.083427088s] [4.083427088s] END
I0322 03:22:50.683202    2308 trace.go:236] Trace[1658610085]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:5a7f4f51-52bd-45a3-b90b-17b3bcf9045a,client:127.0.0.1,protocol:HTTP/2.0,resource:runtimeclasses,scope:cluster,url:/apis/node.k8s.io/v1/runtimeclasses,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:22:46.607) (total time: 4075ms):
Trace[1658610085]: ---"Writing http response done" count:10 4074ms (03:22:50.683)
Trace[1658610085]: [4.075817935s] [4.075817935s] END
I0322 03:22:50.683340    2308 trace.go:236] Trace[1226279173]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:e83a9f0d-27bf-4ffd-b088-06eb3b944a3a,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:namespace,url:/api/v1/namespaces/kube-system/configmaps,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:22:46.615) (total time: 4067ms):
Trace[1226279173]: ---"Writing http response done" count:8 4067ms (03:22:50.683)
Trace[1226279173]: [4.06757763s] [4.06757763s] END
I0322 03:22:50.683463    2308 trace.go:236] Trace[1137829860]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:7569f466-341e-48ad-8582-579bdfa43d16,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:cluster,url:/api/v1/nodes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:22:46.212) (total time: 4470ms):
Trace[1137829860]: ---"Writing http response done" count:1 4469ms (03:22:50.683)
Trace[1137829860]: [4.47094069s] [4.47094069s] END
time="2024-03-22T03:22:50Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:47.715025687 +0000 UTC m=+323.409838897) (total time: 2.988300464s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/limitranges/% false]]"
time="2024-03-22T03:22:50Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:47.71943741 +0000 UTC m=+323.414250632) (total time: 3.028395487s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/leases/% false]]"
time="2024-03-22T03:22:50Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:47.799619441 +0000 UTC m=+323.494432639) (total time: 2.950059124s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/apiextensions.k8s.io/customresourcedefinitions/% false]]"
time="2024-03-22T03:22:50Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:47.870346816 +0000 UTC m=+323.565160014) (total time: 2.894062259s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/deployments/% false]]"
time="2024-03-22T03:22:50Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:46.241224758 +0000 UTC m=+321.936037953) (total time: 4.557421341s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/serviceaccounts/% 589]]"
time="2024-03-22T03:22:50Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:49.230088683 +0000 UTC m=+324.924901913) (total time: 1.573916463s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/clusterrolebindings/% false]]"
I0322 03:22:50.951701    2308 trace.go:236] Trace[324093209]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:561b0908-1dac-425c-bc4f-8cd6f2bc5f4b,client:127.0.0.1,protocol:HTTP/2.0,resource:resourcequotas,scope:cluster,url:/api/v1/resourcequotas,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:22:46.521) (total time: 2884ms):
Trace[324093209]: ---"Writing http response done" count:0 2873ms (03:22:49.405)
Trace[324093209]: [2.884390922s] [2.884390922s] END
I0322 03:22:51.261496    2308 trace.go:236] Trace[45874876]: "Proxy via http_connect protocol over tcp" address:10.42.0.5:10250 (22-Mar-2024 03:22:41.804) (total time: 9442ms):
Trace[45874876]: [9.442868003s] [9.442868003s] END
I0322 03:22:51.322966    2308 trace.go:236] Trace[1952433003]: "Proxy via http_connect protocol over tcp" address:10.42.0.5:10250 (22-Mar-2024 03:22:41.800) (total time: 9519ms):
Trace[1952433003]: [9.519138559s] [9.519138559s] END
I0322 03:22:51.395182    2308 trace.go:236] Trace[289865162]: "Proxy via http_connect protocol over tcp" address:10.42.0.5:10250 (22-Mar-2024 03:22:41.805) (total time: 9583ms):
Trace[289865162]: [9.583290251s] [9.583290251s] END
I0322 03:22:51.867136    2308 trace.go:236] Trace[2016596746]: "List" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:ef5d07a3-0687-4575-b07a-532f8b039825,client:127.0.0.1,protocol:HTTP/2.0,resource:runtimeclasses,scope:cluster,url:/apis/node.k8s.io/v1/runtimeclasses,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:22:47.544) (total time: 1141ms):
Trace[2016596746]: ---"Writing http response done" count:10 1139ms (03:22:48.686)
Trace[2016596746]: [1.141797446s] [1.141797446s] END
I0322 03:22:53.094312    2308 trace.go:236] Trace[568892612]: "Reflector ListAndWatch" name:object-"kube-system"/"coredns-custom" (22-Mar-2024 03:22:38.996) (total time: 11441ms):
Trace[568892612]: ---"Objects listed" error:<nil> 11436ms (03:22:50.433)
Trace[568892612]: [11.441041063s] [11.441041063s] END
I0322 03:22:53.100654    2308 trace.go:236] Trace[1914874922]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:064cb841-c68a-438a-a78b-3e888fe7e3f7,client:127.0.0.1,protocol:HTTP/2.0,resource:csinodes,scope:cluster,url:/apis/storage.k8s.io/v1/csinodes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:22:45.069) (total time: 6143ms):
Trace[1914874922]: ---"About to List from storage" 2395ms (03:22:47.464)
Trace[1914874922]: ---"Writing http response done" count:1 3742ms (03:22:51.213)
Trace[1914874922]: [6.143989497s] [6.143989497s] END
I0322 03:22:53.308880    2308 trace.go:236] Trace[75293069]: "Proxy via http_connect protocol over tcp" address:10.42.0.5:10250 (22-Mar-2024 03:22:41.805) (total time: 11497ms):
Trace[75293069]: [11.49701256s] [11.49701256s] END
W0322 03:22:54.073490    2308 transport.go:301] Unable to cancel request for *otelhttp.Transport
I0322 03:22:54.074090    2308 trace.go:236] Trace[2043048610]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:9ab130fa-1beb-4c42-907d-8b44bce3f27d,client:127.0.0.1,protocol:HTTP/2.0,resource:horizontalpodautoscalers,scope:cluster,url:/apis/autoscaling/v2/horizontalpodautoscalers,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:22:47.646) (total time: 6422ms):
Trace[2043048610]: ["cacher list" audit-id:9ab130fa-1beb-4c42-907d-8b44bce3f27d,type:horizontalpodautoscalers.autoscaling 6420ms (03:22:47.649)
Trace[2043048610]:  ---"watchCache locked acquired" 2125ms (03:22:49.775)]
Trace[2043048610]: ---"Writing http response done" count:0 4289ms (03:22:54.069)
Trace[2043048610]: [6.422692161s] [6.422692161s] END
I0322 03:22:54.185128    2308 trace.go:236] Trace[1702649657]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:41.702) (total time: 12472ms):
Trace[1702649657]: ---"Objects listed" error:<nil> 12470ms (03:22:54.172)
Trace[1702649657]: [12.472540814s] [12.472540814s] END
I0322 03:22:54.199450    2308 trace.go:236] Trace[950643744]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:37.496) (total time: 16702ms):
Trace[950643744]: ---"Objects listed" error:<nil> 16702ms (03:22:54.198)
Trace[950643744]: [16.702066138s] [16.702066138s] END
E0322 03:22:54.280292    2308 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
I0322 03:22:54.352221    2308 trace.go:236] Trace[661253305]: "List" accept:application/json, */*,audit-id:53c8db64-b67d-48fc-9253-17f8c399f537,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:cluster,url:/api/v1/configmaps,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:LIST (22-Mar-2024 03:22:48.114) (total time: 6237ms):
Trace[661253305]: ---"Writing http response done" count:11 6229ms (03:22:54.352)
Trace[661253305]: [6.237882112s] [6.237882112s] END
I0322 03:22:52.007787    2308 trace.go:236] Trace[1108205421]: "Proxy via http_connect protocol over tcp" address:10.42.0.5:10250 (22-Mar-2024 03:22:41.643) (total time: 10346ms):
Trace[1108205421]: [10.346512291s] [10.346512291s] END
time="2024-03-22T03:22:52Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:50.052056148 +0000 UTC m=+325.746869352) (total time: 2.516712528s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/flowschemas/% false]]"
E0322 03:22:54.669809    2308 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 03:22:54.674577    2308 trace.go:236] Trace[1104827166]: "SerializeObject" audit-id:3b15c8c5-5240-4071-8896-9bea3e1b6293,method:PUT,url:/apis/batch/v1/namespaces/kube-system/jobs/helm-install-traefik/status,protocol:HTTP/2.0,mediaType:application/vnd.kubernetes.protobuf,encoder:{"encodeGV":"batch/v1","encoder":"protobuf","name":"versioning"} (22-Mar-2024 03:22:45.774) (total time: 8896ms):
Trace[1104827166]: ---"About to start writing response" size:72 1327ms (03:22:47.102)
Trace[1104827166]: ---"About to start writing response" size:69 5221ms (03:22:52.325)
Trace[1104827166]: ---"Write call failed" writer:responsewriter.outerWithCloseNotifyAndFlush,size:69,firstWrite:false,err:http: Handler timeout 2336ms (03:22:54.662)
Trace[1104827166]: [8.896707236s] [8.896707236s] END
I0322 03:22:54.674853    2308 trace.go:236] Trace[1300058582]: "List" accept:application/json, */*,audit-id:8aa54eab-41fb-4e45-b4bf-2129311ed09e,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:22:52.690) (total time: 1984ms):
Trace[1300058582]: ---"Writing http response done" count:1 1980ms (03:22:54.674)
Trace[1300058582]: [1.984526133s] [1.984526133s] END
E0322 03:22:54.679594    2308 timeout.go:142] post-timeout activity - time-elapsed: 18.885512932s, PUT "/apis/batch/v1/namespaces/kube-system/jobs/helm-install-traefik/status" result: <nil>
I0322 03:22:54.681597    2308 trace.go:236] Trace[787457663]: "List" accept:application/json, */*,audit-id:883887c4-0c81-45ae-a5c2-a34e0f293fce,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:22:52.717) (total time: 1964ms):
Trace[787457663]: ---"Writing http response done" count:1 1960ms (03:22:54.681)
Trace[787457663]: [1.964020247s] [1.964020247s] END
I0322 03:22:54.706440    2308 trace.go:236] Trace[224094315]: "List" accept:application/json, */*,audit-id:d42bac10-7ce4-4994-95cd-2c5c9aac42ee,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:22:52.735) (total time: 1970ms):
Trace[224094315]: ---"Writing http response done" count:1 1970ms (03:22:54.705)
Trace[224094315]: [1.970512182s] [1.970512182s] END
E0322 03:22:52.997544    2308 server.go:310] "Unable to authenticate the request due to an error" err="context canceled"
W0322 03:22:54.944105    2308 transport.go:301] Unable to cancel request for *otelhttp.Transport
time="2024-03-22T03:22:55Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:52.338597575 +0000 UTC m=+328.033410781) (total time: 2.85485065s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/ingressclasses/% 594]]"
I0322 03:22:55.359912    2308 trace.go:236] Trace[2093854111]: "SerializeObject" audit-id:6516f80e-5da4-4af7-98e3-28514f9ead5a,method:GET,url:/apis/rbac.authorization.k8s.io/v1/clusterrolebindings,protocol:HTTP/2.0,mediaType:application/vnd.kubernetes.protobuf,encoder:{"encodeGV":"rbac.authorization.k8s.io/v1","encoder":"protobuf","name":"versioning"} (22-Mar-2024 03:22:47.041) (total time: 8314ms):
Trace[2093854111]: ---"About to start writing response" size:40953 5059ms (03:22:52.101)
Trace[2093854111]: ---"Write call succeeded" writer:responsewriter.outerWithCloseNotifyAndFlush,size:40953,firstWrite:true 3255ms (03:22:55.356)
Trace[2093854111]: [8.31476666s] [8.31476666s] END
I0322 03:22:55.361626    2308 trace.go:236] Trace[1419218276]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:6516f80e-5da4-4af7-98e3-28514f9ead5a,client:127.0.0.1,protocol:HTTP/2.0,resource:clusterrolebindings,scope:cluster,url:/apis/rbac.authorization.k8s.io/v1/clusterrolebindings,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:22:44.716) (total time: 10644ms):
Trace[1419218276]: ["cacher list" audit-id:6516f80e-5da4-4af7-98e3-28514f9ead5a,type:clusterrolebindings.rbac.authorization.k8s.io 10638ms (03:22:44.722)
Trace[1419218276]:  ---"watchCache locked acquired" 2302ms (03:22:47.024)]
Trace[1419218276]: ---"Writing http response done" count:54 8334ms (03:22:55.360)
Trace[1419218276]: [10.644030764s] [10.644030764s] END
I0322 03:22:55.368336    2308 trace.go:236] Trace[575432613]: "SerializeObject" audit-id:fb20ab7b-09af-446b-a22f-be8577cc9729,method:GET,url:/apis/k3s.cattle.io/v1/addons,protocol:HTTP/2.0,mediaType:application/vnd.kubernetes.protobuf,encoder:{"encodeGV":"meta.k8s.io/v1","encoder":"protobuf","name":"versioning"} (22-Mar-2024 03:22:49.093) (total time: 6274ms):
Trace[575432613]: ---"About to start writing response" size:5242 3279ms (03:22:52.372)
Trace[575432613]: ---"Write call succeeded" writer:responsewriter.outerWithCloseNotifyAndFlush,size:5242,firstWrite:true 2995ms (03:22:55.367)
Trace[575432613]: [6.274669266s] [6.274669266s] END
I0322 03:22:55.368870    2308 trace.go:236] Trace[447808683]: "List" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json,audit-id:fb20ab7b-09af-446b-a22f-be8577cc9729,client:127.0.0.1,protocol:HTTP/2.0,resource:addons,scope:cluster,url:/apis/k3s.cattle.io/v1/addons,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/metadata-informers,verb:LIST (22-Mar-2024 03:22:44.830) (total time: 10538ms):
Trace[447808683]: ---"Writing http response done" count:13 10533ms (03:22:55.368)
Trace[447808683]: [10.538420047s] [10.538420047s] END
time="2024-03-22T03:22:52Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:50.187979136 +0000 UTC m=+325.882792337) (total time: 2.540018167s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/configmaps/% false]]"
I0322 03:22:55.715322    2308 trace.go:236] Trace[2086129188]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:b4394aca-90e9-4ee1-9cd4-f32e64fa3106,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:namespace,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:22:46.483) (total time: 7163ms):
Trace[2086129188]: ---"Writing http response done" count:0 4365ms (03:22:53.646)
Trace[2086129188]: [7.163484637s] [7.163484637s] END
time="2024-03-22T03:22:53Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:50.39356831 +0000 UTC m=+326.088381520) (total time: 2.671282351s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/limitranges/% 594]]"
time="2024-03-22T03:22:53Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:49.315481623 +0000 UTC m=+325.010294824) (total time: 1.54158998s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/jobs/% 594]]"
I0322 03:22:56.008929    2308 trace.go:236] Trace[1130934615]: "SerializeObject" audit-id:df153ecb-0f06-442e-9c05-b98526822558,method:GET,url:/api/v1/nodes,protocol:HTTP/2.0,mediaType:application/json,encoder:{"encodeGV":"v1","encoder":"{\"name\":\"json\",\"pretty\":\"false\",\"strict\":\"false\",\"yaml\":\"false\"}","name":"versioning"} (22-Mar-2024 03:22:47.606) (total time: 8397ms):
Trace[1130934615]: ---"About to start writing response" size:5970 2090ms (03:22:49.696)
Trace[1130934615]: ---"Write call succeeded" writer:responsewriter.outerWithCloseNotifyAndFlush,size:5970,firstWrite:true 6305ms (03:22:56.001)
Trace[1130934615]: [8.39755288s] [8.39755288s] END
I0322 03:22:56.010677    2308 trace.go:236] Trace[898220969]: "List" accept:application/json, */*,audit-id:df153ecb-0f06-442e-9c05-b98526822558,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:cluster,url:/api/v1/nodes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:22:47.605) (total time: 8404ms):
Trace[898220969]: ---"Writing http response done" count:1 8404ms (03:22:56.009)
Trace[898220969]: [8.40493217s] [8.40493217s] END
time="2024-03-22T03:22:53Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:50.032765158 +0000 UTC m=+325.727578364) (total time: 1.126187062s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/poddisruptionbudgets/% false]]"
time="2024-03-22T03:22:54Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:49.927883001 +0000 UTC m=+325.622696212) (total time: 2.005297445s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? AND mkv.id <= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC LIMIT 1000 : [[/registry/podtemplates/% 594 false]]"
I0322 03:22:56.054793    2308 trace.go:236] Trace[232753985]: "List" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:dd9fed1e-6bdb-4ea7-adbf-251df9f35d2d,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:22:47.532) (total time: 8521ms):
Trace[232753985]: ---"Writing http response done" count:1 8512ms (03:22:56.054)
Trace[232753985]: [8.521128113s] [8.521128113s] END
time="2024-03-22T03:22:56Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:52.326235859 +0000 UTC m=+328.021049082) (total time: 3.969225387s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/ingressclasses/% false]]"
I0322 03:22:56.768805    2308 trace.go:236] Trace[1128116747]: "List" accept:application/json, */*,audit-id:1fb7e205-f308-4f66-bd33-cf3b0c3da8ff,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:cluster,url:/api/v1/nodes,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:LIST (22-Mar-2024 03:22:48.102) (total time: 6214ms):
Trace[1128116747]: ---"Writing http response done" count:1 3308ms (03:22:54.317)
Trace[1128116747]: [6.214332417s] [6.214332417s] END
I0322 03:22:56.802452    2308 trace.go:236] Trace[1638761631]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:9e5f1112-e986-4a15-8245-e28c6e531a5f,client:127.0.0.1,protocol:HTTP/2.0,resource:clusterroles,scope:cluster,url:/apis/rbac.authorization.k8s.io/v1/clusterroles,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:22:46.565) (total time: 7779ms):
Trace[1638761631]: ["cacher list" audit-id:9e5f1112-e986-4a15-8245-e28c6e531a5f,type:clusterroles.rbac.authorization.k8s.io 7771ms (03:22:46.575)
Trace[1638761631]:  ---"watchCache locked acquired" 2933ms (03:22:49.509)
Trace[1638761631]:  ---"Listed items from cache" count:69 1565ms (03:22:51.075)]
Trace[1638761631]: ---"Writing http response done" count:69 3266ms (03:22:54.344)
Trace[1638761631]: [7.779299229s] [7.779299229s] END
I0322 03:22:57.796815    2308 trace.go:236] Trace[80764568]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:21:02.701) (total time: 115088ms):
Trace[80764568]: ---"Objects listed" error:<nil> 115087ms (03:22:57.788)
Trace[80764568]: [1m55.088105619s] [1m55.088105619s] END
time="2024-03-22T03:22:57Z" level=error msg="Error writing ping" error="write tcp 192.168.56.110:39688->192.168.56.110:6443: i/o timeout"
I0322 03:22:57.928640    2308 trace.go:236] Trace[1205982461]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:e13a8b2c-5b23-4143-a412-68653d7155f1,client:127.0.0.1,protocol:HTTP/2.0,resource:secrets,scope:cluster,url:/api/v1/secrets,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:22:44.703) (total time: 10607ms):
Trace[1205982461]: ---"About to List from storage" 2266ms (03:22:46.970)
Trace[1205982461]: ---"Writing http response done" count:6 8334ms (03:22:55.311)
Trace[1205982461]: [10.607731053s] [10.607731053s] END
I0322 03:22:57.957834    2308 trace.go:236] Trace[435672467]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:21:35.835) (total time: 82115ms):
Trace[435672467]: ---"Objects listed" error:<nil> 82112ms (03:22:57.947)
Trace[435672467]: [1m22.115465862s] [1m22.115465862s] END
I0322 03:22:57.967579    2308 trace.go:236] Trace[1440796720]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:21:36.846) (total time: 81120ms):
Trace[1440796720]: ---"Objects listed" error:<nil> 81120ms (03:22:57.967)
Trace[1440796720]: [1m21.120866521s] [1m21.120866521s] END
I0322 03:22:57.999961    2308 trace.go:236] Trace[1908225636]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:21:02.671) (total time: 115326ms):
Trace[1908225636]: ---"Objects listed" error:<nil> 115319ms (03:22:57.991)
Trace[1908225636]: [1m55.326211402s] [1m55.326211402s] END
I0322 03:22:58.070013    2308 trace.go:236] Trace[831981286]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:21:01.586) (total time: 116480ms):
Trace[831981286]: ---"Objects listed" error:<nil> 116473ms (03:22:58.060)
Trace[831981286]: [1m56.480168326s] [1m56.480168326s] END
time="2024-03-22T03:22:58Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:47.673398236 +0000 UTC m=+323.368211440) (total time: 10.396182999s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/controllers/% false]]"
I0322 03:22:58.078251    2308 trace.go:236] Trace[1334444140]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:20:52.400) (total time: 125678ms):
Trace[1334444140]: ---"Objects listed" error:<nil> 125676ms (03:22:58.076)
Trace[1334444140]: [2m5.678114544s] [2m5.678114544s] END
time="2024-03-22T03:22:58Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:53.118005571 +0000 UTC m=+328.812818771) (total time: 4.963253937s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/horizontalpodautoscalers/% 594]]"
I0322 03:22:58.124077    2308 trace.go:236] Trace[486751103]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:33.941) (total time: 24175ms):
Trace[486751103]: ---"Objects listed" error:<nil> 24171ms (03:22:58.113)
Trace[486751103]: [24.175039845s] [24.175039845s] END
I0322 03:22:58.127864    2308 trace.go:236] Trace[1147112208]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:21:02.300) (total time: 115827ms):
Trace[1147112208]: ---"Objects listed" error:<nil> 115826ms (03:22:58.127)
Trace[1147112208]: [1m55.827348407s] [1m55.827348407s] END
I0322 03:22:58.184140    2308 trace.go:236] Trace[1802461988]: "DeltaFIFO Pop Process" ID:etcdsnapshotfiles.k3s.cattle.io,Depth:21,Reason:slow event handlers blocking the queue (22-Mar-2024 03:22:58.073) (total time: 109ms):
Trace[1802461988]: [109.965324ms] [109.965324ms] END
E0322 03:22:58.694684    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:23:01.073561    2308 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 03:22:58.954257    2308 trace.go:236] Trace[1194016421]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:593928b4-e2be-44ca-93c4-a8adb964884c,client:127.0.0.1,protocol:HTTP/2.0,resource:persistentvolumes,scope:cluster,url:/api/v1/persistentvolumes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:22:46.170) (total time: 12780ms):
Trace[1194016421]: ---"About to List from storage" 2663ms (03:22:48.834)
Trace[1194016421]: ["cacher list" audit-id:593928b4-e2be-44ca-93c4-a8adb964884c,type:persistentvolumes 10115ms (03:22:48.835)]
Trace[1194016421]: ---"Writing http response done" count:0 6927ms (03:22:58.951)
Trace[1194016421]: [12.780333047s] [12.780333047s] END
I0322 03:22:59.057795    2308 trace.go:236] Trace[1902194313]: "List" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:39ba8e56-a778-4dc4-95ce-896923684cdf,client:127.0.0.1,protocol:HTTP/2.0,resource:csidrivers,scope:cluster,url:/apis/storage.k8s.io/v1/csidrivers,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:22:53.896) (total time: 5155ms):
Trace[1902194313]: ["cacher list" audit-id:39ba8e56-a778-4dc4-95ce-896923684cdf,type:csidrivers.storage.k8s.io 5152ms (03:22:53.903)
Trace[1902194313]:  ---"watchCache locked acquired" 2145ms (03:22:56.048)]
Trace[1902194313]: ---"Writing http response done" count:0 1633ms (03:22:59.051)
Trace[1902194313]: [5.155290432s] [5.155290432s] END
I0322 03:22:59.174305    2308 trace.go:236] Trace[1107548564]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:c48fa408-735c-4bb0-b816-37cc9a31e17e,client:127.0.0.1,protocol:HTTP/2.0,resource:volumeattachments,scope:cluster,url:/apis/storage.k8s.io/v1/volumeattachments,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:22:46.227) (total time: 8418ms):
Trace[1107548564]: ["cacher list" audit-id:c48fa408-735c-4bb0-b816-37cc9a31e17e,type:volumeattachments.storage.k8s.io 8417ms (03:22:46.228)
Trace[1107548564]:  ---"watchCache locked acquired" 2816ms (03:22:49.044)]
Trace[1107548564]: ---"Writing http response done" count:0 5597ms (03:22:54.645)
Trace[1107548564]: [8.418018493s] [8.418018493s] END
E0322 03:22:59.278696    2308 controller.go:193] "Failed to update lease" err="Put \"https://127.0.0.1:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/server?timeout=10s\": net/http: request canceled (Client.Timeout exceeded while awaiting headers)"
I0322 03:23:01.099503    2308 trace.go:236] Trace[1621099360]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:49d72ccc-6f7b-46e9-8587-4ad56b45e928,client:127.0.0.1,protocol:HTTP/2.0,resource:ingressclasses,scope:cluster,url:/apis/networking.k8s.io/v1/ingressclasses,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:22:53.790) (total time: 7307ms):
Trace[1621099360]: ---"Writing http response done" count:0 7298ms (03:23:01.098)
Trace[1621099360]: [7.307956334s] [7.307956334s] END
E0322 03:23:01.230526    2308 kubelet_node_status.go:540] "Error updating node status, will retry" err="error getting node \"server\": Get \"https://127.0.0.1:6443/api/v1/nodes/server?timeout=10s\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)"
I0322 03:23:01.430770    2308 trace.go:236] Trace[269792991]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:dbbcad07-3fc2-46cd-9fcb-c5f13a128c9e,client:127.0.0.1,protocol:HTTP/2.0,resource:services,scope:cluster,url:/api/v1/services,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:22:49.744) (total time: 10052ms):
Trace[269792991]: ["cacher list" audit-id:dbbcad07-3fc2-46cd-9fcb-c5f13a128c9e,type:services 8152ms (03:22:51.646)
Trace[269792991]:  ---"watchCache locked acquired" 2357ms (03:22:54.005)]
Trace[269792991]: ---"Writing http response done" count:3 5785ms (03:22:59.797)
Trace[269792991]: [10.052859332s] [10.052859332s] END
I0322 03:23:01.479342    2308 trace.go:236] Trace[316921781]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:21:02.182) (total time: 119291ms):
Trace[316921781]: ---"Objects listed" error:<nil> 119285ms (03:23:01.468)
Trace[316921781]: [1m59.291954538s] [1m59.291954538s] END
I0322 03:23:01.514094    2308 trace.go:236] Trace[1322725288]: "SerializeObject" audit-id:e46da888-c3b3-4bb1-9363-056dc416d866,method:GET,url:/apis/flowcontrol.apiserver.k8s.io/v1beta3/flowschemas,protocol:HTTP/2.0,mediaType:application/vnd.kubernetes.protobuf,encoder:{"encodeGV":"flowcontrol.apiserver.k8s.io/v1beta3","encoder":"protobuf","name":"versioning"} (22-Mar-2024 03:22:49.182) (total time: 12330ms):
Trace[1322725288]: ---"About to start writing response" size:13232 5310ms (03:22:54.492)
Trace[1322725288]: ---"Write call succeeded" writer:responsewriter.outerWithCloseNotifyAndFlush,size:13232,firstWrite:true 7019ms (03:23:01.511)
Trace[1322725288]: [12.330587576s] [12.330587576s] END
I0322 03:23:01.516068    2308 trace.go:236] Trace[284663780]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:e46da888-c3b3-4bb1-9363-056dc416d866,client:127.0.0.1,protocol:HTTP/2.0,resource:flowschemas,scope:cluster,url:/apis/flowcontrol.apiserver.k8s.io/v1beta3/flowschemas,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:22:46.311) (total time: 15204ms):
Trace[284663780]: ---"Writing http response done" count:13 15195ms (03:23:01.515)
Trace[284663780]: [15.204034226s] [15.204034226s] END
I0322 03:23:01.521736    2308 trace.go:236] Trace[1950124696]: "SerializeObject" audit-id:bea6ef48-358e-413f-982e-c4983182e2ad,method:GET,url:/api/v1/secrets,protocol:HTTP/2.0,mediaType:application/vnd.kubernetes.protobuf,encoder:{"encodeGV":"v1","encoder":"protobuf","name":"versioning"} (22-Mar-2024 03:22:48.977) (total time: 12544ms):
Trace[1950124696]: ---"About to start writing response" size:125487 3263ms (03:22:52.240)
Trace[1950124696]: ---"Write call succeeded" writer:responsewriter.outerWithCloseNotifyAndFlush,size:125487,firstWrite:true 9281ms (03:23:01.521)
Trace[1950124696]: [12.544469011s] [12.544469011s] END
I0322 03:23:01.521864    2308 trace.go:236] Trace[811427608]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:bea6ef48-358e-413f-982e-c4983182e2ad,client:127.0.0.1,protocol:HTTP/2.0,resource:secrets,scope:cluster,url:/api/v1/secrets,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:22:46.180) (total time: 15341ms):
Trace[811427608]: ---"Writing http response done" count:6 15332ms (03:23:01.521)
Trace[811427608]: [15.341173376s] [15.341173376s] END
time="2024-03-22T03:22:59Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:49.656360606 +0000 UTC m=+325.351173807) (total time: 10.242644654s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/rolebindings/% 594]]"
I0322 03:22:59.431706    2308 trace.go:236] Trace[937258671]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:37.206) (total time: 20074ms):
Trace[937258671]: ---"Objects listed" error:<nil> 20060ms (03:22:57.267)
Trace[937258671]: [20.07493998s] [20.07493998s] END
time="2024-03-22T03:23:01Z" level=info msg="Slow SQL (started: 2024-03-22 03:23:00.152702551 +0000 UTC m=+335.847515750) (total time: 1.642716223s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/daemonsets/% 594]]"
time="2024-03-22T03:22:59Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:51.012631411 +0000 UTC m=+326.707444613) (total time: 8.893998911s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/daemonsets/% false]]"
I0322 03:22:59.449083    2308 trace.go:236] Trace[1226031506]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:38.848) (total time: 20597ms):
Trace[1226031506]: ---"Objects listed" error:<nil> 20586ms (03:22:59.435)
Trace[1226031506]: [20.597975232s] [20.597975232s] END
time="2024-03-22T03:23:01Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:55.26913871 +0000 UTC m=+330.963951905) (total time: 4.991379848s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? AND mkv.id <= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC LIMIT 1000 : [[/registry/cronjobs/% 595 false]]"
time="2024-03-22T03:22:59Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:53.905533992 +0000 UTC m=+329.600347191) (total time: 6.001496346s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/horizontalpodautoscalers/% false]]"
I0322 03:22:59.596894    2308 trace.go:236] Trace[174917750]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:408e1151-f296-4023-a088-d1a6b1cb397b,client:127.0.0.1,protocol:HTTP/2.0,resource:statefulsets,scope:cluster,url:/apis/apps/v1/statefulsets,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:22:53.898) (total time: 5694ms):
Trace[174917750]: ["cacher list" audit-id:408e1151-f296-4023-a088-d1a6b1cb397b,type:statefulsets.apps 5690ms (03:22:53.903)
Trace[174917750]:  ---"watchCache locked acquired" 2144ms (03:22:56.047)]
Trace[174917750]: ---"Writing http response done" count:0 2185ms (03:22:59.593)
Trace[174917750]: [5.694063229s] [5.694063229s] END
time="2024-03-22T03:22:59Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:54.089214492 +0000 UTC m=+329.784027700) (total time: 5.825456806s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/pods/% 594]]"
I0322 03:22:59.812586    2308 trace.go:236] Trace[68624795]: "SerializeObject" audit-id:06b1188a-413f-47bf-a4e7-cf37e068dd7e,method:GET,url:/apis/batch/v1/jobs,protocol:HTTP/2.0,mediaType:application/json,encoder:{"encodeGV":"batch/v1","encoder":"{\"name\":\"json\",\"pretty\":\"false\",\"strict\":\"false\",\"yaml\":\"false\"}","name":"versioning"} (22-Mar-2024 03:22:53.822) (total time: 5981ms):
Trace[68624795]: ---"About to start writing response" size:16230 4954ms (03:22:58.776)
Trace[68624795]: [5.981929825s] [5.981929825s] END
I0322 03:23:02.043933    2308 trace.go:236] Trace[1441904936]: "List" accept:application/json, */*,audit-id:06b1188a-413f-47bf-a4e7-cf37e068dd7e,client:127.0.0.1,protocol:HTTP/2.0,resource:jobs,scope:cluster,url:/apis/batch/v1/jobs,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:LIST (22-Mar-2024 03:22:50.971) (total time: 11065ms):
Trace[1441904936]: ---"Writing http response done" count:2 11055ms (03:23:02.035)
Trace[1441904936]: [11.065222212s] [11.065222212s] END
time="2024-03-22T03:22:59Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:51.784317401 +0000 UTC m=+327.479130599) (total time: 8.163336076s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/csistoragecapacities/% 594]]"
I0322 03:22:59.819581    2308 trace.go:236] Trace[1690402951]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:6a0f421e-5de3-4189-9f1a-aa69e34202f9,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:cluster,url:/api/v1/configmaps,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:22:50.755) (total time: 9061ms):
Trace[1690402951]: ["cacher list" audit-id:6a0f421e-5de3-4189-9f1a-aa69e34202f9,type:configmaps 9054ms (03:22:50.762)
Trace[1690402951]:  ---"watchCache locked acquired" 2609ms (03:22:53.372)]
Trace[1690402951]: ---"Writing http response done" count:11 6436ms (03:22:59.816)
Trace[1690402951]: [9.061183004s] [9.061183004s] END
I0322 03:22:59.823578    2308 trace.go:236] Trace[939802193]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:116d3dae-08a0-483e-98b1-a941b171d5f6,client:127.0.0.1,protocol:HTTP/2.0,resource:runtimeclasses,scope:cluster,url:/apis/node.k8s.io/v1/runtimeclasses,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:22:52.642) (total time: 7180ms):
Trace[939802193]: ["cacher list" audit-id:116d3dae-08a0-483e-98b1-a941b171d5f6,type:runtimeclasses.node.k8s.io 7171ms (03:22:52.651)
Trace[939802193]:  ---"Listed items from cache" count:10 2003ms (03:22:54.656)]
Trace[939802193]: ---"Writing http response done" count:10 2464ms (03:22:59.823)
Trace[939802193]: [7.180475218s] [7.180475218s] END
I0322 03:22:59.974368    2308 trace.go:236] Trace[1137201602]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:21:00.517) (total time: 117083ms):
Trace[1137201602]: ---"Objects listed" error:<nil> 117075ms (03:22:57.593)
Trace[1137201602]: [1m57.083483629s] [1m57.083483629s] END
time="2024-03-22T03:22:59Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:54.247984261 +0000 UTC m=+329.942797463) (total time: 5.703442401s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/statefulsets/% false]]"
I0322 03:23:00.076159    2308 trace.go:236] Trace[750791843]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:21:01.516) (total time: 118553ms):
Trace[750791843]: ---"Objects listed" error:<nil> 116185ms (03:22:57.702)
Trace[750791843]: [1m58.553474245s] [1m58.553474245s] END
time="2024-03-22T03:22:59Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:54.525372418 +0000 UTC m=+330.220185617) (total time: 5.454936093s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/poddisruptionbudgets/% 594]]"
I0322 03:23:00.137405    2308 trace.go:236] Trace[355329160]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:20:52.088) (total time: 125672ms):
Trace[355329160]: ---"Objects listed" error:<nil> 125672ms (03:22:57.761)
Trace[355329160]: [2m5.67240747s] [2m5.67240747s] END
time="2024-03-22T03:22:59Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:52.286513734 +0000 UTC m=+327.981326940) (total time: 7.69679975s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/flowschemas/% 594]]"
I0322 03:23:00.137462    2308 trace.go:236] Trace[1419743847]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:5031058e-e6ea-46c5-ab9a-d891a802bb00,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:22:53.905) (total time: 4947ms):
Trace[1419743847]: ---"Writing http response done" count:1 4944ms (03:22:58.853)
Trace[1419743847]: [4.947469269s] [4.947469269s] END
I0322 03:22:58.954334    2308 trace.go:236] Trace[866458571]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:fef59802-9119-4718-9896-752ca2a7682e,client:127.0.0.1,protocol:HTTP/2.0,resource:controllerrevisions,scope:cluster,url:/apis/apps/v1/controllerrevisions,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:22:47.288) (total time: 11658ms):
Trace[866458571]: ---"About to List from storage" 1910ms (03:22:49.199)
Trace[866458571]: ---"Writing http response done" count:0 9740ms (03:22:58.947)
Trace[866458571]: [11.658737795s] [11.658737795s] END
I0322 03:23:00.257827    2308 trace.go:236] Trace[1969255449]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:21:52.248) (total time: 65654ms):
Trace[1969255449]: ---"Objects listed" error:<nil> 65639ms (03:22:57.888)
Trace[1969255449]: [1m5.654343111s] [1m5.654343111s] END
time="2024-03-22T03:22:59Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:52.287415711 +0000 UTC m=+327.982228908) (total time: 7.699962766s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/helm.cattle.io/helmchartconfigs/% false]]"
I0322 03:23:00.339904    2308 trace.go:236] Trace[1310048731]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:21:49.837) (total time: 68091ms):
Trace[1310048731]: ---"Objects listed" error:<nil> 68087ms (03:22:57.924)
Trace[1310048731]: [1m8.091129462s] [1m8.091129462s] END
time="2024-03-22T03:22:59Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:50.95924592 +0000 UTC m=+326.654059178) (total time: 9.031191095s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/storageclasses/% 590]]"
I0322 03:23:00.428437    2308 trace.go:236] Trace[688664375]: "List" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json,audit-id:df27e3a1-0bd9-4303-be33-9484374ec4e2,client:127.0.0.1,protocol:HTTP/2.0,resource:helmcharts,scope:cluster,url:/apis/helm.cattle.io/v1/helmcharts,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/metadata-informers,verb:LIST (22-Mar-2024 03:22:46.952) (total time: 12199ms):
Trace[688664375]: ---"About to List from storage" 3117ms (03:22:50.069)
Trace[688664375]: ["cacher list" audit-id:df27e3a1-0bd9-4303-be33-9484374ec4e2,type:helmcharts.helm.cattle.io 9071ms (03:22:50.082)
Trace[688664375]:  ---"watchCache locked acquired" 2539ms (03:22:52.621)]
Trace[688664375]: ---"Writing http response done" count:2 6521ms (03:22:59.151)
Trace[688664375]: [12.199726321s] [12.199726321s] END
time="2024-03-22T03:23:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:51.888004029 +0000 UTC m=+327.582817229) (total time: 8.13883111s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/networkpolicies/% false]]"
I0322 03:23:00.782876    2308 trace.go:236] Trace[1567336878]: "DeltaFIFO Pop Process" ID:helmcharts.helm.cattle.io,Depth:19,Reason:slow event handlers blocking the queue (22-Mar-2024 03:22:58.184) (total time: 300ms):
Trace[1567336878]: [300.778162ms] [300.778162ms] END
time="2024-03-22T03:23:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:54.966827145 +0000 UTC m=+330.661640383) (total time: 5.062626618s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/services/specs/% 594]]"
I0322 03:23:02.238425    2308 trace.go:236] Trace[1420213004]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:74a60349-2d90-4828-8109-193904ed2241,client:127.0.0.1,protocol:HTTP/2.0,resource:replicasets,scope:cluster,url:/apis/apps/v1/replicasets,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/scheduler,verb:LIST (22-Mar-2024 03:22:57.792) (total time: 4441ms):
Trace[1420213004]: ---"Writing http response done" count:3 4438ms (03:23:02.233)
Trace[1420213004]: [4.44104323s] [4.44104323s] END
time="2024-03-22T03:23:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:55.137264363 +0000 UTC m=+330.832077597) (total time: 4.924626973s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/replicasets/% 594]]"
time="2024-03-22T03:23:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:55.156808888 +0000 UTC m=+330.851622086) (total time: 4.908768218s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/helm.cattle.io/helmchartconfigs/% 594]]"
time="2024-03-22T03:23:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:57.208928211 +0000 UTC m=+332.903741417) (total time: 2.865396556s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/minions/% 594]]"
time="2024-03-22T03:23:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:57.280156294 +0000 UTC m=+332.974969498) (total time: 2.801104308s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/prioritylevelconfigurations/% 594]]"
time="2024-03-22T03:23:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:57.550945183 +0000 UTC m=+333.245758399) (total time: 2.530870531s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/validatingwebhookconfigurations/% 594]]"
time="2024-03-22T03:23:00Z" level=error msg="Error writing ping" error="write tcp 192.168.56.110:39688->192.168.56.110:6443: i/o timeout"
time="2024-03-22T03:23:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:53.027313678 +0000 UTC m=+328.722126994) (total time: 4.990865714s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/clusterrolebindings/% 594]]"
time="2024-03-22T03:23:01Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:48.183454423 +0000 UTC m=+323.878267660) (total time: 13.126519583s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/services/specs/% false]]"
time="2024-03-22T03:23:01Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:48.983363841 +0000 UTC m=+324.678177038) (total time: 12.45367774s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/prioritylevelconfigurations/% false]]"
time="2024-03-22T03:23:01Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:49.055252171 +0000 UTC m=+324.750065378) (total time: 10.78621055s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/minions/% false]]"
time="2024-03-22T03:23:01Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:56.892777453 +0000 UTC m=+332.587590650) (total time: 4.71493571s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/services/endpoints/% 594]]"
E0322 03:23:02.294452    2308 kubelet.go:1402] "Container garbage collection failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded"
I0322 03:23:02.455191    2308 trace.go:236] Trace[1663406410]: "Proxy via http_connect protocol over tcp" address:10.42.0.5:10250 (22-Mar-2024 03:22:43.249) (total time: 17255ms):
Trace[1663406410]: [17.255068919s] [17.255068919s] END
I0322 03:23:02.488733    2308 trace.go:236] Trace[834494818]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:38.720) (total time: 22591ms):
Trace[834494818]: ---"Objects listed" error:<nil> 22580ms (03:23:01.301)
Trace[834494818]: [22.591408091s] [22.591408091s] END
I0322 03:23:02.531998    2308 trace.go:236] Trace[24568250]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:35.940) (total time: 25378ms):
Trace[24568250]: ---"Objects listed" error:<nil> 25377ms (03:23:01.318)
Trace[24568250]: [25.378018987s] [25.378018987s] END
I0322 03:23:00.722379    2308 trace.go:236] Trace[1721320970]: "SerializeObject" audit-id:ecfc66d4-1999-42e2-a047-1b1fcde723e1,method:GET,url:/api/v1/namespaces/kube-system/secrets,protocol:HTTP/2.0,mediaType:application/vnd.kubernetes.protobuf,encoder:{"encodeGV":"v1","encoder":"protobuf","name":"versioning"} (22-Mar-2024 03:22:55.703) (total time: 5009ms):
Trace[1721320970]: ---"About to start writing response" size:1982 5006ms (03:23:00.709)
Trace[1721320970]: [5.009532943s] [5.009532943s] END
I0322 03:23:02.780623    2308 trace.go:236] Trace[1981703371]: "List" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:ecfc66d4-1999-42e2-a047-1b1fcde723e1,client:127.0.0.1,protocol:HTTP/2.0,resource:secrets,scope:resource,url:/api/v1/namespaces/kube-system/secrets,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:22:50.807) (total time: 11969ms):
Trace[1981703371]: ---"Writing http response done" count:1 11965ms (03:23:02.777)
Trace[1981703371]: [11.969684978s] [11.969684978s] END
I0322 03:23:03.055191    2308 trace.go:236] Trace[21622213]: "List" accept:application/json, */*,audit-id:7fa664cb-233c-4796-9beb-d573bcad2173,client:127.0.0.1,protocol:HTTP/2.0,resource:helmchartconfigs,scope:cluster,url:/apis/helm.cattle.io/v1/helmchartconfigs,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:LIST (22-Mar-2024 03:22:55.800) (total time: 7251ms):
Trace[21622213]: ---"About to List from storage" 5113ms (03:23:00.914)
Trace[21622213]: ---"Writing http response done" count:0 2134ms (03:23:03.052)
Trace[21622213]: [7.251942774s] [7.251942774s] END
I0322 03:23:03.060538    2308 trace.go:236] Trace[1729020006]: "SerializeObject" audit-id:2f564492-81bb-435e-a2cc-768c0ac4ee1f,method:GET,url:/api/v1/pods,protocol:HTTP/2.0,mediaType:application/vnd.kubernetes.protobuf,encoder:{"encodeGV":"v1","encoder":"protobuf","name":"versioning"} (22-Mar-2024 03:22:53.979) (total time: 6942ms):
Trace[1729020006]: ---"About to start writing response" size:25234 5026ms (03:22:59.006)
Trace[1729020006]: ---"Write call succeeded" writer:responsewriter.outerWithCloseNotifyAndFlush,size:25234,firstWrite:true 1915ms (03:23:00.921)
Trace[1729020006]: [6.942161538s] [6.942161538s] END
I0322 03:23:03.061227    2308 trace.go:236] Trace[1095209977]: "List" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:2f564492-81bb-435e-a2cc-768c0ac4ee1f,client:127.0.0.1,protocol:HTTP/2.0,resource:pods,scope:cluster,url:/api/v1/pods,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:22:51.146) (total time: 11915ms):
Trace[1095209977]: ---"Writing http response done" count:5 11901ms (03:23:03.061)
Trace[1095209977]: [11.915120262s] [11.915120262s] END
I0322 03:23:03.129902    2308 trace.go:236] Trace[1610696710]: "List" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:0bbc7f0d-da68-4ec2-a16f-cd1e14516332,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:22:56.103) (total time: 4846ms):
Trace[1610696710]: ---"Writing http response done" count:1 4394ms (03:23:00.950)
Trace[1610696710]: [4.846662596s] [4.846662596s] END
I0322 03:23:03.237044    2308 trace.go:236] Trace[1440435582]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:c531b975-ab6d-4d88-af32-16cb88e45e39,client:127.0.0.1,protocol:HTTP/2.0,resource:csistoragecapacities,scope:cluster,url:/apis/storage.k8s.io/v1/csistoragecapacities,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:22:52.774) (total time: 6802ms):
Trace[1440435582]: ["cacher list" audit-id:c531b975-ab6d-4d88-af32-16cb88e45e39,type:csistoragecapacities.storage.k8s.io 6350ms (03:22:54.718)]
Trace[1440435582]: ---"Writing http response done" count:0 2209ms (03:22:59.576)
Trace[1440435582]: [6.802228566s] [6.802228566s] END
I0322 03:23:03.264404    2308 trace.go:236] Trace[1947833401]: "SerializeObject" audit-id:4f2c543c-343d-4291-b9d8-b575e316c8dd,method:PATCH,url:/api/v1/namespaces/kube-system/pods/helm-install-traefik-crd-m54x6/status,protocol:HTTP/2.0,mediaType:application/vnd.kubernetes.protobuf,encoder:{"encodeGV":"v1","encoder":"protobuf","name":"versioning"} (22-Mar-2024 03:22:48.740) (total time: 12333ms):
Trace[1947833401]: ---"About to start writing response" size:154 3166ms (03:22:51.906)
Trace[1947833401]: ---"About to start writing response" size:69 9160ms (03:23:01.071)
Trace[1947833401]: [12.333386621s] [12.333386621s] END
I0322 03:23:03.264470    2308 trace.go:236] Trace[930201329]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:4f2c543c-343d-4291-b9d8-b575e316c8dd,client:127.0.0.1,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/kube-system/pods/helm-install-traefik-crd-m54x6/status,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PATCH (22-Mar-2024 03:22:44.324) (total time: 18939ms):
Trace[930201329]: ---"About to apply patch" 4431ms (03:22:50.971)
Trace[930201329]: [18.93944049s] [18.93944049s] END
I0322 03:23:03.563925    2308 trace.go:236] Trace[1823792912]: "DeltaFIFO Pop Process" ID:kube-system/auth-delegator,Depth:11,Reason:slow event handlers blocking the queue (22-Mar-2024 03:23:03.211) (total time: 346ms):
Trace[1823792912]: [346.553292ms] [346.553292ms] END
I0322 03:23:03.884886    2308 trace.go:236] Trace[362675726]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:a5176512-4a0d-4701-9b73-5d7605b36a45,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:namespace,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:22:55.027) (total time: 6784ms):
Trace[362675726]: ---"About to List from storage" 3875ms (03:22:58.902)
Trace[362675726]: ["cacher list" audit-id:a5176512-4a0d-4701-9b73-5d7605b36a45,type:leases.coordination.k8s.io 2906ms (03:22:58.907)
Trace[362675726]:  ---"watchCache locked acquired" 1260ms (03:23:00.167)]
Trace[362675726]: ---"Writing http response done" count:1 1640ms (03:23:01.812)
Trace[362675726]: [6.784874591s] [6.784874591s] END
I0322 03:23:03.915390    2308 trace.go:236] Trace[204924140]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:35.381) (total time: 28531ms):
Trace[204924140]: ---"Objects listed" error:<nil> 28526ms (03:23:03.907)
Trace[204924140]: [28.531670607s] [28.531670607s] END
I0322 03:23:03.936226    2308 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:23:03.940860    2308 trace.go:236] Trace[1593284601]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:33.725) (total time: 30213ms):
Trace[1593284601]: ---"Objects listed" error:<nil> 30212ms (03:23:03.938)
Trace[1593284601]: [30.213913118s] [30.213913118s] END
I0322 03:23:03.948396    2308 trace.go:236] Trace[528274953]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:36.803) (total time: 27144ms):
Trace[528274953]: ---"Objects listed" error:<nil> 27144ms (03:23:03.948)
Trace[528274953]: [27.14462973s] [27.14462973s] END
I0322 03:23:03.969892    2308 trace.go:236] Trace[1806048120]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:39.019) (total time: 24950ms):
Trace[1806048120]: ---"Objects listed" error:<nil> 24948ms (03:23:03.968)
Trace[1806048120]: [24.950240966s] [24.950240966s] END
I0322 03:23:03.978331    2308 trace.go:236] Trace[1333865374]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:37.046) (total time: 26932ms):
Trace[1333865374]: ---"Objects listed" error:<nil> 26932ms (03:23:03.978)
Trace[1333865374]: [26.932176229s] [26.932176229s] END
I0322 03:23:03.982468    2308 trace.go:236] Trace[1966785557]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:cb0eb919-f968-4886-9424-078402eb4ce5,client:127.0.0.1,protocol:HTTP/2.0,resource:limitranges,scope:cluster,url:/api/v1/limitranges,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:23:00.023) (total time: 3958ms):
Trace[1966785557]: ---"Writing http response done" count:0 2367ms (03:23:03.982)
Trace[1966785557]: [3.958296509s] [3.958296509s] END
I0322 03:23:04.237517    2308 trace.go:236] Trace[1722619560]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:37.530) (total time: 26700ms):
Trace[1722619560]: ---"Objects listed" error:<nil> 26693ms (03:23:04.224)
Trace[1722619560]: [26.700312259s] [26.700312259s] END
I0322 03:23:04.256181    2308 trace.go:236] Trace[668805995]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:689588d8-99ac-4262-8635-56d63faaab5f,client:127.0.0.1,protocol:HTTP/2.0,resource:serviceaccounts,scope:cluster,url:/api/v1/serviceaccounts,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:22:52.640) (total time: 11614ms):
Trace[668805995]: ---"About to List from storage" 1989ms (03:22:54.629)
Trace[668805995]: ["cacher list" audit-id:689588d8-99ac-4262-8635-56d63faaab5f,type:serviceaccounts 9623ms (03:22:54.631)]
Trace[668805995]: ---"Writing http response done" count:40 6944ms (03:23:04.254)
Trace[668805995]: [11.614247303s] [11.614247303s] END
I0322 03:23:04.313987    2308 trace.go:236] Trace[961480123]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:da5646fe-fe73-411e-9874-69cb5c602781,client:127.0.0.1,protocol:HTTP/2.0,resource:roles,scope:cluster,url:/apis/rbac.authorization.k8s.io/v1/roles,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:22:49.240) (total time: 15072ms):
Trace[961480123]: ---"About to List from storage" 5336ms (03:22:54.576)
Trace[961480123]: ["cacher list" audit-id:da5646fe-fe73-411e-9874-69cb5c602781,type:roles.rbac.authorization.k8s.io 8157ms (03:22:56.155)
Trace[961480123]:  ---"watchCache locked acquired" 2931ms (03:22:59.090)]
Trace[961480123]: ---"Writing http response done" count:7 5214ms (03:23:04.312)
Trace[961480123]: [15.072389897s] [15.072389897s] END
I0322 03:23:04.489300    2308 trace.go:236] Trace[1956820867]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:43.612) (total time: 20871ms):
Trace[1956820867]: ---"Objects listed" error:<nil> 20866ms (03:23:04.479)
Trace[1956820867]: [20.871249982s] [20.871249982s] END
time="2024-03-22T03:23:02Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:53.039260422 +0000 UTC m=+328.734073639) (total time: 9.342847146s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/cronjobs/% 594]]"
I0322 03:23:04.522939    2308 trace.go:236] Trace[1452321525]: "List" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json,audit-id:74b778d5-2b29-4414-8008-e195d307c28c,client:10.42.0.5,protocol:HTTP/2.0,resource:pods,scope:cluster,url:/api/v1/pods,user-agent:metrics-server/v0.6.3 (linux/amd64) kubernetes/a938798,verb:LIST (22-Mar-2024 03:22:54.596) (total time: 9924ms):
Trace[1452321525]: ---"About to List from storage" 4515ms (03:22:59.111)
Trace[1452321525]: ---"Writing http response done" count:5 5407ms (03:23:04.521)
Trace[1452321525]: [9.9248377s] [9.9248377s] END
time="2024-03-22T03:23:05Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:54.094551592 +0000 UTC m=+329.789364800) (total time: 10.958688005s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/services/endpoints/% false]]"
time="2024-03-22T03:23:03Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:53.227304794 +0000 UTC m=+328.922117996) (total time: 9.913760972s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/k3s.cattle.io/etcdsnapshotfiles/% 594]]"
time="2024-03-22T03:23:03Z" level=info msg="Slow SQL (started: 2024-03-22 03:22:58.863851786 +0000 UTC m=+334.558665038) (total time: 2.900836938s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/configmaps/% 594]]"
time="2024-03-22T03:23:03Z" level=info msg="Slow SQL (started: 2024-03-22 03:23:00.88463918 +0000 UTC m=+336.579452379) (total time: 3.024720534s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/endpointslices/% false]]"
time="2024-03-22T03:23:05Z" level=info msg="Slow SQL (started: 2024-03-22 03:23:01.707689365 +0000 UTC m=+337.402502563) (total time: 1.308691283s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/roles/% false]]"
time="2024-03-22T03:23:06Z" level=info msg="Slow SQL (started: 2024-03-22 03:23:04.299347333 +0000 UTC m=+339.994160543) (total time: 1.931800963s):  SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key' : [[]]"
I0322 03:23:04.904346    2308 trace.go:236] Trace[2023904031]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:ccc18467-9a90-4ad5-82f9-a4afd86db699,client:127.0.0.1,protocol:HTTP/2.0,resource:csidrivers,scope:cluster,url:/apis/storage.k8s.io/v1/csidrivers,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/scheduler,verb:LIST (22-Mar-2024 03:23:02.621) (total time: 2274ms):
Trace[2023904031]: ---"Writing http response done" count:0 2273ms (03:23:04.896)
Trace[2023904031]: [2.274951717s] [2.274951717s] END
I0322 03:23:07.079633    2308 trace.go:236] Trace[1641392994]: "iptables ChainExists" (22-Mar-2024 03:23:03.250) (total time: 3825ms):
Trace[1641392994]: [3.825402061s] [3.825402061s] END
I0322 03:23:07.157984    2308 trace.go:236] Trace[1665981973]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:20:35.452) (total time: 151698ms):
Trace[1665981973]: ---"Objects listed" error:<nil> 151692ms (03:23:07.145)
Trace[1665981973]: [2m31.698433856s] [2m31.698433856s] END
I0322 03:23:07.206126    2308 trace.go:236] Trace[1119243112]: "SerializeObject" audit-id:1f840886-fcca-4931-a05e-1d3d797d222f,method:GET,url:/api/v1/serviceaccounts,protocol:HTTP/2.0,mediaType:application/json,encoder:{"encodeGV":"v1","encoder":"{\"name\":\"json\",\"pretty\":\"false\",\"strict\":\"false\",\"yaml\":\"false\"}","name":"versioning"} (22-Mar-2024 03:23:00.832) (total time: 6373ms):
Trace[1119243112]: ---"About to start writing response" size:13330 2100ms (03:23:02.932)
Trace[1119243112]: ---"Write call succeeded" writer:responsewriter.outerWithCloseNotifyAndFlush,size:13330,firstWrite:true 4272ms (03:23:07.205)
Trace[1119243112]: [6.373525505s] [6.373525505s] END
I0322 03:23:07.206649    2308 trace.go:236] Trace[1245981688]: "List" accept:application/json, */*,audit-id:1f840886-fcca-4931-a05e-1d3d797d222f,client:127.0.0.1,protocol:HTTP/2.0,resource:serviceaccounts,scope:cluster,url:/api/v1/serviceaccounts,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:LIST (22-Mar-2024 03:22:53.745) (total time: 13460ms):
Trace[1245981688]: ---"About to List from storage" 2037ms (03:22:55.783)
Trace[1245981688]: ["cacher list" audit-id:1f840886-fcca-4931-a05e-1d3d797d222f,type:serviceaccounts 11418ms (03:22:55.787)
Trace[1245981688]:  ---"Resized result" 2739ms (03:22:58.532)]
Trace[1245981688]: ---"Writing http response done" count:40 8671ms (03:23:07.206)
Trace[1245981688]: [13.460749299s] [13.460749299s] END
E0322 03:23:07.254113    2308 timeout.go:142] post-timeout activity - time-elapsed: 36.403565736s, PATCH "/api/v1/namespaces/kube-system/pods/helm-install-traefik-crd-m54x6/status" result: <nil>
I0322 03:23:07.293374    2308 trace.go:236] Trace[2064426470]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:34.032) (total time: 33254ms):
Trace[2064426470]: ---"Objects listed" error:<nil> 33247ms (03:23:07.280)
Trace[2064426470]: [33.254823601s] [33.254823601s] END
E0322 03:23:05.051552    2308 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
I0322 03:23:07.710724    2308 trace.go:236] Trace[1953196740]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:38.933) (total time: 28774ms):
Trace[1953196740]: ---"Objects listed" error:<nil> 28768ms (03:23:07.702)
Trace[1953196740]: [28.774788286s] [28.774788286s] END
I0322 03:23:07.742321    2308 trace.go:236] Trace[1891018719]: "List" accept:application/json, */*,audit-id:41ad882f-32db-4416-a041-e9e93094f387,client:127.0.0.1,protocol:HTTP/2.0,resource:namespaces,scope:cluster,url:/api/v1/namespaces,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:23:05.532) (total time: 2209ms):
Trace[1891018719]: ---"Writing http response done" count:4 2204ms (03:23:07.741)
Trace[1891018719]: [2.209809584s] [2.209809584s] END
I0322 03:23:07.743209    2308 trace.go:236] Trace[218636177]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:35.256) (total time: 32486ms):
Trace[218636177]: ---"Objects listed" error:<nil> 32486ms (03:23:07.743)
Trace[218636177]: [32.48632173s] [32.48632173s] END
I0322 03:23:05.061659    2308 trace.go:236] Trace[5572825]: "SerializeObject" audit-id:9b35b9d8-f3ab-4e5b-9fc5-e7c9c6c2c008,method:GET,url:/apis/apiextensions.k8s.io/v1/customresourcedefinitions,protocol:HTTP/2.0,mediaType:application/vnd.kubernetes.protobuf,encoder:{"encodeGV":"apiextensions.k8s.io/v1","encoder":"protobuf","name":"versioning"} (22-Mar-2024 03:22:59.114) (total time: 5941ms):
Trace[5572825]: ---"About to start writing response" size:201593 5315ms (03:23:04.430)
Trace[5572825]: [5.941594711s] [5.941594711s] END
I0322 03:23:07.789994    2308 trace.go:236] Trace[110207281]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:9b35b9d8-f3ab-4e5b-9fc5-e7c9c6c2c008,client:127.0.0.1,protocol:HTTP/2.0,resource:customresourcedefinitions,scope:cluster,url:/apis/apiextensions.k8s.io/v1/customresourcedefinitions,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:22:55.200) (total time: 12587ms):
Trace[110207281]: ["cacher list" audit-id:9b35b9d8-f3ab-4e5b-9fc5-e7c9c6c2c008,type:customresourcedefinitions.apiextensions.k8s.io 10796ms (03:22:56.991)
Trace[110207281]:  ---"watchCache locked acquired" 2116ms (03:22:59.112)]
Trace[110207281]: ---"Writing http response done" count:23 8673ms (03:23:07.787)
Trace[110207281]: [12.587622361s] [12.587622361s] END
I0322 03:23:07.823618    2308 trace.go:236] Trace[1048551865]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:38.372) (total time: 29446ms):
Trace[1048551865]: ---"Objects listed" error:<nil> 29440ms (03:23:07.812)
Trace[1048551865]: [29.446659141s] [29.446659141s] END
I0322 03:23:07.835964    2308 trace.go:236] Trace[734629784]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:31.683) (total time: 36148ms):
Trace[734629784]: ---"Objects listed" error:<nil> 36147ms (03:23:07.830)
Trace[734629784]: [36.148694629s] [36.148694629s] END
I0322 03:23:07.910598    2308 trace.go:236] Trace[177820171]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:4a51c2eb-8909-4c17-82f2-b5e9517b5dac,client:127.0.0.1,protocol:HTTP/2.0,resource:prioritylevelconfigurations,scope:cluster,url:/apis/flowcontrol.apiserver.k8s.io/v1beta3/prioritylevelconfigurations,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:22:57.704) (total time: 7933ms):
Trace[177820171]: ---"About to List from storage" 2350ms (03:23:00.055)
Trace[177820171]: ---"Writing http response done" count:8 5571ms (03:23:05.638)
Trace[177820171]: [7.933847306s] [7.933847306s] END
I0322 03:23:07.949028    2308 trace.go:236] Trace[1035300074]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:37.012) (total time: 30931ms):
Trace[1035300074]: ---"Objects listed" error:<nil> 30929ms (03:23:07.941)
Trace[1035300074]: [30.931662316s] [30.931662316s] END
I0322 03:23:05.130551    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="1.257982065s"
I0322 03:23:05.297623    2308 trace.go:236] Trace[2023231055]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:35.176) (total time: 30114ms):
Trace[2023231055]: ---"Objects listed" error:<nil> 30113ms (03:23:05.289)
Trace[2023231055]: [30.114906793s] [30.114906793s] END
I0322 03:23:05.309560    2308 trace.go:236] Trace[1932828771]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:38.710) (total time: 26598ms):
Trace[1932828771]: ---"Objects listed" error:<nil> 26598ms (03:23:05.309)
Trace[1932828771]: [26.598764134s] [26.598764134s] END
I0322 03:23:05.836047    2308 trace.go:236] Trace[714082917]: "List" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json,audit-id:a8b75899-23e4-4e96-9389-fd460ddc5d4d,client:127.0.0.1,protocol:HTTP/2.0,resource:etcdsnapshotfiles,scope:cluster,url:/apis/k3s.cattle.io/v1/etcdsnapshotfiles,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/metadata-informers,verb:LIST (22-Mar-2024 03:22:51.859) (total time: 11163ms):
Trace[714082917]: ---"About to List from storage" 2350ms (03:22:54.210)
Trace[714082917]: ["cacher list" audit-id:a8b75899-23e4-4e96-9389-fd460ddc5d4d,type:etcdsnapshotfiles.k3s.cattle.io 8812ms (03:22:54.212)]
Trace[714082917]: ---"Writing http response done" count:0 6484ms (03:23:03.023)
Trace[714082917]: [11.163817045s] [11.163817045s] END
E0322 03:23:05.943740    2308 status.go:71] apiserver received an error that is not an metav1.Status: context.deadlineExceededError{}: context deadline exceeded
I0322 03:23:05.963424    2308 trace.go:236] Trace[1853965698]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:21:37.124) (total time: 86081ms):
Trace[1853965698]: ---"Objects listed" error:<nil> 86071ms (03:23:03.195)
Trace[1853965698]: [1m26.081126827s] [1m26.081126827s] END
I0322 03:23:06.246833    2308 trace.go:236] Trace[1545704260]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:41.869) (total time: 21505ms):
Trace[1545704260]: ---"Objects listed" error:<nil> 19251ms (03:23:01.121)
Trace[1545704260]: ---"Objects extracted" 2243ms (03:23:03.368)
Trace[1545704260]: [21.505286821s] [21.505286821s] END
I0322 03:23:06.545982    2308 trace.go:236] Trace[1210069933]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:f86b228e-2678-4231-89cb-7b20ebdaa63f,client:127.0.0.1,protocol:HTTP/2.0,resource:services,scope:cluster,url:/api/v1/services,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/scheduler,verb:LIST (22-Mar-2024 03:22:57.785) (total time: 5942ms):
Trace[1210069933]: ---"Writing http response done" count:3 3575ms (03:23:03.727)
Trace[1210069933]: [5.942652704s] [5.942652704s] END
I0322 03:23:06.670614    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="4.136858959s"
I0322 03:23:06.692388    2308 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:23:06.736715    2308 trace.go:236] Trace[750993498]: "SerializeObject" audit-id:33528b32-5aa1-4800-b66a-5bd585752e78,method:GET,url:/api/v1/namespaces/kube-system/configmaps,protocol:HTTP/2.0,mediaType:application/json,encoder:{"encodeGV":"v1","encoder":"{\"name\":\"json\",\"pretty\":\"false\",\"strict\":\"false\",\"yaml\":\"false\"}","name":"versioning"} (22-Mar-2024 03:23:00.191) (total time: 6538ms):
Trace[750993498]: ---"About to start writing response" size:2124 3775ms (03:23:03.966)
Trace[750993498]: ---"Write call succeeded" writer:responsewriter.outerWithCloseNotifyAndFlush,size:2124,firstWrite:true 2762ms (03:23:06.729)
Trace[750993498]: [6.538917062s] [6.538917062s] END
I0322 03:23:08.099551    2308 trace.go:236] Trace[1923473492]: "List" accept:application/json, */*,audit-id:33528b32-5aa1-4800-b66a-5bd585752e78,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:22:56.080) (total time: 12014ms):
Trace[1923473492]: ["cacher list" audit-id:33528b32-5aa1-4800-b66a-5bd585752e78,type:configmaps 12002ms (03:22:56.092)]
Trace[1923473492]: ---"Writing http response done" count:1 9195ms (03:23:08.094)
Trace[1923473492]: [12.014195729s] [12.014195729s] END
I0322 03:23:06.738849    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="2.131119018s"
E0322 03:23:06.880940    2308 webhook.go:154] Failed to make webhook authenticator request: Post "https://127.0.0.1:6443/apis/authentication.k8s.io/v1/tokenreviews": context deadline exceeded
I0322 03:23:08.123470    2308 trace.go:236] Trace[1069905498]: "List" accept:application/json, */*,audit-id:5ceea9fb-cac2-4504-8dc8-0f831ddcdafe,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:23:04.442) (total time: 3680ms):
Trace[1069905498]: ["cacher list" audit-id:5ceea9fb-cac2-4504-8dc8-0f831ddcdafe,type:configmaps 3675ms (03:23:04.447)
Trace[1069905498]:  ---"watchCache locked acquired" 1977ms (03:23:06.425)]
Trace[1069905498]: ---"Writing http response done" count:1 1696ms (03:23:08.123)
Trace[1069905498]: [3.680596554s] [3.680596554s] END
time="2024-03-22T03:23:06Z" level=info msg="Slow SQL (started: 2024-03-22 03:23:03.526871182 +0000 UTC m=+339.221684382) (total time: 2.830518016s):  SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key' : [[]]"
I0322 03:23:08.833536    2308 trace.go:236] Trace[1951638703]: "List" accept:application/json, */*,audit-id:95961d92-f7c3-4e1b-a558-5c971c9b6bc5,client:10.42.0.5,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps,user-agent:metrics-server/v0.6.3 (linux/amd64) kubernetes/a938798,verb:GET (22-Mar-2024 03:22:57.757) (total time: 11068ms):
Trace[1951638703]: ---"About to List from storage" 2347ms (03:23:00.104)
Trace[1951638703]: ---"Writing http response done" count:1 8717ms (03:23:08.825)
Trace[1951638703]: [11.068146983s] [11.068146983s] END
I0322 03:23:08.833495    2308 trace.go:236] Trace[1118695145]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:5e34e19e-4cad-4bd7-913a-d5263e1a455d,client:127.0.0.1,protocol:HTTP/2.0,resource:statefulsets,scope:cluster,url:/apis/apps/v1/statefulsets,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/scheduler,verb:LIST (22-Mar-2024 03:23:04.897) (total time: 3935ms):
Trace[1118695145]: ---"Writing http response done" count:0 3929ms (03:23:08.832)
Trace[1118695145]: [3.935429622s] [3.935429622s] END
E0322 03:23:07.010006    2308 server.go:310] "Unable to authenticate the request due to an error" err="context canceled"
I0322 03:23:09.141969    2308 trace.go:236] Trace[1416002677]: "List" accept:application/json, */*,audit-id:2ff892bc-2a0b-46b1-a801-6fa603d22d4e,client:10.42.0.5,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps,user-agent:metrics-server/v0.6.3 (linux/amd64) kubernetes/a938798,verb:GET (22-Mar-2024 03:23:05.089) (total time: 4043ms):
Trace[1416002677]: ---"About to List from storage" 2025ms (03:23:07.115)
Trace[1416002677]: ---"Writing http response done" count:1 2016ms (03:23:09.133)
Trace[1416002677]: [4.043726682s] [4.043726682s] END
I0322 03:23:09.158146    2308 trace.go:236] Trace[109633350]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:c8954e3a-c349-474a-92a8-62dee1dd3a47,client:127.0.0.1,protocol:HTTP/2.0,resource:daemonsets,scope:namespace,url:/apis/apps/v1/namespaces/kube-system/daemonsets,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/,verb:LIST (22-Mar-2024 03:23:05.096) (total time: 4061ms):
Trace[109633350]: ---"Writing http response done" count:0 4052ms (03:23:09.158)
Trace[109633350]: [4.061297143s] [4.061297143s] END
time="2024-03-22T03:23:06Z" level=info msg="Slow SQL (started: 2024-03-22 03:23:03.768957719 +0000 UTC m=+339.463770920) (total time: 2.807833298s):  SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key' : [[]]"
time="2024-03-22T03:23:07Z" level=error msg="Error writing ping" error="write tcp 192.168.56.110:39688->192.168.56.110:6443: i/o timeout"
time="2024-03-22T03:23:08Z" level=info msg="Slow SQL (started: 2024-03-22 03:23:03.69246255 +0000 UTC m=+339.387275755) (total time: 2.803313025s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? AND mkv.id <= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC LIMIT 1000 : [[/registry/daemonsets/% 595 false]]"
time="2024-03-22T03:23:08Z" level=info msg="Slow SQL (started: 2024-03-22 03:23:04.078031152 +0000 UTC m=+339.772844360) (total time: 2.794356091s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/endpointslices/% 594]]"
I0322 03:23:09.409528    2308 trace.go:236] Trace[1654160921]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:f9a56154-503a-4aa3-a7ff-7d7f6d07fe64,client:127.0.0.1,protocol:HTTP/2.0,resource:storageclasses,scope:cluster,url:/apis/storage.k8s.io/v1/storageclasses,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/scheduler,verb:LIST (22-Mar-2024 03:23:05.310) (total time: 4090ms):
Trace[1654160921]: ---"Writing http response done" count:1 4089ms (03:23:09.401)
Trace[1654160921]: [4.09067012s] [4.09067012s] END
I0322 03:23:09.423773    2308 trace.go:236] Trace[1863864010]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:20:59.632) (total time: 129790ms):
Trace[1863864010]: ---"Objects listed" error:<nil> 127706ms (03:23:07.339)
Trace[1863864010]: ---"Objects extracted" 2079ms (03:23:09.419)
Trace[1863864010]: [2m9.790499617s] [2m9.790499617s] END
I0322 03:23:09.428201    2308 trace.go:236] Trace[1666198717]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:20:52.229) (total time: 137198ms):
Trace[1666198717]: ---"Objects listed" error:<nil> 137191ms (03:23:09.421)
Trace[1666198717]: [2m17.198485807s] [2m17.198485807s] END
I0322 03:23:09.539783    2308 trace.go:236] Trace[1207101430]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:4d854805-34ab-4b12-8120-343e8548c867,client:127.0.0.1,protocol:HTTP/2.0,resource:poddisruptionbudgets,scope:cluster,url:/apis/policy/v1/poddisruptionbudgets,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/scheduler,verb:LIST (22-Mar-2024 03:23:07.427) (total time: 2110ms):
Trace[1207101430]: ---"Writing http response done" count:0 2103ms (03:23:09.537)
Trace[1207101430]: [2.110130588s] [2.110130588s] END
E0322 03:23:09.572063    2308 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:23:09.574292    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:23:09.575013    2308 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 03:23:09.575332    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
I0322 03:23:09.575459    2308 trace.go:236] Trace[413505619]: "SerializeObject" audit-id:f063c178-f562-4b92-b67a-0b3e1c4d1b0b,method:GET,url:/api,protocol:HTTP/2.0,mediaType:application/json;g=apidiscovery.k8s.io;v=v2beta1;as=APIGroupDiscoveryList,encoder:{"encodeGV":"apidiscovery.k8s.io/v2beta1","encoder":"{\"name\":\"json\",\"pretty\":\"false\",\"strict\":\"false\",\"yaml\":\"false\"}","name":"versioning"} (22-Mar-2024 03:23:00.579) (total time: 6979ms):
Trace[413505619]: ---"About to start writing response" size:6969 4345ms (03:23:04.925)
Trace[413505619]: ---"Write call succeeded" writer:responsewriter.outerWithCloseNotifyAndFlush,size:6969,firstWrite:true 2632ms (03:23:07.557)
Trace[413505619]: [6.979408669s] [6.979408669s] END
I0322 03:23:09.737509    2308 trace.go:236] Trace[58555777]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:39f8dc58-fd7e-4864-a8a5-4205a58fffb2,client:127.0.0.1,protocol:HTTP/2.0,resource:services,scope:cluster,url:/api/v1/services,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:23:07.064) (total time: 2670ms):
Trace[58555777]: ---"Writing http response done" count:3 2663ms (03:23:09.735)
Trace[58555777]: [2.670961867s] [2.670961867s] END
I0322 03:23:09.748054    2308 trace.go:236] Trace[1695850065]: "List" accept:application/json, */*,audit-id:7b77ea93-d3c5-48a1-89a8-526027f18a10,client:127.0.0.1,protocol:HTTP/2.0,resource:addons,scope:cluster,url:/apis/k3s.cattle.io/v1/addons,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:LIST (22-Mar-2024 03:23:00.018) (total time: 9729ms):
Trace[1695850065]: ---"About to List from storage" 1558ms (03:23:01.577)
Trace[1695850065]: ["cacher list" audit-id:7b77ea93-d3c5-48a1-89a8-526027f18a10,type:addons.k3s.cattle.io 8168ms (03:23:01.579)
Trace[1695850065]:  ---"watchCache locked acquired" 1228ms (03:23:02.808)]
Trace[1695850065]: ---"Writing http response done" count:13 6933ms (03:23:09.747)
Trace[1695850065]: [9.729232885s] [9.729232885s] END
I0322 03:23:09.753940    2308 trace.go:236] Trace[1963562080]: "SerializeObject" audit-id:f03a0f87-b058-4c68-a483-ccfd86245679,method:GET,url:/apis/rbac.authorization.k8s.io/v1/clusterrolebindings,protocol:HTTP/2.0,mediaType:application/json,encoder:{"encodeGV":"rbac.authorization.k8s.io/v1","encoder":"{\"name\":\"json\",\"pretty\":\"false\",\"strict\":\"false\",\"yaml\":\"false\"}","name":"versioning"} (22-Mar-2024 03:22:58.523) (total time: 11226ms):
Trace[1963562080]: ---"About to start writing response" size:55288 10560ms (03:23:09.083)
Trace[1963562080]: [11.226516311s] [11.226516311s] END
I0322 03:23:09.754052    2308 trace.go:236] Trace[1004675752]: "List" accept:application/json, */*,audit-id:f03a0f87-b058-4c68-a483-ccfd86245679,client:127.0.0.1,protocol:HTTP/2.0,resource:clusterrolebindings,scope:cluster,url:/apis/rbac.authorization.k8s.io/v1/clusterrolebindings,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:LIST (22-Mar-2024 03:22:50.878) (total time: 18875ms):
Trace[1004675752]: ["cacher list" audit-id:f03a0f87-b058-4c68-a483-ccfd86245679,type:clusterrolebindings.rbac.authorization.k8s.io 18870ms (03:22:50.883)
Trace[1004675752]:  ---"watchCache locked acquired" 2819ms (03:22:53.703)]
Trace[1004675752]: ---"Writing http response done" count:54 16047ms (03:23:09.753)
Trace[1004675752]: [18.875421286s] [18.875421286s] END
I0322 03:23:09.814952    2308 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/helm-install-traefik-t9wf9" containerName="helm"
I0322 03:23:09.821938    2308 trace.go:236] Trace[563421439]: "List" accept:application/json, */*,audit-id:92e9a905-e3d5-45b9-9fb8-78b4c9fc9517,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:23:05.444) (total time: 4376ms):
Trace[563421439]: ---"Writing http response done" count:1 4363ms (03:23:09.821)
Trace[563421439]: [4.376600595s] [4.376600595s] END
I0322 03:23:09.847720    2308 trace.go:236] Trace[662028891]: "List" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:c96b4c9a-5214-4df8-8417-0e27c42f523c,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:23:05.461) (total time: 4381ms):
Trace[662028891]: ---"Writing http response done" count:1 4381ms (03:23:09.843)
Trace[662028891]: [4.38187434s] [4.38187434s] END
I0322 03:23:09.850678    2308 trace.go:236] Trace[212926882]: "List" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:39b40083-4a39-4b6f-b37c-99faaaf99aa8,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:23:05.497) (total time: 2161ms):
Trace[212926882]: ---"Writing http response done" count:1 2161ms (03:23:07.659)
Trace[212926882]: [2.161214737s] [2.161214737s] END
I0322 03:23:09.856450    2308 trace.go:236] Trace[780964975]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:39.332) (total time: 30523ms):
Trace[780964975]: ---"Objects listed" error:<nil> 30520ms (03:23:09.853)
Trace[780964975]: [30.523809286s] [30.523809286s] END
I0322 03:23:09.898822    2308 trace.go:236] Trace[73643230]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:21:15.827) (total time: 114068ms):
Trace[73643230]: ---"Objects listed" error:<nil> 114062ms (03:23:09.889)
Trace[73643230]: [1m54.068548732s] [1m54.068548732s] END
I0322 03:23:10.080864    2308 trace.go:236] Trace[1104478462]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:37.816) (total time: 32259ms):
Trace[1104478462]: ---"Objects listed" error:<nil> 30063ms (03:23:07.879)
Trace[1104478462]: ---"Objects extracted" 2186ms (03:23:10.071)
Trace[1104478462]: [32.259397842s] [32.259397842s] END
I0322 03:23:10.160256    2308 trace.go:236] Trace[558736120]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:8c4ce9f1-49da-46f4-861b-5f1db7d87251,client:127.0.0.1,protocol:HTTP/2.0,resource:persistentvolumes,scope:cluster,url:/api/v1/persistentvolumes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/scheduler,verb:LIST (22-Mar-2024 03:23:06.296) (total time: 3856ms):
Trace[558736120]: ---"Writing http response done" count:0 3854ms (03:23:10.152)
Trace[558736120]: [3.856656208s] [3.856656208s] END
time="2024-03-22T03:23:10Z" level=info msg="Slow SQL (started: 2024-03-22 03:23:05.666131803 +0000 UTC m=+341.360945011) (total time: 2.279911455s):  SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key' : [[]]"
I0322 03:23:10.478834    2308 trace.go:236] Trace[190855045]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:05402d21-e97e-414a-984e-82ff1e9111bf,client:127.0.0.1,protocol:HTTP/2.0,resource:clusterrolebindings,scope:cluster,url:/apis/rbac.authorization.k8s.io/v1/clusterrolebindings,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:22:59.153) (total time: 8899ms):
Trace[190855045]: ["cacher list" audit-id:05402d21-e97e-414a-984e-82ff1e9111bf,type:clusterrolebindings.rbac.authorization.k8s.io 11315ms (03:22:59.156)
Trace[190855045]:  ---"watchCache locked acquired" 1275ms (03:23:00.431)]
Trace[190855045]: ---"Writing http response done" count:54 7617ms (03:23:08.052)
Trace[190855045]: [8.899070554s] [8.899070554s] END
E0322 03:23:10.595184    2308 kubelet.go:2477] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="4m29.727s"
I0322 03:23:11.729846    2308 trace.go:236] Trace[580596889]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:21:01.582) (total time: 130145ms):
Trace[580596889]: ---"Objects listed" error:<nil> 130142ms (03:23:11.724)
Trace[580596889]: [2m10.145099456s] [2m10.145099456s] END
I0322 03:23:11.742638    2308 trace.go:236] Trace[951666083]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:20:52.111) (total time: 139631ms):
Trace[951666083]: ---"Objects listed" error:<nil> 139631ms (03:23:11.742)
Trace[951666083]: [2m19.631185503s] [2m19.631185503s] END
I0322 03:23:11.762481    2308 trace.go:236] Trace[495623462]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:21:01.180) (total time: 130581ms):
Trace[495623462]: ---"Objects listed" error:<nil> 130581ms (03:23:11.762)
Trace[495623462]: [2m10.581599589s] [2m10.581599589s] END
I0322 03:23:11.769738    2308 trace.go:236] Trace[2008939837]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:21:02.519) (total time: 129250ms):
Trace[2008939837]: ---"Objects listed" error:<nil> 129249ms (03:23:11.768)
Trace[2008939837]: [2m9.250157162s] [2m9.250157162s] END
I0322 03:23:11.772185    2308 trace.go:236] Trace[1325078029]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:34.327) (total time: 37444ms):
Trace[1325078029]: ---"Objects listed" error:<nil> 37444ms (03:23:11.772)
Trace[1325078029]: [37.444600309s] [37.444600309s] END
I0322 03:23:11.774754    2308 trace.go:236] Trace[1163990576]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:21:02.221) (total time: 129553ms):
Trace[1163990576]: ---"Objects listed" error:<nil> 129553ms (03:23:11.774)
Trace[1163990576]: [2m9.553660927s] [2m9.553660927s] END
I0322 03:23:11.890697    2308 trace.go:236] Trace[2115075609]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:35.525) (total time: 36355ms):
Trace[2115075609]: ---"Objects listed" error:<nil> 36352ms (03:23:11.878)
Trace[2115075609]: [36.355383956s] [36.355383956s] END
I0322 03:23:11.923523    2308 trace.go:236] Trace[751095216]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:21:02.233) (total time: 129689ms):
Trace[751095216]: ---"Objects listed" error:<nil> 129681ms (03:23:11.914)
Trace[751095216]: [2m9.689253925s] [2m9.689253925s] END
I0322 03:23:11.932663    2308 trace.go:236] Trace[895860282]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:21:01.550) (total time: 130382ms):
Trace[895860282]: ---"Objects listed" error:<nil> 130381ms (03:23:11.931)
Trace[895860282]: [2m10.382315085s] [2m10.382315085s] END
I0322 03:23:11.958930    2308 trace.go:236] Trace[1702884895]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:38.444) (total time: 33514ms):
Trace[1702884895]: ---"Objects listed" error:<nil> 33513ms (03:23:11.958)
Trace[1702884895]: [33.514363528s] [33.514363528s] END
I0322 03:23:11.961865    2308 trace.go:236] Trace[1030293268]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:c232b6cb-4543-4c70-947b-f64930eca05a,client:127.0.0.1,protocol:HTTP/2.0,resource:pods,scope:cluster,url:/api/v1/pods,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/scheduler,verb:LIST (22-Mar-2024 03:23:05.274) (total time: 6686ms):
Trace[1030293268]: ---"Writing http response done" count:5 6672ms (03:23:11.961)
Trace[1030293268]: [6.686869654s] [6.686869654s] END
I0322 03:23:11.962567    2308 trace.go:236] Trace[1206813256]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:39.416) (total time: 32546ms):
Trace[1206813256]: ---"Objects listed" error:<nil> 32546ms (03:23:11.962)
Trace[1206813256]: [32.546319435s] [32.546319435s] END
time="2024-03-22T03:23:10Z" level=info msg="Slow SQL (started: 2024-03-22 03:23:03.647500054 +0000 UTC m=+339.342313248) (total time: 4.513854136s):  SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key' : [[]]"
I0322 03:23:10.714400    2308 trace.go:236] Trace[951989442]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:20b82fe3-6719-4cc8-a71d-26f25f8f6581,client:127.0.0.1,protocol:HTTP/2.0,resource:certificatesigningrequests,scope:cluster,url:/apis/certificates.k8s.io/v1/certificatesigningrequests,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:23:06.490) (total time: 1755ms):
Trace[951989442]: ---"Writing http response done" count:0 1749ms (03:23:08.246)
Trace[951989442]: [1.755637507s] [1.755637507s] END
I0322 03:23:12.090086    2308 trace.go:236] Trace[1276894596]: "List" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:a61cf59f-5a77-4c18-949f-b738446b3b0b,client:127.0.0.1,protocol:HTTP/2.0,resource:services,scope:cluster,url:/api/v1/services,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:23:07.414) (total time: 4671ms):
Trace[1276894596]: ---"Writing http response done" count:3 2603ms (03:23:12.085)
Trace[1276894596]: [4.671102801s] [4.671102801s] END
I0322 03:23:12.093322    2308 trace.go:236] Trace[622955754]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:37.061) (total time: 35031ms):
Trace[622955754]: ---"Objects listed" error:<nil> 35028ms (03:23:12.090)
Trace[622955754]: [35.031323815s] [35.031323815s] END
time="2024-03-22T03:23:10Z" level=info msg="Slow SQL (started: 2024-03-22 03:23:03.147069583 +0000 UTC m=+338.841882781) (total time: 5.147285752s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC LIMIT 500 : [[% 595]]"
I0322 03:23:10.756266    2308 trace.go:236] Trace[1311375072]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:21:14.960) (total time: 115790ms):
Trace[1311375072]: ---"Objects listed" error:<nil> 115788ms (03:23:10.749)
Trace[1311375072]: [1m55.790877811s] [1m55.790877811s] END
time="2024-03-22T03:23:10Z" level=info msg="Slow SQL (started: 2024-03-22 03:23:01.057281588 +0000 UTC m=+336.752094785) (total time: 9.791253959s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/statefulsets/% 594]]"
W0322 03:23:10.786355    2308 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Node: Get "https://127.0.0.1:6443/api/v1/nodes?fieldSelector=metadata.name%3Dserver&resourceVersion=594": net/http: TLS handshake timeout - error from a previous attempt: unexpected EOF
I0322 03:23:12.289574    2308 trace.go:236] Trace[818015460]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:44.006) (total time: 28280ms):
Trace[818015460]: ---"Objects listed" error:Get "https://127.0.0.1:6443/api/v1/nodes?fieldSelector=metadata.name%3Dserver&resourceVersion=594": net/http: TLS handshake timeout - error from a previous attempt: unexpected EOF 26778ms (03:23:10.784)
Trace[818015460]: [28.280630337s] [28.280630337s] END
I0322 03:23:12.292619    2308 trace.go:236] Trace[1905523288]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:e8afe0d2-311e-4403-806d-9f0cdc8f1a2d,client:127.0.0.1,protocol:HTTP/2.0,resource:roles,scope:cluster,url:/apis/rbac.authorization.k8s.io/v1/roles,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:23:06.979) (total time: 5313ms):
Trace[1905523288]: ---"About to List from storage" 2048ms (03:23:09.027)
Trace[1905523288]: ---"Writing http response done" count:7 3248ms (03:23:12.292)
Trace[1905523288]: [5.313428238s] [5.313428238s] END
E0322 03:23:12.294627    2308 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://127.0.0.1:6443/api/v1/nodes?fieldSelector=metadata.name%3Dserver&resourceVersion=594": net/http: TLS handshake timeout - error from a previous attempt: unexpected EOF
I0322 03:23:12.313382    2308 trace.go:236] Trace[743067105]: "DeltaFIFO Pop Process" ID:kube-public/default,Depth:37,Reason:slow event handlers blocking the queue (22-Mar-2024 03:23:12.121) (total time: 190ms):
Trace[743067105]: [190.011792ms] [190.011792ms] END
I0322 03:23:12.346353    2308 trace.go:236] Trace[374075788]: "List" accept:application/json, */*,audit-id:17dbd038-dfc0-4bd8-af16-910aa4da6bc6,client:127.0.0.1,protocol:HTTP/2.0,resource:pods,scope:cluster,url:/api/v1/pods,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:23:08.081) (total time: 4264ms):
Trace[374075788]: ---"Writing http response done" count:5 4258ms (03:23:12.345)
Trace[374075788]: [4.264429551s] [4.264429551s] END
time="2024-03-22T03:23:11Z" level=error msg="Error writing ping" error="write tcp 192.168.56.110:39688->192.168.56.110:6443: i/o timeout"
I0322 03:23:12.408615    2308 trace.go:236] Trace[768008119]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:36.989) (total time: 35413ms):
Trace[768008119]: ---"Objects listed" error:<nil> 33845ms (03:23:10.834)
Trace[768008119]: [35.413217105s] [35.413217105s] END
I0322 03:23:09.141922    2308 trace.go:236] Trace[114151949]: "iptables ChainExists" (22-Mar-2024 03:23:03.250) (total time: 5878ms):
Trace[114151949]: [5.878889451s] [5.878889451s] END
I0322 03:23:11.062138    2308 trace.go:236] Trace[1036450080]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:34.645) (total time: 36412ms):
Trace[1036450080]: ---"Objects listed" error:<nil> 36407ms (03:23:11.052)
Trace[1036450080]: [36.412339648s] [36.412339648s] END
I0322 03:23:12.549962    2308 trace.go:236] Trace[922146989]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:8406116f-4bca-4778-a07e-63fcf2abf223,client:127.0.0.1,protocol:HTTP/2.0,resource:tokenreviews,scope:resource,url:/apis/authentication.k8s.io/v1/tokenreviews,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:POST (22-Mar-2024 03:22:56.138) (total time: 14790ms):
Trace[922146989]: ---"limitedReadBody succeeded" len:1152 4118ms (03:23:00.256)
Trace[922146989]: ---"Conversion done" 1676ms (03:23:01.941)
Trace[922146989]: ---"Write to database call failed" len:1152,err:Timeout: request did not complete within requested timeout - context canceled 3427ms (03:23:05.371)
Trace[922146989]: [14.790653813s] [14.790653813s] END
I0322 03:23:11.238055    2308 trace.go:236] Trace[1773363059]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:38.923) (total time: 30577ms):
Trace[1773363059]: ---"Objects listed" error:<nil> 28505ms (03:23:07.428)
Trace[1773363059]: ---"Objects extracted" 2072ms (03:23:09.500)
Trace[1773363059]: [30.57741045s] [30.57741045s] END
E0322 03:23:12.598727    2308 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 03:23:12.601649    2308 trace.go:236] Trace[1233140991]: "SerializeObject" audit-id:328c8791-522b-44b6-a08d-706391d3e08c,method:PUT,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/server,protocol:HTTP/2.0,mediaType:application/vnd.kubernetes.protobuf,encoder:{"encodeGV":"coordination.k8s.io/v1","encoder":"protobuf","name":"versioning"} (22-Mar-2024 03:23:02.705) (total time: 9893ms):
Trace[1233140991]: ---"About to start writing response" size:69 8238ms (03:23:10.947)
Trace[1233140991]: [9.893535079s] [9.893535079s] END
I0322 03:23:12.602869    2308 trace.go:236] Trace[793274554]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:328c8791-522b-44b6-a08d-706391d3e08c,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/server,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PUT (22-Mar-2024 03:22:50.789) (total time: 21813ms):
Trace[793274554]: ---"About to convert to expected version" 4814ms (03:22:55.609)
Trace[793274554]: ---"About to store object in database" 2695ms (03:22:58.317)
Trace[793274554]: ---"Write to database call failed" len:464,err:Timeout: request did not complete within requested timeout - context canceled 2328ms (03:23:00.645)
Trace[793274554]: [21.813126222s] [21.813126222s] END
I0322 03:23:11.238140    2308 trace.go:236] Trace[267101573]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:968c7814-1dcf-440a-a20d-48d76a77f50d,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:cluster,url:/api/v1/nodes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/,verb:LIST (22-Mar-2024 03:23:05.526) (total time: 5706ms):
Trace[267101573]: ---"Writing http response done" count:1 3530ms (03:23:11.233)
Trace[267101573]: [5.706535208s] [5.706535208s] END
I0322 03:23:11.269531    2308 trace.go:236] Trace[13136005]: "List" accept:application/json, */*,audit-id:d5c8162b-f8a2-47cc-9f25-b87955510fd4,client:127.0.0.1,protocol:HTTP/2.0,resource:helmcharts,scope:cluster,url:/apis/helm.cattle.io/v1/helmcharts,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:LIST (22-Mar-2024 03:23:07.792) (total time: 3476ms):
Trace[13136005]: ["cacher list" audit-id:d5c8162b-f8a2-47cc-9f25-b87955510fd4,type:helmcharts.helm.cattle.io 3469ms (03:23:07.800)
Trace[13136005]:  ---"Listed items from cache" count:2 2182ms (03:23:09.984)]
Trace[13136005]: ---"Writing http response done" count:2 1281ms (03:23:11.269)
Trace[13136005]: [3.476843648s] [3.476843648s] END
I0322 03:23:11.315305    2308 trace.go:236] Trace[1272328989]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:fad336ac-8368-4532-9697-afa3bc1489e6,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:cluster,url:/api/v1/nodes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:23:05.836) (total time: 3791ms):
Trace[1272328989]: ---"Writing http response done" count:1 3788ms (03:23:09.628)
Trace[1272328989]: [3.79153804s] [3.79153804s] END
I0322 03:23:11.319418    2308 trace.go:236] Trace[1699597026]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:bd0b1599-005d-4abe-bfc9-7bb80cf31d2a,client:127.0.0.1,protocol:HTTP/2.0,resource:replicationcontrollers,scope:cluster,url:/api/v1/replicationcontrollers,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/scheduler,verb:LIST (22-Mar-2024 03:23:05.425) (total time: 5893ms):
Trace[1699597026]: ---"About to List from storage" 2170ms (03:23:07.596)
Trace[1699597026]: ---"Writing http response done" count:0 3715ms (03:23:11.319)
Trace[1699597026]: [5.893631153s] [5.893631153s] END
I0322 03:23:11.322608    2308 trace.go:236] Trace[1015874989]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:5c26553c-9582-42cc-b343-0bec58413f30,client:127.0.0.1,protocol:HTTP/2.0,resource:csistoragecapacities,scope:cluster,url:/apis/storage.k8s.io/v1/csistoragecapacities,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/scheduler,verb:LIST (22-Mar-2024 03:23:06.404) (total time: 4918ms):
Trace[1015874989]: ["cacher list" audit-id:5c26553c-9582-42cc-b343-0bec58413f30,type:csistoragecapacities.storage.k8s.io 4908ms (03:23:06.413)
Trace[1015874989]:  ---"watchCache locked acquired" 1183ms (03:23:07.597)]
Trace[1015874989]: ---"Writing http response done" count:0 3718ms (03:23:11.322)
Trace[1015874989]: [4.918378814s] [4.918378814s] END
I0322 03:23:11.335329    2308 trace.go:236] Trace[833759686]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:0c0f544d-3f0f-432e-a2e0-b12773024e27,client:127.0.0.1,protocol:HTTP/2.0,resource:pods,scope:namespace,url:/api/v1/namespaces/kube-system/pods,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/,verb:LIST (22-Mar-2024 03:23:05.789) (total time: 3932ms):
Trace[833759686]: ---"Writing http response done" count:5 3916ms (03:23:09.721)
Trace[833759686]: [3.932935415s] [3.932935415s] END
I0322 03:23:11.347919    2308 trace.go:236] Trace[1691104780]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:48c8f175-4f4c-494f-836d-9b573b4aebd7,client:127.0.0.1,protocol:HTTP/2.0,resource:persistentvolumeclaims,scope:cluster,url:/api/v1/persistentvolumeclaims,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/scheduler,verb:LIST (22-Mar-2024 03:23:06.290) (total time: 5057ms):
Trace[1691104780]: ---"Writing http response done" count:0 2693ms (03:23:11.347)
Trace[1691104780]: [5.057660541s] [5.057660541s] END
I0322 03:23:11.408885    2308 trace.go:236] Trace[1753481946]: "List" accept:application/json, */*,audit-id:f52410bd-a2d0-46f5-98f6-e1fc50f646e4,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:23:06.413) (total time: 4987ms):
Trace[1753481946]: ["cacher list" audit-id:f52410bd-a2d0-46f5-98f6-e1fc50f646e4,type:configmaps 4988ms (03:23:06.414)]
Trace[1753481946]: ---"Writing http response done" count:1 3505ms (03:23:11.400)
Trace[1753481946]: [4.987404876s] [4.987404876s] END
I0322 03:23:11.411366    2308 trace.go:236] Trace[216196667]: "List" accept:application/json, */*,audit-id:4095a3f2-077d-4f2f-9817-e5fafb56603f,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:23:06.264) (total time: 3874ms):
Trace[216196667]: ---"Writing http response done" count:1 3872ms (03:23:10.138)
Trace[216196667]: [3.874592479s] [3.874592479s] END
I0322 03:23:11.561873    2308 trace.go:236] Trace[1473158847]: "List" accept:application/json, */*,audit-id:a2f37a9d-7cbe-40fd-9c7b-666389493c35,client:127.0.0.1,protocol:HTTP/2.0,resource:networkpolicies,scope:cluster,url:/apis/networking.k8s.io/v1/networkpolicies,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:23:07.999) (total time: 3561ms):
Trace[1473158847]: ["cacher list" audit-id:a2f37a9d-7cbe-40fd-9c7b-666389493c35,type:networkpolicies.networking.k8s.io 3553ms (03:23:08.007)
Trace[1473158847]:  ---"watchCache locked acquired" 2391ms (03:23:10.399)]
Trace[1473158847]: ---"Writing http response done" count:0 1160ms (03:23:11.561)
Trace[1473158847]: [3.561787862s] [3.561787862s] END
I0322 03:23:11.562107    2308 trace.go:236] Trace[1281596920]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:3277a8d2-bf2d-4d18-aa82-c49d6992a15f,client:127.0.0.1,protocol:HTTP/2.0,resource:csinodes,scope:cluster,url:/apis/storage.k8s.io/v1/csinodes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/scheduler,verb:LIST (22-Mar-2024 03:23:06.225) (total time: 5328ms):
Trace[1281596920]: ["cacher list" audit-id:3277a8d2-bf2d-4d18-aa82-c49d6992a15f,type:csinodes.storage.k8s.io 5324ms (03:23:06.230)
Trace[1281596920]:  ---"Resized result" 2323ms (03:23:08.555)]
Trace[1281596920]: ---"Writing http response done" count:1 2994ms (03:23:11.553)
Trace[1281596920]: [5.328582459s] [5.328582459s] END
E0322 03:23:11.566719    2308 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:23:12.636946    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:23:12.644244    2308 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 03:23:12.644328    2308 trace.go:236] Trace[1458900511]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:e40a2c38-233a-4db7-a49e-36aff6e17ff9,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/server,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:22:53.746) (total time: 18898ms):
Trace[1458900511]: [18.898278458s] [18.898278458s] END
I0322 03:23:12.682698    2308 trace.go:236] Trace[1574516957]: "List" accept:application/json, */*,audit-id:19987911-be0a-48d8-9757-e1ceffb1e493,client:10.42.0.3,protocol:HTTP/2.0,resource:storageclasses,scope:cluster,url:/apis/storage.k8s.io/v1/storageclasses,user-agent:local-path-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:LIST (22-Mar-2024 03:23:00.546) (total time: 10490ms):
Trace[1574516957]: ---"About to List from storage" 2062ms (03:23:02.608)
Trace[1574516957]: ---"Writing http response done" count:1 8417ms (03:23:11.036)
Trace[1574516957]: [10.490549843s] [10.490549843s] END
E0322 03:23:13.121320    2308 timeout.go:142] post-timeout activity - time-elapsed: 1.254890472s, GET "/api" result: <nil>
I0322 03:23:13.517730    2308 trace.go:236] Trace[1052529762]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:21:00.330) (total time: 131375ms):
Trace[1052529762]: ---"Objects listed" error:<nil> 131375ms (03:23:11.705)
Trace[1052529762]: [2m11.375500144s] [2m11.375500144s] END
I0322 03:23:13.537455    2308 trace.go:236] Trace[1495323143]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:d4b4ca18-34e8-40f0-99ed-011645911efb,client:10.42.0.4,protocol:HTTP/2.0,resource:endpointslices,scope:cluster,url:/apis/discovery.k8s.io/v1/endpointslices,user-agent:coredns/v0.0.0 (linux/amd64) kubernetes/$Format,verb:LIST (22-Mar-2024 03:23:05.774) (total time: 5931ms):
Trace[1495323143]: ["cacher list" audit-id:d4b4ca18-34e8-40f0-99ed-011645911efb,type:endpointslices.discovery.k8s.io 3555ms (03:23:08.151)
Trace[1495323143]:  ---"watchCache locked acquired" 2432ms (03:23:10.585)]
Trace[1495323143]: ---"Writing http response done" count:3 1117ms (03:23:11.706)
Trace[1495323143]: [5.931122058s] [5.931122058s] END
E0322 03:23:13.720475    2308 wrap.go:54] timeout or abort while handling: method=GET URI="/api?timeout=32s" audit-ID="f063c178-f562-4b92-b67a-0b3e1c4d1b0b"
I0322 03:23:13.764234    2308 trace.go:236] Trace[184654743]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:36.518) (total time: 37244ms):
Trace[184654743]: ---"Objects listed" error:<nil> 37243ms (03:23:13.762)
Trace[184654743]: [37.244238045s] [37.244238045s] END
I0322 03:23:13.774852    2308 trace.go:236] Trace[859765255]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:20:48.393) (total time: 145381ms):
Trace[859765255]: ---"Objects listed" error:<nil> 145381ms (03:23:13.774)
Trace[859765255]: [2m25.381805827s] [2m25.381805827s] END
I0322 03:23:14.052156    2308 trace.go:236] Trace[104321719]: "Reflector ListAndWatch" name:object-"kube-system"/"chart-values-traefik" (22-Mar-2024 03:22:35.482) (total time: 38564ms):
Trace[104321719]: ---"Objects listed" error:<nil> 38560ms (03:23:14.043)
Trace[104321719]: [38.564585736s] [38.564585736s] END
I0322 03:23:14.069093    2308 trace.go:236] Trace[1970766604]: "Reflector ListAndWatch" name:object-"kube-system"/"local-path-config" (22-Mar-2024 03:22:35.546) (total time: 38520ms):
Trace[1970766604]: ---"Objects listed" error:<nil> 38520ms (03:23:14.067)
Trace[1970766604]: [38.520786802s] [38.520786802s] END
W0322 03:23:14.083755    2308 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://127.0.0.1:6443/api/v1/services?labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&resourceVersion=590": net/http: TLS handshake timeout - error from a previous attempt: unexpected EOF
I0322 03:23:14.085369    2308 trace.go:236] Trace[2031213230]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:21:02.323) (total time: 131762ms):
Trace[2031213230]: ---"Objects listed" error:Get "https://127.0.0.1:6443/api/v1/services?labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&resourceVersion=590": net/http: TLS handshake timeout - error from a previous attempt: unexpected EOF 131759ms (03:23:14.082)
Trace[2031213230]: [2m11.762188113s] [2m11.762188113s] END
E0322 03:23:14.086951    2308 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://127.0.0.1:6443/api/v1/services?labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&resourceVersion=590": net/http: TLS handshake timeout - error from a previous attempt: unexpected EOF
I0322 03:23:14.159349    2308 trace.go:236] Trace[1977044747]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:3fdfcbe4-9c83-4880-aa96-662832f1a560,client:127.0.0.1,protocol:HTTP/2.0,resource:flowschemas,scope:cluster,url:/apis/flowcontrol.apiserver.k8s.io/v1beta3/flowschemas,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:23:07.985) (total time: 4225ms):
Trace[1977044747]: ---"About to List from storage" 2268ms (03:23:10.253)
Trace[1977044747]: ---"Writing http response done" count:13 1952ms (03:23:12.210)
Trace[1977044747]: [4.225630559s] [4.225630559s] END
I0322 03:23:12.316574    2308 trace.go:236] Trace[1929921010]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:44a81772-6efd-4e25-b3bb-d24aee0b911b,client:127.0.0.1,protocol:HTTP/2.0,resource:clusterroles,scope:cluster,url:/apis/rbac.authorization.k8s.io/v1/clusterroles,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:23:08.732) (total time: 3583ms):
Trace[1929921010]: ["cacher list" audit-id:44a81772-6efd-4e25-b3bb-d24aee0b911b,type:clusterroles.rbac.authorization.k8s.io 3575ms (03:23:08.741)
Trace[1929921010]:  ---"watchCache locked acquired" 1484ms (03:23:10.225)]
Trace[1929921010]: ---"Writing http response done" count:69 827ms (03:23:12.316)
Trace[1929921010]: [3.583827504s] [3.583827504s] END
I0322 03:23:14.385647    2308 trace.go:236] Trace[262449334]: "List" accept:application/json, */*,audit-id:98ff90f8-cad5-4db6-b099-2eeff2417bb2,client:127.0.0.1,protocol:HTTP/2.0,resource:secrets,scope:cluster,url:/api/v1/secrets,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:LIST (22-Mar-2024 03:23:05.624) (total time: 8744ms):
Trace[262449334]: ---"Writing http response done" count:6 6533ms (03:23:14.369)
Trace[262449334]: [8.744762869s] [8.744762869s] END
E0322 03:23:14.684795    2308 timeout.go:142] post-timeout activity - time-elapsed: 5.760340153s, POST "/apis/authentication.k8s.io/v1/tokenreviews" result: <nil>
E0322 03:23:15.031106    2308 kubelet_node_status.go:540] "Error updating node status, will retry" err="error getting node \"server\": Get \"https://127.0.0.1:6443/api/v1/nodes/server?timeout=10s\": context deadline exceeded"
I0322 03:23:16.611261    2308 trace.go:236] Trace[228067212]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:20:58.693) (total time: 137913ms):
Trace[228067212]: ---"Objects listed" error:<nil> 137908ms (03:23:16.601)
Trace[228067212]: [2m17.913525324s] [2m17.913525324s] END
I0322 03:23:17.185027    2308 trace.go:236] Trace[1078514640]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:35.849) (total time: 41327ms):
Trace[1078514640]: ---"Objects listed" error:<nil> 41326ms (03:23:17.176)
Trace[1078514640]: [41.327305389s] [41.327305389s] END
W0322 03:23:17.237443    2308 transport.go:301] Unable to cancel request for *otelhttp.Transport
time="2024-03-22T03:23:14Z" level=info msg="Slow SQL (started: 2024-03-22 03:23:12.552963009 +0000 UTC m=+348.247776214) (total time: 2.106059544s):  SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key' : [[]]"
I0322 03:23:15.132273    2308 trace.go:236] Trace[1923873561]: "List" accept:application/json, */*,audit-id:7a80f628-44d5-45e8-a1ec-1ddf4874fceb,client:10.42.0.3,protocol:HTTP/2.0,resource:persistentvolumeclaims,scope:cluster,url:/api/v1/persistentvolumeclaims,user-agent:local-path-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:LIST (22-Mar-2024 03:23:13.078) (total time: 2046ms):
Trace[1923873561]: ---"Writing http response done" count:0 2044ms (03:23:15.125)
Trace[1923873561]: [2.0467737s] [2.0467737s] END
W0322 03:23:13.124447    2308 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://127.0.0.1:6443/apis/discovery.k8s.io/v1/endpointslices?labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&resourceVersion=589": net/http: TLS handshake timeout - error from a previous attempt: unexpected EOF
I0322 03:23:17.273356    2308 trace.go:236] Trace[53062077]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:35.466) (total time: 41804ms):
Trace[53062077]: ---"Objects listed" error:Get "https://127.0.0.1:6443/apis/discovery.k8s.io/v1/endpointslices?labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&resourceVersion=589": net/http: TLS handshake timeout - error from a previous attempt: unexpected EOF 37656ms (03:23:13.122)
Trace[53062077]: [41.804010183s] [41.804010183s] END
E0322 03:23:17.275935    2308 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://127.0.0.1:6443/apis/discovery.k8s.io/v1/endpointslices?labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&resourceVersion=589": net/http: TLS handshake timeout - error from a previous attempt: unexpected EOF
E0322 03:23:15.571564    2308 timeout.go:142] post-timeout activity - time-elapsed: 18.392986569s, PUT "/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/server" result: <nil>
I0322 03:23:15.893050    2308 trace.go:236] Trace[1603229640]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:35.508) (total time: 38195ms):
Trace[1603229640]: ---"Objects listed" error:<nil> 36344ms (03:23:11.853)
Trace[1603229640]: ---"Objects extracted" 1847ms (03:23:13.700)
Trace[1603229640]: [38.195355135s] [38.195355135s] END
time="2024-03-22T03:23:16Z" level=error msg="Error writing ping" error="write tcp 192.168.56.110:39688->192.168.56.110:6443: i/o timeout"
I0322 03:23:16.273236    2308 trace.go:236] Trace[1971485035]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:34.509) (total time: 41758ms):
Trace[1971485035]: ---"Objects listed" error:<nil> 40438ms (03:23:14.948)
Trace[1971485035]: [41.758040803s] [41.758040803s] END
I0322 03:23:17.691987    2308 trace.go:236] Trace[281077442]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:34.081) (total time: 43601ms):
Trace[281077442]: ---"Objects listed" error:<nil> 43600ms (03:23:17.682)
Trace[281077442]: [43.601469336s] [43.601469336s] END
I0322 03:23:18.023582    2308 trace.go:236] Trace[1872389824]: "Reflector ListAndWatch" name:object-"kube-system"/"coredns" (22-Mar-2024 03:22:37.617) (total time: 38263ms):
Trace[1872389824]: ---"Objects listed" error:<nil> 38261ms (03:23:15.878)
Trace[1872389824]: [38.263251424s] [38.263251424s] END
I0322 03:23:18.179053    2308 trace.go:236] Trace[1262127400]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:39.441) (total time: 37471ms):
Trace[1262127400]: ---"Objects listed" error:<nil> 35916ms (03:23:15.357)
Trace[1262127400]: [37.471645837s] [37.471645837s] END
I0322 03:23:18.203653    2308 trace.go:236] Trace[1620839725]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:38.353) (total time: 38558ms):
Trace[1620839725]: ---"Objects listed" error:<nil> 38554ms (03:23:16.907)
Trace[1620839725]: [38.558440368s] [38.558440368s] END
I0322 03:23:18.276814    2308 trace.go:236] Trace[1467228620]: "Reflector ListAndWatch" name:object-"kube-system"/"chart-content-traefik" (22-Mar-2024 03:22:43.913) (total time: 34359ms):
Trace[1467228620]: ---"Objects listed" error:<nil> 34352ms (03:23:18.266)
Trace[1467228620]: [34.359176834s] [34.359176834s] END
I0322 03:23:18.456132    2308 trace.go:236] Trace[1913390481]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:39.387) (total time: 36745ms):
Trace[1913390481]: ---"Objects listed" error:<nil> 36740ms (03:23:16.128)
Trace[1913390481]: [36.74503331s] [36.74503331s] END
I0322 03:23:20.275532    2308 trace.go:236] Trace[1931105786]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:414df652-635c-4d7f-bff6-ebdcefc7a6aa,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:cluster,url:/api/v1/nodes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/scheduler,verb:LIST (22-Mar-2024 03:23:11.598) (total time: 8676ms):
Trace[1931105786]: ["cacher list" audit-id:414df652-635c-4d7f-bff6-ebdcefc7a6aa,type:nodes 8669ms (03:23:11.605)
Trace[1931105786]:  ---"watchCache locked acquired" 3860ms (03:23:15.466)]
Trace[1931105786]: ---"Writing http response done" count:1 4802ms (03:23:20.274)
Trace[1931105786]: [8.676001239s] [8.676001239s] END
I0322 03:23:20.376328    2308 trace.go:236] Trace[207765789]: "SerializeObject" audit-id:4603d580-46cb-4071-9218-7f7a9c0aa8dc,method:GET,url:/api/v1/namespaces/kube-system/pods/helm-install-traefik-crd-m54x6,protocol:HTTP/2.0,mediaType:application/vnd.kubernetes.protobuf,encoder:{"encodeGV":"v1","encoder":"protobuf","name":"versioning"} (22-Mar-2024 03:23:13.246) (total time: 7121ms):
Trace[207765789]: ---"About to start writing response" size:6211 3649ms (03:23:16.895)
Trace[207765789]: ---"Write call succeeded" writer:responsewriter.outerWithCloseNotifyAndFlush,size:6211,firstWrite:true 3470ms (03:23:20.365)
Trace[207765789]: [7.121013612s] [7.121013612s] END
I0322 03:23:20.377809    2308 trace.go:236] Trace[1974165236]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:4603d580-46cb-4071-9218-7f7a9c0aa8dc,client:127.0.0.1,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/kube-system/pods/helm-install-traefik-crd-m54x6,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:22:47.564) (total time: 32813ms):
Trace[1974165236]: ---"About to write a response" 23950ms (03:23:11.514)
Trace[1974165236]: ---"Writing http response done" 8863ms (03:23:20.377)
Trace[1974165236]: [32.813244783s] [32.813244783s] END
I0322 03:23:20.413490    2308 trace.go:236] Trace[1895402520]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:33.266) (total time: 45531ms):
Trace[1895402520]: ---"Objects listed" error:<nil> 45529ms (03:23:18.795)
Trace[1895402520]: [45.531732145s] [45.531732145s] END
I0322 03:23:20.469242    2308 trace.go:236] Trace[1297137399]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:38.168) (total time: 42300ms):
Trace[1297137399]: ---"Objects listed" error:<nil> 42297ms (03:23:20.465)
Trace[1297137399]: [42.300344201s] [42.300344201s] END
I0322 03:23:20.486883    2308 trace.go:236] Trace[1946141148]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:20:52.947) (total time: 147539ms):
Trace[1946141148]: ---"Objects listed" error:<nil> 147538ms (03:23:20.486)
Trace[1946141148]: [2m27.539109s] [2m27.539109s] END
I0322 03:23:18.722842    2308 trace.go:236] Trace[1475082085]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:20:52.394) (total time: 146320ms):
Trace[1475082085]: ---"Objects listed" error:<nil> 143854ms (03:23:16.248)
Trace[1475082085]: [2m26.320735849s] [2m26.320735849s] END
I0322 03:23:18.832852    2308 trace.go:236] Trace[392786652]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:34.254) (total time: 44570ms):
Trace[392786652]: ---"Objects listed" error:<nil> 44570ms (03:23:18.824)
Trace[392786652]: [44.570595027s] [44.570595027s] END
I0322 03:23:20.705027    2308 trace.go:236] Trace[1161414966]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:43.757) (total time: 35511ms):
Trace[1161414966]: ---"Objects listed" error:<nil> 33878ms (03:23:17.635)
Trace[1161414966]: [35.511951338s] [35.511951338s] END
I0322 03:23:18.834512    2308 trace.go:236] Trace[1408682124]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:35.791) (total time: 43042ms):
Trace[1408682124]: ---"Objects listed" error:<nil> 43040ms (03:23:18.832)
Trace[1408682124]: [43.042499886s] [43.042499886s] END
I0322 03:23:20.797064    2308 trace.go:236] Trace[212975831]: "DeltaFIFO Pop Process" ID:etcdsnapshotfiles.k3s.cattle.io,Depth:21,Reason:slow event handlers blocking the queue (22-Mar-2024 03:23:20.618) (total time: 170ms):
Trace[212975831]: [170.594937ms] [170.594937ms] END
I0322 03:23:18.886200    2308 trace.go:236] Trace[2123056450]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:20:50.442) (total time: 146721ms):
Trace[2123056450]: ---"Objects listed" error:<nil> 146718ms (03:23:17.160)
Trace[2123056450]: [2m26.721796936s] [2m26.721796936s] END
I0322 03:23:18.893846    2308 trace.go:236] Trace[1055181141]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:20:46.773) (total time: 152116ms):
Trace[1055181141]: ---"Objects listed" error:<nil> 152115ms (03:23:18.888)
Trace[1055181141]: [2m32.116726455s] [2m32.116726455s] END
I0322 03:23:20.945551    2308 trace.go:236] Trace[573398047]: "DeltaFIFO Pop Process" ID:helmchartconfigs.helm.cattle.io,Depth:20,Reason:slow event handlers blocking the queue (22-Mar-2024 03:23:20.797) (total time: 136ms):
Trace[573398047]: [136.115292ms] [136.115292ms] END
E0322 03:23:18.927113    2308 controller.go:193] "Failed to update lease" err="Put \"https://127.0.0.1:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/server?timeout=10s\": context deadline exceeded"
I0322 03:23:18.934927    2308 trace.go:236] Trace[1862421592]: "List" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:62431f38-0ac4-4292-9d4c-3d9ddddc81d4,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:23:10.797) (total time: 3390ms):
Trace[1862421592]: ---"Writing http response done" count:1 3388ms (03:23:14.187)
Trace[1862421592]: [3.390172098s] [3.390172098s] END
I0322 03:23:18.980618    2308 trace.go:236] Trace[851322491]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:36.981) (total time: 41992ms):
Trace[851322491]: ---"Objects listed" error:<nil> 41989ms (03:23:18.971)
Trace[851322491]: [41.992900415s] [41.992900415s] END
time="2024-03-22T03:23:20Z" level=error msg="Error writing ping" error="write tcp 192.168.56.110:39688->192.168.56.110:6443: i/o timeout"
I0322 03:23:19.043142    2308 trace.go:236] Trace[2121471564]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:18.167) (total time: 59184ms):
Trace[2121471564]: ---"Objects listed" error:<nil> 59174ms (03:23:17.342)
Trace[2121471564]: [59.184099817s] [59.184099817s] END
I0322 03:23:21.008986    2308 trace.go:236] Trace[1809243060]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:35.425) (total time: 45582ms):
Trace[1809243060]: ---"Objects listed" error:<nil> 45581ms (03:23:21.007)
Trace[1809243060]: [45.582607459s] [45.582607459s] END
E0322 03:23:19.092260    2308 timeout.go:142] post-timeout activity - time-elapsed: 17.310168359s, GET "/api/v1/nodes/server" result: <nil>
I0322 03:23:19.106360    2308 trace.go:236] Trace[1686374424]: "SerializeObject" audit-id:e524ea67-4c28-4ff3-836f-fb15a5ff9ddf,method:GET,url:/api/v1/pods,protocol:HTTP/2.0,mediaType:application/vnd.kubernetes.protobuf,encoder:{"encodeGV":"v1","encoder":"protobuf","name":"versioning"} (22-Mar-2024 03:23:09.107) (total time: 8348ms):
Trace[1686374424]: ---"About to start writing response" size:25234 3464ms (03:23:12.571)
Trace[1686374424]: ---"Write call succeeded" writer:responsewriter.outerWithCloseNotifyAndFlush,size:25234,firstWrite:true 4883ms (03:23:17.454)
Trace[1686374424]: [8.348700397s] [8.348700397s] END
I0322 03:23:21.022913    2308 trace.go:236] Trace[1737712982]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:e524ea67-4c28-4ff3-836f-fb15a5ff9ddf,client:127.0.0.1,protocol:HTTP/2.0,resource:pods,scope:cluster,url:/api/v1/pods,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:23:07.030) (total time: 13991ms):
Trace[1737712982]: ["cacher list" audit-id:e524ea67-4c28-4ff3-836f-fb15a5ff9ddf,type:pods 13984ms (03:23:07.037)
Trace[1737712982]:  ---"watchCache locked acquired" 2053ms (03:23:09.091)]
Trace[1737712982]: ---"Writing http response done" count:5 11930ms (03:23:21.022)
Trace[1737712982]: [13.99183783s] [13.99183783s] END
I0322 03:23:19.315322    2308 trace.go:236] Trace[911486562]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:42.081) (total time: 35577ms):
Trace[911486562]: ---"Objects listed" error:<nil> 35576ms (03:23:17.657)
Trace[911486562]: [35.577667108s] [35.577667108s] END
I0322 03:23:19.624018    2308 trace.go:236] Trace[1747788015]: "SerializeObject" audit-id:fa2e7423-6a2c-43c5-a6c1-778c675a4c99,method:GET,url:/api/v1/namespaces/kube-system/configmaps,protocol:HTTP/2.0,mediaType:application/json,encoder:{"encodeGV":"v1","encoder":"{\"name\":\"json\",\"pretty\":\"false\",\"strict\":\"false\",\"yaml\":\"false\"}","name":"versioning"} (22-Mar-2024 03:23:10.678) (total time: 5080ms):
Trace[1747788015]: ---"About to start writing response" size:2124 2954ms (03:23:13.632)
Trace[1747788015]: ---"Write call succeeded" writer:responsewriter.outerWithCloseNotifyAndFlush,size:2124,firstWrite:true 2124ms (03:23:15.757)
Trace[1747788015]: [5.08010151s] [5.08010151s] END
I0322 03:23:21.030674    2308 trace.go:236] Trace[1760038523]: "List" accept:application/json, */*,audit-id:fa2e7423-6a2c-43c5-a6c1-778c675a4c99,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:23:08.197) (total time: 12833ms):
Trace[1760038523]: ---"Writing http response done" count:1 12823ms (03:23:21.030)
Trace[1760038523]: [12.83358034s] [12.83358034s] END
I0322 03:23:19.770740    2308 trace.go:236] Trace[169043791]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:761c5bce-e8ad-4528-aec0-91f8a3fae216,client:10.42.0.4,protocol:HTTP/2.0,resource:namespaces,scope:cluster,url:/api/v1/namespaces,user-agent:coredns/v0.0.0 (linux/amd64) kubernetes/$Format,verb:LIST (22-Mar-2024 03:23:10.204) (total time: 9563ms):
Trace[169043791]: ["cacher list" audit-id:761c5bce-e8ad-4528-aec0-91f8a3fae216,type:namespaces 9560ms (03:23:10.207)
Trace[169043791]:  ---"watchCache locked acquired" 1764ms (03:23:11.972)]
Trace[169043791]: ---"Writing http response done" count:4 7794ms (03:23:19.767)
Trace[169043791]: [9.563204455s] [9.563204455s] END
I0322 03:23:19.989203    2308 trace.go:236] Trace[780049805]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:36.729) (total time: 40293ms):
Trace[780049805]: ---"Objects listed" error:<nil> 40287ms (03:23:17.017)
Trace[780049805]: [40.293139912s] [40.293139912s] END
E0322 03:23:21.662818    2308 server.go:310] "Unable to authenticate the request due to an error" err="context canceled"
I0322 03:23:21.884959    2308 trace.go:236] Trace[202155209]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:35.502) (total time: 44458ms):
Trace[202155209]: ---"Objects listed" error:<nil> 44452ms (03:23:19.954)
Trace[202155209]: [44.458507098s] [44.458507098s] END
I0322 03:23:22.029148    2308 trace.go:236] Trace[835719025]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:20:56.730) (total time: 143343ms):
Trace[835719025]: ---"Objects listed" error:<nil> 143340ms (03:23:20.071)
Trace[835719025]: [2m23.343927315s] [2m23.343927315s] END
I0322 03:23:22.447189    2308 trace.go:236] Trace[1880039258]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:34.332) (total time: 45912ms):
Trace[1880039258]: ---"Objects listed" error:<nil> 45908ms (03:23:20.241)
Trace[1880039258]: [45.912315386s] [45.912315386s] END
time="2024-03-22T03:23:22Z" level=info msg="Slow SQL (started: 2024-03-22 03:23:16.654167387 +0000 UTC m=+352.348980603) (total time: 4.19985106s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC  : [[/registry/services/specs/% false]]"
I0322 03:23:22.889660    2308 trace.go:236] Trace[80809844]: "DeltaFIFO Pop Process" ID:kube-node-lease/default,Depth:38,Reason:slow event handlers blocking the queue (22-Mar-2024 03:23:22.714) (total time: 165ms):
Trace[80809844]: [165.184032ms] [165.184032ms] END
time="2024-03-22T03:23:23Z" level=info msg="Slow SQL (started: 2024-03-22 03:23:19.008536532 +0000 UTC m=+354.703349729) (total time: 2.714331558s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC LIMIT 1 : [[/registry/masterleases/192.168.56.110 true]]"
I0322 03:23:23.057787    2308 trace.go:236] Trace[1737136875]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:21:22.165) (total time: 120881ms):
Trace[1737136875]: ---"Objects listed" error:<nil> 120880ms (03:23:23.045)
Trace[1737136875]: [2m0.881358482s] [2m0.881358482s] END
I0322 03:23:25.169792    2308 trace.go:236] Trace[621311553]: "List" accept:application/json, */*,audit-id:310fb18e-5026-4ddb-bbda-9203e6d29503,client:10.42.0.3,protocol:HTTP/2.0,resource:persistentvolumes,scope:cluster,url:/api/v1/persistentvolumes,user-agent:local-path-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:LIST (22-Mar-2024 03:23:17.964) (total time: 5325ms):
Trace[621311553]: ---"About to List from storage" 1713ms (03:23:19.677)
Trace[621311553]: ---"Writing http response done" count:0 2032ms (03:23:23.290)
Trace[621311553]: [5.325374193s] [5.325374193s] END
I0322 03:23:25.171551    2308 trace.go:236] Trace[931449867]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:37.996) (total time: 45304ms):
Trace[931449867]: ---"Objects listed" error:<nil> 45300ms (03:23:23.296)
Trace[931449867]: [45.304401118s] [45.304401118s] END
E0322 03:23:25.183698    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
E0322 03:23:25.185447    2308 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:23:25.185577    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:23:25.208871    2308 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 03:23:25.208955    2308 trace.go:236] Trace[634489545]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:ea49e2b7-1990-44a4-9c60-388b936d96e7,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/server,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:23:19.721) (total time: 5487ms):
Trace[634489545]: [5.487525921s] [5.487525921s] END
E0322 03:23:25.210674    2308 timeout.go:142] post-timeout activity - time-elapsed: 11.361705143s, GET "/api/v1/nodes/server" result: <nil>
I0322 03:23:25.255096    2308 trace.go:236] Trace[1026871737]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:41.267) (total time: 43987ms):
Trace[1026871737]: ---"Objects listed" error:<nil> 43986ms (03:23:25.254)
Trace[1026871737]: [43.987812046s] [43.987812046s] END
I0322 03:23:25.289326    2308 trace.go:236] Trace[370287921]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:20:48.301) (total time: 156987ms):
Trace[370287921]: ---"Objects listed" error:<nil> 156987ms (03:23:25.289)
Trace[370287921]: [2m36.987637927s] [2m36.987637927s] END
I0322 03:23:25.291530    2308 trace.go:236] Trace[1134386083]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:42.284) (total time: 43006ms):
Trace[1134386083]: ---"Objects listed" error:<nil> 43006ms (03:23:25.291)
Trace[1134386083]: [43.006537411s] [43.006537411s] END
I0322 03:23:25.297134    2308 trace.go:236] Trace[1177578754]: "List" accept:application/json, */*,audit-id:1490ce05-eb8e-4f6a-b428-61cdb8f71971,client:127.0.0.1,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/default/endpoints,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:23:11.657) (total time: 13639ms):
Trace[1177578754]: ---"About to List from storage" 5873ms (03:23:17.531)
Trace[1177578754]: ["cacher list" audit-id:1490ce05-eb8e-4f6a-b428-61cdb8f71971,type:endpoints 7761ms (03:23:17.535)]
Trace[1177578754]: ---"Writing http response done" count:1 6135ms (03:23:25.297)
Trace[1177578754]: [13.639395494s] [13.639395494s] END
time="2024-03-22T03:23:25Z" level=error msg="Error writing ping" error="write tcp 192.168.56.110:39688->192.168.56.110:6443: i/o timeout"
I0322 03:23:25.387507    2308 trace.go:236] Trace[1875950179]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:21:34.573) (total time: 110813ms):
Trace[1875950179]: ---"Objects listed" error:<nil> 110813ms (03:23:25.387)
Trace[1875950179]: [1m50.813621838s] [1m50.813621838s] END
I0322 03:23:25.439737    2308 trace.go:236] Trace[1256368070]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:41.917) (total time: 43522ms):
Trace[1256368070]: ---"Objects listed" error:<nil> 43522ms (03:23:25.439)
Trace[1256368070]: [43.522075328s] [43.522075328s] END
I0322 03:23:25.486780    2308 trace.go:236] Trace[1504152048]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:21:02.755) (total time: 142731ms):
Trace[1504152048]: ---"Objects listed" error:<nil> 142730ms (03:23:25.486)
Trace[1504152048]: [2m22.731446629s] [2m22.731446629s] END
I0322 03:23:25.496500    2308 trace.go:236] Trace[328179706]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:38.839) (total time: 46657ms):
Trace[328179706]: ---"Objects listed" error:<nil> 46657ms (03:23:25.496)
Trace[328179706]: [46.657328619s] [46.657328619s] END
E0322 03:23:25.519551    2308 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:23:25.520574    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:23:25.521153    2308 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 03:23:25.523775    2308 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:23:25.523817    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
I0322 03:23:25.524865    2308 trace.go:236] Trace[725007453]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:c84544c9-8f6c-4c99-af3f-ae7c4fb03235,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/server,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PUT (22-Mar-2024 03:23:21.707) (total time: 3816ms):
Trace[725007453]: ---"About to convert to expected version" 2179ms (03:23:23.892)
Trace[725007453]: ---"Write to database call failed" len:465,err:Timeout: request did not complete within requested timeout - context canceled 831ms (03:23:25.514)
Trace[725007453]: [3.816939366s] [3.816939366s] END
E0322 03:23:25.527386    2308 timeout.go:142] post-timeout activity - time-elapsed: 7.877156496s, PUT "/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/server" result: <nil>
E0322 03:23:25.561401    2308 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 03:23:25.597208    2308 timeout.go:142] post-timeout activity - time-elapsed: 708.7273ms, POST "/apis/authorization.k8s.io/v1/subjectaccessreviews" result: <nil>
I0322 03:23:23.127751    2308 trace.go:236] Trace[1766781335]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:21:34.352) (total time: 108775ms):
Trace[1766781335]: ---"Objects listed" error:<nil> 108774ms (03:23:23.126)
Trace[1766781335]: [1m48.775089895s] [1m48.775089895s] END
I0322 03:23:25.721686    2308 trace.go:236] Trace[450966938]: "List" accept:application/json, */*,audit-id:9aa8caa2-dd55-4767-abcd-48b520ed5eb8,client:10.42.0.5,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps,user-agent:metrics-server/v0.6.3 (linux/amd64) kubernetes/a938798,verb:GET (22-Mar-2024 03:23:24.937) (total time: 784ms):
Trace[450966938]: ---"Writing http response done" count:1 782ms (03:23:25.721)
Trace[450966938]: [784.215523ms] [784.215523ms] END
I0322 03:23:25.741157    2308 trace.go:236] Trace[795110373]: "Reflector ListAndWatch" name:object-"kube-system"/"kube-root-ca.crt" (22-Mar-2024 03:22:43.769) (total time: 41971ms):
Trace[795110373]: ---"Objects listed" error:<nil> 41971ms (03:23:25.741)
Trace[795110373]: [41.971426889s] [41.971426889s] END
E0322 03:23:25.754130    2308 finisher.go:175] FinishRequest: post-timeout activity - time-elapsed: 24.156857418s, panicked: false, err: context canceled, panic-reason: <nil>
time="2024-03-22T03:23:25Z" level=info msg="Slow SQL (started: 2024-03-22 03:23:24.100035316 +0000 UTC m=+359.794848520) (total time: 1.063166552s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC  : [[/registry/services/specs/% false]]"
I0322 03:23:23.188857    2308 trace.go:236] Trace[108112103]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:20:51.083) (total time: 152104ms):
Trace[108112103]: ---"Objects listed" error:<nil> 152100ms (03:23:23.184)
Trace[108112103]: [2m32.104126511s] [2m32.104126511s] END
I0322 03:23:25.790632    2308 trace.go:236] Trace[494064142]: "SerializeObject" audit-id:2071c0ee-3feb-4f2e-b9cd-f44c6bf52f13,method:GET,url:/api,protocol:HTTP/2.0,mediaType:application/json;g=apidiscovery.k8s.io;v=v2beta1;as=APIGroupDiscoveryList,encoder:{"encodeGV":"apidiscovery.k8s.io/v2beta1","encoder":"{\"name\":\"json\",\"pretty\":\"false\",\"strict\":\"false\",\"yaml\":\"false\"}","name":"versioning"} (22-Mar-2024 03:23:20.188) (total time: 5602ms):
Trace[494064142]: ---"About to start writing response" size:6969 5467ms (03:23:25.656)
Trace[494064142]: [5.602164152s] [5.602164152s] END
I0322 03:23:23.190982    2308 trace.go:236] Trace[45715832]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:39.667) (total time: 42270ms):
Trace[45715832]: ---"Objects listed" error:<nil> 42262ms (03:23:21.929)
Trace[45715832]: [42.270733341s] [42.270733341s] END
I0322 03:23:23.635545    2308 trace.go:236] Trace[684661354]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:69533075-42a0-4334-8bba-dd6d5025f73e,client:10.42.0.4,protocol:HTTP/2.0,resource:services,scope:cluster,url:/api/v1/services,user-agent:coredns/v0.0.0 (linux/amd64) kubernetes/$Format,verb:LIST (22-Mar-2024 03:23:16.282) (total time: 6477ms):
Trace[684661354]: ---"Writing http response done" count:3 3973ms (03:23:22.760)
Trace[684661354]: [6.477999731s] [6.477999731s] END
I0322 03:23:24.478339    2308 trace.go:236] Trace[1769635770]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:35.278) (total time: 47421ms):
Trace[1769635770]: ---"Objects listed" error:<nil> 47416ms (03:23:22.694)
Trace[1769635770]: [47.421932057s] [47.421932057s] END
I0322 03:23:25.853998    2308 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
I0322 03:23:25.854564    2308 event.go:307] "Event occurred" object="kube-system/traefik-crd" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik-crd"
I0322 03:23:24.500898    2308 trace.go:236] Trace[1962852083]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:38.982) (total time: 45515ms):
Trace[1962852083]: ---"Objects listed" error:<nil> 45512ms (03:23:24.494)
Trace[1962852083]: [45.515973888s] [45.515973888s] END
I0322 03:23:24.576160    2308 trace.go:236] Trace[927192988]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:33.177) (total time: 51393ms):
Trace[927192988]: ---"Objects listed" error:<nil> 51392ms (03:23:24.570)
Trace[927192988]: [51.393420585s] [51.393420585s] END
I0322 03:23:24.807341    2308 trace.go:236] Trace[499014088]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:43.866) (total time: 40934ms):
Trace[499014088]: ---"Objects listed" error:<nil> 40928ms (03:23:24.795)
Trace[499014088]: [40.934429075s] [40.934429075s] END
I0322 03:23:24.808106    2308 trace.go:236] Trace[520595457]: "DeltaFIFO Pop Process" ID:kube-system/attachdetach-controller,Depth:36,Reason:slow event handlers blocking the queue (22-Mar-2024 03:23:24.646) (total time: 160ms):
Trace[520595457]: [160.935239ms] [160.935239ms] END
E0322 03:23:24.934580    2308 finisher.go:175] FinishRequest: post-timeout activity - time-elapsed: 19.349093899s, panicked: false, err: <nil>, panic-reason: <nil>
I0322 03:23:24.984285    2308 trace.go:236] Trace[436177730]: "DeltaFIFO Pop Process" ID:kube-system/auth-delegator,Depth:11,Reason:slow event handlers blocking the queue (22-Mar-2024 03:23:24.806) (total time: 175ms):
Trace[436177730]: [175.469108ms] [175.469108ms] END
I0322 03:23:25.053621    2308 trace.go:236] Trace[690606734]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:20:52.784) (total time: 152268ms):
Trace[690606734]: ---"Objects listed" error:<nil> 152265ms (03:23:25.049)
Trace[690606734]: [2m32.268031242s] [2m32.268031242s] END
I0322 03:23:25.053693    2308 trace.go:236] Trace[1498722378]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:34.395) (total time: 48591ms):
Trace[1498722378]: ---"Objects listed" error:<nil> 48587ms (03:23:22.983)
Trace[1498722378]: [48.591218766s] [48.591218766s] END
E0322 03:23:25.872123    2308 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to retrieve openAPI spec, http error: ResponseCode: 500, Body: Internal Server Error: "/openapi/v2": Post "https://10.43.0.1:443/apis/authorization.k8s.io/v1/subjectaccessreviews?timeout=10s": context deadline exceeded
, Header: map[Audit-Id:[2c0b4dfa-7a59-4fd0-8963-e9cac04d1b73] Cache-Control:[no-cache, private] Content-Length:[156] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 22 Mar 2024 03:23:23 GMT] X-Content-Type-Options:[nosniff]]
I0322 03:23:25.872158    2308 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0322 03:23:26.159044    2308 trace.go:236] Trace[178136710]: "Get" accept:application/json, */*,audit-id:ffecb03b-bc69-43d1-8773-9fc86d940b38,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps/coredns,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:GET (22-Mar-2024 03:23:25.078) (total time: 1079ms):
Trace[178136710]: ---"About to write a response" 1079ms (03:23:26.158)
Trace[178136710]: [1.079531801s] [1.079531801s] END
W0322 03:23:26.206675    2308 controller.go:134] slow openapi aggregation of "traefikservices.traefik.io": 43.555636111s
I0322 03:23:26.339051    2308 trace.go:236] Trace[1240185963]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:e115f644-f519-4e41-ba2d-f02f1f209fd6,client:127.0.0.1,protocol:HTTP/2.0,resource:tokenreviews,scope:resource,url:/apis/authentication.k8s.io/v1/tokenreviews,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:POST (22-Mar-2024 03:23:22.291) (total time: 3615ms):
Trace[1240185963]: ---"limitedReadBody succeeded" len:1152 2467ms (03:23:24.758)
Trace[1240185963]: ---"Write to database call succeeded" len:1152 1135ms (03:23:25.904)
Trace[1240185963]: [3.615699403s] [3.615699403s] END
I0322 03:23:26.438834    2308 trace.go:236] Trace[1908616218]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:35.502) (total time: 50935ms):
Trace[1908616218]: ---"Objects listed" error:<nil> 50934ms (03:23:26.437)
Trace[1908616218]: [50.935237506s] [50.935237506s] END
I0322 03:23:26.640980    2308 trace.go:236] Trace[920494125]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:22:37.939) (total time: 48701ms):
Trace[920494125]: ---"Objects listed" error:<nil> 48698ms (03:23:26.637)
Trace[920494125]: [48.701034908s] [48.701034908s] END
I0322 03:23:26.697885    2308 trace.go:236] Trace[182689883]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:85bb9d07-c529-486a-98ca-f1247101c64d,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-7zoch227uv4owxg75pgtnmv3jq,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PUT (22-Mar-2024 03:22:51.922) (total time: 34775ms):
Trace[182689883]: ---"About to store object in database" 2377ms (03:22:54.313)
Trace[182689883]: ["GuaranteedUpdate etcd3" audit-id:85bb9d07-c529-486a-98ca-f1247101c64d,key:/leases/kube-system/apiserver-7zoch227uv4owxg75pgtnmv3jq,type:*coordination.Lease,resource:leases.coordination.k8s.io 27892ms (03:22:58.804)
Trace[182689883]:  ---"initial value restored" 1300ms (03:23:00.105)
Trace[182689883]:  ---"About to Encode" 20547ms (03:23:20.652)
Trace[182689883]:  ---"Txn call failed" err:context deadline exceeded 5652ms (03:23:26.312)]
Trace[182689883]: ---"Write to database call failed" len:590,err:Timeout: request did not complete within requested timeout - context deadline exceeded 384ms (03:23:26.696)
Trace[182689883]: [34.775459834s] [34.775459834s] END
E0322 03:23:26.726795    2308 finisher.go:175] FinishRequest: post-timeout activity - time-elapsed: 6.733s, panicked: false, err: context deadline exceeded, panic-reason: <nil>
I0322 03:23:26.728130    2308 trace.go:236] Trace[985846008]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:df7a5304-56f6-4bbd-bc1c-eca7d005d204,client:127.0.0.1,protocol:HTTP/2.0,resource:services,scope:cluster,url:/api/v1/services,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:23:06.977) (total time: 19750ms):
Trace[985846008]: ---"About to List from storage" 1956ms (03:23:08.934)
Trace[985846008]: ["List(recursive=true) etcd3" audit-id:df7a5304-56f6-4bbd-bc1c-eca7d005d204,key:/services/specs,resourceVersion:,resourceVersionMatch:,limit:0,continue: 16664ms (03:23:10.063)]
Trace[985846008]: ---"Writing http response done" count:3 95ms (03:23:26.728)
Trace[985846008]: [19.75029256s] [19.75029256s] END
E0322 03:23:26.772468    2308 kubelet_node_status.go:540] "Error updating node status, will retry" err="error getting node \"server\": Get \"https://127.0.0.1:6443/api/v1/nodes/server?timeout=10s\": context deadline exceeded"
E0322 03:23:26.774966    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
I0322 03:23:26.775232    2308 trace.go:236] Trace[1820426017]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:75f8e86e-832f-48af-a293-666874cb806e,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/server,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:23:23.234) (total time: 3541ms):
Trace[1820426017]: [3.541140373s] [3.541140373s] END
E0322 03:23:26.842880    2308 finisher.go:175] FinishRequest: post-timeout activity - time-elapsed: 1.285026569s, panicked: false, err: <nil>, panic-reason: <nil>
E0322 03:23:26.877401    2308 wrap.go:54] timeout or abort while handling: method=GET URI="/api/v1/nodes/server?timeout=10s" audit-ID="75f8e86e-832f-48af-a293-666874cb806e"
E0322 03:23:26.880085    2308 timeout.go:142] post-timeout activity - time-elapsed: 11.944s, GET "/api/v1/nodes/server" result: <nil>
E0322 03:23:26.969976    2308 finisher.go:175] FinishRequest: post-timeout activity - time-elapsed: 2.282849688s, panicked: false, err: context canceled, panic-reason: <nil>
I0322 03:23:26.972611    2308 trace.go:236] Trace[1455485320]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:5616337d-6981-419f-aa31-890266dc3471,client:127.0.0.1,protocol:HTTP/2.0,resource:services,scope:cluster,url:/api/v1/services,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:23:15.399) (total time: 11573ms):
Trace[1455485320]: ---"About to List from storage" 1543ms (03:23:16.942)
Trace[1455485320]: ["List(recursive=true) etcd3" audit-id:5616337d-6981-419f-aa31-890266dc3471,key:/services/specs,resourceVersion:,resourceVersionMatch:,limit:0,continue: 8701ms (03:23:18.271)]
Trace[1455485320]: ---"Writing http response done" count:3 749ms (03:23:26.972)
Trace[1455485320]: [11.573385129s] [11.573385129s] END
I0322 03:23:26.996582    2308 trace.go:236] Trace[751309669]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:23:15.295) (total time: 11701ms):
Trace[751309669]: ---"Objects listed" error:<nil> 11699ms (03:23:26.994)
Trace[751309669]: [11.70151054s] [11.70151054s] END
E0322 03:23:27.019220    2308 finisher.go:175] FinishRequest: post-timeout activity - time-elapsed: 40.475798315s, panicked: false, err: context deadline exceeded, panic-reason: <nil>
E0322 03:23:27.027302    2308 controller.go:193] "Failed to update lease" err="Timeout: request did not complete within requested timeout - context deadline exceeded"
I0322 03:23:27.086304    2308 trace.go:236] Trace[1702334781]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.56.110,type:*v1.Endpoints,resource:apiServerIPInfo (22-Mar-2024 03:18:39.993) (total time: 287092ms):
Trace[1702334781]: ---"initial value restored" 255374ms (03:22:55.368)
Trace[1702334781]: ---"Transaction prepared" 15954ms (03:23:11.338)
Trace[1702334781]: ---"Txn call completed" 15739ms (03:23:27.077)
Trace[1702334781]: [4m47.092291046s] [4m47.092291046s] END
I0322 03:23:27.101788    2308 trace.go:236] Trace[1051137349]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:93012c5b-4a57-4dd5-91bf-315396420733,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/server/status,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:node-controller,verb:PUT (22-Mar-2024 03:23:17.018) (total time: 10083ms):
Trace[1051137349]: ---"limitedReadBody succeeded" len:4584 412ms (03:23:17.430)
Trace[1051137349]: ---"Conversion done" 1665ms (03:23:19.103)
Trace[1051137349]: ---"About to store object in database" 1449ms (03:23:20.552)
Trace[1051137349]: ["GuaranteedUpdate etcd3" audit-id:93012c5b-4a57-4dd5-91bf-315396420733,key:/minions/server,type:*core.Node,resource:nodes 6054ms (03:23:21.047)
Trace[1051137349]:  ---"initial value restored" 2113ms (03:23:23.160)
Trace[1051137349]:  ---"About to Encode" 2591ms (03:23:25.752)
Trace[1051137349]:  ---"Txn call completed" 1321ms (03:23:27.076)]
Trace[1051137349]: ---"Writing http response done" 25ms (03:23:27.101)
Trace[1051137349]: [10.083415932s] [10.083415932s] END
I0322 03:23:27.112412    2308 trace.go:236] Trace[1514854004]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:6792727d-367f-4872-ac38-c32c639bc8bc,client:127.0.0.1,protocol:HTTP/2.0,resource:services,scope:resource,url:/api/v1/namespaces/default/services/kubernetes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/,verb:GET (22-Mar-2024 03:23:23.563) (total time: 3548ms):
Trace[1514854004]: ---"About to write a response" 3548ms (03:23:27.112)
Trace[1514854004]: [3.548633141s] [3.548633141s] END
I0322 03:23:27.132187    2308 trace.go:236] Trace[1820174469]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:7239355a-2996-462d-b5e6-df480f5e48e8,client:127.0.0.1,protocol:HTTP/2.0,resource:jobs,scope:resource,url:/apis/batch/v1/namespaces/kube-system/jobs/helm-install-traefik/status,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:job-controller,verb:PUT (22-Mar-2024 03:23:16.781) (total time: 10350ms):
Trace[1820174469]: ---"limitedReadBody succeeded" len:6685 570ms (03:23:17.352)
Trace[1820174469]: ---"About to convert to expected version" 3120ms (03:23:20.472)
Trace[1820174469]: ---"Conversion done" 16ms (03:23:20.489)
Trace[1820174469]: ---"About to store object in database" 1281ms (03:23:21.770)
Trace[1820174469]: ["GuaranteedUpdate etcd3" audit-id:7239355a-2996-462d-b5e6-df480f5e48e8,key:/jobs/kube-system/helm-install-traefik,type:*batch.Job,resource:jobs.batch 2920ms (03:23:24.211)
Trace[1820174469]:  ---"initial value restored" 1023ms (03:23:25.234)
Trace[1820174469]:  ---"About to Encode" 567ms (03:23:25.802)
Trace[1820174469]:  ---"Txn call completed" 1310ms (03:23:27.112)]
Trace[1820174469]: ---"Writing http response done" 19ms (03:23:27.132)
Trace[1820174469]: [10.350448568s] [10.350448568s] END
I0322 03:23:27.230294    2308 trace.go:236] Trace[743051109]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:19973993-cbaa-4366-ba40-149eebdecd67,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/server,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PUT (22-Mar-2024 03:23:24.874) (total time: 2355ms):
Trace[743051109]: ---"limitedReadBody succeeded" len:465 43ms (03:23:24.918)
Trace[743051109]: ["GuaranteedUpdate etcd3" audit-id:19973993-cbaa-4366-ba40-149eebdecd67,key:/leases/kube-node-lease/server,type:*coordination.Lease,resource:leases.coordination.k8s.io 2290ms (03:23:24.940)
Trace[743051109]:  ---"About to Encode" 779ms (03:23:25.720)
Trace[743051109]:  ---"Txn call completed" 1509ms (03:23:27.230)]
Trace[743051109]: [2.355976955s] [2.355976955s] END
I0322 03:23:27.235175    2308 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:23:27.392881    2308 event.go:307] "Event occurred" object="server" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="NodeNotReady" message="Node server status is now: NodeNotReady"
I0322 03:23:27.454448    2308 trace.go:236] Trace[1404646813]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:e79ca8a2-9c3b-46f5-9fc6-7cdb803ea65a,client:127.0.0.1,protocol:HTTP/2.0,resource:apiservices,scope:resource,url:/apis/apiregistration.k8s.io/v1/apiservices/v1beta1.metrics.k8s.io/status,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PUT (22-Mar-2024 03:23:08.814) (total time: 18640ms):
Trace[1404646813]: ---"limitedReadBody succeeded" len:1900 1631ms (03:23:10.445)
Trace[1404646813]: ---"About to convert to expected version" 1122ms (03:23:11.567)
Trace[1404646813]: ["GuaranteedUpdate etcd3" audit-id:e79ca8a2-9c3b-46f5-9fc6-7cdb803ea65a,key:/apiregistration.k8s.io/apiservices/v1beta1.metrics.k8s.io,type:*apiregistration.APIService,resource:apiservices.apiregistration.k8s.io 15457ms (03:23:11.996)
Trace[1404646813]:  ---"initial value restored" 7844ms (03:23:19.841)
Trace[1404646813]:  ---"About to Encode" 6902ms (03:23:26.744)
Trace[1404646813]:  ---"Encode succeeded" len:2399 241ms (03:23:26.986)
Trace[1404646813]:  ---"Txn call completed" 467ms (03:23:27.454)]
Trace[1404646813]: [18.640001284s] [18.640001284s] END
W0322 03:23:27.468240    2308 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:23:27.469798    2308 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
E0322 03:23:27.510960    2308 kubelet.go:2477] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="15.837s"
time="2024-03-22T03:23:27Z" level=info msg="Updating TLS secret for kube-system/k3s-serving (count: 10): map[listener.cattle.io/cn-10.43.0.1:10.43.0.1 listener.cattle.io/cn-127.0.0.1:127.0.0.1 listener.cattle.io/cn-192.168.56.110:192.168.56.110 listener.cattle.io/cn-__1-f16284:::1 listener.cattle.io/cn-kubernetes:kubernetes listener.cattle.io/cn-kubernetes.default:kubernetes.default listener.cattle.io/cn-kubernetes.default.svc:kubernetes.default.svc listener.cattle.io/cn-kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local listener.cattle.io/cn-localhost:localhost listener.cattle.io/cn-server:server listener.cattle.io/fingerprint:SHA1=296AFC6A252966C067401FF9072578D9CE0A72C7]"
E0322 03:23:27.538405    2308 resource_quota_controller.go:440] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I0322 03:23:27.545367    2308 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressrouteudps.traefik.containo.us"
I0322 03:23:27.545753    2308 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="middlewares.traefik.containo.us"
I0322 03:23:27.546098    2308 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressroutes.traefik.io"
I0322 03:23:27.619872    2308 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="serverstransporttcps.traefik.io"
I0322 03:23:27.620159    2308 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="tlsoptions.traefik.containo.us"
I0322 03:23:27.620294    2308 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="tlsstores.traefik.containo.us"
I0322 03:23:27.620410    2308 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="traefikservices.traefik.io"
I0322 03:23:27.620525    2308 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressrouteudps.traefik.io"
I0322 03:23:27.620652    2308 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="serverstransports.traefik.containo.us"
I0322 03:23:27.621239    2308 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressroutetcps.traefik.io"
I0322 03:23:27.621352    2308 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressroutetcps.traefik.containo.us"
I0322 03:23:27.621509    2308 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="serverstransports.traefik.io"
I0322 03:23:27.621613    2308 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="tlsoptions.traefik.io"
I0322 03:23:27.555229    2308 trace.go:236] Trace[1392240575]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:10f6b409-1065-401f-a180-4209123e00a7,client:127.0.0.1,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/kube-system/pods/helm-install-traefik-crd-m54x6/status,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PATCH (22-Mar-2024 03:23:26.736) (total time: 818ms):
Trace[1392240575]: ---"limitedReadBody succeeded" len:90 119ms (03:23:26.855)
Trace[1392240575]: ["GuaranteedUpdate etcd3" audit-id:10f6b409-1065-401f-a180-4209123e00a7,key:/pods/kube-system/helm-install-traefik-crd-m54x6,type:*core.Pod,resource:pods 698ms (03:23:26.856)
Trace[1392240575]:  ---"About to Encode" 275ms (03:23:27.146)
Trace[1392240575]:  ---"Txn call completed" 376ms (03:23:27.522)]
Trace[1392240575]: ---"About to check admission control" 274ms (03:23:27.144)
Trace[1392240575]: ---"Object stored in database" 377ms (03:23:27.522)
Trace[1392240575]: ---"Writing http response done" 32ms (03:23:27.555)
Trace[1392240575]: [818.668921ms] [818.668921ms] END
E0322 03:23:27.588837    2308 available_controller.go:460] v1beta1.metrics.k8s.io failed with: timed out waiting for https://10.42.0.5:10250/apis/metrics.k8s.io/v1beta1
I0322 03:23:27.648313    2308 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:23:27.659378    2308 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="middlewaretcps.traefik.containo.us"
I0322 03:23:27.659749    2308 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="traefikservices.traefik.containo.us"
I0322 03:23:27.659913    2308 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="middlewaretcps.traefik.io"
I0322 03:23:27.660309    2308 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressroutes.traefik.containo.us"
I0322 03:23:27.660473    2308 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="tlsstores.traefik.io"
I0322 03:23:27.660915    2308 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="middlewares.traefik.io"
I0322 03:23:27.663736    2308 shared_informer.go:311] Waiting for caches to sync for resource quota
I0322 03:23:27.667471    2308 trace.go:236] Trace[1550480242]: "List" accept:application/json;as=Table;v=v1;g=meta.k8s.io,application/json;as=Table;v=v1beta1;g=meta.k8s.io,application/json,audit-id:6750cb44-c5a1-4587-834e-cb222f149aa3,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:cluster,url:/api/v1/nodes,user-agent:kubectl/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:23:27.115) (total time: 551ms):
Trace[1550480242]: ---"Writing http response done" count:1 134ms (03:23:27.667)
Trace[1550480242]: [551.524788ms] [551.524788ms] END
I0322 03:23:27.672947    2308 trace.go:236] Trace[830839710]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:97981e0c-45b0-4314-9e0f-a2b9ab3bcdf6,client:127.0.0.1,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:POST (22-Mar-2024 03:23:01.437) (total time: 26235ms):
Trace[830839710]: ---"limitedReadBody succeeded" len:406 1885ms (03:23:03.322)
Trace[830839710]: ---"About to convert to expected version" 2869ms (03:23:06.192)
Trace[830839710]: ["Create etcd3" audit-id:97981e0c-45b0-4314-9e0f-a2b9ab3bcdf6,key:/events/kube-system/metrics-server-67c658944b-8mmfz.17bef80c6130affa,type:*core.Event,resource:events 2392ms (03:23:25.280)
Trace[830839710]:  ---"TransformToStorage succeeded" 2043ms (03:23:27.325)
Trace[830839710]:  ---"Txn call succeeded" 338ms (03:23:27.663)]
Trace[830839710]: [26.235521409s] [26.235521409s] END
I0322 03:23:27.704093    2308 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/coredns-6799fbcd5-tqlcn" containerName="coredns"
I0322 03:23:27.763874    2308 trace.go:236] Trace[1084332611]: "Patch" accept:application/json, */*,audit-id:06ce7dda-b661-4aaf-b07b-c9963a9d38c8,client:127.0.0.1,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events/traefik.17bef7e103c11ddb,user-agent:helm-controller@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:PATCH (22-Mar-2024 03:23:26.740) (total time: 1022ms):
Trace[1084332611]: ["GuaranteedUpdate etcd3" audit-id:06ce7dda-b661-4aaf-b07b-c9963a9d38c8,key:/events/kube-system/traefik.17bef7e103c11ddb,type:*core.Event,resource:events 1018ms (03:23:26.744)
Trace[1084332611]:  ---"initial value restored" 718ms (03:23:27.463)
Trace[1084332611]:  ---"Txn call completed" 298ms (03:23:27.763)]
Trace[1084332611]: ---"Object stored in database" 298ms (03:23:27.763)
Trace[1084332611]: [1.022838339s] [1.022838339s] END
E0322 03:23:27.847054    2308 controller.go:193] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-7zoch227uv4owxg75pgtnmv3jq\": the object has been modified; please apply your changes to the latest version and try again"
I0322 03:23:27.928358    2308 garbagecollector.go:816] "failed to discover some groups" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"
I0322 03:23:27.931037    2308 event.go:307] "Event occurred" object="kube-system/coredns-6799fbcd5-tqlcn" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0322 03:23:27.952887    2308 trace.go:236] Trace[1516942814]: "List" accept:application/json, */*,audit-id:b42ad70a-1fdd-4de4-8646-7eacf579665f,client:10.42.0.2,protocol:HTTP/2.0,resource:secrets,scope:namespace,url:/api/v1/namespaces/kube-system/secrets,user-agent:tiller/v0.0.0 (linux/amd64) kubernetes/$Format,verb:LIST (22-Mar-2024 03:23:27.152) (total time: 800ms):
Trace[1516942814]: ["List(recursive=true) etcd3" audit-id:b42ad70a-1fdd-4de4-8646-7eacf579665f,key:/secrets/kube-system,resourceVersion:,resourceVersionMatch:,limit:0,continue: 800ms (03:23:27.152)]
Trace[1516942814]: [800.819747ms] [800.819747ms] END
time="2024-03-22T03:23:27Z" level=error msg="Remotedialer proxy error" error="websocket: close 1006 (abnormal closure): unexpected EOF"
I0322 03:23:28.346339    2308 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0322 03:23:28.485308    2308 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/metrics-server-67c658944b-8mmfz" containerName="metrics-server"
W0322 03:23:28.494230    2308 handler_proxy.go:93] no RequestInfo found in the context
W0322 03:23:28.599836    2308 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:23:28.610098    2308 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 03:23:28.626056    2308 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E0322 03:23:28.858255    2308 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 03:23:28.858627    2308 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0322 03:23:28.979491    2308 trace.go:236] Trace[1517080714]: "List" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json,audit-id:cf0e4b46-cd75-46aa-8cb6-32c9c646eac6,client:127.0.0.1,protocol:HTTP/2.0,resource:ingressroutes,scope:cluster,url:/apis/traefik.containo.us/v1alpha1/ingressroutes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/metadata-informers,verb:LIST (22-Mar-2024 03:23:28.273) (total time: 704ms):
Trace[1517080714]: ["List(recursive=true) etcd3" audit-id:cf0e4b46-cd75-46aa-8cb6-32c9c646eac6,key:/traefik.containo.us/ingressroutes,resourceVersion:0,resourceVersionMatch:,limit:500,continue: 705ms (03:23:28.273)]
Trace[1517080714]: [704.999267ms] [704.999267ms] END
I0322 03:23:28.980347    2308 trace.go:236] Trace[1736763743]: "List(recursive=true) etcd3" audit-id:,key:/traefik.containo.us/ingressroutes,resourceVersion:,resourceVersionMatch:,limit:10000,continue: (22-Mar-2024 03:23:28.256) (total time: 723ms):
Trace[1736763743]: [723.630891ms] [723.630891ms] END
I0322 03:23:28.987957    2308 trace.go:236] Trace[555331039]: "Patch" accept:application/json, */*,audit-id:a9e8c977-52e1-49c4-a606-b9bbb7c8d674,client:127.0.0.1,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events/traefik-crd.17bef7e103ccc899,user-agent:helm-controller@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:PATCH (22-Mar-2024 03:23:27.897) (total time: 1090ms):
Trace[555331039]: ["GuaranteedUpdate etcd3" audit-id:a9e8c977-52e1-49c4-a606-b9bbb7c8d674,key:/events/kube-system/traefik-crd.17bef7e103ccc899,type:*core.Event,resource:events 1089ms (03:23:27.898)
Trace[555331039]:  ---"initial value restored" 341ms (03:23:28.239)
Trace[555331039]:  ---"Txn call completed" 742ms (03:23:28.982)]
Trace[555331039]: ---"Object stored in database" 744ms (03:23:28.984)
Trace[555331039]: [1.090116583s] [1.090116583s] END
I0322 03:23:28.990805    2308 trace.go:236] Trace[436626592]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:cbc91c0d-21f2-44ff-8428-58765a4acc20,client:127.0.0.1,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/kube-system/pods/local-path-provisioner-6c86858495-mpn68/status,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:node-controller,verb:PUT (22-Mar-2024 03:23:28.023) (total time: 966ms):
Trace[436626592]: ---"limitedReadBody succeeded" len:3464 16ms (03:23:28.040)
Trace[436626592]: ["GuaranteedUpdate etcd3" audit-id:cbc91c0d-21f2-44ff-8428-58765a4acc20,key:/pods/kube-system/local-path-provisioner-6c86858495-mpn68,type:*core.Pod,resource:pods 950ms (03:23:28.040)
Trace[436626592]:  ---"Txn call completed" 869ms (03:23:28.914)]
Trace[436626592]: ---"Write to database call succeeded" len:3464 72ms (03:23:28.990)
Trace[436626592]: [966.680488ms] [966.680488ms] END
I0322 03:23:28.997562    2308 trace.go:236] Trace[1018866798]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:121b2203-1f38-4f43-b31f-0e14a1584751,client:127.0.0.1,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/kube-system/pods/helm-install-traefik-t9wf9/status,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PATCH (22-Mar-2024 03:23:28.195) (total time: 801ms):
Trace[1018866798]: ["GuaranteedUpdate etcd3" audit-id:121b2203-1f38-4f43-b31f-0e14a1584751,key:/pods/kube-system/helm-install-traefik-t9wf9,type:*core.Pod,resource:pods 798ms (03:23:28.199)
Trace[1018866798]:  ---"About to Encode" 141ms (03:23:28.340)
Trace[1018866798]:  ---"Txn call completed" 582ms (03:23:28.923)]
Trace[1018866798]: ---"About to check admission control" 140ms (03:23:28.340)
Trace[1018866798]: ---"Object stored in database" 584ms (03:23:28.924)
Trace[1018866798]: ---"Writing http response done" 72ms (03:23:28.997)
Trace[1018866798]: [801.867367ms] [801.867367ms] END
E0322 03:23:29.013045    2308 available_controller.go:460] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.42.0.5:10250/apis/metrics.k8s.io/v1beta1: Get "https://10.42.0.5:10250/apis/metrics.k8s.io/v1beta1": proxy error from 127.0.0.1:6443 while dialing 10.42.0.5:10250, code 502: 502 Bad Gateway
W0322 03:23:29.080244    2308 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:23:29.081675    2308 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 03:23:29.133933    2308 event.go:307] "Event occurred" object="kube-system/local-path-provisioner-6c86858495-mpn68" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0322 03:23:29.139081    2308 trace.go:236] Trace[276249568]: "Get" accept:application/json, */*,audit-id:701a3b48-be5c-4c51-86f0-98a045c78bfe,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps/coredns,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:GET (22-Mar-2024 03:23:28.427) (total time: 711ms):
Trace[276249568]: ---"About to write a response" 710ms (03:23:29.138)
Trace[276249568]: [711.353798ms] [711.353798ms] END
I0322 03:23:29.207909    2308 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:23:29.251145    2308 trace.go:236] Trace[84672258]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:57e98441-a9db-4e82-9996-03a9e625d0c5,client:127.0.0.1,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:POST (22-Mar-2024 03:23:28.413) (total time: 836ms):
Trace[84672258]: ["Create etcd3" audit-id:57e98441-a9db-4e82-9996-03a9e625d0c5,key:/events/kube-system/coredns-6799fbcd5-tqlcn.17bef826214487c5,type:*core.Event,resource:events 835ms (03:23:28.414)
Trace[84672258]:  ---"Txn call succeeded" 827ms (03:23:29.241)]
Trace[84672258]: [836.026621ms] [836.026621ms] END
I0322 03:23:29.253143    2308 trace.go:236] Trace[629717290]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:3e90e0f6-064a-41ec-b278-13b50ab76fef,client:127.0.0.1,protocol:HTTP/2.0,resource:replicasets,scope:resource,url:/apis/apps/v1/namespaces/kube-system/replicasets/coredns-6799fbcd5/status,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:replicaset-controller,verb:PUT (22-Mar-2024 03:23:28.241) (total time: 1011ms):
Trace[629717290]: ["GuaranteedUpdate etcd3" audit-id:3e90e0f6-064a-41ec-b278-13b50ab76fef,key:/replicasets/kube-system/coredns-6799fbcd5,type:*apps.ReplicaSet,resource:replicasets.apps 926ms (03:23:28.326)
Trace[629717290]:  ---"About to Encode" 98ms (03:23:28.425)
Trace[629717290]:  ---"Txn call completed" 790ms (03:23:29.216)]
Trace[629717290]: ---"Writing http response done" 36ms (03:23:29.253)
Trace[629717290]: [1.011404052s] [1.011404052s] END
I0322 03:23:29.309926    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="1.310763457s"
I0322 03:23:29.404132    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="17.631659ms"
I0322 03:23:29.470166    2308 trace.go:236] Trace[1157719417]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:37a80c50-cc6e-4d3c-9ec0-7c8ff466a3d3,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-7zoch227uv4owxg75pgtnmv3jq,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PUT (22-Mar-2024 03:23:28.432) (total time: 1038ms):
Trace[1157719417]: ["GuaranteedUpdate etcd3" audit-id:37a80c50-cc6e-4d3c-9ec0-7c8ff466a3d3,key:/leases/kube-system/apiserver-7zoch227uv4owxg75pgtnmv3jq,type:*coordination.Lease,resource:leases.coordination.k8s.io 1037ms (03:23:28.432)
Trace[1157719417]:  ---"Txn call completed" 1036ms (03:23:29.469)]
Trace[1157719417]: [1.038054124s] [1.038054124s] END
I0322 03:23:29.477524    2308 trace.go:236] Trace[2081763694]: "List(recursive=true) etcd3" audit-id:,key:/traefik.containo.us/serverstransports,resourceVersion:,resourceVersionMatch:,limit:10000,continue: (22-Mar-2024 03:23:28.487) (total time: 990ms):
Trace[2081763694]: [990.432832ms] [990.432832ms] END
I0322 03:23:29.487723    2308 trace.go:236] Trace[2142788773]: "List" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json,audit-id:ae7215f5-d4a5-4cb2-9ca4-793cf469e3b5,client:127.0.0.1,protocol:HTTP/2.0,resource:serverstransports,scope:cluster,url:/apis/traefik.containo.us/v1alpha1/serverstransports,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/metadata-informers,verb:LIST (22-Mar-2024 03:23:28.436) (total time: 1051ms):
Trace[2142788773]: ["List(recursive=true) etcd3" audit-id:ae7215f5-d4a5-4cb2-9ca4-793cf469e3b5,key:/traefik.containo.us/serverstransports,resourceVersion:0,resourceVersionMatch:,limit:500,continue: 1051ms (03:23:28.436)]
Trace[2142788773]: [1.051673274s] [1.051673274s] END
I0322 03:23:29.580498    2308 trace.go:236] Trace[1463283319]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:48c7ee3a-5203-4a8b-8f2c-648e8b0ddf83,client:127.0.0.1,protocol:HTTP/2.0,resource:services,scope:resource,url:/api/v1/namespaces/kube-system/services/kube-dns,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/,verb:GET (22-Mar-2024 03:23:28.921) (total time: 658ms):
Trace[1463283319]: ---"About to write a response" 658ms (03:23:29.580)
Trace[1463283319]: [658.461197ms] [658.461197ms] END
I0322 03:23:29.655769    2308 trace.go:236] Trace[1157957084]: "Proxy via http_connect protocol over tcp" address:10.42.0.5:10250 (22-Mar-2024 03:23:29.123) (total time: 532ms):
Trace[1157957084]: [532.060481ms] [532.060481ms] END
I0322 03:23:29.716569    2308 trace.go:236] Trace[212639197]: "Proxy via http_connect protocol over tcp" address:10.42.0.5:10250 (22-Mar-2024 03:23:29.120) (total time: 596ms):
Trace[212639197]: [596.0194ms] [596.0194ms] END
I0322 03:23:29.728600    2308 trace.go:236] Trace[919787385]: "Proxy via http_connect protocol over tcp" address:10.42.0.5:10250 (22-Mar-2024 03:23:29.124) (total time: 603ms):
Trace[919787385]: [603.726069ms] [603.726069ms] END
I0322 03:23:29.733921    2308 trace.go:236] Trace[1738727387]: "Proxy via http_connect protocol over tcp" address:10.42.0.5:10250 (22-Mar-2024 03:23:29.125) (total time: 607ms):
Trace[1738727387]: [607.907468ms] [607.907468ms] END
I0322 03:23:29.793381    2308 trace.go:236] Trace[1743011579]: "List" accept:application/json, */*,audit-id:6dfda4b9-ab96-4e58-bf57-2de7a2a70e0a,client:10.42.0.2,protocol:HTTP/2.0,resource:secrets,scope:namespace,url:/api/v1/namespaces/kube-system/secrets,user-agent:Helm/3.12.3,verb:LIST (22-Mar-2024 03:23:29.225) (total time: 568ms):
Trace[1743011579]: ---"Writing http response done" count:1 153ms (03:23:29.793)
Trace[1743011579]: [568.330383ms] [568.330383ms] END
I0322 03:23:29.836094    2308 trace.go:236] Trace[1591931855]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:2e12fb8b-ce11-4c64-9ae0-a6273a584d0d,client:127.0.0.1,protocol:HTTP/2.0,resource:replicasets,scope:resource,url:/apis/apps/v1/namespaces/kube-system/replicasets/local-path-provisioner-6c86858495/status,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:replicaset-controller,verb:PUT (22-Mar-2024 03:23:29.058) (total time: 777ms):
Trace[1591931855]: ["GuaranteedUpdate etcd3" audit-id:2e12fb8b-ce11-4c64-9ae0-a6273a584d0d,key:/replicasets/kube-system/local-path-provisioner-6c86858495,type:*apps.ReplicaSet,resource:replicasets.apps 771ms (03:23:29.064)
Trace[1591931855]:  ---"About to Encode" 221ms (03:23:29.286)
Trace[1591931855]:  ---"Txn call completed" 522ms (03:23:29.809)]
Trace[1591931855]: [777.712157ms] [777.712157ms] END
I0322 03:23:29.878374    2308 trace.go:236] Trace[1652612705]: "Proxy via http_connect protocol over tcp" address:10.42.0.5:10250 (22-Mar-2024 03:23:29.126) (total time: 751ms):
Trace[1652612705]: [751.646573ms] [751.646573ms] END
I0322 03:23:29.899722    2308 trace.go:236] Trace[270181711]: "Proxy via http_connect protocol over tcp" address:10.42.0.5:10250 (22-Mar-2024 03:23:29.084) (total time: 814ms):
Trace[270181711]: [814.730092ms] [814.730092ms] END
I0322 03:23:29.999161    2308 trace.go:236] Trace[1176426850]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:00e342fa-c639-44ff-9eb5-89dec02c95ed,client:127.0.0.1,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/kube-system/pods/metrics-server-67c658944b-8mmfz/status,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:node-controller,verb:PUT (22-Mar-2024 03:23:29.316) (total time: 682ms):
Trace[1176426850]: ---"limitedReadBody succeeded" len:4311 18ms (03:23:29.335)
Trace[1176426850]: ["GuaranteedUpdate etcd3" audit-id:00e342fa-c639-44ff-9eb5-89dec02c95ed,key:/pods/kube-system/metrics-server-67c658944b-8mmfz,type:*core.Pod,resource:pods 663ms (03:23:29.336)
Trace[1176426850]:  ---"Txn call completed" 641ms (03:23:29.987)]
Trace[1176426850]: [682.526211ms] [682.526211ms] END
I0322 03:23:29.999335    2308 trace.go:236] Trace[1782099071]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:5a2b63aa-4aba-4c61-a7e0-73fe21572a32,client:127.0.0.1,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/kube-system/pods/coredns-6799fbcd5-tqlcn,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:23:29.314) (total time: 684ms):
Trace[1782099071]: ---"About to write a response" 673ms (03:23:29.988)
Trace[1782099071]: [684.440744ms] [684.440744ms] END
E0322 03:23:30.085326    2308 cadvisor_stats_provider.go:444] "Partial failure issuing cadvisor.ContainerInfoV2" err="partial failures: [\"/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod24c650bf_5c3c_460f_96f8_53aaa5b4f5a7.slice/cri-containerd-137ac679f6b3f90faab1b0a1c2c2a9a8106f85765376a0e56dfd6b86bfe9bece.scope\": RecentStats: unable to find data in memory cache]"
W0322 03:23:30.088547    2308 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:23:30.092548    2308 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 03:23:30.092814    2308 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0322 03:23:30.179231    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="1.143609375s"
I0322 03:23:30.179882    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="435.799s"
I0322 03:23:30.212781    2308 node_lifecycle_controller.go:1029] "Controller detected that all Nodes are not-Ready. Entering master disruption mode"
I0322 03:23:30.228630    2308 event.go:307] "Event occurred" object="kube-system/metrics-server-67c658944b-8mmfz" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0322 03:23:30.274985    2308 trace.go:236] Trace[882266244]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:762d9118-8559-46c1-8ce3-cade23898652,client:127.0.0.1,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:POST (22-Mar-2024 03:23:29.576) (total time: 698ms):
Trace[882266244]: ["Create etcd3" audit-id:762d9118-8559-46c1-8ce3-cade23898652,key:/events/kube-system/metrics-server-67c658944b-8mmfz.17bef827646674c0,type:*core.Event,resource:events 697ms (03:23:29.577)
Trace[882266244]:  ---"Txn call succeeded" 641ms (03:23:30.218)]
Trace[882266244]: ---"Write to database call succeeded" len:404 34ms (03:23:30.256)
Trace[882266244]: ---"Writing http response done" 18ms (03:23:30.274)
Trace[882266244]: [698.797686ms] [698.797686ms] END
I0322 03:23:30.300409    2308 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
W0322 03:23:30.301143    2308 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:23:30.305497    2308 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 03:23:30.305589    2308 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0322 03:23:30.313161    2308 trace.go:236] Trace[2061696856]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:30197063-f85a-4ed7-b7d6-c92fecb459a3,client:127.0.0.1,protocol:HTTP/2.0,resource:jobs,scope:resource,url:/apis/batch/v1/namespaces/kube-system/jobs/helm-install-traefik-crd/status,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:job-controller,verb:PUT (22-Mar-2024 03:23:29.299) (total time: 1014ms):
Trace[2061696856]: ---"limitedReadBody succeeded" len:6701 17ms (03:23:29.317)
Trace[2061696856]: ["GuaranteedUpdate etcd3" audit-id:30197063-f85a-4ed7-b7d6-c92fecb459a3,key:/jobs/kube-system/helm-install-traefik-crd,type:*batch.Job,resource:jobs.batch 994ms (03:23:29.318)
Trace[2061696856]:  ---"Txn call completed" 908ms (03:23:30.243)]
Trace[2061696856]: ---"Write to database call succeeded" len:6701 63ms (03:23:30.306)
Trace[2061696856]: [1.01400374s] [1.01400374s] END
E0322 03:23:30.481459    2308 available_controller.go:460] v1beta1.metrics.k8s.io failed with: Operation cannot be fulfilled on apiservices.apiregistration.k8s.io "v1beta1.metrics.k8s.io": the object has been modified; please apply your changes to the latest version and try again
I0322 03:23:30.552687    2308 trace.go:236] Trace[973535080]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:d639eef8-eeb6-4d19-9fbd-dd995636d488,client:127.0.0.1,protocol:HTTP/2.0,resource:deployments,scope:resource,url:/apis/apps/v1/namespaces/kube-system/deployments/coredns/status,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:deployment-controller,verb:PUT (22-Mar-2024 03:23:29.615) (total time: 936ms):
Trace[973535080]: ["GuaranteedUpdate etcd3" audit-id:d639eef8-eeb6-4d19-9fbd-dd995636d488,key:/deployments/kube-system/coredns,type:*apps.Deployment,resource:deployments.apps 923ms (03:23:29.628)
Trace[973535080]:  ---"About to Encode" 370ms (03:23:29.999)
Trace[973535080]:  ---"Txn call completed" 510ms (03:23:30.509)]
Trace[973535080]: ---"Writing http response done" 42ms (03:23:30.552)
Trace[973535080]: [936.876685ms] [936.876685ms] END
I0322 03:23:30.576278    2308 trace.go:236] Trace[2089909908]: "Patch" accept:application/vnd.kubernetes.protobuf, */*,audit-id:aeed1829-5dc3-4189-b3d4-4d4a9bfebb05,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/server,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:node-controller,verb:PATCH (22-Mar-2024 03:23:29.559) (total time: 1016ms):
Trace[2089909908]: ["GuaranteedUpdate etcd3" audit-id:aeed1829-5dc3-4189-b3d4-4d4a9bfebb05,key:/minions/server,type:*core.Node,resource:nodes 969ms (03:23:29.606)
Trace[2089909908]:  ---"About to Encode" 477ms (03:23:30.087)
Trace[2089909908]:  ---"Txn call completed" 452ms (03:23:30.539)]
Trace[2089909908]: ---"About to check admission control" 339ms (03:23:29.949)
Trace[2089909908]: ---"Object stored in database" 625ms (03:23:30.574)
Trace[2089909908]: [1.016947304s] [1.016947304s] END
I0322 03:23:30.680543    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="554.855828ms"
I0322 03:23:30.680631    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="46.254s"
I0322 03:23:30.700458    2308 trace.go:236] Trace[1614752912]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.56.110,type:*v1.Endpoints,resource:apiServerIPInfo (22-Mar-2024 03:23:29.022) (total time: 1677ms):
Trace[1614752912]: ---"initial value restored" 636ms (03:23:29.658)
Trace[1614752912]: ---"Transaction prepared" 429ms (03:23:30.089)
Trace[1614752912]: ---"Txn call completed" 610ms (03:23:30.700)
Trace[1614752912]: [1.677878783s] [1.677878783s] END
I0322 03:23:30.997506    2308 trace.go:236] Trace[761584202]: "Create" accept:application/vnd.kubernetes.protobuf, */*,audit-id:c4acf077-bfba-4c3f-8eb9-b35cecf283b8,client:127.0.0.1,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:node-controller,verb:POST (22-Mar-2024 03:23:30.495) (total time: 502ms):
Trace[761584202]: [502.086471ms] [502.086471ms] END
I0322 03:23:31.038365    2308 trace.go:236] Trace[1016510665]: "Proxy via http_connect protocol over tcp" address:10.42.0.5:10250 (22-Mar-2024 03:23:30.498) (total time: 539ms):
Trace[1016510665]: [539.999877ms] [539.999877ms] END
I0322 03:23:31.055160    2308 trace.go:236] Trace[791073659]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:58ea1fa2-0b3b-413a-86ad-7ff3dca4a609,client:127.0.0.1,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events/coredns-6799fbcd5-tqlcn.17bef82617ac193a,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PATCH (22-Mar-2024 03:23:30.508) (total time: 547ms):
Trace[791073659]: ["GuaranteedUpdate etcd3" audit-id:58ea1fa2-0b3b-413a-86ad-7ff3dca4a609,key:/events/kube-system/coredns-6799fbcd5-tqlcn.17bef82617ac193a,type:*core.Event,resource:events 543ms (03:23:30.511)
Trace[791073659]:  ---"initial value restored" 380ms (03:23:30.891)
Trace[791073659]:  ---"Txn call completed" 160ms (03:23:31.053)]
Trace[791073659]: ---"Object stored in database" 162ms (03:23:31.054)
Trace[791073659]: [547.075382ms] [547.075382ms] END
I0322 03:23:31.088580    2308 trace.go:236] Trace[1823480342]: "Patch" accept:application/vnd.kubernetes.protobuf, */*,audit-id:9869eb8e-74cc-4327-ae3f-dd87f4fd988f,client:127.0.0.1,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/kube-system/pods/helm-install-traefik-crd-m54x6,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:job-controller,verb:PATCH (22-Mar-2024 03:23:30.509) (total time: 578ms):
Trace[1823480342]: ---"limitedReadBody succeeded" len:89 21ms (03:23:30.530)
Trace[1823480342]: ["GuaranteedUpdate etcd3" audit-id:9869eb8e-74cc-4327-ae3f-dd87f4fd988f,key:/pods/kube-system/helm-install-traefik-crd-m54x6,type:*core.Pod,resource:pods 557ms (03:23:30.530)
Trace[1823480342]:  ---"About to Encode" 107ms (03:23:30.638)
Trace[1823480342]:  ---"Txn call completed" 432ms (03:23:31.071)]
Trace[1823480342]: ---"Object stored in database" 536ms (03:23:31.071)
Trace[1823480342]: ---"Writing http response done" 16ms (03:23:31.088)
Trace[1823480342]: [578.732159ms] [578.732159ms] END
I0322 03:23:31.090847    2308 trace.go:236] Trace[2103364313]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:6c6af922-a80a-4751-82d0-0a03fcc82f08,client:127.0.0.1,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/kube-system/pods/coredns-6799fbcd5-tqlcn/status,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PATCH (22-Mar-2024 03:23:30.322) (total time: 766ms):
Trace[2103364313]: ["GuaranteedUpdate etcd3" audit-id:6c6af922-a80a-4751-82d0-0a03fcc82f08,key:/pods/kube-system/coredns-6799fbcd5-tqlcn,type:*core.Pod,resource:pods 756ms (03:23:30.332)
Trace[2103364313]:  ---"About to Encode" 146ms (03:23:30.482)
Trace[2103364313]:  ---"Txn call completed" 579ms (03:23:31.072)]
Trace[2103364313]: ---"Object stored in database" 732ms (03:23:31.072)
Trace[2103364313]: ---"Writing http response done" 17ms (03:23:31.089)
Trace[2103364313]: [766.958914ms] [766.958914ms] END
I0322 03:23:31.177754    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="98.096s"
I0322 03:23:31.205838    2308 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:23:31.416429    2308 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:23:31.437620    2308 event.go:307] "Event occurred" object="kube-system/helm-install-traefik-crd" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="Completed" message="Job completed"
I0322 03:23:31.569403    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="60.805s"
I0322 03:23:32.086835    2308 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
E0322 03:23:32.218640    2308 available_controller.go:460] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.42.0.5:10250/apis/metrics.k8s.io/v1beta1: bad status from https://10.42.0.5:10250/apis/metrics.k8s.io/v1beta1: 403
W0322 03:23:32.242507    2308 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:23:32.244280    2308 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
E0322 03:23:32.245230    2308 handler_proxy.go:137] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
I0322 03:23:32.695247    2308 shared_informer.go:318] Caches are synced for resource quota
I0322 03:23:32.749925    2308 shared_informer.go:318] Caches are synced for garbage collector
W0322 03:23:32.761482    2308 handler_proxy.go:93] no RequestInfo found in the context
time="2024-03-22T03:23:32Z" level=info msg="certificate CN=serverworker signed by CN=k3s-server-ca@1711077444: notBefore=2024-03-22 03:17:24 +0000 UTC notAfter=2025-03-22 03:23:32 +0000 UTC"
E0322 03:23:32.762413    2308 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
E0322 03:23:32.762503    2308 handler_proxy.go:137] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
time="2024-03-22T03:23:32Z" level=info msg="Connecting to proxy" url="wss://192.168.56.110:6443/v1-k3s/connect"
I0322 03:23:32.986557    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="297.301417ms"
I0322 03:23:32.986788    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="180.357s"
I0322 03:23:33.110061    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="144.525s"
time="2024-03-22T03:23:33Z" level=info msg="Handling backend connection request [server]"
W0322 03:23:33.296115    2308 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:23:33.298117    2308 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 03:23:33.298136    2308 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0322 03:23:33.298373    2308 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:23:33.300353    2308 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 03:23:33.300376    2308 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0322 03:23:33.539129    2308 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
time="2024-03-22T03:23:33Z" level=info msg="certificate CN=system:node:serverworker,O=system:nodes signed by CN=k3s-client-ca@1711077444: notBefore=2024-03-22 03:17:24 +0000 UTC notAfter=2025-03-22 03:23:33 +0000 UTC"
W0322 03:23:34.030925    2308 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:23:34.035517    2308 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 03:23:34.038311    2308 trace.go:236] Trace[1668130291]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:3ddff8d2-ec93-4fbf-a16f-abe5cab3c465,client:127.0.0.1,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events/metrics-server-67c658944b-8mmfz.17bef82954f12c5c,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PATCH (22-Mar-2024 03:23:33.391) (total time: 644ms):
Trace[1668130291]: ["GuaranteedUpdate etcd3" audit-id:3ddff8d2-ec93-4fbf-a16f-abe5cab3c465,key:/events/kube-system/metrics-server-67c658944b-8mmfz.17bef82954f12c5c,type:*core.Event,resource:events 642ms (03:23:33.393)
Trace[1668130291]:  ---"initial value restored" 300ms (03:23:33.694)
Trace[1668130291]:  ---"Txn call completed" 299ms (03:23:34.034)]
Trace[1668130291]: ---"Object stored in database" 330ms (03:23:34.035)
Trace[1668130291]: [644.710626ms] [644.710626ms] END
E0322 03:23:34.038468    2308 handler_proxy.go:137] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
I0322 03:23:34.080027    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="214.911386ms"
I0322 03:23:34.080178    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="76.775s"
W0322 03:23:35.066110    2308 handler_proxy.go:93] no RequestInfo found in the context
W0322 03:23:35.079994    2308 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:23:35.084344    2308 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 03:23:35.084421    2308 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E0322 03:23:35.090064    2308 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 03:23:35.090207    2308 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0322 03:23:35.287060    2308 node_lifecycle_controller.go:1048] "Controller detected that some Nodes are Ready. Exiting master disruption mode"
time="2024-03-22T03:23:36Z" level=info msg="Handling backend connection request [serverworker]"
I0322 03:23:37.217429    2308 actual_state_of_world.go:547] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"serverworker\" does not exist"
I0322 03:23:37.376058    2308 node_controller.go:431] Initializing node serverworker with cloud provider
E0322 03:23:37.381055    2308 node_controller.go:240] error syncing 'serverworker': failed to get instance metadata for node serverworker: address annotations not yet set, requeuing
I0322 03:23:37.425430    2308 node_controller.go:431] Initializing node serverworker with cloud provider
E0322 03:23:37.425799    2308 node_controller.go:240] error syncing 'serverworker': failed to get instance metadata for node serverworker: address annotations not yet set, requeuing
I0322 03:23:37.451120    2308 node_controller.go:431] Initializing node serverworker with cloud provider
E0322 03:23:37.451590    2308 node_controller.go:240] error syncing 'serverworker': failed to get instance metadata for node serverworker: address annotations not yet set, requeuing
I0322 03:23:37.509704    2308 node_controller.go:431] Initializing node serverworker with cloud provider
E0322 03:23:37.510848    2308 node_controller.go:240] error syncing 'serverworker': failed to get instance metadata for node serverworker: address annotations not yet set, requeuing
I0322 03:23:37.573987    2308 node_controller.go:431] Initializing node serverworker with cloud provider
E0322 03:23:37.574282    2308 node_controller.go:240] error syncing 'serverworker': failed to get instance metadata for node serverworker: address annotations not yet set, requeuing
I0322 03:23:37.784862    2308 node_controller.go:431] Initializing node serverworker with cloud provider
I0322 03:23:39.338260    2308 trace.go:236] Trace[872012227]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:7ce750f3-205c-44e5-975f-ae79af22da6e,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:23:36.918) (total time: 2411ms):
Trace[872012227]: ---"About to write a response" 543ms (03:23:37.462)
Trace[872012227]: ---"Writing http response done" 1866ms (03:23:39.328)
Trace[872012227]: [2.411014475s] [2.411014475s] END
E0322 03:23:40.521659    2308 node_controller.go:240] error syncing 'serverworker': failed to get instance metadata for node serverworker: address annotations not yet set, requeuing
time="2024-03-22T03:23:40Z" level=info msg="Slow SQL (started: 2024-03-22 03:23:38.829886389 +0000 UTC m=+374.524699596) (total time: 1.36952323s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC LIMIT 500 : [[% 668]]"
I0322 03:23:41.277110    2308 event.go:307] "Event occurred" object="serverworker" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node serverworker event: Registered Node serverworker in Controller"
I0322 03:23:41.445434    2308 node_controller.go:431] Initializing node serverworker with cloud provider
I0322 03:23:41.212603    2308 node_lifecycle_controller.go:877] "Missing timestamp for Node. Assuming now as a timestamp" node="serverworker"
I0322 03:23:42.109495    2308 trace.go:236] Trace[1876515788]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:5583d3ed-bbd7-492f-b471-48d902d9e44d,client:192.168.56.111,protocol:HTTP/2.0,resource:csinodes,scope:resource,url:/apis/storage.k8s.io/v1/csinodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:23:36.880) (total time: 5222ms):
Trace[1876515788]: [5.222714671s] [5.222714671s] END
E0322 03:23:42.596007    2308 node_controller.go:240] error syncing 'serverworker': failed to get instance metadata for node serverworker: address annotations not yet set, requeuing
I0322 03:23:42.608422    2308 node_controller.go:431] Initializing node serverworker with cloud provider
W0322 03:23:59.527471    2308 transport.go:301] Unable to cancel request for *otelhttp.Transport
E0322 03:23:52.627411    2308 node_controller.go:240] error syncing 'serverworker': failed to get instance metadata for node serverworker: address annotations not yet set, requeuing
E0322 03:24:01.105778    2308 health_controller.go:162] Metrics Controller heartbeat missed
time="2024-03-22T03:24:08Z" level=info msg="Slow SQL (started: 2024-03-22 03:23:45.396006842 +0000 UTC m=+381.090820042) (total time: 8.021012467s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC LIMIT 500 : [[% 668]]"
W0322 03:23:55.122480    2308 transport.go:301] Unable to cancel request for *otelhttp.Transport
time="2024-03-22T03:24:12Z" level=info msg="Slow SQL (started: 2024-03-22 03:23:47.035356387 +0000 UTC m=+382.730169585) (total time: 9.287585841s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC LIMIT 1 : [[/registry/masterleases/192.168.56.110 true]]"
E0322 03:24:14.524960    2308 health_controller.go:162] Metrics Controller heartbeat missed
I0322 03:24:54.388291    2308 node_controller.go:431] Initializing node serverworker with cloud provider
I0322 03:24:54.533240    2308 trace.go:236] Trace[936718678]: "Calculate volume metrics of config-volume for pod kube-system/local-path-provisioner-6c86858495-mpn68" (22-Mar-2024 03:24:24.656) (total time: 29871ms):
Trace[936718678]: [29.871491192s] [29.871491192s] END
W0322 03:24:54.669035    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:54.676790    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:54.903724    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PersistentVolume ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I0322 03:24:54.944642    2308 request.go:697] Waited for 29.328273528s due to client-side throttling, not priority and fairness, request: GET:https://127.0.0.1:6444/api
E0322 03:24:55.042282    2308 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:24:55.046406    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
W0322 03:24:55.163958    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ServiceAccount ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.168791    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.169417    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.197036    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.HelmChartConfig ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.197912    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CSINode ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:24:55.216382    2308 kubelet_node_status.go:540] "Error updating node status, will retry" err="error getting node \"server\": Get \"https://127.0.0.1:6443/api/v1/nodes/server?resourceVersion=0&timeout=10s\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)"
W0322 03:24:55.216801    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Secret ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.444425    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Namespace ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.468444    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.596509    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1beta3.FlowSchema ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.598087    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Lease ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.606465    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.HelmChart ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.607259    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Job ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.607864    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.DaemonSet ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.610827    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.611200    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.LimitRange ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.616812    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ClusterRole ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.617507    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CustomResourceDefinition ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.617823    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.618075    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.621539    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.636400    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.700075    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ClusterRoleBinding ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.701468    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.703600    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.706682    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.707078    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.EndpointSlice ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.708934    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Addon ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.710087    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Role ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.710791    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ValidatingWebhookConfiguration ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.711987    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ServiceAccount ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.712789    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.RoleBinding ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.713605    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1beta3.PriorityLevelConfiguration ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.714399    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.714873    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Secret ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.716171    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Endpoints ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.718521    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.RoleBinding ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.719024    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.719270    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ClusterRole ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.719915    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.NetworkPolicy ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.720314    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.721926    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ResourceQuota ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.722604    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.723217    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Job ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.733132    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CSIDriver ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.734143    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Lease ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.735042    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.735377    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Role ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.735695    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.StorageClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.736057    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.736564    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.737479    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PriorityClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.737922    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ReplicaSet ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.738227    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CronJob ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.738618    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CertificateSigningRequest ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.739541    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.740046    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v2.HorizontalPodAutoscaler ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.740930    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1beta3.FlowSchema ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.741955    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.742370    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.742752    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.743398    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.IngressClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.743160    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.MutatingWebhookConfiguration ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.744006    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.RuntimeClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.744201    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.IngressClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.744379    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ClusterRoleBinding ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.744550    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.StorageClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.744725    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.744908    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ResourceQuota ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.745089    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Lease ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.745309    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Endpoints ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.745794    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ReplicationController ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.746029    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1beta3.PriorityLevelConfiguration ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.746314    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.746516    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.746822    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.747196    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Ingress ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.747450    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PodDisruptionBudget ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.747984    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.LimitRange ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.748481    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.748900    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.749371    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.749607    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.750038    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ClusterRoleBinding ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.750674    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.751312    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.VolumeAttachment ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.751851    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.752198    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ServiceAccount ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.752475    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ControllerRevision ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.753113    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.StatefulSet ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.753461    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PersistentVolumeClaim ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.753691    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.754125    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Deployment ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.754418    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.MutatingWebhookConfiguration ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.754769    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CSIStorageCapacity ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.755488    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.755862    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PersistentVolume ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.758933    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Secret ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.759591    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.EndpointSlice ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.764187    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.764700    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.765033    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.765291    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.765664    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.RuntimeClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.766191    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.DaemonSet ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.766509    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ValidatingWebhookConfiguration ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.766707    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Namespace ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.766896    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.777302    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.745530    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PriorityClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.745619    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.VolumeAttachment ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.777935    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.778228    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.778504    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PodTemplate ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.782177    2308 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:24:55.790588    2308 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 03:24:55.790791    2308 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0322 03:24:55.811632    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CSIDriver ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.873736    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.877286    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I0322 03:24:55.923116    2308 trace.go:236] Trace[1453718975]: "iptables ChainExists" (22-Mar-2024 03:24:31.780) (total time: 24141ms):
Trace[1453718975]: [24.141018018s] [24.141018018s] END
E0322 03:24:55.947534    2308 finisher.go:175] FinishRequest: post-timeout activity - time-elapsed: 23.969857621s, panicked: false, err: context deadline exceeded, panic-reason: <nil>
W0322 03:24:55.980762    2308 reflector.go:458] object-"kube-system"/"kube-root-ca.crt": watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:55.981914    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.EndpointSlice ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I0322 03:24:55.983182    2308 trace.go:236] Trace[1063088600]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.56.110,type:*v1.Endpoints,resource:apiServerIPInfo (22-Mar-2024 03:23:39.675) (total time: 76307ms):
Trace[1063088600]: [1m16.307520921s] [1m16.307520921s] END
E0322 03:24:55.986025    2308 controller.go:164] unable to sync kubernetes service: rpc error: code = Unavailable desc = keepalive ping failed to receive ACK within timeout
W0322 03:24:55.988183    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.APIService ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:24:56.002573    2308 handler_proxy.go:137] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
W0322 03:24:56.018598    2308 reflector.go:458] object-"kube-system"/"local-path-config": watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:24:56.024744    2308 ttl_controller.go:226] Patch "https://127.0.0.1:6444/api/v1/nodes/serverworker": http2: client connection lost
I0322 03:24:56.026281    2308 log.go:245] http: TLS handshake error from 10.42.0.5:35450: EOF
W0322 03:24:56.028288    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:56.045093    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:24:56.048842    2308 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:24:56.050053    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:24:56.050665    2308 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 03:24:56.051636    2308 trace.go:236] Trace[1618191578]: "SerializeObject" audit-id:11b3735a-1448-41f9-b4a1-58423db6add0,method:POST,url:/api/v1/namespaces/default/events,protocol:HTTP/2.0,mediaType:application/vnd.kubernetes.protobuf,encoder:{"encodeGV":"v1","encoder":"protobuf","name":"versioning"} (22-Mar-2024 03:23:44.344) (total time: 71706ms):
Trace[1618191578]: ---"About to start writing response" size:576 71703ms (03:24:56.048)
Trace[1618191578]: [1m11.70682773s] [1m11.70682773s] END
I0322 03:24:56.052213    2308 trace.go:236] Trace[1071217335]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:11b3735a-1448-41f9-b4a1-58423db6add0,client:192.168.56.111,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/default/events,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:POST (22-Mar-2024 03:23:36.955) (total time: 79095ms):
Trace[1071217335]: ["Create etcd3" audit-id:11b3735a-1448-41f9-b4a1-58423db6add0,key:/events/default/serverworker.17bef83599be4773,type:*core.Event,resource:events 78978ms (03:23:37.073)
Trace[1071217335]:  ---"Txn call succeeded" 3593ms (03:23:40.700)
Trace[1071217335]:  ---"decode succeeded" len:573 970ms (03:23:41.671)]
Trace[1071217335]: ---"Write to database call succeeded" len:266 275ms (03:23:41.946)
Trace[1071217335]: ---"About to write a response" 464ms (03:23:42.410)
Trace[1071217335]: ---"Writing http response done" 73641ms (03:24:56.051)
Trace[1071217335]: [1m19.095995981s] [1m19.095995981s] END
E0322 03:24:45.779214    2308 health_controller.go:162] Metrics Controller heartbeat missed
E0322 03:24:56.070439    2308 health_controller.go:162] Metrics Controller heartbeat missed
E0322 03:24:56.070671    2308 health_controller.go:162] Metrics Controller heartbeat missed
W0322 03:24:56.081949    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.RuntimeClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:24:56.086253    2308 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, context deadline exceeded]"
E0322 03:24:56.088155    2308 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:24:56.088304    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:24:56.090641    2308 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 03:24:56.092213    2308 timeout.go:142] post-timeout activity - time-elapsed: 146.191102ms, PATCH "/api/v1/nodes/serverworker" result: <nil>
W0322 03:24:56.100558    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:24:47.794323    2308 controller.go:193] "Failed to update lease" err="Put \"https://127.0.0.1:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/server?timeout=10s\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)"
W0322 03:24:56.107041    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:56.113276    2308 reflector.go:458] object-"kube-system"/"coredns-custom": watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:56.114848    2308 reflector.go:458] object-"kube-system"/"chart-values-traefik": watch of *v1.Secret ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:56.115145    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CSIStorageCapacity ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:56.115766    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:56.115967    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PersistentVolume ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:56.116097    2308 handler_proxy.go:93] no RequestInfo found in the context
W0322 03:24:56.116854    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:56.114260    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ReplicaSet ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:56.117421    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:56.117642    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.StorageClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:56.117881    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CSIDriver ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:56.118092    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PodDisruptionBudget ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:56.118351    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.StatefulSet ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:56.118758    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CSINode ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:56.119142    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ReplicationController ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:56.119385    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:24:56.120507    2308 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 03:24:56.120598    2308 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E0322 03:24:56.123478    2308 event.go:289] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"server.17bef82d64d2e9c9", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"server", UID:"server", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"ContainerGCFailed", Message:"rpc error: code = DeadlineExceeded desc = context deadline exceeded", Source:v1.EventSource{Component:"kubelet", Host:"server"}, FirstTimestamp:time.Date(2024, time.March, 22, 3, 23, 2, 296127945, time.Local), LastTimestamp:time.Date(2024, time.March, 22, 3, 23, 2, 296127945, time.Local), Count:1, Type:"Warning", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"kubelet", ReportingInstance:"server"}': 'Post "https://127.0.0.1:6443/api/v1/namespaces/default/events": http2: client connection lost'(may retry after sleeping)
W0322 03:24:56.115036    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Namespace ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:56.127650    2308 reflector.go:458] object-"kube-system"/"chart-content-traefik": watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:56.127835    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:56.135115    2308 reflector.go:458] object-"kube-system"/"coredns": watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:56.136922    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:24:56.168018    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PersistentVolumeClaim ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:24:56.184566    2308 controller_utils.go:203] unable to taint [&Taint{Key:node.kubernetes.io/not-ready,Value:,Effect:NoExecute,TimeAdded:2024-03-22 03:23:47.330088231 +0000 UTC m=+383.024901426,}] unresponsive Node "serverworker": Get "https://127.0.0.1:6444/api/v1/nodes/serverworker?resourceVersion=0": http2: client connection lost
E0322 03:24:56.215304    2308 node_controller.go:240] error syncing 'serverworker': failed to get instance metadata for node serverworker: address annotations not yet set, requeuing
W0322 03:24:56.261501    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Namespace ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:24:54.642682    2308 event.go:289] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"serverworker.17bef836664bdd6b", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"serverworker", UID:"321ef136-363d-4561-9f3c-232a328b81e2", APIVersion:"v1", ResourceVersion:"", FieldPath:""}, Reason:"RegisteredNode", Message:"Node serverworker event: Registered Node serverworker in Controller", Source:v1.EventSource{Component:"node-controller", Host:""}, FirstTimestamp:time.Date(2024, time.March, 22, 3, 23, 40, 975537515, time.Local), LastTimestamp:time.Date(2024, time.March, 22, 3, 23, 40, 975537515, time.Local), Count:1, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"node-controller", ReportingInstance:""}': 'Post "https://127.0.0.1:6444/api/v1/namespaces/default/events": http2: client connection lost'(may retry after sleeping)
I0322 03:24:56.325978    2308 trace.go:236] Trace[1396255479]: "DeltaFIFO Pop Process" ID:v1.helm.cattle.io,Depth:25,Reason:slow event handlers blocking the queue (22-Mar-2024 03:24:08.002) (total time: 9136ms):
Trace[1396255479]: [9.136412411s] [9.136412411s] END
E0322 03:24:57.270764    2308 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:24:57.276371    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:24:57.278272    2308 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 03:24:57.281189    2308 trace.go:236] Trace[829224855]: "Patch" accept:application/vnd.kubernetes.protobuf, */*,audit-id:6a32c155-1322-448d-a038-c4389a24fa93,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:node-controller,verb:PATCH (22-Mar-2024 03:23:59.976) (total time: 57302ms):
Trace[829224855]: ---"limitedReadBody succeeded" len:63 54819ms (03:24:54.795)
Trace[829224855]: [57.302599417s] [57.302599417s] END
E0322 03:24:57.416513    2308 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:24:57.428738    2308 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:24:57.429164    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:24:57.430193    2308 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 03:24:57.432246    2308 trace.go:236] Trace[489891729]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:6d2ac3bb-17e8-4c58-b2de-309044efbb5c,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-7zoch227uv4owxg75pgtnmv3jq,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PUT (22-Mar-2024 03:24:56.480) (total time: 949ms):
Trace[489891729]: ---"Write to database call failed" len:590,err:Timeout: request did not complete within requested timeout - context deadline exceeded 939ms (03:24:57.428)
Trace[489891729]: [949.501592ms] [949.501592ms] END
E0322 03:24:57.435247    2308 timeout.go:142] post-timeout activity - time-elapsed: 2.988953602s, PUT "/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-7zoch227uv4owxg75pgtnmv3jq" result: <nil>
time="2024-03-22T03:24:56Z" level=info msg="Slow SQL (started: 2024-03-22 03:24:09.905695813 +0000 UTC m=+405.600509012) (total time: 46.433423397s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC  : [[/registry/masterleases/192.168.56.110 false]]"
W0322 03:24:56.387305    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I0322 03:24:57.482819    2308 trace.go:236] Trace[1245080802]: "DeltaFIFO Pop Process" ID:v2.autoscaling,Depth:23,Reason:slow event handlers blocking the queue (22-Mar-2024 03:24:57.235) (total time: 246ms):
Trace[1245080802]: [246.980402ms] [246.980402ms] END
I0322 03:24:57.504229    2308 node_controller.go:431] Initializing node serverworker with cloud provider
E0322 03:24:57.519029    2308 timeout.go:142] post-timeout activity - time-elapsed: 1.57744144s, PATCH "/api/v1/nodes/serverworker" result: <nil>
I0322 03:24:57.595167    2308 scope.go:117] "RemoveContainer" containerID="99f14d58a0ef229261a3734019bc51448bd530d4cf41690d804f452d725e4532"
time="2024-03-22T03:24:56Z" level=warning msg="Unable to fetch coredns config map: Get \"https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps/coredns\": http2: client connection lost"
I0322 03:24:56.389833    2308 trace.go:236] Trace[418277783]: "iptables ChainExists" (22-Mar-2024 03:24:02.715) (total time: 53674ms):
Trace[418277783]: [53.674467585s] [53.674467585s] END
W0322 03:24:56.390114    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
time="2024-03-22T03:24:56Z" level=info msg="Slow SQL (started: 2024-03-22 03:24:55.169317188 +0000 UTC m=+450.864130398) (total time: 1.303716545s): INSERT INTO kine(name, created, deleted, create_revision, prev_revision, lease, value, old_value) values(?, ?, ?, ?, ?, ?, ?, ?) : [[/registry/masterleases/192.168.56.110 0 1 597 629 15 [107 56 115 0 10 15 10 2 118 49 18 9 69 110 100 112 111 105 110 116 115 18 40 10 16 10 0 18 0 26 0 34 0 42 0 50 0 56 2 66 0 18 20 10 18 10 14 49 57 50 46 49 54 56 46 53 54 46 49 49 48 26 0 26 0 34 0] [107 56 115 0 10 15 10 2 118 49 18 9 69 110 100 112 111 105 110 116 115 18 40 10 16 10 0 18 0 26 0 34 0 42 0 50 0 56 2 66 0 18 20 10 18 10 14 49 57 50 46 49 54 56 46 53 54 46 49 49 48 26 0 26 0 34 0]]]"
time="2024-03-22T03:24:57Z" level=error msg="error while range on /registry/ingress/ /registry/ingress/: context canceled"
E0322 03:24:56.426638    2308 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 03:24:57.682289    2308 trace.go:236] Trace[664194175]: "SerializeObject" audit-id:93c240d6-ffb4-4020-919f-b6911c5a7157,method:GET,url:/api/v1/nodes/serverworker,protocol:HTTP/2.0,mediaType:application/vnd.kubernetes.protobuf,encoder:{"encodeGV":"v1","encoder":"protobuf","name":"versioning"} (22-Mar-2024 03:23:42.639) (total time: 75037ms):
Trace[664194175]: ---"About to start writing response" size:1853 11292ms (03:23:53.931)
Trace[664194175]: ---"Write call failed" writer:responsewriter.outerWithCloseNotifyAndFlush,size:1853,firstWrite:true,err:http: Handler timeout 32290ms (03:24:26.222)
Trace[664194175]: ---"About to start writing response" size:69 30201ms (03:24:56.424)
Trace[664194175]: [1m15.037961676s] [1m15.037961676s] END
I0322 03:24:57.684294    2308 trace.go:236] Trace[1394994057]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:93c240d6-ffb4-4020-919f-b6911c5a7157,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:23:41.474) (total time: 76209ms):
Trace[1394994057]: ---"About to Get from storage" 625ms (03:23:42.100)
Trace[1394994057]: ---"Writing http response done" 75580ms (03:24:57.684)
Trace[1394994057]: [1m16.209231102s] [1m16.209231102s] END
E0322 03:24:56.484915    2308 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:24:57.707110    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:24:57.709618    2308 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 03:24:57.710545    2308 trace.go:236] Trace[1079298372]: "Update" accept:application/json, */*,audit-id:ebb61976-733d-470f-824e-2f7b741c46e5,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PUT (22-Mar-2024 03:23:40.871) (total time: 76838ms):
Trace[1079298372]: ---"limitedReadBody succeeded" len:3315 1526ms (03:23:42.398)
Trace[1079298372]: ---"About to convert to expected version" 758ms (03:23:43.156)
Trace[1079298372]: ---"Conversion done" 71035ms (03:24:54.191)
Trace[1079298372]: [1m16.838006349s] [1m16.838006349s] END
I0322 03:24:56.523282    2308 trace.go:236] Trace[322124987]: "Calculate volume metrics of content for pod kube-system/helm-install-traefik-t9wf9" (22-Mar-2024 03:24:55.358) (total time: 1161ms):
Trace[322124987]: [1.16149028s] [1.16149028s] END
E0322 03:24:56.594049    2308 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, context canceled]"
W0322 03:24:56.604153    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.NetworkPolicy ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:24:56.633882    2308 timeout.go:142] post-timeout activity - time-elapsed: 692.923761ms, POST "/api/v1/namespaces/default/events" result: <nil>
W0322 03:24:56.871733    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:24:56.883888    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
E0322 03:24:57.770048    2308 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:24:57.771945    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:24:57.772124    2308 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 03:24:56.892508    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
I0322 03:24:57.777396    2308 trace.go:236] Trace[2027692103]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:a4be0bb1-ef60-479e-8540-dced829cd279,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:24:56.876) (total time: 896ms):
Trace[2027692103]: [896.250491ms] [896.250491ms] END
E0322 03:24:57.778166    2308 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:24:57.778310    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:24:57.780399    2308 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 03:24:57.780528    2308 trace.go:236] Trace[224877995]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:7e0261c9-fe3c-4fe7-9ff1-e33ba5c34904,client:192.168.56.111,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:24:56.884) (total time: 896ms):
Trace[224877995]: [896.484455ms] [896.484455ms] END
E0322 03:24:56.894659    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
E0322 03:24:57.783015    2308 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:24:56.894831    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
E0322 03:24:56.894990    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
E0322 03:24:56.895105    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
E0322 03:24:56.895234    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
E0322 03:24:56.895343    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
E0322 03:24:57.784009    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:24:57.784324    2308 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:24:57.784441    2308 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:24:56.895461    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
E0322 03:24:56.895567    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
E0322 03:24:57.784811    2308 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:24:57.784983    2308 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 03:24:57.785095    2308 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:24:57.785773    2308 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:24:57.786690    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:24:57.786841    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:24:57.788025    2308 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 03:24:57.788267    2308 trace.go:236] Trace[1599165390]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:6a2f9dfa-f501-4a17-8874-402f8d7a5013,client:192.168.56.111,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:24:56.894) (total time: 893ms):
Trace[1599165390]: [893.479838ms] [893.479838ms] END
E0322 03:24:56.901112    2308 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:24:56.911462    2308 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:24:56.965678    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
E0322 03:24:56.996622    2308 controller.go:193] "Failed to update lease" err="Put \"https://127.0.0.1:6444/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-7zoch227uv4owxg75pgtnmv3jq\": http2: client connection lost"
E0322 03:24:57.803641    2308 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:24:57.803862    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:24:57.804197    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:24:57.804375    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:24:57.804512    2308 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:24:57.804607    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
I0322 03:24:57.804788    2308 trace.go:236] Trace[946497507]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:34e93048-a038-4fe2-a0f1-d56e044b2cc4,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:24:56.894) (total time: 910ms):
Trace[946497507]: [910.255436ms] [910.255436ms] END
E0322 03:24:57.804964    2308 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 03:24:57.805056    2308 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 03:24:57.805174    2308 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 03:24:57.083567    2308 status.go:71] apiserver received an error that is not an metav1.Status: context.deadlineExceededError{}: context deadline exceeded
E0322 03:24:57.805438    2308 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 03:24:57.805514    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:24:57.805665    2308 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 03:24:57.806440    2308 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:24:57.806638    2308 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 03:24:57.806749    2308 trace.go:236] Trace[380233367]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:bb09f859-ffd6-49e6-b351-688717865b99,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:24:56.895) (total time: 911ms):
Trace[380233367]: [911.545441ms] [911.545441ms] END
E0322 03:24:57.806893    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:24:57.807046    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:24:57.807148    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:24:57.807292    2308 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
W0322 03:24:57.139345    2308 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Endpoints ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:24:57.807820    2308 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 03:24:57.808020    2308 trace.go:236] Trace[411886227]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:82037b28-9b2a-43a3-a8ed-28df0831b86e,client:192.168.56.111,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:24:56.895) (total time: 912ms):
Trace[411886227]: [912.711402ms] [912.711402ms] END
I0322 03:24:57.808229    2308 trace.go:236] Trace[959792443]: "List" accept:application/json, */*,audit-id:f8205652-b2df-4873-910f-63cb4d04a72a,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:24:56.805) (total time: 1002ms):
Trace[959792443]: ---"Writing http response done" count:1 995ms (03:24:57.807)
Trace[959792443]: [1.002347906s] [1.002347906s] END
I0322 03:24:57.809202    2308 trace.go:236] Trace[369537827]: "Get" accept:application/json, */*,audit-id:b67d759c-7e4a-4906-bded-26d66695a31c,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps/coredns,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:GET (22-Mar-2024 03:24:56.203) (total time: 1605ms):
Trace[369537827]: [1.605599318s] [1.605599318s] END
E0322 03:24:57.812172    2308 timeout.go:142] post-timeout activity - time-elapsed: 1.870210918s, GET "/api/v1/namespaces/kube-system/configmaps/coredns" result: <nil>
I0322 03:24:57.812392    2308 trace.go:236] Trace[2129149317]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:28470e48-c203-4a61-ab10-bee4843158e9,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:24:56.894) (total time: 917ms):
Trace[2129149317]: [917.426589ms] [917.426589ms] END
I0322 03:24:57.812635    2308 trace.go:236] Trace[1961848321]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:c05c6a14-7342-44c8-956f-c8ddf693b717,client:192.168.56.111,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:24:56.895) (total time: 917ms):
Trace[1961848321]: [917.090476ms] [917.090476ms] END
I0322 03:24:57.812814    2308 trace.go:236] Trace[953705817]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:b0e9afb0-f15d-4f35-9442-f9655ce6b579,client:192.168.56.111,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:24:56.895) (total time: 917ms):
Trace[953705817]: [917.740408ms] [917.740408ms] END
I0322 03:24:57.153206    2308 log.go:245] http: TLS handshake error from 10.42.0.5:40534: EOF
I0322 03:24:57.174570    2308 trace.go:236] Trace[1993442866]: "iptables ChainExists" (22-Mar-2024 03:24:31.746) (total time: 25426ms):
Trace[1993442866]: [25.426299013s] [25.426299013s] END
E0322 03:24:57.176867    2308 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:24:57.813995    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:24:57.208261    2308 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
I0322 03:24:57.814303    2308 trace.go:236] Trace[1622506056]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:50766887-5476-4cec-a41b-f1d03231fd0f,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:24:56.895) (total time: 918ms):
Trace[1622506056]: [918.870482ms] [918.870482ms] END
E0322 03:24:57.814457    2308 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 03:24:57.814618    2308 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 03:24:57.815005    2308 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 03:24:57.815199    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
I0322 03:24:57.815342    2308 trace.go:236] Trace[91451633]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:9ed67817-eb91-4282-b2df-df71525c4433,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:24:56.895) (total time: 919ms):
Trace[91451633]: [919.663932ms] [919.663932ms] END
E0322 03:24:57.815512    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
I0322 03:24:57.815691    2308 trace.go:236] Trace[814096200]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:ddc0aeea-833d-4950-aa38-79662538c390,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/server,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PUT (22-Mar-2024 03:24:56.903) (total time: 912ms):
Trace[814096200]: [912.01543ms] [912.01543ms] END
I0322 03:24:57.816414    2308 trace.go:236] Trace[1565908744]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:0b5a8614-e327-4d53-87a8-2f8cf2433260,client:127.0.0.1,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/default/events,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:POST (22-Mar-2024 03:23:36.911) (total time: 80904ms):
Trace[1565908744]: ["Create etcd3" audit-id:0b5a8614-e327-4d53-87a8-2f8cf2433260,key:/events/default/server.17bef82d64d2e9c9,type:*core.Event,resource:events 76035ms (03:23:41.780)
Trace[1565908744]:  ---"About to Encode" 721ms (03:23:42.501)
Trace[1565908744]:  ---"Encode succeeded" len:570 904ms (03:23:43.406)
Trace[1565908744]:  ---"Txn call failed" err:context deadline exceeded 70965ms (03:24:54.373)]
Trace[1565908744]: ---"Write to database call failed" len:263,err:Timeout: request did not complete within requested timeout - context deadline exceeded 1551ms (03:24:55.924)
Trace[1565908744]: [1m20.904697115s] [1m20.904697115s] END
E0322 03:24:57.816742    2308 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 03:24:57.817098    2308 trace.go:236] Trace[1829260864]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:5b4b933a-495c-40ea-acc0-6253cc7b554c,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:24:56.965) (total time: 851ms):
Trace[1829260864]: [851.507648ms] [851.507648ms] END
E0322 03:24:57.817299    2308 timeout.go:142] post-timeout activity - time-elapsed: 1.878086214s, POST "/api/v1/namespaces/default/events" result: <nil>
E0322 03:24:57.817570    2308 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 03:24:57.818639    2308 timeout.go:142] post-timeout activity - time-elapsed: 2.352344331s, GET "/apis" result: <nil>
I0322 03:24:57.930343    2308 trace.go:236] Trace[1672742974]: "iptables ChainExists" (22-Mar-2024 03:24:03.203) (total time: 54723ms):
Trace[1672742974]: [54.723237365s] [54.723237365s] END
E0322 03:24:58.101195    2308 status.go:71] apiserver received an error that is not an metav1.Status: context.deadlineExceededError{}: context deadline exceeded
E0322 03:24:58.105057    2308 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:24:58.105943    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:24:58.108378    2308 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 03:24:58.111658    2308 trace.go:236] Trace[1674755737]: "Get" accept:application/json, */*,audit-id:13fb5ce2-15ac-4394-a378-130be44e54e4,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:24:57.358) (total time: 750ms):
Trace[1674755737]: [750.009098ms] [750.009098ms] END
E0322 03:24:58.115092    2308 timeout.go:142] post-timeout activity - time-elapsed: 2.219667453s, GET "/api/v1/nodes/serverworker" result: <nil>
E0322 03:24:58.174750    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:24:58.176825    2308 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 03:24:58.177181    2308 trace.go:236] Trace[1238950508]: "List" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:cf1cf94b-3072-4ba1-9660-2c32532a92c9,client:192.168.56.111,protocol:HTTP/2.0,resource:pods,scope:cluster,url:/api/v1/pods,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:24:56.461) (total time: 1715ms):
Trace[1238950508]: ---"Writing http response done" count:0 769ms (03:24:58.176)
Trace[1238950508]: [1.715479939s] [1.715479939s] END
E0322 03:24:58.240892    2308 timeout.go:142] post-timeout activity - time-elapsed: 45.25538196s, GET "/api/v1/nodes/serverworker" result: <nil>
E0322 03:24:58.324070    2308 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:24:58.326572    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:24:58.326616    2308 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 03:24:58.334895    2308 timeout.go:142] post-timeout activity - time-elapsed: 2.229592083s, PUT "/api/v1/nodes/serverworker" result: <nil>
E0322 03:24:58.334931    2308 timeout.go:142] post-timeout activity - time-elapsed: 1.806616186s, GET "/api/v1/nodes/server" result: <nil>
E0322 03:24:58.340974    2308 timeout.go:142] post-timeout activity - time-elapsed: 2.174748648s, GET "/api/v1/nodes/serverworker" result: <nil>
E0322 03:24:58.346917    2308 timeout.go:142] post-timeout activity - time-elapsed: 2.241682858s, GET "/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/serverworker" result: <nil>
E0322 03:24:58.353736    2308 timeout.go:142] post-timeout activity - time-elapsed: 2.392659279s, GET "/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/serverworker" result: <nil>
E0322 03:24:58.356957    2308 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:24:58.356979    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:24:58.401886    2308 timeout.go:142] post-timeout activity - time-elapsed: 2.441034161s, GET "/api/v1/nodes/serverworker" result: <nil>
E0322 03:24:58.401962    2308 timeout.go:142] post-timeout activity - time-elapsed: 4.10575791s, GET "/api/v1/pods" result: <nil>
E0322 03:24:58.417812    2308 finisher.go:175] FinishRequest: post-timeout activity - time-elapsed: 6.4s, panicked: false, err: <nil>, panic-reason: <nil>
E0322 03:24:58.417973    2308 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 03:24:58.419087    2308 timeout.go:142] post-timeout activity - time-elapsed: 2.252887515s, POST "/apis/authentication.k8s.io/v1/tokenreviews" result: <nil>
E0322 03:24:58.419263    2308 timeout.go:142] post-timeout activity - time-elapsed: 2.449811773s, GET "/api/v1/nodes/serverworker" result: <nil>
E0322 03:24:58.420517    2308 timeout.go:142] post-timeout activity - time-elapsed: 2.450032526s, GET "/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/serverworker" result: <nil>
E0322 03:24:58.422280    2308 timeout.go:142] post-timeout activity - time-elapsed: 2.461068496s, GET "/api/v1/nodes/serverworker" result: <nil>
E0322 03:24:58.422526    2308 timeout.go:142] post-timeout activity - time-elapsed: 2.451550697s, GET "/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/serverworker" result: <nil>
E0322 03:24:58.422662    2308 timeout.go:142] post-timeout activity - time-elapsed: 2.453557024s, GET "/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/serverworker" result: <nil>
E0322 03:24:58.422783    2308 timeout.go:142] post-timeout activity - time-elapsed: 2.451950653s, GET "/api/v1/nodes/serverworker" result: <nil>
E0322 03:24:58.422906    2308 timeout.go:142] post-timeout activity - time-elapsed: 2.450086668s, GET "/api/v1/nodes/serverworker" result: <nil>
E0322 03:24:58.422992    2308 timeout.go:142] post-timeout activity - time-elapsed: 1.854962977s, PUT "/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/server" result: <nil>
E0322 03:24:58.423011    2308 timeout.go:142] post-timeout activity - time-elapsed: 1.453687921s, GET "/api/v1/nodes/serverworker" result: <nil>
E0322 03:24:58.451411    2308 node_controller.go:240] error syncing 'serverworker': failed to get instance metadata for node serverworker: address annotations not yet set, requeuing
E0322 03:24:58.460491    2308 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:24:58.462222    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:24:58.462284    2308 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 03:24:58.759670    2308 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
I0322 03:24:58.776589    2308 trace.go:236] Trace[1897542207]: "List" accept:application/json, */*,audit-id:4eb4facf-373c-4a9f-8527-7e415c5783aa,client:192.168.56.111,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/default/endpoints,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:24:58.060) (total time: 713ms):
Trace[1897542207]: ---"Writing http response done" count:1 711ms (03:24:58.773)
Trace[1897542207]: [713.78542ms] [713.78542ms] END
E0322 03:24:59.031938    2308 timeout.go:142] post-timeout activity - time-elapsed: 3.112336397s, POST "/api/v1/namespaces/default/events" result: <nil>
I0322 03:24:59.284336    2308 trace.go:236] Trace[1293797908]: "List" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:42f2013c-de9c-4629-ab8f-498c149f96e6,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:24:57.745) (total time: 1531ms):
Trace[1293797908]: ---"Writing http response done" count:1 1523ms (03:24:59.277)
Trace[1293797908]: [1.531289665s] [1.531289665s] END
E0322 03:24:59.545288    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:24:59.547605    2308 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 03:24:59.549118    2308 trace.go:236] Trace[503583472]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:2f9c766e-3047-4628-bdda-47cb177facfc,client:192.168.56.111,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:POST (22-Mar-2024 03:24:58.032) (total time: 1515ms):
Trace[503583472]: [1.515898028s] [1.515898028s] END
E0322 03:24:59.550736    2308 timeout.go:142] post-timeout activity - time-elapsed: 3.446626433s, POST "/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases" result: <nil>
E0322 03:24:59.675129    2308 finisher.go:175] FinishRequest: post-timeout activity - time-elapsed: 2.599853336s, panicked: false, err: context canceled, panic-reason: <nil>
I0322 03:24:59.870259    2308 trace.go:236] Trace[1559621788]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:fd9ce9be-3d14-4a56-83bc-ded13ee4acee,client:192.168.56.111,protocol:HTTP/2.0,resource:tokenreviews,scope:resource,url:/apis/authentication.k8s.io/v1/tokenreviews,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:POST (22-Mar-2024 03:24:58.762) (total time: 1103ms):
Trace[1559621788]: ---"Write to database call succeeded" len:1152 1089ms (03:24:59.860)
Trace[1559621788]: [1.103270914s] [1.103270914s] END
I0322 03:25:00.041539    2308 trace.go:236] Trace[902750690]: "List" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:f603c2c1-cbe6-4ea0-9cd1-ed7406904fdf,client:192.168.56.111,protocol:HTTP/2.0,resource:services,scope:cluster,url:/api/v1/services,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:24:58.285) (total time: 1749ms):
Trace[902750690]: ["cacher list" audit-id:f603c2c1-cbe6-4ea0-9cd1-ed7406904fdf,type:services 1746ms (03:24:58.289)
Trace[902750690]:  ---"Listed items from cache" count:3 632ms (03:24:58.923)]
Trace[902750690]: ---"Writing http response done" count:3 1108ms (03:25:00.034)
Trace[902750690]: [1.749047076s] [1.749047076s] END
I0322 03:25:00.048262    2308 trace.go:236] Trace[2101035827]: "List" accept:application/json, */*,audit-id:31d8c908-4241-449e-ab47-647828ecf4f9,client:127.0.0.1,protocol:HTTP/2.0,resource:jobs,scope:cluster,url:/apis/batch/v1/jobs,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:LIST (22-Mar-2024 03:24:59.250) (total time: 798ms):
Trace[2101035827]: ---"Writing http response done" count:2 796ms (03:25:00.048)
Trace[2101035827]: [798.117369ms] [798.117369ms] END
I0322 03:25:00.049053    2308 trace.go:236] Trace[1084794268]: "List" accept:application/json, */*,audit-id:dc0e5717-1c44-40c5-9d0a-8d23dd878ef6,client:127.0.0.1,protocol:HTTP/2.0,resource:clusterrolebindings,scope:cluster,url:/apis/rbac.authorization.k8s.io/v1/clusterrolebindings,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:LIST (22-Mar-2024 03:24:59.183) (total time: 865ms):
Trace[1084794268]: ["cacher list" audit-id:dc0e5717-1c44-40c5-9d0a-8d23dd878ef6,type:clusterrolebindings.rbac.authorization.k8s.io 863ms (03:24:59.185)
Trace[1084794268]:  ---"watchCache locked acquired" 626ms (03:24:59.811)]
Trace[1084794268]: ---"Writing http response done" count:54 237ms (03:25:00.049)
Trace[1084794268]: [865.031ms] [865.031ms] END
I0322 03:25:00.049444    2308 trace.go:236] Trace[917744885]: "List" accept:application/json, */*,audit-id:f51dab68-3352-4523-ac7a-004ed7f29117,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:cluster,url:/api/v1/nodes,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:LIST (22-Mar-2024 03:24:59.212) (total time: 837ms):
Trace[917744885]: ---"Writing http response done" count:2 831ms (03:25:00.049)
Trace[917744885]: [837.388477ms] [837.388477ms] END
I0322 03:25:00.727601    2308 trace.go:236] Trace[666223933]: "List" accept:application/json, */*,audit-id:b3757184-1545-4636-abb4-da4624e3cafd,client:127.0.0.1,protocol:HTTP/2.0,resource:helmchartconfigs,scope:cluster,url:/apis/helm.cattle.io/v1/helmchartconfigs,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:LIST (22-Mar-2024 03:24:59.206) (total time: 1516ms):
Trace[666223933]: ---"About to List from storage" 613ms (03:24:59.819)
Trace[666223933]: ["cacher list" audit-id:b3757184-1545-4636-abb4-da4624e3cafd,type:helmchartconfigs.helm.cattle.io 903ms (03:24:59.819)]
Trace[666223933]: [1.516981018s] [1.516981018s] END
I0322 03:25:00.778080    2308 trace.go:236] Trace[1242172968]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:868966d1-68eb-4297-91f2-64d6c84d0fe7,client:127.0.0.1,protocol:HTTP/2.0,resource:csidrivers,scope:cluster,url:/apis/storage.k8s.io/v1/csidrivers,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:24:59.824) (total time: 953ms):
Trace[1242172968]: ["cacher list" audit-id:868966d1-68eb-4297-91f2-64d6c84d0fe7,type:csidrivers.storage.k8s.io 953ms (03:24:59.824)]
Trace[1242172968]: [953.225894ms] [953.225894ms] END
I0322 03:25:00.787890    2308 trace.go:236] Trace[1785963049]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:bed42ec7-a110-4fb5-bc53-c5f09b39e482,client:127.0.0.1,protocol:HTTP/2.0,resource:podtemplates,scope:cluster,url:/api/v1/podtemplates,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:24:59.825) (total time: 962ms):
Trace[1785963049]: ["cacher list" audit-id:bed42ec7-a110-4fb5-bc53-c5f09b39e482,type:podtemplates 962ms (03:24:59.825)]
Trace[1785963049]: [962.512451ms] [962.512451ms] END
I0322 03:25:00.815329    2308 trace.go:236] Trace[2020361182]: "List" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json,audit-id:d82529d0-1511-41a7-9023-ed16783b30fb,client:127.0.0.1,protocol:HTTP/2.0,resource:helmcharts,scope:cluster,url:/apis/helm.cattle.io/v1/helmcharts,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/metadata-informers,verb:LIST (22-Mar-2024 03:24:59.828) (total time: 986ms):
Trace[2020361182]: ["cacher list" audit-id:d82529d0-1511-41a7-9023-ed16783b30fb,type:helmcharts.helm.cattle.io 986ms (03:24:59.828)]
Trace[2020361182]: [986.744013ms] [986.744013ms] END
I0322 03:25:00.820096    2308 trace.go:236] Trace[823558565]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:95109c8c-3eed-4b13-a435-e08804f1266b,client:127.0.0.1,protocol:HTTP/2.0,resource:mutatingwebhookconfigurations,scope:cluster,url:/apis/admissionregistration.k8s.io/v1/mutatingwebhookconfigurations,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:24:59.829) (total time: 990ms):
Trace[823558565]: ["cacher list" audit-id:95109c8c-3eed-4b13-a435-e08804f1266b,type:mutatingwebhookconfigurations.admissionregistration.k8s.io 990ms (03:24:59.829)]
Trace[823558565]: [990.885661ms] [990.885661ms] END
I0322 03:25:00.823183    2308 trace.go:236] Trace[1096835072]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:36e698a2-a460-4bea-8698-eeb01c113e81,client:127.0.0.1,protocol:HTTP/2.0,resource:endpointslices,scope:cluster,url:/apis/discovery.k8s.io/v1/endpointslices,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:24:59.830) (total time: 992ms):
Trace[1096835072]: ["cacher list" audit-id:36e698a2-a460-4bea-8698-eeb01c113e81,type:endpointslices.discovery.k8s.io 991ms (03:24:59.831)]
Trace[1096835072]: [992.138571ms] [992.138571ms] END
I0322 03:25:00.826319    2308 trace.go:236] Trace[1869299521]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:279a1e43-01f9-452e-b2c9-9013d46c45b2,client:127.0.0.1,protocol:HTTP/2.0,resource:poddisruptionbudgets,scope:cluster,url:/apis/policy/v1/poddisruptionbudgets,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:24:59.832) (total time: 993ms):
Trace[1869299521]: ["cacher list" audit-id:279a1e43-01f9-452e-b2c9-9013d46c45b2,type:poddisruptionbudgets.policy 992ms (03:24:59.833)]
Trace[1869299521]: [993.38121ms] [993.38121ms] END
I0322 03:25:00.828920    2308 trace.go:236] Trace[1139726192]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:fd748b08-edc1-4aae-9625-1ffc79aaf731,client:127.0.0.1,protocol:HTTP/2.0,resource:networkpolicies,scope:cluster,url:/apis/networking.k8s.io/v1/networkpolicies,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:24:59.837) (total time: 991ms):
Trace[1139726192]: ["cacher list" audit-id:fd748b08-edc1-4aae-9625-1ffc79aaf731,type:networkpolicies.networking.k8s.io 991ms (03:24:59.837)]
Trace[1139726192]: [991.510866ms] [991.510866ms] END
I0322 03:25:00.830995    2308 trace.go:236] Trace[723187309]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:bab080e7-f29d-4440-a777-824881cef188,client:127.0.0.1,protocol:HTTP/2.0,resource:persistentvolumeclaims,scope:cluster,url:/api/v1/persistentvolumeclaims,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:24:59.837) (total time: 992ms):
Trace[723187309]: ["cacher list" audit-id:bab080e7-f29d-4440-a777-824881cef188,type:persistentvolumeclaims 992ms (03:24:59.838)]
Trace[723187309]: [992.742273ms] [992.742273ms] END
I0322 03:25:00.831364    2308 trace.go:236] Trace[805788143]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:202f65c6-fa08-46a2-bdd3-e3e4b90dd668,client:127.0.0.1,protocol:HTTP/2.0,resource:csidrivers,scope:cluster,url:/apis/storage.k8s.io/v1/csidrivers,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/scheduler,verb:LIST (22-Mar-2024 03:24:59.872) (total time: 958ms):
Trace[805788143]: ["cacher list" audit-id:202f65c6-fa08-46a2-bdd3-e3e4b90dd668,type:csidrivers.storage.k8s.io 958ms (03:24:59.872)]
Trace[805788143]: [958.390383ms] [958.390383ms] END
I0322 03:25:00.834273    2308 trace.go:236] Trace[916900051]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:0d9aa08b-b0f1-4195-8c0c-5ec72c62ba12,client:127.0.0.1,protocol:HTTP/2.0,resource:poddisruptionbudgets,scope:cluster,url:/apis/policy/v1/poddisruptionbudgets,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/scheduler,verb:LIST (22-Mar-2024 03:24:59.876) (total time: 957ms):
Trace[916900051]: ["cacher list" audit-id:0d9aa08b-b0f1-4195-8c0c-5ec72c62ba12,type:poddisruptionbudgets.policy 957ms (03:24:59.876)]
Trace[916900051]: [957.933735ms] [957.933735ms] END
I0322 03:25:00.835016    2308 trace.go:236] Trace[1536135792]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:bc4fbefa-9478-4dcb-9323-c7b2238d2e1c,client:127.0.0.1,protocol:HTTP/2.0,resource:persistentvolumeclaims,scope:cluster,url:/api/v1/persistentvolumeclaims,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/scheduler,verb:LIST (22-Mar-2024 03:24:59.877) (total time: 957ms):
Trace[1536135792]: ["cacher list" audit-id:bc4fbefa-9478-4dcb-9323-c7b2238d2e1c,type:persistentvolumeclaims 957ms (03:24:59.877)]
Trace[1536135792]: [957.867907ms] [957.867907ms] END
I0322 03:25:00.838337    2308 trace.go:236] Trace[1232324770]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:60a94b99-c0c4-4680-8621-5fb8f9c0712f,client:127.0.0.1,protocol:HTTP/2.0,resource:namespaces,scope:cluster,url:/api/v1/namespaces,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/scheduler,verb:LIST (22-Mar-2024 03:24:59.877) (total time: 960ms):
Trace[1232324770]: ["cacher list" audit-id:60a94b99-c0c4-4680-8621-5fb8f9c0712f,type:namespaces 959ms (03:24:59.878)]
Trace[1232324770]: [960.375896ms] [960.375896ms] END
I0322 03:25:00.839219    2308 trace.go:236] Trace[1127024737]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:835fe763-e0fd-4117-895c-d4de7e07d0c1,client:127.0.0.1,protocol:HTTP/2.0,resource:csinodes,scope:cluster,url:/apis/storage.k8s.io/v1/csinodes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/scheduler,verb:LIST (22-Mar-2024 03:24:59.880) (total time: 958ms):
Trace[1127024737]: ["cacher list" audit-id:835fe763-e0fd-4117-895c-d4de7e07d0c1,type:csinodes.storage.k8s.io 957ms (03:24:59.881)]
Trace[1127024737]: [958.223964ms] [958.223964ms] END
I0322 03:25:00.841011    2308 trace.go:236] Trace[1840765806]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:db8ebb4a-c982-4c27-ae61-4363821e6e01,client:127.0.0.1,protocol:HTTP/2.0,resource:persistentvolumes,scope:cluster,url:/api/v1/persistentvolumes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/scheduler,verb:LIST (22-Mar-2024 03:24:59.888) (total time: 952ms):
Trace[1840765806]: ["cacher list" audit-id:db8ebb4a-c982-4c27-ae61-4363821e6e01,type:persistentvolumes 952ms (03:24:59.888)]
Trace[1840765806]: [952.970929ms] [952.970929ms] END
I0322 03:25:00.842290    2308 trace.go:236] Trace[1126137259]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:9f5a5e8c-2ec7-4b75-854c-d0708c26dbce,client:127.0.0.1,protocol:HTTP/2.0,resource:storageclasses,scope:cluster,url:/apis/storage.k8s.io/v1/storageclasses,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/scheduler,verb:LIST (22-Mar-2024 03:24:59.888) (total time: 953ms):
Trace[1126137259]: ["cacher list" audit-id:9f5a5e8c-2ec7-4b75-854c-d0708c26dbce,type:storageclasses.storage.k8s.io 953ms (03:24:59.888)]
Trace[1126137259]: [953.508348ms] [953.508348ms] END
I0322 03:25:00.843253    2308 trace.go:236] Trace[144195601]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:ddd7b785-bf87-49a2-83d6-f200f2c36743,client:127.0.0.1,protocol:HTTP/2.0,resource:csistoragecapacities,scope:cluster,url:/apis/storage.k8s.io/v1/csistoragecapacities,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/scheduler,verb:LIST (22-Mar-2024 03:24:59.889) (total time: 953ms):
Trace[144195601]: ["cacher list" audit-id:ddd7b785-bf87-49a2-83d6-f200f2c36743,type:csistoragecapacities.storage.k8s.io 953ms (03:24:59.889)]
Trace[144195601]: [953.450621ms] [953.450621ms] END
I0322 03:25:00.844000    2308 trace.go:236] Trace[2020715503]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:8a026e0e-38bc-462a-8f5b-fc212d43563a,client:127.0.0.1,protocol:HTTP/2.0,resource:statefulsets,scope:cluster,url:/apis/apps/v1/statefulsets,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/scheduler,verb:LIST (22-Mar-2024 03:24:59.891) (total time: 952ms):
Trace[2020715503]: ["cacher list" audit-id:8a026e0e-38bc-462a-8f5b-fc212d43563a,type:statefulsets.apps 952ms (03:24:59.891)]
Trace[2020715503]: [952.327532ms] [952.327532ms] END
I0322 03:25:00.844746    2308 trace.go:236] Trace[785811059]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:59d77532-84be-42a4-8576-84160535f4bc,client:127.0.0.1,protocol:HTTP/2.0,resource:replicationcontrollers,scope:cluster,url:/api/v1/replicationcontrollers,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/scheduler,verb:LIST (22-Mar-2024 03:24:59.892) (total time: 952ms):
Trace[785811059]: ["cacher list" audit-id:59d77532-84be-42a4-8576-84160535f4bc,type:replicationcontrollers 951ms (03:24:59.892)]
Trace[785811059]: [952.240128ms] [952.240128ms] END
I0322 03:25:00.845235    2308 trace.go:236] Trace[2072670616]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:84bfa979-1387-4fc4-b583-a6b766eadf8d,client:127.0.0.1,protocol:HTTP/2.0,resource:endpointslices,scope:cluster,url:/apis/discovery.k8s.io/v1/endpointslices,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/,verb:LIST (22-Mar-2024 03:24:59.893) (total time: 951ms):
Trace[2072670616]: ["cacher list" audit-id:84bfa979-1387-4fc4-b583-a6b766eadf8d,type:endpointslices.discovery.k8s.io 951ms (03:24:59.893)]
Trace[2072670616]: [951.49115ms] [951.49115ms] END
I0322 03:25:00.846550    2308 trace.go:236] Trace[741693216]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:96a5f124-c631-473c-90f4-4c306778ca64,client:127.0.0.1,protocol:HTTP/2.0,resource:daemonsets,scope:namespace,url:/apis/apps/v1/namespaces/kube-system/daemonsets,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/,verb:LIST (22-Mar-2024 03:24:59.896) (total time: 949ms):
Trace[741693216]: ["cacher list" audit-id:96a5f124-c631-473c-90f4-4c306778ca64,type:daemonsets.apps 949ms (03:24:59.897)]
Trace[741693216]: [949.843559ms] [949.843559ms] END
I0322 03:25:00.851221    2308 trace.go:236] Trace[440681501]: "List" accept:application/json, */*,audit-id:c1169479-3daf-4887-a11d-850edc782f4a,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:24:59.901) (total time: 949ms):
Trace[440681501]: ["cacher list" audit-id:c1169479-3daf-4887-a11d-850edc782f4a,type:configmaps 949ms (03:24:59.901)]
Trace[440681501]: [949.956908ms] [949.956908ms] END
I0322 03:25:00.852015    2308 trace.go:236] Trace[1674723424]: "List" accept:application/json, */*,audit-id:bc31d466-9287-4190-a3d2-2fd8d88a61ee,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:24:59.902) (total time: 949ms):
Trace[1674723424]: ["cacher list" audit-id:bc31d466-9287-4190-a3d2-2fd8d88a61ee,type:configmaps 949ms (03:24:59.902)]
Trace[1674723424]: [949.730108ms] [949.730108ms] END
I0322 03:25:00.852812    2308 trace.go:236] Trace[2020372351]: "List" accept:application/json, */*,audit-id:70fe257b-2fd3-48b2-a1a7-9b0d2d2c43a8,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:24:59.903) (total time: 949ms):
Trace[2020372351]: ["cacher list" audit-id:70fe257b-2fd3-48b2-a1a7-9b0d2d2c43a8,type:configmaps 949ms (03:24:59.903)]
Trace[2020372351]: [949.130161ms] [949.130161ms] END
I0322 03:25:00.853607    2308 trace.go:236] Trace[2039206648]: "List" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:37cd7f11-3a54-4320-8506-5736623599cc,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:24:59.942) (total time: 910ms):
Trace[2039206648]: ["cacher list" audit-id:37cd7f11-3a54-4320-8506-5736623599cc,type:configmaps 910ms (03:24:59.942)]
Trace[2039206648]: [910.633235ms] [910.633235ms] END
I0322 03:25:00.859390    2308 trace.go:236] Trace[411464316]: "List" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:ebc030b0-f4a2-4d51-b170-87a639c10ca2,client:127.0.0.1,protocol:HTTP/2.0,resource:secrets,scope:resource,url:/api/v1/namespaces/kube-system/secrets,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:24:59.995) (total time: 864ms):
Trace[411464316]: ["cacher list" audit-id:ebc030b0-f4a2-4d51-b170-87a639c10ca2,type:secrets 861ms (03:24:59.997)]
Trace[411464316]: [864.089227ms] [864.089227ms] END
I0322 03:25:00.860051    2308 trace.go:236] Trace[995485158]: "List" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:08ed8288-690e-4c64-a4ac-1d547bb481bf,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:25:00.000) (total time: 859ms):
Trace[995485158]: ["cacher list" audit-id:08ed8288-690e-4c64-a4ac-1d547bb481bf,type:configmaps 859ms (03:25:00.000)]
Trace[995485158]: [859.487195ms] [859.487195ms] END
I0322 03:25:00.867659    2308 trace.go:236] Trace[1828537490]: "List" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:82912240-4a0b-4adb-b95b-31d30287cfba,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:25:00.002) (total time: 865ms):
Trace[1828537490]: ["cacher list" audit-id:82912240-4a0b-4adb-b95b-31d30287cfba,type:configmaps 865ms (03:25:00.002)]
Trace[1828537490]: [865.236087ms] [865.236087ms] END
I0322 03:25:00.868227    2308 trace.go:236] Trace[492398526]: "List" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:2691d623-2813-4fc3-98cd-91cad71266c1,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:25:00.003) (total time: 864ms):
Trace[492398526]: ["cacher list" audit-id:2691d623-2813-4fc3-98cd-91cad71266c1,type:configmaps 864ms (03:25:00.004)]
Trace[492398526]: [864.226962ms] [864.226962ms] END
I0322 03:25:00.876246    2308 trace.go:236] Trace[928160560]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:bef9a7db-e953-42a0-94ee-d16d0283a957,client:127.0.0.1,protocol:HTTP/2.0,resource:storageclasses,scope:cluster,url:/apis/storage.k8s.io/v1/storageclasses,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:25:00.060) (total time: 815ms):
Trace[928160560]: ["cacher list" audit-id:bef9a7db-e953-42a0-94ee-d16d0283a957,type:storageclasses.storage.k8s.io 815ms (03:25:00.061)]
Trace[928160560]: [815.644884ms] [815.644884ms] END
I0322 03:25:00.883325    2308 trace.go:236] Trace[2114496058]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:c0b44fc3-14f0-4ef0-8a59-edc36e220e1a,client:127.0.0.1,protocol:HTTP/2.0,resource:ingressclasses,scope:cluster,url:/apis/networking.k8s.io/v1/ingressclasses,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:25:00.071) (total time: 811ms):
Trace[2114496058]: ["cacher list" audit-id:c0b44fc3-14f0-4ef0-8a59-edc36e220e1a,type:ingressclasses.networking.k8s.io 810ms (03:25:00.072)]
Trace[2114496058]: [811.311884ms] [811.311884ms] END
I0322 03:25:00.883936    2308 trace.go:236] Trace[1714487921]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:fd06fbce-ac2c-4f6e-a659-30771c810da7,client:127.0.0.1,protocol:HTTP/2.0,resource:csistoragecapacities,scope:cluster,url:/apis/storage.k8s.io/v1/csistoragecapacities,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:25:00.074) (total time: 809ms):
Trace[1714487921]: ["cacher list" audit-id:fd06fbce-ac2c-4f6e-a659-30771c810da7,type:csistoragecapacities.storage.k8s.io 809ms (03:25:00.074)]
Trace[1714487921]: [809.714519ms] [809.714519ms] END
I0322 03:25:00.884284    2308 trace.go:236] Trace[2080208243]: "List" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:5db49b52-421d-4967-9558-cdab3e711c30,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:25:00.075) (total time: 808ms):
Trace[2080208243]: ["cacher list" audit-id:5db49b52-421d-4967-9558-cdab3e711c30,type:configmaps 808ms (03:25:00.076)]
Trace[2080208243]: [808.267585ms] [808.267585ms] END
I0322 03:25:00.884751    2308 trace.go:236] Trace[124764967]: "List" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:9253bfcf-fe03-4d73-9b47-4ffd091866f9,client:127.0.0.1,protocol:HTTP/2.0,resource:csidrivers,scope:cluster,url:/apis/storage.k8s.io/v1/csidrivers,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:25:00.076) (total time: 808ms):
Trace[124764967]: ["cacher list" audit-id:9253bfcf-fe03-4d73-9b47-4ffd091866f9,type:csidrivers.storage.k8s.io 808ms (03:25:00.076)]
Trace[124764967]: [808.314035ms] [808.314035ms] END
I0322 03:25:00.885145    2308 trace.go:236] Trace[1099245058]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:fe9fcbef-5823-4343-a983-e82077a1b746,client:127.0.0.1,protocol:HTTP/2.0,resource:endpoints,scope:cluster,url:/api/v1/endpoints,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:25:00.078) (total time: 806ms):
Trace[1099245058]: ["cacher list" audit-id:fe9fcbef-5823-4343-a983-e82077a1b746,type:endpoints 806ms (03:25:00.078)]
Trace[1099245058]: [806.648191ms] [806.648191ms] END
I0322 03:25:00.885565    2308 trace.go:236] Trace[913156883]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:bd6eb908-9c18-4f2e-9035-e4ae57647792,client:127.0.0.1,protocol:HTTP/2.0,resource:daemonsets,scope:cluster,url:/apis/apps/v1/daemonsets,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:25:00.079) (total time: 805ms):
Trace[913156883]: ["cacher list" audit-id:bd6eb908-9c18-4f2e-9035-e4ae57647792,type:daemonsets.apps 805ms (03:25:00.079)]
Trace[913156883]: [805.569996ms] [805.569996ms] END
I0322 03:25:00.889675    2308 trace.go:236] Trace[858439388]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:4098024a-67f3-492a-b46e-16d1e05afde1,client:127.0.0.1,protocol:HTTP/2.0,resource:horizontalpodautoscalers,scope:cluster,url:/apis/autoscaling/v2/horizontalpodautoscalers,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:25:00.081) (total time: 808ms):
Trace[858439388]: ["cacher list" audit-id:4098024a-67f3-492a-b46e-16d1e05afde1,type:horizontalpodautoscalers.autoscaling 808ms (03:25:00.081)]
Trace[858439388]: [808.473231ms] [808.473231ms] END
I0322 03:25:00.891061    2308 trace.go:236] Trace[1598716882]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:eca09bdb-ca3a-441d-bec8-153043494325,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:cluster,url:/apis/coordination.k8s.io/v1/leases,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:25:00.082) (total time: 808ms):
Trace[1598716882]: ["cacher list" audit-id:eca09bdb-ca3a-441d-bec8-153043494325,type:leases.coordination.k8s.io 808ms (03:25:00.082)]
Trace[1598716882]: [808.747724ms] [808.747724ms] END
I0322 03:25:00.893590    2308 trace.go:236] Trace[1457528992]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:82b781c2-1b74-4a3a-b2a9-58b27e53151d,client:127.0.0.1,protocol:HTTP/2.0,resource:priorityclasses,scope:cluster,url:/apis/scheduling.k8s.io/v1/priorityclasses,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:25:00.083) (total time: 810ms):
Trace[1457528992]: ["cacher list" audit-id:82b781c2-1b74-4a3a-b2a9-58b27e53151d,type:priorityclasses.scheduling.k8s.io 807ms (03:25:00.085)]
Trace[1457528992]: [810.268042ms] [810.268042ms] END
I0322 03:25:00.894259    2308 trace.go:236] Trace[545471229]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:bb543819-ed9e-4bc6-bf67-c4c3fc70e8e3,client:127.0.0.1,protocol:HTTP/2.0,resource:ingresses,scope:cluster,url:/apis/networking.k8s.io/v1/ingresses,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:25:00.088) (total time: 805ms):
Trace[545471229]: ["cacher list" audit-id:bb543819-ed9e-4bc6-bf67-c4c3fc70e8e3,type:ingresses.networking.k8s.io 805ms (03:25:00.088)]
Trace[545471229]: [805.558348ms] [805.558348ms] END
I0322 03:25:00.898662    2308 trace.go:236] Trace[1846895058]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:a2f45322-8976-4b82-a246-6c2e7d6e183a,client:127.0.0.1,protocol:HTTP/2.0,resource:volumeattachments,scope:cluster,url:/apis/storage.k8s.io/v1/volumeattachments,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:25:00.108) (total time: 790ms):
Trace[1846895058]: ["cacher list" audit-id:a2f45322-8976-4b82-a246-6c2e7d6e183a,type:volumeattachments.storage.k8s.io 789ms (03:25:00.109)]
Trace[1846895058]: [790.25837ms] [790.25837ms] END
I0322 03:25:00.899292    2308 trace.go:236] Trace[986279906]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:8ec9d767-daef-4dac-b9ae-8316654abb0b,client:127.0.0.1,protocol:HTTP/2.0,resource:namespaces,scope:cluster,url:/api/v1/namespaces,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:25:00.112) (total time: 787ms):
Trace[986279906]: ["cacher list" audit-id:8ec9d767-daef-4dac-b9ae-8316654abb0b,type:namespaces 786ms (03:25:00.112)]
Trace[986279906]: [787.064376ms] [787.064376ms] END
I0322 03:25:00.899635    2308 trace.go:236] Trace[1477422595]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:b4ede284-0d0f-4924-8235-eff1593a6171,client:127.0.0.1,protocol:HTTP/2.0,resource:statefulsets,scope:cluster,url:/apis/apps/v1/statefulsets,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:25:00.115) (total time: 784ms):
Trace[1477422595]: ["cacher list" audit-id:b4ede284-0d0f-4924-8235-eff1593a6171,type:statefulsets.apps 783ms (03:25:00.115)]
Trace[1477422595]: [784.113837ms] [784.113837ms] END
I0322 03:25:00.901032    2308 trace.go:236] Trace[635894555]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:b72a0c7f-f2aa-4afc-ab0f-f541b6e9d989,client:127.0.0.1,protocol:HTTP/2.0,resource:limitranges,scope:cluster,url:/api/v1/limitranges,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:25:00.117) (total time: 783ms):
Trace[635894555]: ["cacher list" audit-id:b72a0c7f-f2aa-4afc-ab0f-f541b6e9d989,type:limitranges 783ms (03:25:00.117)]
Trace[635894555]: [783.420796ms] [783.420796ms] END
I0322 03:25:00.901937    2308 trace.go:236] Trace[232811618]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:385e11fb-9399-4f27-b9be-3fa6447595e9,client:127.0.0.1,protocol:HTTP/2.0,resource:validatingwebhookconfigurations,scope:cluster,url:/apis/admissionregistration.k8s.io/v1/validatingwebhookconfigurations,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:25:00.125) (total time: 776ms):
Trace[232811618]: ["cacher list" audit-id:385e11fb-9399-4f27-b9be-3fa6447595e9,type:validatingwebhookconfigurations.admissionregistration.k8s.io 776ms (03:25:00.125)]
Trace[232811618]: [776.7895ms] [776.7895ms] END
I0322 03:25:00.902870    2308 trace.go:236] Trace[432681250]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:5af9c419-c1c0-4940-a236-7c5c8d8ddd18,client:127.0.0.1,protocol:HTTP/2.0,resource:controllerrevisions,scope:cluster,url:/apis/apps/v1/controllerrevisions,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:25:00.130) (total time: 772ms):
Trace[432681250]: ["cacher list" audit-id:5af9c419-c1c0-4940-a236-7c5c8d8ddd18,type:controllerrevisions.apps 772ms (03:25:00.130)]
Trace[432681250]: [772.305428ms] [772.305428ms] END
I0322 03:25:00.904969    2308 trace.go:236] Trace[212068978]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:233a687f-a40b-451a-82d1-adcaee6eb7d5,client:127.0.0.1,protocol:HTTP/2.0,resource:persistentvolumes,scope:cluster,url:/api/v1/persistentvolumes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:25:00.137) (total time: 767ms):
Trace[212068978]: ["cacher list" audit-id:233a687f-a40b-451a-82d1-adcaee6eb7d5,type:persistentvolumes 766ms (03:25:00.138)]
Trace[212068978]: [767.764186ms] [767.764186ms] END
I0322 03:25:00.905666    2308 trace.go:236] Trace[2110970336]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:2cc46b07-ba82-4238-954b-61cfe416cdc1,client:127.0.0.1,protocol:HTTP/2.0,resource:resourcequotas,scope:cluster,url:/api/v1/resourcequotas,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:25:00.143) (total time: 761ms):
Trace[2110970336]: ["cacher list" audit-id:2cc46b07-ba82-4238-954b-61cfe416cdc1,type:resourcequotas 761ms (03:25:00.143)]
Trace[2110970336]: [761.959017ms] [761.959017ms] END
I0322 03:25:00.906468    2308 trace.go:236] Trace[537154502]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:bfc628f6-32c3-4584-a1fa-c37a51422e8c,client:127.0.0.1,protocol:HTTP/2.0,resource:csinodes,scope:cluster,url:/apis/storage.k8s.io/v1/csinodes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:25:00.150) (total time: 755ms):
Trace[537154502]: ["cacher list" audit-id:bfc628f6-32c3-4584-a1fa-c37a51422e8c,type:csinodes.storage.k8s.io 755ms (03:25:00.150)]
Trace[537154502]: [755.529474ms] [755.529474ms] END
I0322 03:25:00.907562    2308 trace.go:236] Trace[1658167481]: "List" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json,audit-id:a8999e79-e6d7-4d42-a64c-a68ca5f87192,client:127.0.0.1,protocol:HTTP/2.0,resource:helmchartconfigs,scope:cluster,url:/apis/helm.cattle.io/v1/helmchartconfigs,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/metadata-informers,verb:LIST (22-Mar-2024 03:25:00.173) (total time: 734ms):
Trace[1658167481]: ["cacher list" audit-id:a8999e79-e6d7-4d42-a64c-a68ca5f87192,type:helmchartconfigs.helm.cattle.io 734ms (03:25:00.173)]
Trace[1658167481]: [734.330726ms] [734.330726ms] END
I0322 03:25:00.931412    2308 trace.go:236] Trace[774950338]: "List" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json,audit-id:c5d2db31-b8cd-4012-82e8-0d02028104ff,client:127.0.0.1,protocol:HTTP/2.0,resource:serverstransports,scope:cluster,url:/apis/traefik.io/v1alpha1/serverstransports,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/metadata-informers,verb:LIST (22-Mar-2024 03:25:00.184) (total time: 746ms):
Trace[774950338]: ["cacher list" audit-id:c5d2db31-b8cd-4012-82e8-0d02028104ff,type:serverstransports.traefik.io 746ms (03:25:00.184)]
Trace[774950338]: [746.825755ms] [746.825755ms] END
I0322 03:25:00.932708    2308 trace.go:236] Trace[2022866290]: "List" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json,audit-id:284e01f5-3a2d-4eff-b7ea-bc5c7daa4979,client:127.0.0.1,protocol:HTTP/2.0,resource:ingressroutes,scope:cluster,url:/apis/traefik.containo.us/v1alpha1/ingressroutes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/metadata-informers,verb:LIST (22-Mar-2024 03:25:00.186) (total time: 746ms):
Trace[2022866290]: ["cacher list" audit-id:284e01f5-3a2d-4eff-b7ea-bc5c7daa4979,type:ingressroutes.traefik.containo.us 746ms (03:25:00.186)]
Trace[2022866290]: [746.165295ms] [746.165295ms] END
I0322 03:25:00.966048    2308 trace.go:236] Trace[426173855]: "List" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json,audit-id:73c192c8-9b64-4a87-b0bb-b27a895e91ff,client:127.0.0.1,protocol:HTTP/2.0,resource:traefikservices,scope:cluster,url:/apis/traefik.containo.us/v1alpha1/traefikservices,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/metadata-informers,verb:LIST (22-Mar-2024 03:24:59.822) (total time: 1143ms):
Trace[426173855]: ["cacher list" audit-id:73c192c8-9b64-4a87-b0bb-b27a895e91ff,type:traefikservices.traefik.containo.us 1143ms (03:24:59.822)]
Trace[426173855]: [1.143191567s] [1.143191567s] END
I0322 03:25:00.967674    2308 trace.go:236] Trace[509937204]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:8722638f-e01d-4ee2-aca4-b2a04c1733c7,client:127.0.0.1,protocol:HTTP/2.0,resource:customresourcedefinitions,scope:cluster,url:/apis/apiextensions.k8s.io/v1/customresourcedefinitions,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:24:59.735) (total time: 1231ms):
Trace[509937204]: ---"Writing http response done" count:23 1226ms (03:25:00.967)
Trace[509937204]: [1.23145855s] [1.23145855s] END
I0322 03:25:00.969711    2308 trace.go:236] Trace[1662005096]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:20f62c91-e099-462e-87e6-57446bbc274d,client:127.0.0.1,protocol:HTTP/2.0,resource:replicasets,scope:cluster,url:/apis/apps/v1/replicasets,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/scheduler,verb:LIST (22-Mar-2024 03:24:59.873) (total time: 1095ms):
Trace[1662005096]: ["cacher list" audit-id:20f62c91-e099-462e-87e6-57446bbc274d,type:replicasets.apps 1095ms (03:24:59.873)]
Trace[1662005096]: ---"Writing http response done" count:3 137ms (03:25:00.969)
Trace[1662005096]: [1.09594339s] [1.09594339s] END
I0322 03:25:00.970039    2308 trace.go:236] Trace[1963838709]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:54829e40-ac41-4779-a94f-b8389b0340f5,client:127.0.0.1,protocol:HTTP/2.0,resource:services,scope:cluster,url:/api/v1/services,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/scheduler,verb:LIST (22-Mar-2024 03:24:59.874) (total time: 1095ms):
Trace[1963838709]: ["cacher list" audit-id:54829e40-ac41-4779-a94f-b8389b0340f5,type:services 1094ms (03:24:59.875)]
Trace[1963838709]: ---"Writing http response done" count:3 137ms (03:25:00.970)
Trace[1963838709]: [1.095315105s] [1.095315105s] END
I0322 03:25:00.970849    2308 trace.go:236] Trace[973649281]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:4c6c3d24-2e0f-4823-884a-b7108f7458d8,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:cluster,url:/api/v1/nodes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/,verb:LIST (22-Mar-2024 03:24:59.894) (total time: 1076ms):
Trace[973649281]: ["cacher list" audit-id:4c6c3d24-2e0f-4823-884a-b7108f7458d8,type:nodes 1076ms (03:24:59.894)]
Trace[973649281]: ---"Writing http response done" count:2 125ms (03:25:00.970)
Trace[973649281]: [1.076249619s] [1.076249619s] END
I0322 03:25:00.971277    2308 trace.go:236] Trace[1715442309]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:f0c4ebf4-7e4d-437e-ab07-f6227b3b03ee,client:127.0.0.1,protocol:HTTP/2.0,resource:services,scope:cluster,url:/api/v1/services,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:24:59.896) (total time: 1075ms):
Trace[1715442309]: ["cacher list" audit-id:f0c4ebf4-7e4d-437e-ab07-f6227b3b03ee,type:services 1075ms (03:24:59.896)]
Trace[1715442309]: ---"Writing http response done" count:3 125ms (03:25:00.971)
Trace[1715442309]: [1.075243335s] [1.075243335s] END
I0322 03:25:00.971646    2308 trace.go:236] Trace[797524768]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:ff15aae4-b7c5-462c-b509-6b7d2e9cc49c,client:127.0.0.1,protocol:HTTP/2.0,resource:pods,scope:namespace,url:/api/v1/namespaces/kube-system/pods,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/,verb:LIST (22-Mar-2024 03:24:59.897) (total time: 1073ms):
Trace[797524768]: ["cacher list" audit-id:ff15aae4-b7c5-462c-b509-6b7d2e9cc49c,type:pods 1073ms (03:24:59.897)]
Trace[797524768]: ---"Writing http response done" count:5 124ms (03:25:00.971)
Trace[797524768]: [1.073567069s] [1.073567069s] END
I0322 03:25:00.972060    2308 trace.go:236] Trace[669178431]: "List" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:5fe06bd5-cd6e-4d1f-a7fa-16f6ebdc716c,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:24:59.981) (total time: 990ms):
Trace[669178431]: ["cacher list" audit-id:5fe06bd5-cd6e-4d1f-a7fa-16f6ebdc716c,type:nodes 989ms (03:24:59.982)]
Trace[669178431]: ---"Writing http response done" count:1 115ms (03:25:00.972)
Trace[669178431]: [990.88398ms] [990.88398ms] END
I0322 03:25:00.972341    2308 trace.go:236] Trace[900054770]: "List" accept:application/json, */*,audit-id:ebe6b061-e93f-46c7-bc94-fdbe9c6a8759,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:cluster,url:/api/v1/nodes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:24:59.983) (total time: 988ms):
Trace[900054770]: ["cacher list" audit-id:ebe6b061-e93f-46c7-bc94-fdbe9c6a8759,type:nodes 988ms (03:24:59.983)]
Trace[900054770]: ---"Writing http response done" count:2 115ms (03:25:00.972)
Trace[900054770]: [988.889049ms] [988.889049ms] END
I0322 03:25:00.972553    2308 trace.go:236] Trace[1087005879]: "List" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:ca9649e9-2e6a-4487-a48f-9a0a60897bd4,client:127.0.0.1,protocol:HTTP/2.0,resource:runtimeclasses,scope:cluster,url:/apis/node.k8s.io/v1/runtimeclasses,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:24:59.984) (total time: 987ms):
Trace[1087005879]: ["cacher list" audit-id:ca9649e9-2e6a-4487-a48f-9a0a60897bd4,type:runtimeclasses.node.k8s.io 986ms (03:24:59.985)]
Trace[1087005879]: ---"Writing http response done" count:10 115ms (03:25:00.972)
Trace[1087005879]: [987.670327ms] [987.670327ms] END
I0322 03:25:00.972824    2308 trace.go:236] Trace[177672992]: "List" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:74ce6c8a-c841-4ee2-b102-2488ce002871,client:127.0.0.1,protocol:HTTP/2.0,resource:pods,scope:cluster,url:/api/v1/pods,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:24:59.989) (total time: 983ms):
Trace[177672992]: ["cacher list" audit-id:74ce6c8a-c841-4ee2-b102-2488ce002871,type:pods 982ms (03:24:59.990)]
Trace[177672992]: ---"Writing http response done" count:5 114ms (03:25:00.972)
Trace[177672992]: [983.25998ms] [983.25998ms] END
I0322 03:25:00.972977    2308 trace.go:236] Trace[831155013]: "List" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:062719a3-7823-4158-8ff1-e1efbe1cf893,client:127.0.0.1,protocol:HTTP/2.0,resource:services,scope:cluster,url:/api/v1/services,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:24:59.992) (total time: 980ms):
Trace[831155013]: ["cacher list" audit-id:062719a3-7823-4158-8ff1-e1efbe1cf893,type:services 979ms (03:24:59.993)]
Trace[831155013]: ---"Writing http response done" count:3 113ms (03:25:00.972)
Trace[831155013]: [980.23946ms] [980.23946ms] END
I0322 03:25:00.982431    2308 trace.go:236] Trace[318230058]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:a4f6e3d5-ca58-4203-871a-c57576842fdf,client:127.0.0.1,protocol:HTTP/2.0,resource:replicasets,scope:cluster,url:/apis/apps/v1/replicasets,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:24:59.826) (total time: 1155ms):
Trace[318230058]: ["cacher list" audit-id:a4f6e3d5-ca58-4203-871a-c57576842fdf,type:replicasets.apps 1155ms (03:24:59.827)]
Trace[318230058]: ---"Writing http response done" count:3 191ms (03:25:00.982)
Trace[318230058]: [1.155620112s] [1.155620112s] END
I0322 03:25:01.000797    2308 trace.go:236] Trace[929816920]: "List" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json,audit-id:0be634ee-6b0e-4567-a89b-0ace6e36c38c,client:127.0.0.1,protocol:HTTP/2.0,resource:ingressrouteudps,scope:cluster,url:/apis/traefik.containo.us/v1alpha1/ingressrouteudps,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/metadata-informers,verb:LIST (22-Mar-2024 03:24:59.826) (total time: 1174ms):
Trace[929816920]: ["cacher list" audit-id:0be634ee-6b0e-4567-a89b-0ace6e36c38c,type:ingressrouteudps.traefik.containo.us 1174ms (03:24:59.826)]
Trace[929816920]: [1.174410817s] [1.174410817s] END
I0322 03:25:01.001332    2308 trace.go:236] Trace[224477663]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:4aa03afb-136c-4cfd-b035-74cf2578f313,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:cluster,url:/api/v1/nodes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:24:59.960) (total time: 1041ms):
Trace[224477663]: ["cacher list" audit-id:4aa03afb-136c-4cfd-b035-74cf2578f313,type:nodes 1041ms (03:24:59.960)]
Trace[224477663]: ---"Writing http response done" count:2 145ms (03:25:01.001)
Trace[224477663]: [1.041295371s] [1.041295371s] END
I0322 03:25:01.002102    2308 trace.go:236] Trace[1144361723]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:f98f2c17-667d-458e-a4ee-7105fe530a0c,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:cluster,url:/api/v1/nodes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:24:59.898) (total time: 1103ms):
Trace[1144361723]: ["cacher list" audit-id:f98f2c17-667d-458e-a4ee-7105fe530a0c,type:nodes 1103ms (03:24:59.898)]
Trace[1144361723]: ---"Writing http response done" count:2 154ms (03:25:01.002)
Trace[1144361723]: [1.103299157s] [1.103299157s] END
I0322 03:25:01.002533    2308 trace.go:236] Trace[1887143888]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:8a9de466-8fb6-40e4-854e-946fba2f88f3,client:127.0.0.1,protocol:HTTP/2.0,resource:serviceaccounts,scope:cluster,url:/api/v1/serviceaccounts,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:25:00.010) (total time: 992ms):
Trace[1887143888]: ["cacher list" audit-id:8a9de466-8fb6-40e4-854e-946fba2f88f3,type:serviceaccounts 991ms (03:25:00.011)]
Trace[1887143888]: ---"Writing http response done" count:40 133ms (03:25:01.002)
Trace[1887143888]: [992.087349ms] [992.087349ms] END
I0322 03:25:01.003073    2308 trace.go:236] Trace[1870712967]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:d0b888c6-2863-433c-a4ed-d4250e82efb8,client:127.0.0.1,protocol:HTTP/2.0,resource:clusterrolebindings,scope:cluster,url:/apis/rbac.authorization.k8s.io/v1/clusterrolebindings,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:25:00.100) (total time: 902ms):
Trace[1870712967]: ["cacher list" audit-id:d0b888c6-2863-433c-a4ed-d4250e82efb8,type:clusterrolebindings.rbac.authorization.k8s.io 901ms (03:25:00.101)]
Trace[1870712967]: ---"Writing http response done" count:54 108ms (03:25:01.003)
Trace[1870712967]: [902.238301ms] [902.238301ms] END
I0322 03:25:01.003522    2308 trace.go:236] Trace[1743150474]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:0e168f94-b879-404b-b20d-795253acf8bf,client:127.0.0.1,protocol:HTTP/2.0,resource:prioritylevelconfigurations,scope:cluster,url:/apis/flowcontrol.apiserver.k8s.io/v1beta3/prioritylevelconfigurations,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:25:00.105) (total time: 898ms):
Trace[1743150474]: ["cacher list" audit-id:0e168f94-b879-404b-b20d-795253acf8bf,type:prioritylevelconfigurations.flowcontrol.apiserver.k8s.io 897ms (03:25:00.105)]
Trace[1743150474]: ---"Writing http response done" count:8 106ms (03:25:01.003)
Trace[1743150474]: [898.145085ms] [898.145085ms] END
I0322 03:25:01.004024    2308 trace.go:236] Trace[1309189205]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:0f57e222-53dd-4338-8c43-f09b4c0e76c7,client:127.0.0.1,protocol:HTTP/2.0,resource:services,scope:cluster,url:/api/v1/services,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:25:00.106) (total time: 897ms):
Trace[1309189205]: ["cacher list" audit-id:0f57e222-53dd-4338-8c43-f09b4c0e76c7,type:services 897ms (03:25:00.106)]
Trace[1309189205]: ---"Writing http response done" count:3 106ms (03:25:01.004)
Trace[1309189205]: [897.196507ms] [897.196507ms] END
I0322 03:25:01.004357    2308 trace.go:236] Trace[70529509]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:1068e238-7414-443b-800e-ab4aa60cd6fd,client:127.0.0.1,protocol:HTTP/2.0,resource:clusterroles,scope:cluster,url:/apis/rbac.authorization.k8s.io/v1/clusterroles,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:25:00.134) (total time: 869ms):
Trace[70529509]: ["cacher list" audit-id:1068e238-7414-443b-800e-ab4aa60cd6fd,type:clusterroles.rbac.authorization.k8s.io 869ms (03:25:00.135)]
Trace[70529509]: ---"Writing http response done" count:69 100ms (03:25:01.004)
Trace[70529509]: [869.340353ms] [869.340353ms] END
I0322 03:25:01.004789    2308 trace.go:236] Trace[1628708538]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:499a6aae-f85d-43de-918d-93627e63a66e,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:cluster,url:/api/v1/configmaps,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:25:00.147) (total time: 856ms):
Trace[1628708538]: ["cacher list" audit-id:499a6aae-f85d-43de-918d-93627e63a66e,type:configmaps 856ms (03:25:00.147)]
Trace[1628708538]: ---"Writing http response done" count:11 98ms (03:25:01.004)
Trace[1628708538]: [856.957557ms] [856.957557ms] END
I0322 03:25:01.005177    2308 trace.go:236] Trace[1536381114]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:0e96edfe-fdef-4bba-8c9b-4b95675a2c2c,client:127.0.0.1,protocol:HTTP/2.0,resource:pods,scope:cluster,url:/api/v1/pods,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:25:00.169) (total time: 836ms):
Trace[1536381114]: ["cacher list" audit-id:0e96edfe-fdef-4bba-8c9b-4b95675a2c2c,type:pods 835ms (03:25:00.170)]
Trace[1536381114]: ---"Writing http response done" count:5 98ms (03:25:01.005)
Trace[1536381114]: [836.012957ms] [836.012957ms] END
I0322 03:25:01.005486    2308 trace.go:236] Trace[952206359]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:1ecf22c2-8c3e-4744-8996-7ac003c07c2a,client:127.0.0.1,protocol:HTTP/2.0,resource:deployments,scope:cluster,url:/apis/apps/v1/deployments,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:25:00.175) (total time: 830ms):
Trace[952206359]: ["cacher list" audit-id:1ecf22c2-8c3e-4744-8996-7ac003c07c2a,type:deployments.apps 829ms (03:25:00.175)]
Trace[952206359]: ---"Writing http response done" count:3 97ms (03:25:01.005)
Trace[952206359]: [830.143962ms] [830.143962ms] END
I0322 03:25:01.005822    2308 trace.go:236] Trace[1889203432]: "List" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json,audit-id:5d18b230-ec8b-44e8-8161-87c36e82185c,client:127.0.0.1,protocol:HTTP/2.0,resource:addons,scope:cluster,url:/apis/k3s.cattle.io/v1/addons,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/metadata-informers,verb:LIST (22-Mar-2024 03:25:00.176) (total time: 828ms):
Trace[1889203432]: ["cacher list" audit-id:5d18b230-ec8b-44e8-8161-87c36e82185c,type:addons.k3s.cattle.io 828ms (03:25:00.176)]
Trace[1889203432]: ---"Writing http response done" count:13 97ms (03:25:01.005)
Trace[1889203432]: [828.943016ms] [828.943016ms] END
I0322 03:25:01.006100    2308 trace.go:236] Trace[159572584]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:0ed39404-606d-4c71-b917-1e418193be2f,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:cluster,url:/api/v1/nodes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:25:00.179) (total time: 826ms):
Trace[159572584]: ["cacher list" audit-id:0ed39404-606d-4c71-b917-1e418193be2f,type:nodes 826ms (03:25:00.179)]
Trace[159572584]: ---"Writing http response done" count:2 96ms (03:25:01.006)
Trace[159572584]: [826.723808ms] [826.723808ms] END
I0322 03:25:01.006860    2308 trace.go:236] Trace[35797999]: "List" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json,audit-id:71a6abba-7ac3-45af-a062-33389a0a0cb2,client:127.0.0.1,protocol:HTTP/2.0,resource:ingressroutetcps,scope:cluster,url:/apis/traefik.io/v1alpha1/ingressroutetcps,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/metadata-informers,verb:LIST (22-Mar-2024 03:24:59.829) (total time: 1176ms):
Trace[35797999]: ["cacher list" audit-id:71a6abba-7ac3-45af-a062-33389a0a0cb2,type:ingressroutetcps.traefik.io 1176ms (03:24:59.830)]
Trace[35797999]: [1.176847812s] [1.176847812s] END
I0322 03:25:01.007279    2308 trace.go:236] Trace[655719420]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:0560f02b-a732-45f1-9525-fb2ae234d9bf,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:cluster,url:/api/v1/nodes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/scheduler,verb:LIST (22-Mar-2024 03:24:59.884) (total time: 1122ms):
Trace[655719420]: ["cacher list" audit-id:0560f02b-a732-45f1-9525-fb2ae234d9bf,type:nodes 1122ms (03:24:59.884)]
Trace[655719420]: ---"Writing http response done" count:2 167ms (03:25:01.007)
Trace[655719420]: [1.122792198s] [1.122792198s] END
I0322 03:25:01.007588    2308 trace.go:236] Trace[994006910]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:898cd0b6-6c88-4823-8a13-5e3d1cdd344f,client:127.0.0.1,protocol:HTTP/2.0,resource:pods,scope:cluster,url:/api/v1/pods,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/scheduler,verb:LIST (22-Mar-2024 03:24:59.885) (total time: 1121ms):
Trace[994006910]: ["cacher list" audit-id:898cd0b6-6c88-4823-8a13-5e3d1cdd344f,type:pods 1120ms (03:24:59.886)]
Trace[994006910]: ---"Writing http response done" count:4 167ms (03:25:01.007)
Trace[994006910]: [1.121885688s] [1.121885688s] END
I0322 03:25:01.007886    2308 trace.go:236] Trace[718128237]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:169598fe-f5b2-4064-8af5-89247bf6babc,client:127.0.0.1,protocol:HTTP/2.0,resource:jobs,scope:cluster,url:/apis/batch/v1/jobs,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:25:00.053) (total time: 954ms):
Trace[718128237]: ["cacher list" audit-id:169598fe-f5b2-4064-8af5-89247bf6babc,type:jobs.batch 954ms (03:25:00.053)]
Trace[718128237]: ---"Writing http response done" count:2 138ms (03:25:01.007)
Trace[718128237]: [954.753991ms] [954.753991ms] END
I0322 03:25:01.008211    2308 trace.go:236] Trace[1516717562]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:7bc06dc8-5867-4f3f-a924-3981335f7e5e,client:127.0.0.1,protocol:HTTP/2.0,resource:flowschemas,scope:cluster,url:/apis/flowcontrol.apiserver.k8s.io/v1beta3/flowschemas,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:25:00.054) (total time: 953ms):
Trace[1516717562]: ["cacher list" audit-id:7bc06dc8-5867-4f3f-a924-3981335f7e5e,type:flowschemas.flowcontrol.apiserver.k8s.io 952ms (03:25:00.055)]
Trace[1516717562]: ---"Writing http response done" count:13 134ms (03:25:01.008)
Trace[1516717562]: [953.557053ms] [953.557053ms] END
I0322 03:25:01.008352    2308 trace.go:236] Trace[841279847]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:e13c839a-ccc1-410f-92c5-821d6615dc66,client:127.0.0.1,protocol:HTTP/2.0,resource:runtimeclasses,scope:cluster,url:/apis/node.k8s.io/v1/runtimeclasses,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:25:00.080) (total time: 927ms):
Trace[841279847]: ["cacher list" audit-id:e13c839a-ccc1-410f-92c5-821d6615dc66,type:runtimeclasses.node.k8s.io 927ms (03:25:00.080)]
Trace[841279847]: ---"Writing http response done" count:10 122ms (03:25:01.008)
Trace[841279847]: [927.832324ms] [927.832324ms] END
I0322 03:25:01.008604    2308 trace.go:236] Trace[1887130579]: "List" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json,audit-id:1fd80be3-fe31-4eaa-b26a-0c488861aa60,client:127.0.0.1,protocol:HTTP/2.0,resource:customresourcedefinitions,scope:cluster,url:/apis/apiextensions.k8s.io/v1/customresourcedefinitions,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/metadata-informers,verb:LIST (22-Mar-2024 03:25:00.063) (total time: 945ms):
Trace[1887130579]: ["cacher list" audit-id:1fd80be3-fe31-4eaa-b26a-0c488861aa60,type:customresourcedefinitions.apiextensions.k8s.io 939ms (03:25:00.068)]
Trace[1887130579]: ---"Writing http response done" count:23 131ms (03:25:01.008)
Trace[1887130579]: [945.316121ms] [945.316121ms] END
I0322 03:25:01.009054    2308 trace.go:236] Trace[838410987]: "List" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json,audit-id:9c14bc9c-de36-4810-9218-c0677b19f226,client:127.0.0.1,protocol:HTTP/2.0,resource:ingressroutetcps,scope:cluster,url:/apis/traefik.containo.us/v1alpha1/ingressroutetcps,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/metadata-informers,verb:LIST (22-Mar-2024 03:24:59.836) (total time: 1172ms):
Trace[838410987]: ["cacher list" audit-id:9c14bc9c-de36-4810-9218-c0677b19f226,type:ingressroutetcps.traefik.containo.us 1172ms (03:24:59.836)]
Trace[838410987]: [1.17213639s] [1.17213639s] END
I0322 03:25:01.009788    2308 trace.go:236] Trace[893085801]: "List" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json,audit-id:5bddeb50-1733-497b-977b-9ffc4b1a1a66,client:127.0.0.1,protocol:HTTP/2.0,resource:middlewaretcps,scope:cluster,url:/apis/traefik.io/v1alpha1/middlewaretcps,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/metadata-informers,verb:LIST (22-Mar-2024 03:24:59.839) (total time: 1169ms):
Trace[893085801]: ["cacher list" audit-id:5bddeb50-1733-497b-977b-9ffc4b1a1a66,type:middlewaretcps.traefik.io 1169ms (03:24:59.839)]
Trace[893085801]: [1.169984519s] [1.169984519s] END
I0322 03:25:01.010477    2308 trace.go:236] Trace[1434022696]: "List" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json,audit-id:ef35a3b4-40b7-4e27-aec3-743e56c8987e,client:127.0.0.1,protocol:HTTP/2.0,resource:ingressrouteudps,scope:cluster,url:/apis/traefik.io/v1alpha1/ingressrouteudps,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/metadata-informers,verb:LIST (22-Mar-2024 03:25:00.045) (total time: 964ms):
Trace[1434022696]: ["cacher list" audit-id:ef35a3b4-40b7-4e27-aec3-743e56c8987e,type:ingressrouteudps.traefik.io 963ms (03:25:00.046)]
Trace[1434022696]: [964.728451ms] [964.728451ms] END
I0322 03:25:01.010823    2308 trace.go:236] Trace[1663692236]: "List" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json,audit-id:2c30f032-ad1a-42d8-852e-036d0cf212c4,client:127.0.0.1,protocol:HTTP/2.0,resource:traefikservices,scope:cluster,url:/apis/traefik.io/v1alpha1/traefikservices,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/metadata-informers,verb:LIST (22-Mar-2024 03:25:00.081) (total time: 929ms):
Trace[1663692236]: ["cacher list" audit-id:2c30f032-ad1a-42d8-852e-036d0cf212c4,type:traefikservices.traefik.io 928ms (03:25:00.081)]
Trace[1663692236]: [929.027488ms] [929.027488ms] END
I0322 03:25:01.011197    2308 trace.go:236] Trace[497631222]: "List" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json,audit-id:d9e085d0-857a-4af3-bc01-b5ee2483450c,client:127.0.0.1,protocol:HTTP/2.0,resource:middlewaretcps,scope:cluster,url:/apis/traefik.containo.us/v1alpha1/middlewaretcps,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/metadata-informers,verb:LIST (22-Mar-2024 03:25:00.082) (total time: 928ms):
Trace[497631222]: ["cacher list" audit-id:d9e085d0-857a-4af3-bc01-b5ee2483450c,type:middlewaretcps.traefik.containo.us 928ms (03:25:00.082)]
Trace[497631222]: [928.392289ms] [928.392289ms] END
I0322 03:25:01.011616    2308 trace.go:236] Trace[202367470]: "List" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json,audit-id:083ec5b6-c29d-458b-b317-eb1c98da0a66,client:127.0.0.1,protocol:HTTP/2.0,resource:ingressroutes,scope:cluster,url:/apis/traefik.io/v1alpha1/ingressroutes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/metadata-informers,verb:LIST (22-Mar-2024 03:25:00.119) (total time: 892ms):
Trace[202367470]: ["cacher list" audit-id:083ec5b6-c29d-458b-b317-eb1c98da0a66,type:ingressroutes.traefik.io 892ms (03:25:00.119)]
Trace[202367470]: [892.166858ms] [892.166858ms] END
I0322 03:25:01.012194    2308 trace.go:236] Trace[1404321197]: "List" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json,audit-id:d9a171a8-ad60-429c-a77d-151530489401,client:127.0.0.1,protocol:HTTP/2.0,resource:tlsstores,scope:cluster,url:/apis/traefik.io/v1alpha1/tlsstores,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/metadata-informers,verb:LIST (22-Mar-2024 03:25:00.121) (total time: 890ms):
Trace[1404321197]: ["cacher list" audit-id:d9a171a8-ad60-429c-a77d-151530489401,type:tlsstores.traefik.io 890ms (03:25:00.121)]
Trace[1404321197]: [890.999259ms] [890.999259ms] END
I0322 03:25:01.012707    2308 trace.go:236] Trace[123362401]: "List" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json,audit-id:4f95634b-00a3-4858-9344-12b94ee8cdb1,client:127.0.0.1,protocol:HTTP/2.0,resource:serverstransports,scope:cluster,url:/apis/traefik.containo.us/v1alpha1/serverstransports,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/metadata-informers,verb:LIST (22-Mar-2024 03:25:00.123) (total time: 889ms):
Trace[123362401]: ["cacher list" audit-id:4f95634b-00a3-4858-9344-12b94ee8cdb1,type:serverstransports.traefik.containo.us 889ms (03:25:00.123)]
Trace[123362401]: [889.680343ms] [889.680343ms] END
I0322 03:25:01.013123    2308 trace.go:236] Trace[818465831]: "List" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json,audit-id:010d0447-8096-4951-b643-f1da93ebfd37,client:127.0.0.1,protocol:HTTP/2.0,resource:middlewares,scope:cluster,url:/apis/traefik.containo.us/v1alpha1/middlewares,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/metadata-informers,verb:LIST (22-Mar-2024 03:25:00.133) (total time: 879ms):
Trace[818465831]: ["cacher list" audit-id:010d0447-8096-4951-b643-f1da93ebfd37,type:middlewares.traefik.containo.us 879ms (03:25:00.133)]
Trace[818465831]: [879.924849ms] [879.924849ms] END
I0322 03:25:01.013427    2308 trace.go:236] Trace[1126479346]: "List" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json,audit-id:f1684cc7-7a22-4438-ad80-8b2dbd796e01,client:127.0.0.1,protocol:HTTP/2.0,resource:serverstransporttcps,scope:cluster,url:/apis/traefik.io/v1alpha1/serverstransporttcps,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/metadata-informers,verb:LIST (22-Mar-2024 03:25:00.172) (total time: 841ms):
Trace[1126479346]: ["cacher list" audit-id:f1684cc7-7a22-4438-ad80-8b2dbd796e01,type:serverstransporttcps.traefik.io 841ms (03:25:00.172)]
Trace[1126479346]: [841.409463ms] [841.409463ms] END
I0322 03:25:01.013813    2308 trace.go:236] Trace[846285154]: "List" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json,audit-id:ef5f2215-ac62-47b4-aca2-2519d9238150,client:127.0.0.1,protocol:HTTP/2.0,resource:middlewares,scope:cluster,url:/apis/traefik.io/v1alpha1/middlewares,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/metadata-informers,verb:LIST (22-Mar-2024 03:25:00.174) (total time: 839ms):
Trace[846285154]: ["cacher list" audit-id:ef5f2215-ac62-47b4-aca2-2519d9238150,type:middlewares.traefik.io 839ms (03:25:00.174)]
Trace[846285154]: [839.558819ms] [839.558819ms] END
I0322 03:25:01.016289    2308 trace.go:236] Trace[799933968]: "List" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json,audit-id:49876cb0-c6ca-4925-954c-0b4b31414467,client:127.0.0.1,protocol:HTTP/2.0,resource:tlsstores,scope:cluster,url:/apis/traefik.containo.us/v1alpha1/tlsstores,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/metadata-informers,verb:LIST (22-Mar-2024 03:25:00.178) (total time: 837ms):
Trace[799933968]: ["cacher list" audit-id:49876cb0-c6ca-4925-954c-0b4b31414467,type:tlsstores.traefik.containo.us 838ms (03:25:00.178)]
Trace[799933968]: [837.910517ms] [837.910517ms] END
I0322 03:25:01.016821    2308 trace.go:236] Trace[2117999847]: "List" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json,audit-id:f8c054db-dfa4-42ce-876f-94cd18b81871,client:127.0.0.1,protocol:HTTP/2.0,resource:tlsoptions,scope:cluster,url:/apis/traefik.io/v1alpha1/tlsoptions,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/metadata-informers,verb:LIST (22-Mar-2024 03:25:00.182) (total time: 834ms):
Trace[2117999847]: ["cacher list" audit-id:f8c054db-dfa4-42ce-876f-94cd18b81871,type:tlsoptions.traefik.io 834ms (03:25:00.182)]
Trace[2117999847]: [834.369255ms] [834.369255ms] END
I0322 03:25:01.049092    2308 trace.go:236] Trace[679181986]: "List" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json,audit-id:05b856c1-5ac9-445f-9334-0c860adaed92,client:127.0.0.1,protocol:HTTP/2.0,resource:etcdsnapshotfiles,scope:cluster,url:/apis/k3s.cattle.io/v1/etcdsnapshotfiles,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/metadata-informers,verb:LIST (22-Mar-2024 03:25:00.003) (total time: 1045ms):
Trace[679181986]: [1.045138827s] [1.045138827s] END
I0322 03:25:01.166654    2308 event.go:307] "Event occurred" object="kube-system/traefik-crd" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik-crd"
I0322 03:25:01.209823    2308 trace.go:236] Trace[857356751]: "List" accept:application/json, */*,audit-id:0888324e-c80b-48ba-978a-7dcb2968ea1d,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:25:00.670) (total time: 539ms):
Trace[857356751]: ---"Writing http response done" count:1 537ms (03:25:01.209)
Trace[857356751]: [539.090166ms] [539.090166ms] END
E0322 03:25:01.211128    2308 finisher.go:175] FinishRequest: post-timeout activity - time-elapsed: 7.00134122s, panicked: false, err: context deadline exceeded, panic-reason: <nil>
I0322 03:25:01.211603    2308 trace.go:236] Trace[1370951349]: "List" accept:application/json, */*,audit-id:d7bcb7b1-b1ab-4aca-a22a-2b34406a9dc9,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:25:00.679) (total time: 532ms):
Trace[1370951349]: ---"Writing http response done" count:1 532ms (03:25:01.211)
Trace[1370951349]: [532.290785ms] [532.290785ms] END
E0322 03:25:01.221850    2308 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:25:01.223758    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:25:01.224367    2308 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 03:25:01.225977    2308 trace.go:236] Trace[384305083]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:37167b8f-c09c-479a-8dbe-34e1d2bbfa24,client:192.168.56.111,protocol:HTTP/2.0,resource:tokenreviews,scope:resource,url:/apis/authentication.k8s.io/v1/tokenreviews,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:POST (22-Mar-2024 03:24:59.403) (total time: 1821ms):
Trace[384305083]: ---"About to store object in database" 894ms (03:25:00.307)
Trace[384305083]: [1.821925124s] [1.821925124s] END
E0322 03:25:01.227051    2308 timeout.go:142] post-timeout activity - time-elapsed: 4.085710669s, POST "/apis/authentication.k8s.io/v1/tokenreviews" result: <nil>
E0322 03:25:01.228642    2308 finisher.go:175] FinishRequest: post-timeout activity - time-elapsed: 4.605323827s, panicked: false, err: context deadline exceeded, panic-reason: <nil>
I0322 03:25:01.260848    2308 trace.go:236] Trace[1335790302]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:cf4f7b1a-0d06-4fd2-b63a-952fdcbda046,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:24:56.826) (total time: 4434ms):
Trace[1335790302]: ---"About to write a response" 4433ms (03:25:01.259)
Trace[1335790302]: [4.434703588s] [4.434703588s] END
E0322 03:25:01.297966    2308 finisher.go:175] FinishRequest: post-timeout activity - time-elapsed: 651.819778ms, panicked: false, err: <nil>, panic-reason: <nil>
I0322 03:25:01.309583    2308 trace.go:236] Trace[900677269]: "List" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json,audit-id:57b79272-64a9-440a-b9e1-8e2045aebba9,client:127.0.0.1,protocol:HTTP/2.0,resource:tlsoptions,scope:cluster,url:/apis/traefik.containo.us/v1alpha1/tlsoptions,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/metadata-informers,verb:LIST (22-Mar-2024 03:24:59.815) (total time: 1493ms):
Trace[900677269]: ["cacher list" audit-id:57b79272-64a9-440a-b9e1-8e2045aebba9,type:tlsoptions.traefik.containo.us 1491ms (03:24:59.817)]
Trace[900677269]: ---"Writing http response done" count:0 798ms (03:25:01.308)
Trace[900677269]: [1.493647155s] [1.493647155s] END
I0322 03:25:01.408821    2308 node_controller.go:431] Initializing node serverworker with cloud provider
I0322 03:25:01.482146    2308 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
I0322 03:25:01.523727    2308 trace.go:236] Trace[417761745]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:a037016a-483f-4228-9cba-6e6a7838e3bb,client:127.0.0.1,protocol:HTTP/2.0,resource:certificatesigningrequests,scope:cluster,url:/apis/certificates.k8s.io/v1/certificatesigningrequests,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:24:59.823) (total time: 1697ms):
Trace[417761745]: ["cacher list" audit-id:a037016a-483f-4228-9cba-6e6a7838e3bb,type:certificatesigningrequests.certificates.k8s.io 1697ms (03:24:59.823)]
Trace[417761745]: ---"Writing http response done" count:0 773ms (03:25:01.520)
Trace[417761745]: [1.697321739s] [1.697321739s] END
E0322 03:25:01.577984    2308 finisher.go:175] FinishRequest: post-timeout activity - time-elapsed: 3.964932135s, panicked: false, err: context deadline exceeded, panic-reason: <nil>
I0322 03:25:01.976441    2308 trace.go:236] Trace[591510934]: "List" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json,audit-id:25449cb2-67c9-42ce-b493-03eeeffad569,client:127.0.0.1,protocol:HTTP/2.0,resource:apiservices,scope:cluster,url:/apis/apiregistration.k8s.io/v1/apiservices,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/metadata-informers,verb:LIST (22-Mar-2024 03:24:59.827) (total time: 2145ms):
Trace[591510934]: ["cacher list" audit-id:25449cb2-67c9-42ce-b493-03eeeffad569,type:apiservices.apiregistration.k8s.io 2145ms (03:24:59.827)]
Trace[591510934]: ---"Writing http response done" count:26 1171ms (03:25:01.972)
Trace[591510934]: [2.145066875s] [2.145066875s] END
E0322 03:25:02.049632    2308 node_controller.go:240] error syncing 'serverworker': failed to get instance metadata for node serverworker: address annotations not yet set, requeuing
I0322 03:25:02.078536    2308 trace.go:236] Trace[1997109374]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:fa946e3a-2918-4e3e-aac7-1c639f47a092,client:127.0.0.1,protocol:HTTP/2.0,resource:replicationcontrollers,scope:cluster,url:/api/v1/replicationcontrollers,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:25:01.486) (total time: 591ms):
Trace[1997109374]: ---"Writing http response done" count:0 580ms (03:25:02.077)
Trace[1997109374]: [591.619953ms] [591.619953ms] END
I0322 03:25:02.110612    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="13.235162ms"
time="2024-03-22T03:25:02Z" level=info msg="Slow SQL (started: 2024-03-22 03:25:00.872191532 +0000 UTC m=+456.567004776) (total time: 1.263861236s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC  : [[/registry/minions/serverworker false]]"
I0322 03:25:02.138578    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="87.996s"
I0322 03:25:02.138832    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="38.725s"
time="2024-03-22T03:25:02Z" level=info msg="Slow SQL (started: 2024-03-22 03:25:01.038440768 +0000 UTC m=+456.733253968) (total time: 1.217309509s): SELECT SUM(pgsize) FROM dbstat : [[]]"
I0322 03:25:02.885210    2308 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:25:02.885559    2308 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:25:02.888128    2308 trace.go:236] Trace[507763022]: "DeltaFIFO Pop Process" ID:kube-node-lease/default,Depth:38,Reason:slow event handlers blocking the queue (22-Mar-2024 03:25:02.462) (total time: 423ms):
Trace[507763022]: [423.093753ms] [423.093753ms] END
I0322 03:25:02.888699    2308 trace.go:236] Trace[1720518562]: "List" accept:application/json;as=Table;v=v1;g=meta.k8s.io,application/json;as=Table;v=v1beta1;g=meta.k8s.io,application/json,audit-id:7eec99ed-2390-4a85-958c-0f0973f2341f,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:cluster,url:/api/v1/nodes,user-agent:kubectl/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:24:58.707) (total time: 4180ms):
Trace[1720518562]: ["List(recursive=true) etcd3" audit-id:7eec99ed-2390-4a85-958c-0f0973f2341f,key:/minions,resourceVersion:,resourceVersionMatch:,limit:500,continue: 3353ms (03:24:59.534)]
Trace[1720518562]: ---"Writing http response done" count:2 1752ms (03:25:02.888)
Trace[1720518562]: [4.180809885s] [4.180809885s] END
E0322 03:25:02.947563    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
E0322 03:25:02.950107    2308 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:25:02.950223    2308 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:25:02.951790    2308 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 03:25:02.952043    2308 trace.go:236] Trace[321694620]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:2fbe2db1-8639-4113-9ea9-d0decd781bd7,client:192.168.56.111,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:24:58.513) (total time: 4438ms):
Trace[321694620]: [4.438855175s] [4.438855175s] END
E0322 03:25:02.953486    2308 timeout.go:142] post-timeout activity - time-elapsed: 14.494764ms, GET "/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/serverworker" result: <nil>
I0322 03:25:02.971846    2308 trace.go:236] Trace[1373314907]: "DeltaFIFO Pop Process" ID:cluster-admin,Depth:67,Reason:slow event handlers blocking the queue (22-Mar-2024 03:25:02.687) (total time: 283ms):
Trace[1373314907]: [283.340082ms] [283.340082ms] END
I0322 03:25:02.976978    2308 trace.go:236] Trace[1054441580]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:f9160987-b4d3-4209-a265-5a1328e995b1,client:192.168.56.111,protocol:HTTP/2.0,resource:csinodes,scope:resource,url:/apis/storage.k8s.io/v1/csinodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:24:58.429) (total time: 4547ms):
Trace[1054441580]: [4.547143014s] [4.547143014s] END
I0322 03:25:03.075231    2308 trace.go:236] Trace[118365349]: "DeltaFIFO Pop Process" ID:kube-system/attachdetach-controller,Depth:36,Reason:slow event handlers blocking the queue (22-Mar-2024 03:25:02.969) (total time: 104ms):
Trace[118365349]: [104.805892ms] [104.805892ms] END
E0322 03:25:03.255778    2308 kubelet.go:2477] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1m23.799s"
I0322 03:25:03.332001    2308 trace.go:236] Trace[1267787234]: "Update" accept:application/json, */*,audit-id:39d97f98-1e13-4836-83df-fbd62c6e9433,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PUT (22-Mar-2024 03:24:59.597) (total time: 3724ms):
Trace[1267787234]: ---"limitedReadBody succeeded" len:3315 14ms (03:24:59.612)
Trace[1267787234]: ["GuaranteedUpdate etcd3" audit-id:39d97f98-1e13-4836-83df-fbd62c6e9433,key:/minions/serverworker,type:*core.Node,resource:nodes 3701ms (03:24:59.621)
Trace[1267787234]:  ---"About to Encode" 978ms (03:25:00.599)
Trace[1267787234]:  ---"Encode succeeded" len:2444 507ms (03:25:01.106)
Trace[1267787234]:  ---"Txn call completed" 2213ms (03:25:03.321)]
Trace[1267787234]: [3.724753181s] [3.724753181s] END
I0322 03:25:03.472322    2308 trace.go:236] Trace[808698645]: "List" accept:application/json, */*,audit-id:66cfa31a-d5f4-44ce-a385-bd835bc569b4,client:127.0.0.1,protocol:HTTP/2.0,resource:pods,scope:cluster,url:/api/v1/pods,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:25:02.762) (total time: 709ms):
Trace[808698645]: ["cacher list" audit-id:66cfa31a-d5f4-44ce-a385-bd835bc569b4,type:pods 707ms (03:25:02.764)
Trace[808698645]:  ---"watchCache locked acquired" 529ms (03:25:03.294)]
Trace[808698645]: ---"Writing http response done" count:5 177ms (03:25:03.472)
Trace[808698645]: [709.233413ms] [709.233413ms] END
I0322 03:25:03.472783    2308 trace.go:236] Trace[1586032946]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:fcbda234-597d-44bf-9777-42de567c1d73,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/server,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:24:59.944) (total time: 3523ms):
Trace[1586032946]: ---"About to write a response" 3376ms (03:25:03.320)
Trace[1586032946]: ---"Writing http response done" 147ms (03:25:03.467)
Trace[1586032946]: [3.523481009s] [3.523481009s] END
I0322 03:25:03.496636    2308 trace.go:236] Trace[1833652626]: "DeltaFIFO Pop Process" ID:etcdsnapshotfiles.k3s.cattle.io,Depth:21,Reason:slow event handlers blocking the queue (22-Mar-2024 03:25:03.372) (total time: 122ms):
Trace[1833652626]: [122.804845ms] [122.804845ms] END
I0322 03:25:03.756894    2308 trace.go:236] Trace[1109935790]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:a9c4c92c-1ffa-4104-9930-61c05ee41d70,client:127.0.0.1,protocol:HTTP/2.0,resource:cronjobs,scope:cluster,url:/apis/batch/v1/cronjobs,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:25:03.200) (total time: 551ms):
Trace[1109935790]: ---"Writing http response done" count:0 547ms (03:25:03.752)
Trace[1109935790]: [551.601917ms] [551.601917ms] END
I0322 03:25:03.764519    2308 trace.go:236] Trace[963573351]: "List" accept:application/json, */*,audit-id:cbe4afba-0e71-4e8e-b442-eb1070fcf08f,client:127.0.0.1,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/default/endpoints,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:25:02.568) (total time: 655ms):
Trace[963573351]: ---"Writing http response done" count:1 650ms (03:25:03.224)
Trace[963573351]: [655.365272ms] [655.365272ms] END
I0322 03:25:04.551903    2308 node_controller.go:431] Initializing node serverworker with cloud provider
I0322 03:25:04.594626    2308 trace.go:236] Trace[378897501]: "Get" accept:application/json, */*,audit-id:dac64bd4-c75c-414a-a1d8-cde5fe59079f,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps/coredns,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:GET (22-Mar-2024 03:25:03.316) (total time: 1276ms):
Trace[378897501]: ---"About to write a response" 1274ms (03:25:04.591)
Trace[378897501]: [1.276706317s] [1.276706317s] END
I0322 03:25:04.604016    2308 trace.go:236] Trace[1279339293]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:02935c05-0261-4d2a-9f04-f9d1da691000,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/server/status,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:node-controller,verb:PUT (22-Mar-2024 03:24:59.544) (total time: 5058ms):
Trace[1279339293]: ---"limitedReadBody succeeded" len:4817 72ms (03:24:59.617)
Trace[1279339293]: ["GuaranteedUpdate etcd3" audit-id:02935c05-0261-4d2a-9f04-f9d1da691000,key:/minions/server,type:*core.Node,resource:nodes 4982ms (03:24:59.621)
Trace[1279339293]:  ---"About to Encode" 984ms (03:25:00.606)
Trace[1279339293]:  ---"Txn call completed" 3710ms (03:25:04.318)]
Trace[1279339293]: ---"Writing http response done" 281ms (03:25:04.603)
Trace[1279339293]: [5.058926996s] [5.058926996s] END
I0322 03:25:04.676341    2308 trace.go:236] Trace[214486885]: "Get" accept:application/json, */*,audit-id:b8b24fd7-431f-4a80-b20e-7ca85430794d,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:24:58.031) (total time: 6645ms):
Trace[214486885]: ---"About to write a response" 6262ms (03:25:04.293)
Trace[214486885]: ---"Writing http response done" 382ms (03:25:04.676)
Trace[214486885]: [6.645043511s] [6.645043511s] END
I0322 03:25:04.684651    2308 trace.go:236] Trace[225529849]: "List" accept:application/json, */*,audit-id:d6d2f894-7575-4521-b057-f71dda071407,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:25:03.920) (total time: 763ms):
Trace[225529849]: ---"Writing http response done" count:1 759ms (03:25:04.684)
Trace[225529849]: [763.901617ms] [763.901617ms] END
I0322 03:25:04.719983    2308 trace.go:236] Trace[1923123181]: "Get" accept:application/json, */*,audit-id:98b6b3da-42f0-4f73-90d3-564581885046,client:127.0.0.1,protocol:HTTP/2.0,resource:secrets,scope:resource,url:/api/v1/namespaces/kube-system/secrets/k3s-serving,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:GET (22-Mar-2024 03:25:01.401) (total time: 3318ms):
Trace[1923123181]: ---"About to write a response" 3317ms (03:25:04.719)
Trace[1923123181]: [3.318458079s] [3.318458079s] END
I0322 03:25:04.728072    2308 trace.go:236] Trace[1866068555]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:345ae651-d30e-481e-bd29-89154307e695,client:127.0.0.1,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/default/events,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:POST (22-Mar-2024 03:24:59.988) (total time: 4739ms):
Trace[1866068555]: ---"limitedReadBody succeeded" len:263 62ms (03:25:00.050)
Trace[1866068555]: ["Create etcd3" audit-id:345ae651-d30e-481e-bd29-89154307e695,key:/events/default/server.17bef82d64d2e9c9,type:*core.Event,resource:events 3596ms (03:25:01.131)
Trace[1866068555]:  ---"TransformToStorage succeeded" 1892ms (03:25:03.025)
Trace[1866068555]:  ---"Txn call succeeded" 1695ms (03:25:04.720)]
Trace[1866068555]: [4.739291289s] [4.739291289s] END
I0322 03:25:04.802540    2308 trace.go:236] Trace[1066162949]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:25080364-1019-42ff-bb48-7165a8d02c66,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/server,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PUT (22-Mar-2024 03:24:59.957) (total time: 4844ms):
Trace[1066162949]: ["GuaranteedUpdate etcd3" audit-id:25080364-1019-42ff-bb48-7165a8d02c66,key:/leases/kube-node-lease/server,type:*coordination.Lease,resource:leases.coordination.k8s.io 3779ms (03:25:01.022)
Trace[1066162949]:  ---"About to Encode" 1225ms (03:25:02.249)
Trace[1066162949]:  ---"Txn call completed" 2549ms (03:25:04.800)]
Trace[1066162949]: [4.844516824s] [4.844516824s] END
I0322 03:25:04.804045    2308 trace.go:236] Trace[2022834781]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:9818ccec-5a51-4aa9-976e-3cbb4d1cb50a,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-7zoch227uv4owxg75pgtnmv3jq,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PUT (22-Mar-2024 03:24:59.684) (total time: 5119ms):
Trace[2022834781]: ["GuaranteedUpdate etcd3" audit-id:9818ccec-5a51-4aa9-976e-3cbb4d1cb50a,key:/leases/kube-system/apiserver-7zoch227uv4owxg75pgtnmv3jq,type:*coordination.Lease,resource:leases.coordination.k8s.io 4853ms (03:24:59.950)
Trace[2022834781]:  ---"initial value restored" 1054ms (03:25:01.004)
Trace[2022834781]:  ---"About to Encode" 2109ms (03:25:03.114)
Trace[2022834781]:  ---"Txn call completed" 1590ms (03:25:04.706)]
Trace[2022834781]: ---"Write to database call succeeded" len:590 91ms (03:25:04.803)
Trace[2022834781]: [5.119353433s] [5.119353433s] END
I0322 03:25:04.872988    2308 trace.go:236] Trace[114538894]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:f327d6a7-495e-4b79-bbd1-7866b9cb8e6d,client:192.168.56.111,protocol:HTTP/2.0,resource:subjectaccessreviews,scope:resource,url:/apis/authorization.k8s.io/v1/subjectaccessreviews,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:POST (22-Mar-2024 03:25:02.934) (total time: 1938ms):
Trace[114538894]: ---"About to convert to expected version" 574ms (03:25:03.510)
Trace[114538894]: ---"Write to database call succeeded" len:461 1347ms (03:25:04.862)
Trace[114538894]: [1.938343373s] [1.938343373s] END
E0322 03:25:04.978017    2308 resource_quota_controller.go:440] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
time="2024-03-22T03:25:05Z" level=info msg="Updating TLS secret for kube-system/k3s-serving (count: 10): map[listener.cattle.io/cn-10.43.0.1:10.43.0.1 listener.cattle.io/cn-127.0.0.1:127.0.0.1 listener.cattle.io/cn-192.168.56.110:192.168.56.110 listener.cattle.io/cn-__1-f16284:::1 listener.cattle.io/cn-kubernetes:kubernetes listener.cattle.io/cn-kubernetes.default:kubernetes.default listener.cattle.io/cn-kubernetes.default.svc:kubernetes.default.svc listener.cattle.io/cn-kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local listener.cattle.io/cn-localhost:localhost listener.cattle.io/cn-server:server listener.cattle.io/fingerprint:SHA1=296AFC6A252966C067401FF9072578D9CE0A72C7]"
I0322 03:25:05.163518    2308 event.go:307] "Event occurred" object="server" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="NodeNotReady" message="Node server status is now: NodeNotReady"
I0322 03:25:05.203512    2308 trace.go:236] Trace[164479967]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:dcf652a5-c527-4a15-b5f3-31784fad0e5e,client:127.0.0.1,protocol:HTTP/2.0,resource:services,scope:cluster,url:/api/v1/services,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:25:04.509) (total time: 690ms):
Trace[164479967]: ---"Writing http response done" count:3 686ms (03:25:05.200)
Trace[164479967]: [690.962161ms] [690.962161ms] END
I0322 03:25:05.252384    2308 trace.go:236] Trace[1302523962]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:4a37d84e-387d-422f-b8f2-8757a3747e98,client:127.0.0.1,protocol:HTTP/2.0,resource:resourcequotas,scope:namespace,url:/api/v1/namespaces/kube-node-lease/resourcequotas,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:25:03.236) (total time: 1879ms):
Trace[1302523962]: ["List(recursive=true) etcd3" audit-id:4a37d84e-387d-422f-b8f2-8757a3747e98,key:/resourcequotas/kube-node-lease,resourceVersion:,resourceVersionMatch:,limit:0,continue: 1878ms (03:25:03.240)]
Trace[1302523962]: [1.879596804s] [1.879596804s] END
E0322 03:25:05.328250    2308 kubelet.go:2477] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.706s"
E0322 03:25:05.835251    2308 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = NotFound desc = an error occurred when try to find container \"99f14d58a0ef229261a3734019bc51448bd530d4cf41690d804f452d725e4532\": not found" containerID="99f14d58a0ef229261a3734019bc51448bd530d4cf41690d804f452d725e4532"
I0322 03:25:05.997643    2308 trace.go:236] Trace[107698318]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.56.110,type:*v1.Endpoints,resource:apiServerIPInfo (22-Mar-2024 03:24:55.986) (total time: 10005ms):
Trace[107698318]: ---"initial value restored" 5653ms (03:25:01.640)
Trace[107698318]: ---"Transaction prepared" 2569ms (03:25:04.214)
Trace[107698318]: ---"Txn call completed" 1772ms (03:25:05.986)
Trace[107698318]: [10.005752065s] [10.005752065s] END
I0322 03:25:06.098655    2308 trace.go:236] Trace[1501936349]: "Patch" accept:application/vnd.kubernetes.protobuf, */*,audit-id:48c557a9-7e9a-42ad-8e4f-2f151c16c763,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:node-controller,verb:PATCH (22-Mar-2024 03:25:03.363) (total time: 2730ms):
Trace[1501936349]: ---"limitedReadBody succeeded" len:308 135ms (03:25:03.498)
Trace[1501936349]: ["GuaranteedUpdate etcd3" audit-id:48c557a9-7e9a-42ad-8e4f-2f151c16c763,key:/minions/serverworker,type:*core.Node,resource:nodes 2555ms (03:25:03.537)]
Trace[1501936349]: ---"About to check admission control" 1438ms (03:25:04.978)
Trace[1501936349]: [2.730392244s] [2.730392244s] END
I0322 03:25:06.105147    2308 trace.go:236] Trace[711183797]: "Update" accept:application/json, */*,audit-id:88f95dae-0bdd-4245-a248-74ae036e696e,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:PUT (22-Mar-2024 03:24:59.268) (total time: 6835ms):
Trace[711183797]: ---"limitedReadBody succeeded" len:2870 21ms (03:24:59.290)
Trace[711183797]: ---"Conversion done" 550ms (03:24:59.842)
Trace[711183797]: ["GuaranteedUpdate etcd3" audit-id:88f95dae-0bdd-4245-a248-74ae036e696e,key:/minions/serverworker,type:*core.Node,resource:nodes 4504ms (03:25:01.599)
Trace[711183797]:  ---"initial value restored" 555ms (03:25:02.155)
Trace[711183797]:  ---"About to Encode" 1500ms (03:25:03.655)
Trace[711183797]:  ---"Txn call completed" 1264ms (03:25:04.920)]
Trace[711183797]: ---"Write to database call failed" len:2870,err:Operation cannot be fulfilled on nodes "serverworker": the object has been modified; please apply your changes to the latest version and try again 134ms (03:25:06.101)
Trace[711183797]: [6.83540642s] [6.83540642s] END
I0322 03:25:06.108603    2308 trace.go:236] Trace[679198742]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:4a90e482-55bd-487c-bb46-4424cc02a30e,client:127.0.0.1,protocol:HTTP/2.0,resource:services,scope:resource,url:/api/v1/namespaces/default/services/kubernetes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/,verb:GET (22-Mar-2024 03:25:02.824) (total time: 3284ms):
Trace[679198742]: ---"About to write a response" 2912ms (03:25:05.736)
Trace[679198742]: ---"Writing http response done" 371ms (03:25:06.108)
Trace[679198742]: [3.284301503s] [3.284301503s] END
I0322 03:25:06.118992    2308 trace.go:236] Trace[1413075246]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:a108b392-3cf6-4d3a-8a5f-7104f26833da,client:192.168.56.111,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/default/events,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:POST (22-Mar-2024 03:24:58.347) (total time: 7771ms):
Trace[1413075246]: ["Create etcd3" audit-id:a108b392-3cf6-4d3a-8a5f-7104f26833da,key:/events/default/serverworker.17bef83599be4773,type:*core.Event,resource:events 5631ms (03:25:00.487)
Trace[1413075246]:  ---"TransformToStorage succeeded" 2452ms (03:25:02.942)
Trace[1413075246]:  ---"Txn call succeeded" 1891ms (03:25:04.833)]
Trace[1413075246]: ---"Write to database call failed" len:266,err:events "serverworker.17bef83599be4773" already exists 933ms (03:25:05.767)
Trace[1413075246]: [7.771192311s] [7.771192311s] END
I0322 03:25:06.211485    2308 trace.go:236] Trace[568675355]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:dd61601e-5cc4-4699-9c80-0aa528db7d6f,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:25:04.674) (total time: 1537ms):
Trace[568675355]: ---"About to write a response" 1537ms (03:25:06.211)
Trace[568675355]: [1.537200977s] [1.537200977s] END
I0322 03:25:06.215725    2308 trace.go:236] Trace[1190523421]: "Create" accept:application/vnd.kubernetes.protobuf, */*,audit-id:28c209da-2b31-406d-a91c-28acfc830abf,client:127.0.0.1,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/default/events,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:node-controller,verb:POST (22-Mar-2024 03:25:03.359) (total time: 2855ms):
Trace[1190523421]: ---"limitedReadBody succeeded" len:307 125ms (03:25:03.485)
Trace[1190523421]: ["Create etcd3" audit-id:28c209da-2b31-406d-a91c-28acfc830abf,key:/events/default/serverworker.17bef836664bdd6b,type:*core.Event,resource:events 2143ms (03:25:04.071)
Trace[1190523421]:  ---"Txn call succeeded" 1543ms (03:25:05.620)
Trace[1190523421]:  ---"decode succeeded" len:577 396ms (03:25:06.017)]
Trace[1190523421]: ---"Writing http response done" 196ms (03:25:06.214)
Trace[1190523421]: [2.855501887s] [2.855501887s] END
E0322 03:25:06.298593    2308 finisher.go:175] FinishRequest: post-timeout activity - time-elapsed: 8.037232455s, panicked: false, err: context deadline exceeded, panic-reason: <nil>
I0322 03:25:06.370455    2308 trace.go:236] Trace[153991819]: "Patch" accept:application/json, */*,audit-id:2814cbd9-c875-4caa-a58a-3a0a2a76fdb7,client:127.0.0.1,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events/traefik-crd.17bef7e103ccc899,user-agent:helm-controller@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:PATCH (22-Mar-2024 03:25:01.469) (total time: 4897ms):
Trace[153991819]: ["GuaranteedUpdate etcd3" audit-id:2814cbd9-c875-4caa-a58a-3a0a2a76fdb7,key:/events/kube-system/traefik-crd.17bef7e103ccc899,type:*core.Event,resource:events 4881ms (03:25:01.485)
Trace[153991819]:  ---"initial value restored" 3319ms (03:25:04.805)
Trace[153991819]:  ---"About to Encode" 202ms (03:25:05.007)
Trace[153991819]:  ---"Encode succeeded" len:673 241ms (03:25:05.249)
Trace[153991819]:  ---"Txn call completed" 1113ms (03:25:06.362)]
Trace[153991819]: ---"About to check admission control" 198ms (03:25:05.004)
Trace[153991819]: ---"Object stored in database" 1360ms (03:25:06.364)
Trace[153991819]: [4.897879051s] [4.897879051s] END
W0322 03:25:06.391996    2308 controller.go:134] slow openapi aggregation of "addons.k3s.cattle.io": 2.988477781s
I0322 03:25:06.456139    2308 trace.go:236] Trace[380416233]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:6fc86c04-2661-4025-a909-e8da9f5ca5ea,client:127.0.0.1,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/kube-system/pods/coredns-6799fbcd5-tqlcn,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:25:04.665) (total time: 1790ms):
Trace[380416233]: ---"About to write a response" 1710ms (03:25:06.376)
Trace[380416233]: [1.790451439s] [1.790451439s] END
E0322 03:25:06.496497    2308 kubelet.go:2477] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.163s"
I0322 03:25:06.680857    2308 trace.go:236] Trace[477942454]: "Get" accept:application/json, */*,audit-id:0c530280-1327-4b2f-b230-3c46d70a32cd,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps/coredns,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:GET (22-Mar-2024 03:25:06.164) (total time: 513ms):
Trace[477942454]: ---"About to write a response" 510ms (03:25:06.674)
Trace[477942454]: [513.095899ms] [513.095899ms] END
I0322 03:25:06.823384    2308 trace.go:236] Trace[850099216]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:d8bc56a2-e844-4dad-9881-32407e5b07dd,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker/status,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PATCH (22-Mar-2024 03:25:02.868) (total time: 3953ms):
Trace[850099216]: ["GuaranteedUpdate etcd3" audit-id:d8bc56a2-e844-4dad-9881-32407e5b07dd,key:/minions/serverworker,type:*core.Node,resource:nodes 3927ms (03:25:02.894)
Trace[850099216]:  ---"About to Encode" 772ms (03:25:03.672)
Trace[850099216]:  ---"Txn call completed" 1289ms (03:25:04.962)
Trace[850099216]:  ---"About to Encode" 297ms (03:25:05.260)
Trace[850099216]:  ---"Txn call completed" 1561ms (03:25:06.821)]
Trace[850099216]: ---"About to check admission control" 771ms (03:25:03.671)
Trace[850099216]: ---"About to apply patch" 1291ms (03:25:04.962)
Trace[850099216]: ---"Object stored in database" 1855ms (03:25:06.822)
Trace[850099216]: [3.95337914s] [3.95337914s] END
I0322 03:25:06.843369    2308 trace.go:236] Trace[1354629302]: "Get" accept:application/json, */*,audit-id:5e1ed4f4-aa9b-495f-be53-b833441388ab,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:25:06.262) (total time: 580ms):
Trace[1354629302]: ---"About to write a response" 580ms (03:25:06.843)
Trace[1354629302]: [580.529131ms] [580.529131ms] END
I0322 03:25:06.987657    2308 trace.go:236] Trace[348264935]: "Update" accept:application/json, */*,audit-id:d22f5454-3c30-4f2c-ada3-a35d5aca13c2,client:127.0.0.1,protocol:HTTP/2.0,resource:secrets,scope:resource,url:/api/v1/namespaces/kube-system/secrets/k3s-serving,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:PUT (22-Mar-2024 03:25:06.218) (total time: 768ms):
Trace[348264935]: ["GuaranteedUpdate etcd3" audit-id:d22f5454-3c30-4f2c-ada3-a35d5aca13c2,key:/secrets/kube-system/k3s-serving,type:*core.Secret,resource:secrets 752ms (03:25:06.234)
Trace[348264935]:  ---"About to Encode" 268ms (03:25:06.505)]
Trace[348264935]: [768.281193ms] [768.281193ms] END
I0322 03:25:07.022851    2308 trace.go:236] Trace[1723388296]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:0f27b827-a811-4b26-b04a-fcf34b1bca23,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/node-controller,verb:PUT (22-Mar-2024 03:25:05.834) (total time: 1188ms):
Trace[1723388296]: ---"limitedReadBody succeeded" len:2479 216ms (03:25:06.050)
Trace[1723388296]: ["GuaranteedUpdate etcd3" audit-id:0f27b827-a811-4b26-b04a-fcf34b1bca23,key:/minions/serverworker,type:*core.Node,resource:nodes 739ms (03:25:06.283)
Trace[1723388296]:  ---"Txn call completed" 549ms (03:25:06.844)]
Trace[1723388296]: [1.188392678s] [1.188392678s] END
I0322 03:25:07.029096    2308 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/coredns-6799fbcd5-tqlcn" containerName="coredns"
I0322 03:25:07.081791    2308 trace.go:236] Trace[1509541136]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:9215d5f7-2072-4329-a7cb-e01b2e439ff4,client:127.0.0.1,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/default/endpoints/kubernetes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:25:06.523) (total time: 558ms):
Trace[1509541136]: ---"About to write a response" 556ms (03:25:07.079)
Trace[1509541136]: [558.567581ms] [558.567581ms] END
I0322 03:25:07.093930    2308 garbagecollector.go:816] "failed to discover some groups" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"
I0322 03:25:07.251265    2308 trace.go:236] Trace[271641348]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:354db54b-093f-479c-8c6f-1e297b1ea121,client:192.168.56.111,protocol:HTTP/2.0,resource:csinodes,scope:resource,url:/apis/storage.k8s.io/v1/csinodes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:POST (22-Mar-2024 03:25:06.541) (total time: 703ms):
Trace[271641348]: ["Create etcd3" audit-id:354db54b-093f-479c-8c6f-1e297b1ea121,key:/csinodes/serverworker,type:*storage.CSINode,resource:csinodes.storage.k8s.io 690ms (03:25:06.556)
Trace[271641348]:  ---"Txn call succeeded" 662ms (03:25:07.219)]
Trace[271641348]: ---"Write to database call succeeded" len:332 15ms (03:25:07.237)
Trace[271641348]: [703.921893ms] [703.921893ms] END
W0322 03:25:07.457434    2308 controller.go:134] slow openapi aggregation of "etcdsnapshotfiles.k3s.cattle.io": 1.061296831s
I0322 03:25:07.482166    2308 trace.go:236] Trace[1729677791]: "Get" accept:application/json, */*,audit-id:7be99ac9-1795-4021-9000-af119f88771d,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps/coredns,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:GET (22-Mar-2024 03:25:06.913) (total time: 565ms):
Trace[1729677791]: ---"About to write a response" 559ms (03:25:07.473)
Trace[1729677791]: [565.302851ms] [565.302851ms] END
I0322 03:25:07.533674    2308 trace.go:236] Trace[1040021188]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:d4db944c-405f-46e0-b127-49aa7986203f,client:127.0.0.1,protocol:HTTP/2.0,resource:services,scope:resource,url:/api/v1/namespaces/kube-system/services/kube-dns,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/,verb:GET (22-Mar-2024 03:25:06.717) (total time: 816ms):
Trace[1040021188]: ---"About to write a response" 816ms (03:25:07.533)
Trace[1040021188]: [816.398265ms] [816.398265ms] END
I0322 03:25:07.641466    2308 trace.go:236] Trace[1565334646]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:530a4440-1117-4ab1-a106-90361e172b2c,client:127.0.0.1,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/kube-system/pods/coredns-6799fbcd5-tqlcn/status,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:node-controller,verb:PUT (22-Mar-2024 03:25:06.078) (total time: 1561ms):
Trace[1565334646]: ---"Conversion done" 185ms (03:25:06.272)
Trace[1565334646]: ["GuaranteedUpdate etcd3" audit-id:530a4440-1117-4ab1-a106-90361e172b2c,key:/pods/kube-system/coredns-6799fbcd5-tqlcn,type:*core.Pod,resource:pods 1363ms (03:25:06.276)
Trace[1565334646]:  ---"About to Encode" 508ms (03:25:06.786)
Trace[1565334646]:  ---"Encode succeeded" len:5127 189ms (03:25:06.976)
Trace[1565334646]:  ---"Txn call completed" 614ms (03:25:07.590)]
Trace[1565334646]: ---"Writing http response done" 44ms (03:25:07.640)
Trace[1565334646]: [1.561515736s] [1.561515736s] END
I0322 03:25:07.641863    2308 trace.go:236] Trace[1395314024]: "Patch" accept:application/vnd.kubernetes.protobuf, */*,audit-id:e5d900a2-80ee-4033-a7b4-3fde316090f2,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:ttl-controller,verb:PATCH (22-Mar-2024 03:24:59.537) (total time: 8097ms):
Trace[1395314024]: ["GuaranteedUpdate etcd3" audit-id:e5d900a2-80ee-4033-a7b4-3fde316090f2,key:/minions/serverworker,type:*core.Node,resource:nodes 7122ms (03:25:00.513)
Trace[1395314024]:  ---"initial value restored" 780ms (03:25:01.293)
Trace[1395314024]:  ---"About to Encode" 2836ms (03:25:04.129)
Trace[1395314024]:  ---"Txn call completed" 767ms (03:25:04.898)
Trace[1395314024]:  ---"About to Encode" 1531ms (03:25:06.432)
Trace[1395314024]:  ---"Txn call completed" 284ms (03:25:06.718)
Trace[1395314024]:  ---"About to Encode" 124ms (03:25:06.846)
Trace[1395314024]:  ---"Txn call completed" 786ms (03:25:07.633)]
Trace[1395314024]: ---"About to check admission control" 2286ms (03:25:03.580)
Trace[1395314024]: ---"About to apply patch" 1321ms (03:25:04.901)
Trace[1395314024]: ---"About to check admission control" 1375ms (03:25:06.277)
Trace[1395314024]: ---"About to apply patch" 446ms (03:25:06.723)
Trace[1395314024]: ---"About to check admission control" 14ms (03:25:06.737)
Trace[1395314024]: ---"Object stored in database" 897ms (03:25:07.635)
Trace[1395314024]: [8.09798878s] [8.09798878s] END
I0322 03:25:07.644189    2308 trace.go:236] Trace[155405403]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:21a5cb40-7119-444a-9e3f-b17db0285299,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:node-controller,verb:GET (22-Mar-2024 03:25:06.809) (total time: 835ms):
Trace[155405403]: ---"About to write a response" 835ms (03:25:07.644)
Trace[155405403]: [835.148095ms] [835.148095ms] END
I0322 03:25:07.671098    2308 trace.go:236] Trace[248060540]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:45ab6317-3bcb-46b4-bc6a-5716a6f3ef5e,client:192.168.56.111,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/default/events,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:POST (22-Mar-2024 03:25:06.520) (total time: 1150ms):
Trace[248060540]: ["Create etcd3" audit-id:45ab6317-3bcb-46b4-bc6a-5716a6f3ef5e,key:/events/default/serverworker.17bef835a010b124,type:*core.Event,resource:events 1012ms (03:25:06.658)
Trace[248060540]:  ---"Txn call succeeded" 900ms (03:25:07.561)]
Trace[248060540]: ---"Write to database call succeeded" len:287 109ms (03:25:07.670)
Trace[248060540]: [1.150375421s] [1.150375421s] END
I0322 03:25:07.677182    2308 trace.go:236] Trace[47392627]: "Patch" accept:application/vnd.kubernetes.protobuf, */*,audit-id:53ff6851-c7ec-467a-8d41-3feadb975626,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/server,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:node-controller,verb:PATCH (22-Mar-2024 03:25:06.698) (total time: 978ms):
Trace[47392627]: ---"limitedReadBody succeeded" len:156 37ms (03:25:06.735)
Trace[47392627]: ["GuaranteedUpdate etcd3" audit-id:53ff6851-c7ec-467a-8d41-3feadb975626,key:/minions/server,type:*core.Node,resource:nodes 932ms (03:25:06.744)
Trace[47392627]:  ---"Txn call completed" 884ms (03:25:07.642)]
Trace[47392627]: ---"Object stored in database" 892ms (03:25:07.643)
Trace[47392627]: ---"Writing http response done" 33ms (03:25:07.677)
Trace[47392627]: [978.376574ms] [978.376574ms] END
I0322 03:25:08.015935    2308 trace.go:236] Trace[1454970340]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:b00aeeaa-247e-48d8-a34b-37d96a40d278,client:127.0.0.1,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events/coredns-6799fbcd5-tqlcn.17bef82617ac193a,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PATCH (22-Mar-2024 03:25:06.261) (total time: 1749ms):
Trace[1454970340]: ["GuaranteedUpdate etcd3" audit-id:b00aeeaa-247e-48d8-a34b-37d96a40d278,key:/events/kube-system/coredns-6799fbcd5-tqlcn.17bef82617ac193a,type:*core.Event,resource:events 1747ms (03:25:06.263)
Trace[1454970340]:  ---"initial value restored" 687ms (03:25:06.950)
Trace[1454970340]:  ---"About to Encode" 473ms (03:25:07.424)
Trace[1454970340]:  ---"Txn call completed" 490ms (03:25:07.917)]
Trace[1454970340]: ---"About to check admission control" 158ms (03:25:07.108)
Trace[1454970340]: ---"Object stored in database" 890ms (03:25:07.999)
Trace[1454970340]: [1.749975632s] [1.749975632s] END
I0322 03:25:08.034948    2308 event.go:307] "Event occurred" object="kube-system/coredns-6799fbcd5-tqlcn" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0322 03:25:08.730578    2308 trace.go:236] Trace[1089651083]: "List(recursive=true) etcd3" audit-id:,key:/masterleases/,resourceVersion:0,resourceVersionMatch:NotOlderThan,limit:0,continue: (22-Mar-2024 03:25:07.403) (total time: 1321ms):
Trace[1089651083]: [1.321106484s] [1.321106484s] END
I0322 03:25:08.794954    2308 trace.go:236] Trace[2098281766]: "Patch" accept:application/vnd.kubernetes.protobuf, */*,audit-id:488571d3-83b5-4ac1-828a-50d8a3c033a3,client:127.0.0.1,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/default/events/server.17bef833347b0cd7,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:node-controller,verb:PATCH (22-Mar-2024 03:25:06.697) (total time: 2096ms):
Trace[2098281766]: ---"limitedReadBody succeeded" len:102 44ms (03:25:06.742)
Trace[2098281766]: ["GuaranteedUpdate etcd3" audit-id:488571d3-83b5-4ac1-828a-50d8a3c033a3,key:/events/default/server.17bef833347b0cd7,type:*core.Event,resource:events 2050ms (03:25:06.743)
Trace[2098281766]:  ---"initial value restored" 801ms (03:25:07.544)
Trace[2098281766]:  ---"Txn call completed" 1222ms (03:25:08.781)]
Trace[2098281766]: ---"Object stored in database" 1233ms (03:25:08.788)
Trace[2098281766]: [2.096908063s] [2.096908063s] END
I0322 03:25:09.207811    2308 trace.go:236] Trace[1461753359]: "Patch" accept:application/json, */*,audit-id:8a1495c2-7dc5-40d9-8e0c-ee0fa8157188,client:127.0.0.1,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events/traefik.17bef7e103c11ddb,user-agent:helm-controller@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:PATCH (22-Mar-2024 03:25:06.880) (total time: 2320ms):
Trace[1461753359]: ["GuaranteedUpdate etcd3" audit-id:8a1495c2-7dc5-40d9-8e0c-ee0fa8157188,key:/events/kube-system/traefik.17bef7e103c11ddb,type:*core.Event,resource:events 2320ms (03:25:06.882)
Trace[1461753359]:  ---"initial value restored" 592ms (03:25:07.474)
Trace[1461753359]:  ---"About to Encode" 108ms (03:25:07.583)
Trace[1461753359]:  ---"Txn call completed" 1606ms (03:25:09.190)]
Trace[1461753359]: ---"About to check admission control" 108ms (03:25:07.583)
Trace[1461753359]: ---"Object stored in database" 1611ms (03:25:09.194)
Trace[1461753359]: [2.320554929s] [2.320554929s] END
I0322 03:25:09.497920    2308 trace.go:236] Trace[15079857]: "Patch" accept:application/vnd.kubernetes.protobuf, */*,audit-id:a028d4b6-bd63-4442-9d4e-d5321f9abed1,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:node-controller,verb:PATCH (22-Mar-2024 03:24:59.545) (total time: 9945ms):
Trace[15079857]: ---"limitedReadBody succeeded" len:63 44ms (03:24:59.590)
Trace[15079857]: ["GuaranteedUpdate etcd3" audit-id:a028d4b6-bd63-4442-9d4e-d5321f9abed1,key:/minions/serverworker,type:*core.Node,resource:nodes 9897ms (03:24:59.593)
Trace[15079857]:  ---"About to Encode" 3596ms (03:25:03.192)
Trace[15079857]:  ---"Txn call completed" 1737ms (03:25:04.932)
Trace[15079857]:  ---"About to Encode" 301ms (03:25:05.234)
Trace[15079857]:  ---"Encode succeeded" len:2532 329ms (03:25:05.564)
Trace[15079857]:  ---"Txn call completed" 1260ms (03:25:06.825)
Trace[15079857]:  ---"Txn call completed" 812ms (03:25:07.645)
Trace[15079857]:  ---"About to Encode" 137ms (03:25:07.782)
Trace[15079857]:  ---"Txn call completed" 1694ms (03:25:09.478)]
Trace[15079857]: ---"About to check admission control" 2742ms (03:25:02.337)
Trace[15079857]: ---"About to apply patch" 2595ms (03:25:04.933)
Trace[15079857]: ---"About to apply patch" 1884ms (03:25:06.825)
Trace[15079857]: ---"About to apply patch" 813ms (03:25:07.645)
Trace[15079857]: ---"About to check admission control" 12ms (03:25:07.657)
Trace[15079857]: ---"Object stored in database" 1822ms (03:25:09.480)
Trace[15079857]: ---"Writing http response done" 10ms (03:25:09.490)
Trace[15079857]: [9.945089139s] [9.945089139s] END
I0322 03:25:09.956376    2308 trace.go:236] Trace[515158951]: "Get" accept:application/json, */*,audit-id:436bde69-c085-4b31-a590-a023d8b856da,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:25:07.744) (total time: 1940ms):
Trace[515158951]: ---"About to write a response" 1934ms (03:25:09.678)
Trace[515158951]: [1.940186191s] [1.940186191s] END
I0322 03:25:10.084213    2308 trace.go:236] Trace[1710841060]: "Get" accept:application/json, */*,audit-id:c18e8b9d-a2df-440b-acae-78e3cec5888f,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps/coredns,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:GET (22-Mar-2024 03:25:08.845) (total time: 1234ms):
Trace[1710841060]: ---"About to write a response" 1233ms (03:25:10.078)
Trace[1710841060]: [1.234471111s] [1.234471111s] END
I0322 03:25:10.457305    2308 scope.go:117] "RemoveContainer" containerID="7794e189bc1b2b2e4251a637e10afba15513bb8034f56b5f81f0d281a4148b00"
I0322 03:25:10.643357    2308 trace.go:236] Trace[2045647587]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:31d8ae7f-a4d9-4a6f-9d76-e10428b049c8,client:127.0.0.1,protocol:HTTP/2.0,resource:services,scope:resource,url:/api/v1/namespaces/kube-system/services/metrics-server,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/,verb:GET (22-Mar-2024 03:25:08.845) (total time: 1617ms):
Trace[2045647587]: ---"About to write a response" 1222ms (03:25:10.067)
Trace[2045647587]: ---"Writing http response done" 394ms (03:25:10.462)
Trace[2045647587]: [1.617105466s] [1.617105466s] END
I0322 03:25:10.670970    2308 range_allocator.go:380] "Set node PodCIDR" node="serverworker" podCIDRs=["10.42.1.0/24"]
E0322 03:25:10.750341    2308 kubelet.go:2477] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="2.155s"
I0322 03:25:10.969910    2308 trace.go:236] Trace[1656644132]: "Update" accept:application/json, */*,audit-id:b8b0de2e-47c8-422c-bcad-0fe391c440f6,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps/coredns,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:PUT (22-Mar-2024 03:25:09.175) (total time: 1789ms):
Trace[1656644132]: ---"limitedReadBody succeeded" len:2110 29ms (03:25:09.204)
Trace[1656644132]: ["GuaranteedUpdate etcd3" audit-id:b8b0de2e-47c8-422c-bcad-0fe391c440f6,key:/configmaps/kube-system/coredns,type:*core.ConfigMap,resource:configmaps 1753ms (03:25:09.212)
Trace[1656644132]:  ---"About to Encode" 204ms (03:25:09.418)
Trace[1656644132]:  ---"Encode succeeded" len:1787 199ms (03:25:09.618)
Trace[1656644132]:  ---"Txn call completed" 1155ms (03:25:10.773)]
Trace[1656644132]: ---"Writing http response done" 188ms (03:25:10.964)
Trace[1656644132]: [1.789620007s] [1.789620007s] END
E0322 03:25:11.000729    2308 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"helm\" with CrashLoopBackOff: \"back-off 20s restarting failed container=helm pod=helm-install-traefik-t9wf9_kube-system(598e1d6b-dd9f-40b6-8abc-7ed9692a3a85)\"" pod="kube-system/helm-install-traefik-t9wf9" podUID="598e1d6b-dd9f-40b6-8abc-7ed9692a3a85"
I0322 03:25:11.037725    2308 trace.go:236] Trace[729494404]: "List" accept:application/json, */*,audit-id:171c7e3b-3857-4d57-ba2e-8abc412179bf,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:cluster,url:/api/v1/nodes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:25:10.290) (total time: 747ms):
Trace[729494404]: ---"Writing http response done" count:2 457ms (03:25:11.037)
Trace[729494404]: [747.257809ms] [747.257809ms] END
I0322 03:25:11.417436    2308 trace.go:236] Trace[260453298]: "Patch" accept:application/vnd.kubernetes.protobuf, */*,audit-id:e2d0eb64-eaef-4009-8172-8f970ad93349,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:node-controller,verb:PATCH (22-Mar-2024 03:25:09.335) (total time: 2070ms):
Trace[260453298]: ["GuaranteedUpdate etcd3" audit-id:e2d0eb64-eaef-4009-8172-8f970ad93349,key:/minions/serverworker,type:*core.Node,resource:nodes 1889ms (03:25:09.516)]
Trace[260453298]: ---"About to check admission control" 319ms (03:25:09.841)
Trace[260453298]: [2.070083714s] [2.070083714s] END
I0322 03:25:11.458533    2308 trace.go:236] Trace[1372471237]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:0c5bff83-c038-48b6-9a09-8b94d2baecb1,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/node-controller,verb:PUT (22-Mar-2024 03:25:08.729) (total time: 2728ms):
Trace[1372471237]: ---"limitedReadBody succeeded" len:2792 272ms (03:25:09.001)
Trace[1372471237]: ---"Conversion done" 157ms (03:25:09.166)
Trace[1372471237]: ["GuaranteedUpdate etcd3" audit-id:0c5bff83-c038-48b6-9a09-8b94d2baecb1,key:/minions/serverworker,type:*core.Node,resource:nodes 2233ms (03:25:09.224)]
Trace[1372471237]: ---"Write to database call failed" len:2792,err:Operation cannot be fulfilled on nodes "serverworker": the object has been modified; please apply your changes to the latest version and try again 46ms (03:25:11.348)
Trace[1372471237]: [2.728249328s] [2.728249328s] END
I0322 03:25:11.533240    2308 trace.go:236] Trace[1511787014]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:4f7ef7b2-f39e-4045-9ab7-1362aab1c1d6,client:127.0.0.1,protocol:HTTP/2.0,resource:endpointslices,scope:resource,url:/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/kubernetes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:25:09.474) (total time: 2058ms):
Trace[1511787014]: ---"About to write a response" 2057ms (03:25:11.531)
Trace[1511787014]: [2.058708989s] [2.058708989s] END
I0322 03:25:11.564619    2308 trace.go:236] Trace[862865744]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:99029f00-568a-4338-8fec-1ce86cde3915,client:192.168.56.111,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/default/events,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:POST (22-Mar-2024 03:25:09.511) (total time: 2052ms):
Trace[862865744]: ["Create etcd3" audit-id:99029f00-568a-4338-8fec-1ce86cde3915,key:/events/default/serverworker.17bef835a010d541,type:*core.Event,resource:events 1177ms (03:25:10.386)
Trace[862865744]:  ---"Txn call succeeded" 1171ms (03:25:11.558)]
Trace[862865744]: [2.052516283s] [2.052516283s] END
time="2024-03-22T03:25:11Z" level=info msg="Updated coredns node hosts entry [192.168.56.111 serverworker]"
I0322 03:25:11.625707    2308 trace.go:236] Trace[507063021]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:b2e0c0be-ddcf-4736-9a1b-0c9eaff81d76,client:192.168.56.111,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:25:09.963) (total time: 1662ms):
Trace[507063021]: [1.662274965s] [1.662274965s] END
I0322 03:25:11.927303    2308 trace.go:236] Trace[459675381]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:b50a78e0-9811-43fa-8dcc-4c68d576b117,client:127.0.0.1,protocol:HTTP/2.0,resource:endpointslices,scope:resource,url:/apis/discovery.k8s.io/v1/namespaces/kube-system/endpointslices/kube-dns-kmwdv,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:endpointslice-controller,verb:PUT (22-Mar-2024 03:25:09.505) (total time: 2418ms):
Trace[459675381]: ["GuaranteedUpdate etcd3" audit-id:b50a78e0-9811-43fa-8dcc-4c68d576b117,key:/endpointslices/kube-system/kube-dns-kmwdv,type:*discovery.EndpointSlice,resource:endpointslices.discovery.k8s.io 2258ms (03:25:09.665)
Trace[459675381]:  ---"initial value restored" 265ms (03:25:09.930)
Trace[459675381]:  ---"About to Encode" 1439ms (03:25:11.370)
Trace[459675381]:  ---"Txn call completed" 545ms (03:25:11.916)]
Trace[459675381]: [2.418716206s] [2.418716206s] END
E0322 03:25:11.935344    2308 kubelet.go:2477] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.039s"
W0322 03:25:11.981670    2308 controller.go:134] slow openapi aggregation of "helmchartconfigs.helm.cattle.io": 4.256576158s
I0322 03:25:12.227285    2308 trace.go:236] Trace[906181467]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:9534eda0-1e9f-465b-ba60-493762885377,client:127.0.0.1,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/kube-system/pods/coredns-6799fbcd5-tqlcn/status,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PATCH (22-Mar-2024 03:25:06.887) (total time: 5337ms):
Trace[906181467]: ["GuaranteedUpdate etcd3" audit-id:9534eda0-1e9f-465b-ba60-493762885377,key:/pods/kube-system/coredns-6799fbcd5-tqlcn,type:*core.Pod,resource:pods 5336ms (03:25:06.888)
Trace[906181467]:  ---"About to Encode" 639ms (03:25:07.528)
Trace[906181467]:  ---"Txn call completed" 451ms (03:25:07.981)
Trace[906181467]:  ---"About to Encode" 3347ms (03:25:11.339)
Trace[906181467]:  ---"Txn call completed" 776ms (03:25:12.118)]
Trace[906181467]: ---"About to check admission control" 424ms (03:25:07.313)
Trace[906181467]: ---"About to apply patch" 678ms (03:25:07.992)
Trace[906181467]: ---"About to check admission control" 2010ms (03:25:10.002)
Trace[906181467]: ---"Object stored in database" 2178ms (03:25:12.181)
Trace[906181467]: ---"Writing http response done" 43ms (03:25:12.225)
Trace[906181467]: [5.337542694s] [5.337542694s] END
I0322 03:25:12.281754    2308 trace.go:236] Trace[21254676]: "Get" accept:application/json, */*,audit-id:3e9e0e9b-c035-4c20-aca4-c4645ee336b2,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:25:10.392) (total time: 1641ms):
Trace[21254676]: ---"About to write a response" 1160ms (03:25:11.552)
Trace[21254676]: ---"Writing http response done" 481ms (03:25:12.033)
Trace[21254676]: [1.641717414s] [1.641717414s] END
I0322 03:25:12.286031    2308 trace.go:236] Trace[2145819405]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:b2ab214b-df2f-49b9-9296-03c81acf6ec9,client:127.0.0.1,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/kube-dns,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:endpoint-controller,verb:PUT (22-Mar-2024 03:25:10.355) (total time: 1930ms):
Trace[2145819405]: ["GuaranteedUpdate etcd3" audit-id:b2ab214b-df2f-49b9-9296-03c81acf6ec9,key:/services/endpoints/kube-system/kube-dns,type:*core.Endpoints,resource:endpoints 1908ms (03:25:10.377)
Trace[2145819405]:  ---"About to Encode" 1118ms (03:25:11.498)
Trace[2145819405]:  ---"Txn call completed" 779ms (03:25:12.285)]
Trace[2145819405]: [1.930000291s] [1.930000291s] END
I0322 03:25:12.286703    2308 trace.go:236] Trace[246819276]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:7964dc4e-1dec-4686-ac88-64e03f411f2b,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker/status,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PATCH (22-Mar-2024 03:25:07.668) (total time: 4618ms):
Trace[246819276]: ---"limitedReadBody succeeded" len:557 28ms (03:25:07.696)
Trace[246819276]: ["GuaranteedUpdate etcd3" audit-id:7964dc4e-1dec-4686-ac88-64e03f411f2b,key:/minions/serverworker,type:*core.Node,resource:nodes 4580ms (03:25:07.706)
Trace[246819276]:  ---"About to Encode" 1869ms (03:25:09.579)
Trace[246819276]:  ---"Txn call completed" 1039ms (03:25:10.618)
Trace[246819276]:  ---"About to Encode" 814ms (03:25:11.436)
Trace[246819276]:  ---"Txn call completed" 848ms (03:25:12.286)]
Trace[246819276]: ---"About to check admission control" 1203ms (03:25:08.913)
Trace[246819276]: ---"About to apply patch" 1709ms (03:25:10.622)
Trace[246819276]: ---"About to check admission control" 279ms (03:25:10.901)
Trace[246819276]: ---"Object stored in database" 1384ms (03:25:12.286)
Trace[246819276]: [4.618487323s] [4.618487323s] END
I0322 03:25:12.300867    2308 trace.go:236] Trace[2139984119]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:99a07541-05ff-4d3c-a050-afcf814554e5,client:127.0.0.1,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/kube-system/pods/helm-install-traefik-t9wf9/status,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:node-controller,verb:PUT (22-Mar-2024 03:25:09.504) (total time: 2795ms):
Trace[2139984119]: ---"limitedReadBody succeeded" len:6137 215ms (03:25:09.720)
Trace[2139984119]: ["GuaranteedUpdate etcd3" audit-id:99a07541-05ff-4d3c-a050-afcf814554e5,key:/pods/kube-system/helm-install-traefik-t9wf9,type:*core.Pod,resource:pods 2282ms (03:25:10.017)
Trace[2139984119]:  ---"initial value restored" 340ms (03:25:10.358)
Trace[2139984119]:  ---"About to Encode" 1029ms (03:25:11.388)
Trace[2139984119]:  ---"Txn call completed" 712ms (03:25:12.102)
Trace[2139984119]:  ---"decode succeeded" len:6134 151ms (03:25:12.254)]
Trace[2139984119]: ---"Writing http response done" 45ms (03:25:12.300)
Trace[2139984119]: [2.795564626s] [2.795564626s] END
I0322 03:25:12.327909    2308 trace.go:236] Trace[300046128]: "Get" accept:application/json, */*,audit-id:3f76539d-e1c7-4daa-823e-5fde989ed323,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:25:11.197) (total time: 1130ms):
Trace[300046128]: ---"About to write a response" 1052ms (03:25:12.255)
Trace[300046128]: [1.130085529s] [1.130085529s] END
I0322 03:25:12.389563    2308 trace.go:236] Trace[1934586546]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:96bd4234-39eb-40ca-a0e1-a13a16877a6c,client:127.0.0.1,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events/metrics-server-67c658944b-8mmfz.17bef82954f12c5c,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PATCH (22-Mar-2024 03:25:09.412) (total time: 2976ms):
Trace[1934586546]: ["GuaranteedUpdate etcd3" audit-id:96bd4234-39eb-40ca-a0e1-a13a16877a6c,key:/events/kube-system/metrics-server-67c658944b-8mmfz.17bef82954f12c5c,type:*core.Event,resource:events 2971ms (03:25:09.417)
Trace[1934586546]:  ---"initial value restored" 1314ms (03:25:10.732)
Trace[1934586546]:  ---"About to Encode" 782ms (03:25:11.514)
Trace[1934586546]:  ---"Txn call completed" 873ms (03:25:12.388)]
Trace[1934586546]: ---"About to check admission control" 772ms (03:25:11.511)
Trace[1934586546]: ---"Object stored in database" 877ms (03:25:12.388)
Trace[1934586546]: [2.976953219s] [2.976953219s] END
I0322 03:25:12.396557    2308 event.go:307] "Event occurred" object="kube-system/helm-install-traefik-t9wf9" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0322 03:25:12.432460    2308 topologycache.go:237] "Can't get CPU or zone information for node" node="serverworker"
I0322 03:25:12.463449    2308 trace.go:236] Trace[1881640947]: "Patch" accept:application/vnd.kubernetes.protobuf, */*,audit-id:c5f107b4-8ed2-4c87-83cd-9d8d1783fbad,client:127.0.0.1,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events/coredns-6799fbcd5-tqlcn.17bef8335cc3f591,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:node-controller,verb:PATCH (22-Mar-2024 03:25:10.245) (total time: 2217ms):
Trace[1881640947]: ---"Recorded the audit event" 214ms (03:25:10.467)
Trace[1881640947]: ["GuaranteedUpdate etcd3" audit-id:c5f107b4-8ed2-4c87-83cd-9d8d1783fbad,key:/events/kube-system/coredns-6799fbcd5-tqlcn.17bef8335cc3f591,type:*core.Event,resource:events 1994ms (03:25:10.469)
Trace[1881640947]:  ---"initial value restored" 735ms (03:25:11.204)
Trace[1881640947]:  ---"About to Encode" 318ms (03:25:11.522)
Trace[1881640947]:  ---"Txn call completed" 940ms (03:25:12.463)]
Trace[1881640947]: ---"About to check admission control" 317ms (03:25:11.522)
Trace[1881640947]: ---"Object stored in database" 940ms (03:25:12.463)
Trace[1881640947]: [2.217879501s] [2.217879501s] END
I0322 03:25:12.487095    2308 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:25:12.832117    2308 trace.go:236] Trace[454583398]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:ac99455e-0ace-4717-b142-56caf8a0d7be,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:node-controller,verb:GET (22-Mar-2024 03:25:12.296) (total time: 529ms):
Trace[454583398]: ---"About to write a response" 514ms (03:25:12.811)
Trace[454583398]: [529.157177ms] [529.157177ms] END
I0322 03:25:12.921902    2308 trace.go:236] Trace[2032684845]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:a2be8b26-a477-4ae5-bec8-34564e63c7ed,client:192.168.56.111,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/default/events,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:POST (22-Mar-2024 03:25:12.272) (total time: 649ms):
Trace[2032684845]: ["Create etcd3" audit-id:a2be8b26-a477-4ae5-bec8-34564e63c7ed,key:/events/default/serverworker.17bef835a010e50c,type:*core.Event,resource:events 513ms (03:25:12.408)
Trace[2032684845]:  ---"Txn call succeeded" 499ms (03:25:12.917)]
Trace[2032684845]: [649.026701ms] [649.026701ms] END
I0322 03:25:13.034833    2308 trace.go:236] Trace[175106366]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:7accd5c2-dfe7-45dc-ae84-f7134cd85c69,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:25:11.951) (total time: 1082ms):
Trace[175106366]: ---"About to write a response" 1079ms (03:25:13.030)
Trace[175106366]: [1.082219768s] [1.082219768s] END
I0322 03:25:13.053045    2308 trace.go:236] Trace[72239106]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:1e32d2a8-6ddd-4218-8017-de13342968bb,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:25:12.397) (total time: 655ms):
Trace[72239106]: ---"About to write a response" 655ms (03:25:13.052)
Trace[72239106]: [655.132719ms] [655.132719ms] END
I0322 03:25:13.082872    2308 scope.go:117] "RemoveContainer" containerID="10bf002df908d7fdba72bbaa57e714397a780057c9557cc9f05de8abc2684e1b"
I0322 03:25:13.137485    2308 trace.go:236] Trace[2107505800]: "Get" accept:application/json, */*,audit-id:73942c1c-8a91-4aa6-a426-7eab674e6662,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:25:12.231) (total time: 904ms):
Trace[2107505800]: ---"About to write a response" 830ms (03:25:13.062)
Trace[2107505800]: [904.548608ms] [904.548608ms] END
I0322 03:25:13.235085    2308 trace.go:236] Trace[828284436]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:340bc1c4-20d5-4a88-8eb3-e6bf94ab40eb,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/node-controller,verb:PUT (22-Mar-2024 03:25:12.298) (total time: 936ms):
Trace[828284436]: ["GuaranteedUpdate etcd3" audit-id:340bc1c4-20d5-4a88-8eb3-e6bf94ab40eb,key:/minions/serverworker,type:*core.Node,resource:nodes 803ms (03:25:12.431)]
Trace[828284436]: [936.702357ms] [936.702357ms] END
I0322 03:25:13.259600    2308 status_manager.go:317] "Container readiness changed for unknown container" pod="kube-system/coredns-6799fbcd5-tqlcn" containerID="containerd://137ac679f6b3f90faab1b0a1c2c2a9a8106f85765376a0e56dfd6b86bfe9bece"
I0322 03:25:13.264335    2308 trace.go:236] Trace[1375478580]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:a3a30d8c-b839-4519-bc16-b9c91a1dcb88,client:127.0.0.1,protocol:HTTP/2.0,resource:replicasets,scope:resource,url:/apis/apps/v1/namespaces/kube-system/replicasets/coredns-6799fbcd5/status,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:replicaset-controller,verb:PUT (22-Mar-2024 03:25:10.268) (total time: 2994ms):
Trace[1375478580]: ---"limitedReadBody succeeded" len:6114 47ms (03:25:10.316)
Trace[1375478580]: ---"Conversion done" 22ms (03:25:10.340)
Trace[1375478580]: ---"About to store object in database" 280ms (03:25:10.620)
Trace[1375478580]: ["GuaranteedUpdate etcd3" audit-id:a3a30d8c-b839-4519-bc16-b9c91a1dcb88,key:/replicasets/kube-system/coredns-6799fbcd5,type:*apps.ReplicaSet,resource:replicasets.apps 2358ms (03:25:10.905)
Trace[1375478580]:  ---"initial value restored" 565ms (03:25:11.470)
Trace[1375478580]:  ---"About to Encode" 1043ms (03:25:12.514)
Trace[1375478580]:  ---"Txn call completed" 484ms (03:25:13.002)
Trace[1375478580]:  ---"decode succeeded" len:6063 161ms (03:25:13.164)]
Trace[1375478580]: ---"Write to database call succeeded" len:6114 14ms (03:25:13.179)
Trace[1375478580]: ---"Writing http response done" 84ms (03:25:13.263)
Trace[1375478580]: [2.994414682s] [2.994414682s] END
I0322 03:25:13.434670    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="4.562529136s"
I0322 03:25:13.437831    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="2.364513ms"
I0322 03:25:13.445632    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="1.68451ms"
I0322 03:25:13.531899    2308 trace.go:236] Trace[469764018]: "Update" accept:application/json, */*,audit-id:6d31fb0d-9dc6-480a-8b6d-ae6fca274fc4,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:PUT (22-Mar-2024 03:25:12.224) (total time: 1301ms):
Trace[469764018]: ---"limitedReadBody succeeded" len:3483 153ms (03:25:12.377)
Trace[469764018]: ["GuaranteedUpdate etcd3" audit-id:6d31fb0d-9dc6-480a-8b6d-ae6fca274fc4,key:/minions/serverworker,type:*core.Node,resource:nodes 967ms (03:25:12.558)]
Trace[469764018]: ---"Write to database call failed" len:3483,err:Operation cannot be fulfilled on nodes "serverworker": the object has been modified; please apply your changes to the latest version and try again 90ms (03:25:13.522)
Trace[469764018]: [1.301439939s] [1.301439939s] END
I0322 03:25:13.581559    2308 trace.go:236] Trace[1275485107]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:29bf5ce3-77f0-4334-aeef-f9e632ca66bc,client:127.0.0.1,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/kube-system/pods/helm-install-traefik-t9wf9,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:25:12.528) (total time: 1052ms):
Trace[1275485107]: ---"About to write a response" 865ms (03:25:13.393)
Trace[1275485107]: ---"Writing http response done" 187ms (03:25:13.580)
Trace[1275485107]: [1.052639558s] [1.052639558s] END
I0322 03:25:13.600111    2308 trace.go:236] Trace[2035797335]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:733b0b8d-b9e3-4229-9cf3-6dc38a97c6eb,client:127.0.0.1,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/kube-system/pods/local-path-provisioner-6c86858495-mpn68/status,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:node-controller,verb:PUT (22-Mar-2024 03:25:12.737) (total time: 861ms):
Trace[2035797335]: ["GuaranteedUpdate etcd3" audit-id:733b0b8d-b9e3-4229-9cf3-6dc38a97c6eb,key:/pods/kube-system/local-path-provisioner-6c86858495-mpn68,type:*core.Pod,resource:pods 767ms (03:25:12.832)
Trace[2035797335]:  ---"Txn call completed" 512ms (03:25:13.382)
Trace[2035797335]:  ---"decode succeeded" len:3461 207ms (03:25:13.590)]
Trace[2035797335]: [861.507323ms] [861.507323ms] END
I0322 03:25:14.001642    2308 node_lifecycle_controller.go:1029] "Controller detected that all Nodes are not-Ready. Entering master disruption mode"
I0322 03:25:14.015218    2308 event.go:307] "Event occurred" object="kube-system/local-path-provisioner-6c86858495-mpn68" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
W0322 03:25:14.044925    2308 controller.go:134] slow openapi aggregation of "ingressroutes.traefik.containo.us": 1.069054953s
I0322 03:25:14.073377    2308 trace.go:236] Trace[186091635]: "Create" accept:application/vnd.kubernetes.protobuf, */*,audit-id:9a519c40-b187-4717-9110-c98bdd213ac7,client:127.0.0.1,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:node-controller,verb:POST (22-Mar-2024 03:25:12.853) (total time: 1128ms):
Trace[186091635]: ["Create etcd3" audit-id:9a519c40-b187-4717-9110-c98bdd213ac7,key:/events/kube-system/helm-install-traefik-t9wf9.17bef84baed0aed2,type:*core.Event,resource:events 1055ms (03:25:12.926)
Trace[186091635]:  ---"Txn call succeeded" 1045ms (03:25:13.972)]
Trace[186091635]: [1.128347115s] [1.128347115s] END
I0322 03:25:14.140743    2308 trace.go:236] Trace[1533447444]: "Patch" accept:application/vnd.kubernetes.protobuf, */*,audit-id:715aba57-1898-48e2-8ba8-5de2f2de0ba0,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:node-controller,verb:PATCH (22-Mar-2024 03:25:13.170) (total time: 968ms):
Trace[1533447444]: ---"limitedReadBody succeeded" len:308 52ms (03:25:13.222)
Trace[1533447444]: ["GuaranteedUpdate etcd3" audit-id:715aba57-1898-48e2-8ba8-5de2f2de0ba0,key:/minions/serverworker,type:*core.Node,resource:nodes 915ms (03:25:13.222)
Trace[1533447444]:  ---"About to Encode" 569ms (03:25:13.792)
Trace[1533447444]:  ---"Txn call completed" 343ms (03:25:14.136)]
Trace[1533447444]: ---"About to check admission control" 17ms (03:25:13.240)
Trace[1533447444]: ---"Object stored in database" 896ms (03:25:14.136)
Trace[1533447444]: [968.013047ms] [968.013047ms] END
I0322 03:25:14.154056    2308 trace.go:236] Trace[42787532]: "Patch" accept:application/vnd.kubernetes.protobuf, */*,audit-id:a927cd82-8cef-4244-8a0b-f68690b230eb,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:node-controller,verb:PATCH (22-Mar-2024 03:25:13.161) (total time: 992ms):
Trace[42787532]: ["GuaranteedUpdate etcd3" audit-id:a927cd82-8cef-4244-8a0b-f68690b230eb,key:/minions/serverworker,type:*core.Node,resource:nodes 954ms (03:25:13.199)
Trace[42787532]:  ---"About to Encode" 523ms (03:25:13.727)
Trace[42787532]:  ---"Txn call completed" 413ms (03:25:14.141)]
Trace[42787532]: ---"About to check admission control" 520ms (03:25:13.725)
Trace[42787532]: ---"About to apply patch" 416ms (03:25:14.141)
Trace[42787532]: [992.947971ms] [992.947971ms] END
I0322 03:25:14.157770    2308 trace.go:236] Trace[32907378]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.56.110,type:*v1.Endpoints,resource:apiServerIPInfo (22-Mar-2024 03:25:12.132) (total time: 2025ms):
Trace[32907378]: ---"initial value restored" 703ms (03:25:12.836)
Trace[32907378]: ---"Transaction prepared" 401ms (03:25:13.237)
Trace[32907378]: ---"Txn call completed" 918ms (03:25:14.156)
Trace[32907378]: [2.02559829s] [2.02559829s] END
I0322 03:25:14.276152    2308 trace.go:236] Trace[760729627]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:8bf80591-99f9-42df-bc2a-d160f5a2284e,client:192.168.56.111,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:POST (22-Mar-2024 03:25:13.315) (total time: 957ms):
Trace[760729627]: ---"limitedReadBody succeeded" len:184 147ms (03:25:13.463)
Trace[760729627]: [957.655149ms] [957.655149ms] END
I0322 03:25:14.353854    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="589.870294ms"
I0322 03:25:14.679143    2308 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:25:14.679236    2308 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:25:14.683833    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="145.165306ms"
I0322 03:25:14.704101    2308 trace.go:236] Trace[2098046188]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:16fc0166-a8fc-44d0-891c-dfc0a564478b,client:127.0.0.1,protocol:HTTP/2.0,resource:deployments,scope:resource,url:/apis/apps/v1/namespaces/kube-system/deployments/coredns/status,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:deployment-controller,verb:PUT (22-Mar-2024 03:25:14.091) (total time: 609ms):
Trace[2098046188]: ["GuaranteedUpdate etcd3" audit-id:16fc0166-a8fc-44d0-891c-dfc0a564478b,key:/deployments/kube-system/coredns,type:*apps.Deployment,resource:deployments.apps 603ms (03:25:14.098)
Trace[2098046188]:  ---"About to Encode" 74ms (03:25:14.172)
Trace[2098046188]:  ---"Txn call completed" 224ms (03:25:14.397)
Trace[2098046188]:  ---"decode succeeded" len:6530 217ms (03:25:14.614)]
Trace[2098046188]: ---"Write to database call succeeded" len:6552 81ms (03:25:14.696)
Trace[2098046188]: [609.679996ms] [609.679996ms] END
I0322 03:25:14.810856    2308 trace.go:236] Trace[23381517]: "Patch" accept:application/json, */*,audit-id:416300bd-6d34-4b27-a180-4c4c8abbc5e3,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker/status,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PATCH (22-Mar-2024 03:25:13.881) (total time: 926ms):
Trace[23381517]: ["GuaranteedUpdate etcd3" audit-id:416300bd-6d34-4b27-a180-4c4c8abbc5e3,key:/minions/serverworker,type:*core.Node,resource:nodes 781ms (03:25:14.027)
Trace[23381517]:  ---"initial value restored" 67ms (03:25:14.094)
Trace[23381517]:  ---"About to Encode" 77ms (03:25:14.172)
Trace[23381517]:  ---"Txn call completed" 139ms (03:25:14.311)
Trace[23381517]:  ---"Txn call completed" 456ms (03:25:14.772)]
Trace[23381517]: ---"About to apply patch" 215ms (03:25:14.313)
Trace[23381517]: ---"Object stored in database" 458ms (03:25:14.773)
Trace[23381517]: ---"Writing http response done" 34ms (03:25:14.808)
Trace[23381517]: [926.82874ms] [926.82874ms] END
I0322 03:25:14.817982    2308 trace.go:236] Trace[250110818]: "Create" accept:application/vnd.kubernetes.protobuf, */*,audit-id:b23d0587-47ff-43c3-804a-be5d712ef5d1,client:192.168.56.111,protocol:HTTP/2.0,resource:events,scope:resource,url:/apis/events.k8s.io/v1/namespaces/default/events,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:POST (22-Mar-2024 03:25:13.298) (total time: 1518ms):
Trace[250110818]: ["Create etcd3" audit-id:b23d0587-47ff-43c3-804a-be5d712ef5d1,key:/events/default/serverworker.17bef84c16a8caae,type:*core.Event,resource:events 1028ms (03:25:13.788)
Trace[250110818]:  ---"TransformToStorage succeeded" 427ms (03:25:14.217)
Trace[250110818]:  ---"Txn call succeeded" 417ms (03:25:14.634)]
Trace[250110818]: ---"Writing http response done" 177ms (03:25:14.817)
Trace[250110818]: [1.518556728s] [1.518556728s] END
I0322 03:25:14.913481    2308 trace.go:236] Trace[561235829]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:19c2cacf-88bc-4b77-9339-224e4ce55578,client:127.0.0.1,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events/metrics-server-67c658944b-8mmfz.17bef80c6130affa,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PATCH (22-Mar-2024 03:25:13.266) (total time: 1644ms):
Trace[561235829]: ---"limitedReadBody succeeded" len:152 142ms (03:25:13.408)
Trace[561235829]: ["GuaranteedUpdate etcd3" audit-id:19c2cacf-88bc-4b77-9339-224e4ce55578,key:/events/kube-system/metrics-server-67c658944b-8mmfz.17bef80c6130affa,type:*core.Event,resource:events 1490ms (03:25:13.419)
Trace[561235829]:  ---"initial value restored" 873ms (03:25:14.293)
Trace[561235829]:  ---"Txn call completed" 612ms (03:25:14.906)]
Trace[561235829]: ---"Object stored in database" 612ms (03:25:14.906)
Trace[561235829]: [1.644057775s] [1.644057775s] END
I0322 03:25:14.940321    2308 kube.go:482] Creating the node lease for IPv4. This is the n.Spec.PodCIDRs: [10.42.1.0/24]
I0322 03:25:14.944891    2308 trace.go:236] Trace[1262721988]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:336930e9-59c7-44ef-b59f-92eb7125203b,client:192.168.56.111,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/default/events/serverworker.17bef835a010b124,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PATCH (22-Mar-2024 03:25:13.302) (total time: 1642ms):
Trace[1262721988]: ["GuaranteedUpdate etcd3" audit-id:336930e9-59c7-44ef-b59f-92eb7125203b,key:/events/default/serverworker.17bef835a010b124,type:*core.Event,resource:events 1458ms (03:25:13.486)
Trace[1262721988]:  ---"initial value restored" 758ms (03:25:14.245)
Trace[1262721988]:  ---"Txn call completed" 648ms (03:25:14.904)]
Trace[1262721988]: ---"Object stored in database" 690ms (03:25:14.944)
Trace[1262721988]: [1.642700788s] [1.642700788s] END
I0322 03:25:14.953494    2308 subnet.go:160] Batch elem [0] is { lease.Event{Type:0, Lease:lease.Lease{EnableIPv4:true, EnableIPv6:false, Subnet:ip.IP4Net{IP:0xa2a0100, PrefixLen:0x18}, IPv6Subnet:ip.IP6Net{IP:(*ip.IP6)(nil), PrefixLen:0x0}, Attrs:lease.LeaseAttrs{PublicIP:0xa00020f, PublicIPv6:(*ip.IP6)(nil), BackendType:"vxlan", BackendData:json.RawMessage{0x7b, 0x22, 0x56, 0x4e, 0x49, 0x22, 0x3a, 0x31, 0x2c, 0x22, 0x56, 0x74, 0x65, 0x70, 0x4d, 0x41, 0x43, 0x22, 0x3a, 0x22, 0x35, 0x32, 0x3a, 0x66, 0x64, 0x3a, 0x61, 0x30, 0x3a, 0x61, 0x62, 0x3a, 0x31, 0x33, 0x3a, 0x35, 0x63, 0x22, 0x7d}, BackendV6Data:json.RawMessage(nil)}, Expiration:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Asof:0}} }
I0322 03:25:15.274543    2308 trace.go:236] Trace[1171497152]: "List(recursive=true) etcd3" audit-id:,key:/masterleases/,resourceVersion:0,resourceVersionMatch:NotOlderThan,limit:0,continue: (22-Mar-2024 03:25:14.465) (total time: 642ms):
Trace[1171497152]: [642.408707ms] [642.408707ms] END
I0322 03:25:15.323501    2308 trace.go:236] Trace[616484565]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:7d30dee9-458d-4c67-ae79-f2a6608ac909,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:node-controller,verb:GET (22-Mar-2024 03:25:14.756) (total time: 566ms):
Trace[616484565]: ---"About to write a response" 385ms (03:25:15.141)
Trace[616484565]: ---"Writing http response done" 181ms (03:25:15.322)
Trace[616484565]: [566.382195ms] [566.382195ms] END
I0322 03:25:15.441434    2308 trace.go:236] Trace[423263796]: "Patch" accept:application/vnd.kubernetes.protobuf, */*,audit-id:e1dee8ad-bfd5-42d9-bdc3-4213eaec74d0,client:127.0.0.1,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events/local-path-provisioner-6c86858495-mpn68.17bef833a31c9a1e,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:node-controller,verb:PATCH (22-Mar-2024 03:25:14.307) (total time: 1130ms):
Trace[423263796]: ["GuaranteedUpdate etcd3" audit-id:e1dee8ad-bfd5-42d9-bdc3-4213eaec74d0,key:/events/kube-system/local-path-provisioner-6c86858495-mpn68.17bef833a31c9a1e,type:*core.Event,resource:events 1120ms (03:25:14.318)
Trace[423263796]:  ---"initial value restored" 561ms (03:25:14.879)
Trace[423263796]:  ---"About to Encode" 107ms (03:25:14.986)
Trace[423263796]:  ---"Txn call completed" 449ms (03:25:15.436)]
Trace[423263796]: ---"About to check admission control" 96ms (03:25:14.976)
Trace[423263796]: ---"Object stored in database" 460ms (03:25:15.436)
Trace[423263796]: [1.130070524s] [1.130070524s] END
I0322 03:25:15.448021    2308 trace.go:236] Trace[1392216480]: "Update" accept:application/json, */*,audit-id:407792d5-5745-4d56-9742-e59a1dc1a206,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:PUT (22-Mar-2024 03:25:14.871) (total time: 576ms):
Trace[1392216480]: ---"limitedReadBody succeeded" len:4142 52ms (03:25:14.923)
Trace[1392216480]: ["GuaranteedUpdate etcd3" audit-id:407792d5-5745-4d56-9742-e59a1dc1a206,key:/minions/serverworker,type:*core.Node,resource:nodes 518ms (03:25:14.929)]
Trace[1392216480]: [576.263895ms] [576.263895ms] END
I0322 03:25:15.449676    2308 trace.go:236] Trace[1312808627]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:6a0b6448-b339-46cd-89bd-f5b2b9ecd34c,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/node-controller,verb:PUT (22-Mar-2024 03:25:14.861) (total time: 588ms):
Trace[1312808627]: ---"limitedReadBody succeeded" len:3075 86ms (03:25:14.948)
Trace[1312808627]: [588.156825ms] [588.156825ms] END
I0322 03:25:15.490713    2308 trace.go:236] Trace[399037321]: "Get" accept:application/json, */*,audit-id:ad60eba7-5089-460a-bd71-ead49674085d,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:25:14.863) (total time: 626ms):
Trace[399037321]: ---"About to write a response" 578ms (03:25:15.442)
Trace[399037321]: [626.675753ms] [626.675753ms] END
I0322 03:25:15.683568    2308 trace.go:236] Trace[1668832209]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:cf16b0db-2e66-4357-ab67-62730c491936,client:127.0.0.1,protocol:HTTP/2.0,resource:deployments,scope:resource,url:/apis/apps/v1/namespaces/kube-system/deployments/local-path-provisioner/status,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:deployment-controller,verb:PUT (22-Mar-2024 03:25:14.862) (total time: 815ms):
Trace[1668832209]: ---"Conversion done" 127ms (03:25:14.994)
Trace[1668832209]: ["GuaranteedUpdate etcd3" audit-id:cf16b0db-2e66-4357-ab67-62730c491936,key:/deployments/kube-system/local-path-provisioner,type:*apps.Deployment,resource:deployments.apps 679ms (03:25:15.000)
Trace[1668832209]:  ---"About to Encode" 472ms (03:25:15.474)
Trace[1668832209]:  ---"Txn call completed" 145ms (03:25:15.623)]
Trace[1668832209]: ---"Writing http response done" 54ms (03:25:15.678)
Trace[1668832209]: [815.762088ms] [815.762088ms] END
I0322 03:25:16.060559    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="3.966316ms"
I0322 03:25:16.098570    2308 topologycache.go:237] "Can't get CPU or zone information for node" node="serverworker"
I0322 03:25:16.206874    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="91.844488ms"
I0322 03:25:16.206982    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="64.804s"
I0322 03:25:16.270446    2308 node_controller.go:502] Successfully initialized node serverworker with cloud provider
I0322 03:25:16.272689    2308 event.go:307] "Event occurred" object="serverworker" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="Synced" message="Node synced successfully"
I0322 03:25:17.250665    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="77.348586ms"
I0322 03:25:17.253622    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="2.40881ms"
I0322 03:25:17.482218    2308 handler.go:275] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0322 03:25:18.451612    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="13.931558ms"
I0322 03:25:18.451899    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="127.58s"
I0322 03:25:19.222257    2308 node_lifecycle_controller.go:1048] "Controller detected that some Nodes are Ready. Exiting master disruption mode"
I0322 03:25:24.376691    2308 scope.go:117] "RemoveContainer" containerID="7794e189bc1b2b2e4251a637e10afba15513bb8034f56b5f81f0d281a4148b00"
I0322 03:25:24.402526    2308 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/helm-install-traefik-t9wf9" containerName="helm"
I0322 03:25:24.411565    2308 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:25:25.322673    2308 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:25:25.712212    2308 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:25:29.347080    2308 alloc.go:330] "allocated clusterIPs" service="kube-system/traefik" clusterIPs={"IPv4":"10.43.192.163"}
I0322 03:25:29.461230    2308 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Service" apiVersion="v1" type="Normal" reason="EnsuringLoadBalancer" message="Ensuring load balancer"
I0322 03:25:30.870976    2308 trace.go:236] Trace[550292070]: "Patch" accept:application/vnd.kubernetes.protobuf, */*,audit-id:a8e786d8-9435-4799-89a6-1802dda0f7a1,client:127.0.0.1,protocol:HTTP/2.0,resource:services,scope:resource,url:/api/v1/namespaces/kube-system/services/traefik/status,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/service-controller,verb:PATCH (22-Mar-2024 03:25:30.195) (total time: 671ms):
Trace[550292070]: ---"Recorded the audit event" 18ms (03:25:30.216)
Trace[550292070]: ["GuaranteedUpdate etcd3" audit-id:a8e786d8-9435-4799-89a6-1802dda0f7a1,key:/services/specs/kube-system/traefik,type:*core.Service,resource:services 638ms (03:25:30.229)
Trace[550292070]:  ---"About to Encode" 490ms (03:25:30.722)
Trace[550292070]:  ---"Encode succeeded" len:1535 65ms (03:25:30.788)
Trace[550292070]:  ---"Txn call completed" 78ms (03:25:30.867)]
Trace[550292070]: ---"About to check admission control" 183ms (03:25:30.416)
Trace[550292070]: ---"Object stored in database" 450ms (03:25:30.867)
Trace[550292070]: [671.645154ms] [671.645154ms] END
I0322 03:25:31.135621    2308 trace.go:236] Trace[1235228345]: "Create" accept:application/json,audit-id:6cfe1e91-cf18-4c81-8340-94593ee3dd0e,client:10.42.0.2,protocol:HTTP/2.0,resource:ingressclasses,scope:resource,url:/apis/networking.k8s.io/v1/ingressclasses,user-agent:Helm/3.12.3,verb:POST (22-Mar-2024 03:25:30.604) (total time: 528ms):
Trace[1235228345]: ---"limitedReadBody succeeded" len:475 60ms (03:25:30.664)
Trace[1235228345]: ---"Conversion done" 71ms (03:25:30.741)
Trace[1235228345]: ---"Writing http response done" 52ms (03:25:31.132)
Trace[1235228345]: [528.099369ms] [528.099369ms] END
I0322 03:25:31.213785    2308 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set traefik-f4564c4f4 to 1"
I0322 03:25:31.326046    2308 controller.go:624] quota admission added evaluator for: daemonsets.apps
I0322 03:25:31.331146    2308 controller.go:624] quota admission added evaluator for: ingressroutes.traefik.io
I0322 03:25:31.379346    2308 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Service" apiVersion="v1" type="Normal" reason="AppliedDaemonSet" message="Applied LoadBalancer DaemonSet kube-system/svclb-traefik-290a977b"
I0322 03:25:31.388825    2308 event.go:307] "Event occurred" object="kube-system/traefik-f4564c4f4" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: traefik-f4564c4f4-wtfts"
I0322 03:25:31.426250    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="265.109528ms"
I0322 03:25:31.427996    2308 controller.go:624] quota admission added evaluator for: controllerrevisions.apps
I0322 03:25:31.454507    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="28.117538ms"
I0322 03:25:31.454734    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="47.294s"
I0322 03:25:31.502173    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="98.892s"
I0322 03:25:31.505257    2308 event.go:307] "Event occurred" object="kube-system/svclb-traefik-290a977b" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: svclb-traefik-290a977b-phx87"
I0322 03:25:31.533910    2308 event.go:307] "Event occurred" object="kube-system/svclb-traefik-290a977b" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: svclb-traefik-290a977b-hfkdf"
I0322 03:25:31.621620    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="67.005s"
I0322 03:25:31.627716    2308 topology_manager.go:215] "Topology Admit Handler" podUID="19ab54ce-ae54-4a36-88d8-9339ee518b72" podNamespace="kube-system" podName="svclb-traefik-290a977b-phx87"
E0322 03:25:31.628926    2308 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="416389e0-ea56-4446-afa6-333902153fd1" containerName="helm"
I0322 03:25:31.629353    2308 memory_manager.go:346] "RemoveStaleState removing state" podUID="416389e0-ea56-4446-afa6-333902153fd1" containerName="helm"
I0322 03:25:32.393761    2308 scope.go:117] "RemoveContainer" containerID="7794e189bc1b2b2e4251a637e10afba15513bb8034f56b5f81f0d281a4148b00"
I0322 03:25:32.428021    2308 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:25:33.472270    2308 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:25:33.473688    2308 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:25:33.634182    2308 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:25:33.696880    2308 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"values\" (UniqueName: \"kubernetes.io/secret/598e1d6b-dd9f-40b6-8abc-7ed9692a3a85-values\") pod \"598e1d6b-dd9f-40b6-8abc-7ed9692a3a85\" (UID: \"598e1d6b-dd9f-40b6-8abc-7ed9692a3a85\") "
I0322 03:25:33.696955    2308 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-w58d6\" (UniqueName: \"kubernetes.io/projected/598e1d6b-dd9f-40b6-8abc-7ed9692a3a85-kube-api-access-w58d6\") pod \"598e1d6b-dd9f-40b6-8abc-7ed9692a3a85\" (UID: \"598e1d6b-dd9f-40b6-8abc-7ed9692a3a85\") "
I0322 03:25:33.697010    2308 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/598e1d6b-dd9f-40b6-8abc-7ed9692a3a85-klipper-cache\") pod \"598e1d6b-dd9f-40b6-8abc-7ed9692a3a85\" (UID: \"598e1d6b-dd9f-40b6-8abc-7ed9692a3a85\") "
I0322 03:25:33.697054    2308 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/598e1d6b-dd9f-40b6-8abc-7ed9692a3a85-tmp\") pod \"598e1d6b-dd9f-40b6-8abc-7ed9692a3a85\" (UID: \"598e1d6b-dd9f-40b6-8abc-7ed9692a3a85\") "
I0322 03:25:33.697080    2308 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/598e1d6b-dd9f-40b6-8abc-7ed9692a3a85-klipper-config\") pod \"598e1d6b-dd9f-40b6-8abc-7ed9692a3a85\" (UID: \"598e1d6b-dd9f-40b6-8abc-7ed9692a3a85\") "
I0322 03:25:33.697103    2308 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"content\" (UniqueName: \"kubernetes.io/configmap/598e1d6b-dd9f-40b6-8abc-7ed9692a3a85-content\") pod \"598e1d6b-dd9f-40b6-8abc-7ed9692a3a85\" (UID: \"598e1d6b-dd9f-40b6-8abc-7ed9692a3a85\") "
I0322 03:25:33.697130    2308 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/598e1d6b-dd9f-40b6-8abc-7ed9692a3a85-klipper-helm\") pod \"598e1d6b-dd9f-40b6-8abc-7ed9692a3a85\" (UID: \"598e1d6b-dd9f-40b6-8abc-7ed9692a3a85\") "
I0322 03:25:33.706328    2308 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/598e1d6b-dd9f-40b6-8abc-7ed9692a3a85-values" (OuterVolumeSpecName: "values") pod "598e1d6b-dd9f-40b6-8abc-7ed9692a3a85" (UID: "598e1d6b-dd9f-40b6-8abc-7ed9692a3a85"). InnerVolumeSpecName "values". PluginName "kubernetes.io/secret", VolumeGidValue ""
I0322 03:25:33.706613    2308 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/598e1d6b-dd9f-40b6-8abc-7ed9692a3a85-klipper-helm" (OuterVolumeSpecName: "klipper-helm") pod "598e1d6b-dd9f-40b6-8abc-7ed9692a3a85" (UID: "598e1d6b-dd9f-40b6-8abc-7ed9692a3a85"). InnerVolumeSpecName "klipper-helm". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0322 03:25:33.712287    2308 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/598e1d6b-dd9f-40b6-8abc-7ed9692a3a85-klipper-config" (OuterVolumeSpecName: "klipper-config") pod "598e1d6b-dd9f-40b6-8abc-7ed9692a3a85" (UID: "598e1d6b-dd9f-40b6-8abc-7ed9692a3a85"). InnerVolumeSpecName "klipper-config". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0322 03:25:33.714651    2308 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/configmap/598e1d6b-dd9f-40b6-8abc-7ed9692a3a85-content" (OuterVolumeSpecName: "content") pod "598e1d6b-dd9f-40b6-8abc-7ed9692a3a85" (UID: "598e1d6b-dd9f-40b6-8abc-7ed9692a3a85"). InnerVolumeSpecName "content". PluginName "kubernetes.io/configmap", VolumeGidValue ""
I0322 03:25:33.718156    2308 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/598e1d6b-dd9f-40b6-8abc-7ed9692a3a85-kube-api-access-w58d6" (OuterVolumeSpecName: "kube-api-access-w58d6") pod "598e1d6b-dd9f-40b6-8abc-7ed9692a3a85" (UID: "598e1d6b-dd9f-40b6-8abc-7ed9692a3a85"). InnerVolumeSpecName "kube-api-access-w58d6". PluginName "kubernetes.io/projected", VolumeGidValue ""
I0322 03:25:33.718536    2308 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/598e1d6b-dd9f-40b6-8abc-7ed9692a3a85-tmp" (OuterVolumeSpecName: "tmp") pod "598e1d6b-dd9f-40b6-8abc-7ed9692a3a85" (UID: "598e1d6b-dd9f-40b6-8abc-7ed9692a3a85"). InnerVolumeSpecName "tmp". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0322 03:25:33.721593    2308 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/598e1d6b-dd9f-40b6-8abc-7ed9692a3a85-klipper-cache" (OuterVolumeSpecName: "klipper-cache") pod "598e1d6b-dd9f-40b6-8abc-7ed9692a3a85" (UID: "598e1d6b-dd9f-40b6-8abc-7ed9692a3a85"). InnerVolumeSpecName "klipper-cache". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0322 03:25:33.798503    2308 reconciler_common.go:300] "Volume detached for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/598e1d6b-dd9f-40b6-8abc-7ed9692a3a85-klipper-helm\") on node \"server\" DevicePath \"\""
I0322 03:25:33.798536    2308 reconciler_common.go:300] "Volume detached for volume \"values\" (UniqueName: \"kubernetes.io/secret/598e1d6b-dd9f-40b6-8abc-7ed9692a3a85-values\") on node \"server\" DevicePath \"\""
I0322 03:25:33.798582    2308 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-w58d6\" (UniqueName: \"kubernetes.io/projected/598e1d6b-dd9f-40b6-8abc-7ed9692a3a85-kube-api-access-w58d6\") on node \"server\" DevicePath \"\""
I0322 03:25:33.798594    2308 reconciler_common.go:300] "Volume detached for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/598e1d6b-dd9f-40b6-8abc-7ed9692a3a85-tmp\") on node \"server\" DevicePath \"\""
I0322 03:25:33.798603    2308 reconciler_common.go:300] "Volume detached for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/598e1d6b-dd9f-40b6-8abc-7ed9692a3a85-klipper-cache\") on node \"server\" DevicePath \"\""
I0322 03:25:33.798609    2308 reconciler_common.go:300] "Volume detached for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/598e1d6b-dd9f-40b6-8abc-7ed9692a3a85-klipper-config\") on node \"server\" DevicePath \"\""
I0322 03:25:33.798618    2308 reconciler_common.go:300] "Volume detached for volume \"content\" (UniqueName: \"kubernetes.io/configmap/598e1d6b-dd9f-40b6-8abc-7ed9692a3a85-content\") on node \"server\" DevicePath \"\""
I0322 03:25:34.422697    2308 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="145567761f3c16244d0cdfb88d83c26d17d22112a7765191bbfb7b782d559985"
I0322 03:25:34.438282    2308 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:25:34.483835    2308 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:25:34.495400    2308 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:25:34.507199    2308 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:25:34.509849    2308 event.go:307] "Event occurred" object="kube-system/helm-install-traefik" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="Completed" message="Job completed"
I0322 03:25:36.457294    2308 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/svclb-traefik-290a977b-phx87" containerName="lb-tcp-80"
I0322 03:25:36.593664    2308 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/svclb-traefik-290a977b-phx87" containerName="lb-tcp-443"
I0322 03:25:37.467327    2308 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/svclb-traefik-290a977b-phx87" podStartSLOduration=2.157775179 podCreationTimestamp="2024-03-22 03:25:31 +0000 UTC" firstStartedPulling="2024-03-22 03:25:32.145194443 +0000 UTC m=+487.840007642" lastFinishedPulling="2024-03-22 03:25:36.451024382 +0000 UTC m=+492.145837576" observedRunningTime="2024-03-22 03:25:37.461832127 +0000 UTC m=+493.156645335" watchObservedRunningTime="2024-03-22 03:25:37.463605113 +0000 UTC m=+493.158418312"
I0322 03:25:37.571925    2308 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Service" apiVersion="v1" type="Normal" reason="UpdatedLoadBalancer" message="Updated LoadBalancer with new IPs: [] -> [192.168.56.110]"
I0322 03:25:42.106979    2308 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Service" apiVersion="v1" type="Normal" reason="UpdatedLoadBalancer" message="Updated LoadBalancer with new IPs: [192.168.56.110] -> [192.168.56.110 192.168.56.111]"
I0322 03:25:44.081124    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="759.637s"
I0322 03:25:45.120526    2308 endpointslice_controller.go:310] "Error syncing endpoint slices for service, retrying" key="kube-system/traefik" err="EndpointSlice informer cache is out of date"
I0322 03:25:45.120879    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="25.698479ms"
I0322 03:25:45.124574    2308 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="79.901s"
time="2024-03-22T03:28:20Z" level=info msg="Starting k3s v1.28.7+k3s1 (051b14b2)"
time="2024-03-22T03:28:20Z" level=info msg="Configuring sqlite3 database connection pooling: maxIdleConns=2, maxOpenConns=0, connMaxLifetime=0s"
time="2024-03-22T03:28:20Z" level=info msg="Configuring database table schema and indexes, this may take a moment..."
time="2024-03-22T03:28:20Z" level=info msg="Database tables and indexes are up to date"
time="2024-03-22T03:28:20Z" level=info msg="Kine available at unix://kine.sock"
time="2024-03-22T03:28:20Z" level=info msg="generated self-signed CA certificate CN=k3s-client-ca@1711078100: notBefore=2024-03-22 03:28:20.115875809 +0000 UTC notAfter=2034-03-20 03:28:20.115875809 +0000 UTC"
time="2024-03-22T03:28:20Z" level=info msg="certificate CN=system:admin,O=system:masters signed by CN=k3s-client-ca@1711078100: notBefore=2024-03-22 03:28:20 +0000 UTC notAfter=2025-03-22 03:28:20 +0000 UTC"
time="2024-03-22T03:28:20Z" level=info msg="certificate CN=system:k3s-supervisor,O=system:masters signed by CN=k3s-client-ca@1711078100: notBefore=2024-03-22 03:28:20 +0000 UTC notAfter=2025-03-22 03:28:20 +0000 UTC"
time="2024-03-22T03:28:20Z" level=info msg="certificate CN=system:kube-controller-manager signed by CN=k3s-client-ca@1711078100: notBefore=2024-03-22 03:28:20 +0000 UTC notAfter=2025-03-22 03:28:20 +0000 UTC"
time="2024-03-22T03:28:20Z" level=info msg="certificate CN=system:kube-scheduler signed by CN=k3s-client-ca@1711078100: notBefore=2024-03-22 03:28:20 +0000 UTC notAfter=2025-03-22 03:28:20 +0000 UTC"
time="2024-03-22T03:28:20Z" level=info msg="certificate CN=system:apiserver,O=system:masters signed by CN=k3s-client-ca@1711078100: notBefore=2024-03-22 03:28:20 +0000 UTC notAfter=2025-03-22 03:28:20 +0000 UTC"
time="2024-03-22T03:28:20Z" level=info msg="certificate CN=system:kube-proxy signed by CN=k3s-client-ca@1711078100: notBefore=2024-03-22 03:28:20 +0000 UTC notAfter=2025-03-22 03:28:20 +0000 UTC"
time="2024-03-22T03:28:20Z" level=info msg="certificate CN=system:k3s-controller signed by CN=k3s-client-ca@1711078100: notBefore=2024-03-22 03:28:20 +0000 UTC notAfter=2025-03-22 03:28:20 +0000 UTC"
time="2024-03-22T03:28:20Z" level=info msg="certificate CN=k3s-cloud-controller-manager signed by CN=k3s-client-ca@1711078100: notBefore=2024-03-22 03:28:20 +0000 UTC notAfter=2025-03-22 03:28:20 +0000 UTC"
time="2024-03-22T03:28:20Z" level=info msg="generated self-signed CA certificate CN=k3s-server-ca@1711078100: notBefore=2024-03-22 03:28:20.123045902 +0000 UTC notAfter=2034-03-20 03:28:20.123045902 +0000 UTC"
time="2024-03-22T03:28:20Z" level=info msg="certificate CN=kube-apiserver signed by CN=k3s-server-ca@1711078100: notBefore=2024-03-22 03:28:20 +0000 UTC notAfter=2025-03-22 03:28:20 +0000 UTC"
time="2024-03-22T03:28:20Z" level=info msg="generated self-signed CA certificate CN=k3s-request-header-ca@1711078100: notBefore=2024-03-22 03:28:20.124528744 +0000 UTC notAfter=2034-03-20 03:28:20.124528744 +0000 UTC"
time="2024-03-22T03:28:20Z" level=info msg="certificate CN=system:auth-proxy signed by CN=k3s-request-header-ca@1711078100: notBefore=2024-03-22 03:28:20 +0000 UTC notAfter=2025-03-22 03:28:20 +0000 UTC"
time="2024-03-22T03:28:20Z" level=info msg="generated self-signed CA certificate CN=etcd-server-ca@1711078100: notBefore=2024-03-22 03:28:20.125600421 +0000 UTC notAfter=2034-03-20 03:28:20.125600421 +0000 UTC"
time="2024-03-22T03:28:20Z" level=info msg="certificate CN=etcd-client signed by CN=etcd-server-ca@1711078100: notBefore=2024-03-22 03:28:20 +0000 UTC notAfter=2025-03-22 03:28:20 +0000 UTC"
time="2024-03-22T03:28:20Z" level=info msg="generated self-signed CA certificate CN=etcd-peer-ca@1711078100: notBefore=2024-03-22 03:28:20.126686751 +0000 UTC notAfter=2034-03-20 03:28:20.126686751 +0000 UTC"
time="2024-03-22T03:28:20Z" level=info msg="certificate CN=etcd-peer signed by CN=etcd-peer-ca@1711078100: notBefore=2024-03-22 03:28:20 +0000 UTC notAfter=2025-03-22 03:28:20 +0000 UTC"
time="2024-03-22T03:28:20Z" level=info msg="certificate CN=etcd-server signed by CN=etcd-server-ca@1711078100: notBefore=2024-03-22 03:28:20 +0000 UTC notAfter=2025-03-22 03:28:20 +0000 UTC"
time="2024-03-22T03:28:20Z" level=info msg="Saving cluster bootstrap data to datastore"
time="2024-03-22T03:28:20Z" level=info msg="certificate CN=k3s,O=k3s signed by CN=k3s-server-ca@1711078100: notBefore=2024-03-22 03:28:20 +0000 UTC notAfter=2025-03-22 03:28:20 +0000 UTC"
time="2024-03-22T03:28:20Z" level=warning msg="dynamiclistener [::]:6443: no cached certificate available for preload - deferring certificate load until storage initialization or first client request"
time="2024-03-22T03:28:20Z" level=info msg="Active TLS secret / (ver=) (count 10): map[listener.cattle.io/cn-10.43.0.1:10.43.0.1 listener.cattle.io/cn-127.0.0.1:127.0.0.1 listener.cattle.io/cn-192.168.56.110:192.168.56.110 listener.cattle.io/cn-__1-f16284:::1 listener.cattle.io/cn-kubernetes:kubernetes listener.cattle.io/cn-kubernetes.default:kubernetes.default listener.cattle.io/cn-kubernetes.default.svc:kubernetes.default.svc listener.cattle.io/cn-kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local listener.cattle.io/cn-localhost:localhost listener.cattle.io/cn-server:server listener.cattle.io/fingerprint:SHA1=3EF84C4C3E5D7F82BBAF806634968BEF8CE80EDA]"
time="2024-03-22T03:28:20Z" level=info msg="Running kube-apiserver --advertise-address=192.168.56.110 --advertise-port=6443 --allow-privileged=true --anonymous-auth=false --api-audiences=https://kubernetes.default.svc.cluster.local,k3s --authorization-mode=Node,RBAC --bind-address=127.0.0.1 --cert-dir=/var/lib/rancher/k3s/server/tls/temporary-certs --client-ca-file=/var/lib/rancher/k3s/server/tls/client-ca.crt --egress-selector-config-file=/var/lib/rancher/k3s/server/etc/egress-selector-config.yaml --enable-admission-plugins=NodeRestriction --enable-aggregator-routing=true --enable-bootstrap-token-auth=true --etcd-servers=unix://kine.sock --kubelet-certificate-authority=/var/lib/rancher/k3s/server/tls/server-ca.crt --kubelet-client-certificate=/var/lib/rancher/k3s/server/tls/client-kube-apiserver.crt --kubelet-client-key=/var/lib/rancher/k3s/server/tls/client-kube-apiserver.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --profiling=false --proxy-client-cert-file=/var/lib/rancher/k3s/server/tls/client-auth-proxy.crt --proxy-client-key-file=/var/lib/rancher/k3s/server/tls/client-auth-proxy.key --requestheader-allowed-names=system:auth-proxy --requestheader-client-ca-file=/var/lib/rancher/k3s/server/tls/request-header-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6444 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/var/lib/rancher/k3s/server/tls/service.key --service-account-signing-key-file=/var/lib/rancher/k3s/server/tls/service.current.key --service-cluster-ip-range=10.43.0.0/16 --service-node-port-range=30000-32767 --storage-backend=etcd3 --tls-cert-file=/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.crt --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 --tls-private-key-file=/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.key"
time="2024-03-22T03:28:20Z" level=info msg="Running kube-scheduler --authentication-kubeconfig=/var/lib/rancher/k3s/server/cred/scheduler.kubeconfig --authorization-kubeconfig=/var/lib/rancher/k3s/server/cred/scheduler.kubeconfig --bind-address=127.0.0.1 --kubeconfig=/var/lib/rancher/k3s/server/cred/scheduler.kubeconfig --leader-elect=false --profiling=false --secure-port=10259"
I0322 03:28:20.503401    2280 options.go:220] external host was not specified, using 192.168.56.110
time="2024-03-22T03:28:20Z" level=info msg="Running kube-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/var/lib/rancher/k3s/server/cred/controller.kubeconfig --authorization-kubeconfig=/var/lib/rancher/k3s/server/cred/controller.kubeconfig --bind-address=127.0.0.1 --cluster-cidr=10.42.0.0/16 --cluster-signing-kube-apiserver-client-cert-file=/var/lib/rancher/k3s/server/tls/client-ca.nochain.crt --cluster-signing-kube-apiserver-client-key-file=/var/lib/rancher/k3s/server/tls/client-ca.key --cluster-signing-kubelet-client-cert-file=/var/lib/rancher/k3s/server/tls/client-ca.nochain.crt --cluster-signing-kubelet-client-key-file=/var/lib/rancher/k3s/server/tls/client-ca.key --cluster-signing-kubelet-serving-cert-file=/var/lib/rancher/k3s/server/tls/server-ca.nochain.crt --cluster-signing-kubelet-serving-key-file=/var/lib/rancher/k3s/server/tls/server-ca.key --cluster-signing-legacy-unknown-cert-file=/var/lib/rancher/k3s/server/tls/server-ca.nochain.crt --cluster-signing-legacy-unknown-key-file=/var/lib/rancher/k3s/server/tls/server-ca.key --configure-cloud-routes=false --controllers=*,tokencleaner,-service,-route,-cloud-node-lifecycle --kubeconfig=/var/lib/rancher/k3s/server/cred/controller.kubeconfig --leader-elect=false --profiling=false --root-ca-file=/var/lib/rancher/k3s/server/tls/server-ca.crt --secure-port=10257 --service-account-private-key-file=/var/lib/rancher/k3s/server/tls/service.current.key --service-cluster-ip-range=10.43.0.0/16 --use-service-account-credentials=true"
time="2024-03-22T03:28:20Z" level=info msg="Running cloud-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/var/lib/rancher/k3s/server/cred/cloud-controller.kubeconfig --authorization-kubeconfig=/var/lib/rancher/k3s/server/cred/cloud-controller.kubeconfig --bind-address=127.0.0.1 --cloud-config=/var/lib/rancher/k3s/server/etc/cloud-config.yaml --cloud-provider=k3s --cluster-cidr=10.42.0.0/16 --configure-cloud-routes=false --controllers=*,-route --feature-gates=CloudDualStackNodeIPs=true --kubeconfig=/var/lib/rancher/k3s/server/cred/cloud-controller.kubeconfig --leader-elect=false --leader-elect-resource-name=k3s-cloud-controller-manager --node-status-update-frequency=1m0s --profiling=false"
I0322 03:28:20.504114    2280 server.go:156] Version: v1.28.7+k3s1
I0322 03:28:20.504152    2280 server.go:158] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
time="2024-03-22T03:28:20Z" level=info msg="Server node token is available at /var/lib/rancher/k3s/server/token"
time="2024-03-22T03:28:20Z" level=info msg="To join server node to cluster: k3s server -s https://10.0.2.15:6443 -t ${SERVER_NODE_TOKEN}"
time="2024-03-22T03:28:20Z" level=info msg="Agent node token is available at /var/lib/rancher/k3s/server/agent-token"
time="2024-03-22T03:28:20Z" level=info msg="To join agent node to cluster: k3s agent -s https://10.0.2.15:6443 -t ${AGENT_NODE_TOKEN}"
time="2024-03-22T03:28:20Z" level=info msg="Wrote kubeconfig /etc/rancher/k3s/k3s.yaml"
time="2024-03-22T03:28:20Z" level=info msg="Run: k3s kubectl"
time="2024-03-22T03:28:20Z" level=info msg="Waiting for API server to become available"
I0322 03:28:21.139734    2280 plugins.go:158] Loaded 12 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,TaintNodesByCondition,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,MutatingAdmissionWebhook.
I0322 03:28:21.139856    2280 plugins.go:161] Loaded 13 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,ClusterTrustBundleAttest,CertificateSubjectRestriction,ValidatingAdmissionPolicy,ValidatingAdmissionWebhook,ResourceQuota.
I0322 03:28:21.140431    2280 instance.go:298] Using reconciler: lease
I0322 03:28:21.140778    2280 shared_informer.go:311] Waiting for caches to sync for node_authorizer
I0322 03:28:21.148311    2280 handler.go:275] Adding GroupVersion apiextensions.k8s.io v1 to ResourceManager
W0322 03:28:21.148442    2280 genericapiserver.go:744] Skipping API apiextensions.k8s.io/v1beta1 because it has no resources.
time="2024-03-22T03:28:21Z" level=info msg="Password verified locally for node server"
time="2024-03-22T03:28:21Z" level=info msg="certificate CN=server signed by CN=k3s-server-ca@1711078100: notBefore=2024-03-22 03:28:20 +0000 UTC notAfter=2025-03-22 03:28:21 +0000 UTC"
I0322 03:28:21.247228    2280 handler.go:275] Adding GroupVersion  v1 to ResourceManager
I0322 03:28:21.247424    2280 instance.go:709] API group "internal.apiserver.k8s.io" is not enabled, skipping.
I0322 03:28:21.411899    2280 instance.go:709] API group "resource.k8s.io" is not enabled, skipping.
I0322 03:28:21.418103    2280 handler.go:275] Adding GroupVersion authentication.k8s.io v1 to ResourceManager
W0322 03:28:21.418215    2280 genericapiserver.go:744] Skipping API authentication.k8s.io/v1beta1 because it has no resources.
W0322 03:28:21.418268    2280 genericapiserver.go:744] Skipping API authentication.k8s.io/v1alpha1 because it has no resources.
I0322 03:28:21.418748    2280 handler.go:275] Adding GroupVersion authorization.k8s.io v1 to ResourceManager
W0322 03:28:21.418811    2280 genericapiserver.go:744] Skipping API authorization.k8s.io/v1beta1 because it has no resources.
I0322 03:28:21.419574    2280 handler.go:275] Adding GroupVersion autoscaling v2 to ResourceManager
I0322 03:28:21.420241    2280 handler.go:275] Adding GroupVersion autoscaling v1 to ResourceManager
W0322 03:28:21.420293    2280 genericapiserver.go:744] Skipping API autoscaling/v2beta1 because it has no resources.
W0322 03:28:21.420299    2280 genericapiserver.go:744] Skipping API autoscaling/v2beta2 because it has no resources.
I0322 03:28:21.421547    2280 handler.go:275] Adding GroupVersion batch v1 to ResourceManager
W0322 03:28:21.421601    2280 genericapiserver.go:744] Skipping API batch/v1beta1 because it has no resources.
I0322 03:28:21.422252    2280 handler.go:275] Adding GroupVersion certificates.k8s.io v1 to ResourceManager
W0322 03:28:21.422298    2280 genericapiserver.go:744] Skipping API certificates.k8s.io/v1beta1 because it has no resources.
W0322 03:28:21.422641    2280 genericapiserver.go:744] Skipping API certificates.k8s.io/v1alpha1 because it has no resources.
I0322 03:28:21.423107    2280 handler.go:275] Adding GroupVersion coordination.k8s.io v1 to ResourceManager
W0322 03:28:21.423153    2280 genericapiserver.go:744] Skipping API coordination.k8s.io/v1beta1 because it has no resources.
W0322 03:28:21.423315    2280 genericapiserver.go:744] Skipping API discovery.k8s.io/v1beta1 because it has no resources.
I0322 03:28:21.423735    2280 handler.go:275] Adding GroupVersion discovery.k8s.io v1 to ResourceManager
I0322 03:28:21.425074    2280 handler.go:275] Adding GroupVersion networking.k8s.io v1 to ResourceManager
W0322 03:28:21.425162    2280 genericapiserver.go:744] Skipping API networking.k8s.io/v1beta1 because it has no resources.
W0322 03:28:21.425171    2280 genericapiserver.go:744] Skipping API networking.k8s.io/v1alpha1 because it has no resources.
I0322 03:28:21.425968    2280 handler.go:275] Adding GroupVersion node.k8s.io v1 to ResourceManager
W0322 03:28:21.426005    2280 genericapiserver.go:744] Skipping API node.k8s.io/v1beta1 because it has no resources.
W0322 03:28:21.426010    2280 genericapiserver.go:744] Skipping API node.k8s.io/v1alpha1 because it has no resources.
I0322 03:28:21.426683    2280 handler.go:275] Adding GroupVersion policy v1 to ResourceManager
W0322 03:28:21.426701    2280 genericapiserver.go:744] Skipping API policy/v1beta1 because it has no resources.
I0322 03:28:21.428226    2280 handler.go:275] Adding GroupVersion rbac.authorization.k8s.io v1 to ResourceManager
W0322 03:28:21.428252    2280 genericapiserver.go:744] Skipping API rbac.authorization.k8s.io/v1beta1 because it has no resources.
W0322 03:28:21.428260    2280 genericapiserver.go:744] Skipping API rbac.authorization.k8s.io/v1alpha1 because it has no resources.
I0322 03:28:21.428673    2280 handler.go:275] Adding GroupVersion scheduling.k8s.io v1 to ResourceManager
W0322 03:28:21.428691    2280 genericapiserver.go:744] Skipping API scheduling.k8s.io/v1beta1 because it has no resources.
W0322 03:28:21.428695    2280 genericapiserver.go:744] Skipping API scheduling.k8s.io/v1alpha1 because it has no resources.
I0322 03:28:21.430304    2280 handler.go:275] Adding GroupVersion storage.k8s.io v1 to ResourceManager
W0322 03:28:21.430319    2280 genericapiserver.go:744] Skipping API storage.k8s.io/v1beta1 because it has no resources.
W0322 03:28:21.430323    2280 genericapiserver.go:744] Skipping API storage.k8s.io/v1alpha1 because it has no resources.
I0322 03:28:21.431196    2280 handler.go:275] Adding GroupVersion flowcontrol.apiserver.k8s.io v1beta3 to ResourceManager
I0322 03:28:21.432081    2280 handler.go:275] Adding GroupVersion flowcontrol.apiserver.k8s.io v1beta2 to ResourceManager
W0322 03:28:21.432094    2280 genericapiserver.go:744] Skipping API flowcontrol.apiserver.k8s.io/v1beta1 because it has no resources.
W0322 03:28:21.432098    2280 genericapiserver.go:744] Skipping API flowcontrol.apiserver.k8s.io/v1alpha1 because it has no resources.
I0322 03:28:21.434589    2280 handler.go:275] Adding GroupVersion apps v1 to ResourceManager
W0322 03:28:21.434604    2280 genericapiserver.go:744] Skipping API apps/v1beta2 because it has no resources.
W0322 03:28:21.434609    2280 genericapiserver.go:744] Skipping API apps/v1beta1 because it has no resources.
I0322 03:28:21.435255    2280 handler.go:275] Adding GroupVersion admissionregistration.k8s.io v1 to ResourceManager
W0322 03:28:21.435268    2280 genericapiserver.go:744] Skipping API admissionregistration.k8s.io/v1beta1 because it has no resources.
W0322 03:28:21.435272    2280 genericapiserver.go:744] Skipping API admissionregistration.k8s.io/v1alpha1 because it has no resources.
I0322 03:28:21.435791    2280 handler.go:275] Adding GroupVersion events.k8s.io v1 to ResourceManager
W0322 03:28:21.435803    2280 genericapiserver.go:744] Skipping API events.k8s.io/v1beta1 because it has no resources.
I0322 03:28:21.438521    2280 handler.go:275] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
W0322 03:28:21.438555    2280 genericapiserver.go:744] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
time="2024-03-22T03:28:21Z" level=info msg="certificate CN=system:node:server,O=system:nodes signed by CN=k3s-client-ca@1711078100: notBefore=2024-03-22 03:28:20 +0000 UTC notAfter=2025-03-22 03:28:21 +0000 UTC"
time="2024-03-22T03:28:21Z" level=info msg="Module overlay was already loaded"
time="2024-03-22T03:28:21Z" level=info msg="Module br_netfilter was already loaded"
time="2024-03-22T03:28:21Z" level=info msg="Set sysctl 'net/ipv4/conf/all/forwarding' to 1"
time="2024-03-22T03:28:21Z" level=info msg="Set sysctl 'net/netfilter/nf_conntrack_max' to 131072"
time="2024-03-22T03:28:21Z" level=info msg="Set sysctl 'net/netfilter/nf_conntrack_tcp_timeout_established' to 86400"
time="2024-03-22T03:28:21Z" level=info msg="Set sysctl 'net/netfilter/nf_conntrack_tcp_timeout_close_wait' to 3600"
time="2024-03-22T03:28:21Z" level=info msg="Logging containerd to /var/lib/rancher/k3s/agent/containerd/containerd.log"
time="2024-03-22T03:28:21Z" level=info msg="Running containerd -c /var/lib/rancher/k3s/agent/etc/containerd/config.toml -a /run/k3s/containerd/containerd.sock --state /run/k3s/containerd --root /var/lib/rancher/k3s/agent/containerd"
I0322 03:28:22.036409    2280 secure_serving.go:213] Serving securely on 127.0.0.1:6444
I0322 03:28:22.036725    2280 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/rancher/k3s/server/tls/request-header-ca.crt"
I0322 03:28:22.038219    2280 controller.go:116] Starting legacy_token_tracking_controller
I0322 03:28:22.038240    2280 shared_informer.go:311] Waiting for caches to sync for configmaps
I0322 03:28:22.039606    2280 gc_controller.go:78] Starting apiserver lease garbage collector
I0322 03:28:22.036738    2280 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/rancher/k3s/server/tls/client-ca.crt"
I0322 03:28:22.039889    2280 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0322 03:28:22.039953    2280 shared_informer.go:311] Waiting for caches to sync for cluster_authentication_trust_controller
I0322 03:28:22.040159    2280 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0322 03:28:22.040210    2280 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0322 03:28:22.040337    2280 handler_discovery.go:412] Starting ResourceDiscoveryManager
I0322 03:28:22.040717    2280 aggregator.go:164] waiting for initial CRD sync...
I0322 03:28:22.040786    2280 controller.go:80] Starting OpenAPI V3 AggregationController
I0322 03:28:22.041076    2280 customresource_discovery_controller.go:289] Starting DiscoveryController
I0322 03:28:22.041244    2280 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/rancher/k3s/server/tls/client-auth-proxy.crt::/var/lib/rancher/k3s/server/tls/client-auth-proxy.key"
I0322 03:28:22.041886    2280 gc_controller.go:78] Starting apiserver lease garbage collector
I0322 03:28:22.036774    2280 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.crt::/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.key"
I0322 03:28:22.036792    2280 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0322 03:28:22.036863    2280 available_controller.go:423] Starting AvailableConditionController
I0322 03:28:22.047486    2280 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0322 03:28:22.047763    2280 controller.go:78] Starting OpenAPI AggregationController
I0322 03:28:22.047881    2280 apf_controller.go:374] Starting API Priority and Fairness config controller
I0322 03:28:22.048000    2280 system_namespaces_controller.go:67] Starting system namespaces controller
I0322 03:28:22.048105    2280 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/rancher/k3s/server/tls/client-ca.crt"
I0322 03:28:22.048296    2280 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/rancher/k3s/server/tls/request-header-ca.crt"
I0322 03:28:22.048722    2280 crdregistration_controller.go:111] Starting crd-autoregister controller
I0322 03:28:22.048776    2280 shared_informer.go:311] Waiting for caches to sync for crd-autoregister
I0322 03:28:22.049376    2280 controller.go:134] Starting OpenAPI controller
I0322 03:28:22.049970    2280 controller.go:85] Starting OpenAPI V3 controller
I0322 03:28:22.050085    2280 naming_controller.go:291] Starting NamingConditionController
I0322 03:28:22.050240    2280 establishing_controller.go:76] Starting EstablishingController
I0322 03:28:22.050342    2280 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0322 03:28:22.050423    2280 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0322 03:28:22.050617    2280 crd_finalizer.go:266] Starting CRDFinalizer
I0322 03:28:22.139986    2280 shared_informer.go:318] Caches are synced for cluster_authentication_trust_controller
I0322 03:28:22.140300    2280 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0322 03:28:22.147605    2280 cache.go:39] Caches are synced for AvailableConditionController controller
I0322 03:28:22.148908    2280 shared_informer.go:318] Caches are synced for crd-autoregister
I0322 03:28:22.149091    2280 apf_controller.go:379] Running API Priority and Fairness config worker
I0322 03:28:22.149098    2280 apf_controller.go:382] Running API Priority and Fairness periodic rebalancing process
I0322 03:28:22.149324    2280 aggregator.go:166] initial CRD sync complete...
I0322 03:28:22.149346    2280 autoregister_controller.go:141] Starting autoregister controller
I0322 03:28:22.149351    2280 cache.go:32] Waiting for caches to sync for autoregister controller
I0322 03:28:22.149356    2280 cache.go:39] Caches are synced for autoregister controller
I0322 03:28:22.152446    2280 controller.go:624] quota admission added evaluator for: namespaces
E0322 03:28:22.221773    2280 controller.go:95] Unable to perform initial Kubernetes service initialization: namespaces "default" not found
I0322 03:28:22.232219    2280 controller.go:624] quota admission added evaluator for: leases.coordination.k8s.io
I0322 03:28:22.239078    2280 shared_informer.go:318] Caches are synced for configmaps
I0322 03:28:22.240893    2280 shared_informer.go:318] Caches are synced for node_authorizer
time="2024-03-22T03:28:22Z" level=info msg="containerd is now running"
time="2024-03-22T03:28:22Z" level=info msg="Running kubelet --address=0.0.0.0 --allowed-unsafe-sysctls=net.ipv4.ip_forward,net.ipv6.conf.all.forwarding --anonymous-auth=false --authentication-token-webhook=true --authorization-mode=Webhook --cgroup-driver=systemd --client-ca-file=/var/lib/rancher/k3s/agent/client-ca.crt --cloud-provider=external --cluster-dns=10.43.0.10 --cluster-domain=cluster.local --container-runtime-endpoint=unix:///run/k3s/containerd/containerd.sock --containerd=/run/k3s/containerd/containerd.sock --eviction-hard=imagefs.available<5%,nodefs.available<5% --eviction-minimum-reclaim=imagefs.available=10%,nodefs.available=10% --fail-swap-on=false --feature-gates=CloudDualStackNodeIPs=true --healthz-bind-address=127.0.0.1 --hostname-override=server --kubeconfig=/var/lib/rancher/k3s/agent/kubelet.kubeconfig --node-ip=192.168.56.110 --node-labels= --pod-infra-container-image=rancher/mirrored-pause:3.6 --pod-manifest-path=/var/lib/rancher/k3s/agent/pod-manifests --read-only-port=0 --resolv-conf=/run/systemd/resolve/resolv.conf --serialize-image-pulls=false --tls-cert-file=/var/lib/rancher/k3s/agent/serving-kubelet.crt --tls-private-key-file=/var/lib/rancher/k3s/agent/serving-kubelet.key"
time="2024-03-22T03:28:22Z" level=info msg="Connecting to proxy" url="wss://127.0.0.1:6443/v1-k3s/connect"
time="2024-03-22T03:28:22Z" level=info msg="Handling backend connection request [server]"
time="2024-03-22T03:28:22Z" level=info msg="Waiting to retrieve kube-proxy configuration; server is not ready: https://127.0.0.1:6443/v1-k3s/readyz: 500 Internal Server Error"
I0322 03:28:23.050795    2280 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0322 03:28:23.053902    2280 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0322 03:28:23.053980    2280 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0322 03:28:23.460872    2280 controller.go:624] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0322 03:28:23.496243    2280 controller.go:624] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0322 03:28:23.639147    2280 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.43.0.1"}
W0322 03:28:23.643765    2280 lease.go:263] Resetting endpoints for master service "kubernetes" to [192.168.56.110]
I0322 03:28:23.644766    2280 controller.go:624] quota admission added evaluator for: endpoints
I0322 03:28:23.649285    2280 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
Flag --cloud-provider has been deprecated, will be removed in 1.25 or later, in favor of removing cloud provider code from Kubelet.
Flag --containerd has been deprecated, This is a cadvisor flag that was mistakenly registered with the Kubelet. Due to legacy concerns, it will follow the standard CLI deprecation timeline before being removed.
Flag --pod-infra-container-image has been deprecated, will be removed in a future release. Image garbage collector will get sandbox image information from CRI.
I0322 03:28:23.932177    2280 server.go:202] "--pod-infra-container-image will not be pruned by the image garbage collector in kubelet and should also be set in the remote runtime"
I0322 03:28:23.934082    2280 server.go:462] "Kubelet version" kubeletVersion="v1.28.7+k3s1"
I0322 03:28:23.934107    2280 server.go:464] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0322 03:28:23.936024    2280 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/rancher/k3s/agent/client-ca.crt"
I0322 03:28:23.942868    2280 server.go:720] "--cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /"
I0322 03:28:23.943902    2280 container_manager_linux.go:265] "Container manager verified user specified cgroup-root exists" cgroupRoot=[]
I0322 03:28:23.944228    2280 container_manager_linux.go:270] "Creating Container Manager object based on Node Config" nodeConfig={"RuntimeCgroupsName":"","SystemCgroupsName":"","KubeletCgroupsName":"","KubeletOOMScoreAdj":-999,"ContainerRuntime":"","CgroupsPerQOS":true,"CgroupRoot":"/","CgroupDriver":"systemd","KubeletRootDir":"/var/lib/kubelet","ProtectKernelDefaults":false,"KubeReservedCgroupName":"","SystemReservedCgroupName":"","ReservedSystemCPUs":{},"EnforceNodeAllocatable":{"pods":{}},"KubeReserved":null,"SystemReserved":null,"HardEvictionThresholds":[{"Signal":"nodefs.available","Operator":"LessThan","Value":{"Quantity":null,"Percentage":0.05},"GracePeriod":0,"MinReclaim":null},{"Signal":"imagefs.available","Operator":"LessThan","Value":{"Quantity":null,"Percentage":0.05},"GracePeriod":0,"MinReclaim":null}],"QOSReserved":{},"CPUManagerPolicy":"none","CPUManagerPolicyOptions":null,"TopologyManagerScope":"container","CPUManagerReconcilePeriod":10000000000,"ExperimentalMemoryManagerPolicy":"None","ExperimentalMemoryManagerReservedMemory":null,"PodPidsLimit":-1,"EnforceCPULimits":true,"CPUCFSQuotaPeriod":100000000,"TopologyManagerPolicy":"none","TopologyManagerPolicyOptions":null}
I0322 03:28:23.944307    2280 topology_manager.go:138] "Creating topology manager with none policy"
I0322 03:28:23.944365    2280 container_manager_linux.go:301] "Creating device plugin manager"
I0322 03:28:23.944944    2280 state_mem.go:36] "Initialized new in-memory state store"
I0322 03:28:23.945558    2280 kubelet.go:393] "Attempting to sync node with API server"
I0322 03:28:23.945706    2280 kubelet.go:298] "Adding static pod path" path="/var/lib/rancher/k3s/agent/pod-manifests"
I0322 03:28:23.945843    2280 kubelet.go:309] "Adding apiserver pod source"
I0322 03:28:23.945924    2280 apiserver.go:42] "Waiting for node sync before watching apiserver pods"
I0322 03:28:23.947728    2280 apiserver.go:52] "Watching apiserver"
I0322 03:28:23.948426    2280 kuberuntime_manager.go:257] "Container runtime initialized" containerRuntime="containerd" version="v1.7.11-k3s2" apiVersion="v1"
W0322 03:28:23.950421    2280 probe.go:268] Flexvolume plugin directory at /usr/libexec/kubernetes/kubelet-plugins/volume/exec/ does not exist. Recreating.
I0322 03:28:23.951636    2280 server.go:1227] "Started kubelet"
I0322 03:28:23.954716    2280 fs_resource_analyzer.go:67] "Starting FS ResourceAnalyzer"
E0322 03:28:23.957072    2280 cri_stats_provider.go:448] "Failed to get the info of the filesystem with mountpoint" err="unable to find data in memory cache" mountpoint="/var/lib/rancher/k3s/agent/containerd/io.containerd.snapshotter.v1.overlayfs"
E0322 03:28:23.957153    2280 kubelet.go:1431] "Image garbage collection failed once. Stats initialization may not have completed yet" err="invalid capacity 0 on image filesystem"
I0322 03:28:23.968241    2280 server.go:162] "Starting to listen" address="0.0.0.0" port=10250
I0322 03:28:23.968673    2280 volume_manager.go:291] "Starting Kubelet Volume Manager"
I0322 03:28:23.969196    2280 desired_state_of_world_populator.go:151] "Desired state populator starts to run"
I0322 03:28:23.969472    2280 reconciler_new.go:29] "Reconciler: start to sync state"
I0322 03:28:23.969827    2280 server.go:462] "Adding debug handlers to kubelet server"
I0322 03:28:23.970542    2280 ratelimit.go:65] "Setting rate limiting for podresources endpoint" qps=100 burstTokens=10
I0322 03:28:23.972913    2280 server.go:233] "Starting to serve the podresources API" endpoint="unix:/var/lib/kubelet/pod-resources/kubelet.sock"
E0322 03:28:23.988680    2280 nodelease.go:49] "Failed to get node when trying to set owner ref to the node lease" err="nodes \"server\" not found" node="server"
I0322 03:28:24.008632    2280 kubelet_network_linux.go:50] "Initialized iptables rules." protocol="IPv4"
I0322 03:28:24.031371    2280 kubelet_network_linux.go:50] "Initialized iptables rules." protocol="IPv6"
I0322 03:28:24.034297    2280 status_manager.go:217] "Starting to sync pod status with apiserver"
I0322 03:28:24.034334    2280 kubelet.go:2303] "Starting kubelet main sync loop"
E0322 03:28:24.034383    2280 kubelet.go:2327] "Skipping pod synchronization" err="[container runtime status check may not have completed yet, PLEG is not healthy: pleg has yet to be successful]"
I0322 03:28:24.070573    2280 kubelet_node_status.go:70] "Attempting to register node" node="server"
I0322 03:28:24.077571    2280 kubelet_node_status.go:73] "Successfully registered node" node="server"
I0322 03:28:24.080454    2280 cpu_manager.go:214] "Starting CPU manager" policy="none"
I0322 03:28:24.080469    2280 cpu_manager.go:215] "Reconciling" reconcilePeriod="10s"
I0322 03:28:24.080489    2280 state_mem.go:36] "Initialized new in-memory state store"
I0322 03:28:24.084879    2280 policy_none.go:49] "None policy: Start"
time="2024-03-22T03:28:24Z" level=info msg="Kube API server is now running"
time="2024-03-22T03:28:24Z" level=info msg="ETCD server is now running"
time="2024-03-22T03:28:24Z" level=info msg="k3s is up and running"
time="2024-03-22T03:28:24Z" level=info msg="Creating k3s-supervisor event broadcaster"
time="2024-03-22T03:28:24Z" level=info msg="Waiting for cloud-controller-manager privileges to become available"
time="2024-03-22T03:28:24Z" level=info msg="Annotations and labels have been set successfully on node: server"
time="2024-03-22T03:28:24Z" level=info msg="Starting flannel with backend vxlan"
I0322 03:28:24.107414    2280 memory_manager.go:169] "Starting memorymanager" policy="None"
I0322 03:28:24.107647    2280 state_mem.go:35] "Initializing new in-memory state store"
time="2024-03-22T03:28:24Z" level=info msg="Applying CRD addons.k3s.cattle.io"
E0322 03:28:24.154316    2280 kubelet.go:2327] "Skipping pod synchronization" err="container runtime status check may not have completed yet"
time="2024-03-22T03:28:24Z" level=info msg="Applying CRD etcdsnapshotfiles.k3s.cattle.io"
I0322 03:28:24.208835    2280 handler.go:275] Adding GroupVersion k3s.cattle.io v1 to ResourceManager
I0322 03:28:24.238436    2280 manager.go:471] "Failed to read data from checkpoint" checkpoint="kubelet_internal_checkpoint" err="checkpoint is not found"
W0322 03:28:24.239102    2280 watcher.go:93] Error while processing event ("/sys/fs/cgroup/devices/kubepods.slice/kubepods-besteffort.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/devices/kubepods.slice/kubepods-besteffort.slice: no such file or directory
I0322 03:28:24.246490    2280 plugin_manager.go:118] "Starting Kubelet Plugin Manager"
time="2024-03-22T03:28:24Z" level=info msg="Applying CRD helmcharts.helm.cattle.io"
I0322 03:28:24.278776    2280 handler.go:275] Adding GroupVersion k3s.cattle.io v1 to ResourceManager
time="2024-03-22T03:28:24Z" level=info msg="Applying CRD helmchartconfigs.helm.cattle.io"
I0322 03:28:24.374863    2280 handler.go:275] Adding GroupVersion helm.cattle.io v1 to ResourceManager
time="2024-03-22T03:28:24Z" level=info msg="Waiting for CRD helmchartconfigs.helm.cattle.io to become available"
I0322 03:28:24.411086    2280 handler.go:275] Adding GroupVersion helm.cattle.io v1 to ResourceManager
I0322 03:28:24.415855    2280 desired_state_of_world_populator.go:159] "Finished populating initial desired state of world"
I0322 03:28:24.516748    2280 kubelet_node_status.go:493] "Fast updating node status as it just became ready"
time="2024-03-22T03:28:24Z" level=info msg="Done waiting for CRD helmchartconfigs.helm.cattle.io to become available"
time="2024-03-22T03:28:24Z" level=info msg="Writing static file: /var/lib/rancher/k3s/server/static/charts/traefik-crd-25.0.2+up25.0.0.tgz"
time="2024-03-22T03:28:24Z" level=info msg="Writing static file: /var/lib/rancher/k3s/server/static/charts/traefik-25.0.2+up25.0.0.tgz"
time="2024-03-22T03:28:24Z" level=info msg="Failed to get existing traefik HelmChart" error="helmcharts.helm.cattle.io \"traefik\" not found"
time="2024-03-22T03:28:24Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/auth-delegator.yaml"
time="2024-03-22T03:28:24Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-service.yaml"
time="2024-03-22T03:28:24Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/resource-reader.yaml"
time="2024-03-22T03:28:24Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/aggregated-metrics-reader.yaml"
time="2024-03-22T03:28:24Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/metrics-apiservice.yaml"
time="2024-03-22T03:28:24Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-deployment.yaml"
time="2024-03-22T03:28:24Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/rolebindings.yaml"
time="2024-03-22T03:28:24Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/runtimes.yaml"
time="2024-03-22T03:28:24Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/traefik.yaml"
time="2024-03-22T03:28:24Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/ccm.yaml"
time="2024-03-22T03:28:24Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/coredns.yaml"
time="2024-03-22T03:28:24Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/local-storage.yaml"
time="2024-03-22T03:28:24Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/auth-reader.yaml"
time="2024-03-22T03:28:24Z" level=info msg="Starting dynamiclistener CN filter node controller"
time="2024-03-22T03:28:24Z" level=info msg="Tunnel server egress proxy mode: agent"
time="2024-03-22T03:28:25Z" level=info msg="Starting k3s.cattle.io/v1, Kind=Addon controller"
time="2024-03-22T03:28:25Z" level=info msg="Creating deploy event broadcaster"
time="2024-03-22T03:28:25Z" level=info msg="Creating helm-controller event broadcaster"
time="2024-03-22T03:28:25Z" level=info msg="Starting /v1, Kind=Node controller"
time="2024-03-22T03:28:25Z" level=warning msg="Unable to fetch coredns config map: configmaps \"coredns\" not found"
time="2024-03-22T03:28:25Z" level=info msg="Cluster dns configmap has been set successfully"
time="2024-03-22T03:28:25Z" level=info msg="Starting helm.cattle.io/v1, Kind=HelmChart controller"
time="2024-03-22T03:28:25Z" level=info msg="Starting helm.cattle.io/v1, Kind=HelmChartConfig controller"
time="2024-03-22T03:28:25Z" level=info msg="Starting rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding controller"
time="2024-03-22T03:28:25Z" level=info msg="Starting batch/v1, Kind=Job controller"
time="2024-03-22T03:28:25Z" level=info msg="Starting /v1, Kind=Secret controller"
time="2024-03-22T03:28:25Z" level=info msg="Starting /v1, Kind=ConfigMap controller"
time="2024-03-22T03:28:25Z" level=info msg="Starting /v1, Kind=ServiceAccount controller"
I0322 03:28:25.294430    2280 serving.go:355] Generated self-signed cert in-memory
time="2024-03-22T03:28:25Z" level=warning msg="Unable to fetch coredns config map: configmaps \"coredns\" not found"
time="2024-03-22T03:28:25Z" level=info msg="Updating TLS secret for kube-system/k3s-serving (count: 10): map[listener.cattle.io/cn-10.43.0.1:10.43.0.1 listener.cattle.io/cn-127.0.0.1:127.0.0.1 listener.cattle.io/cn-192.168.56.110:192.168.56.110 listener.cattle.io/cn-__1-f16284:::1 listener.cattle.io/cn-kubernetes:kubernetes listener.cattle.io/cn-kubernetes.default:kubernetes.default listener.cattle.io/cn-kubernetes.default.svc:kubernetes.default.svc listener.cattle.io/cn-kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local listener.cattle.io/cn-localhost:localhost listener.cattle.io/cn-server:server listener.cattle.io/fingerprint:SHA1=3EF84C4C3E5D7F82BBAF806634968BEF8CE80EDA]"
time="2024-03-22T03:28:25Z" level=info msg="Updating TLS secret for kube-system/k3s-serving (count: 10): map[listener.cattle.io/cn-10.43.0.1:10.43.0.1 listener.cattle.io/cn-127.0.0.1:127.0.0.1 listener.cattle.io/cn-192.168.56.110:192.168.56.110 listener.cattle.io/cn-__1-f16284:::1 listener.cattle.io/cn-kubernetes:kubernetes listener.cattle.io/cn-kubernetes.default:kubernetes.default listener.cattle.io/cn-kubernetes.default.svc:kubernetes.default.svc listener.cattle.io/cn-kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local listener.cattle.io/cn-localhost:localhost listener.cattle.io/cn-server:server listener.cattle.io/fingerprint:SHA1=3EF84C4C3E5D7F82BBAF806634968BEF8CE80EDA]"
time="2024-03-22T03:28:26Z" level=info msg="Active TLS secret kube-system/k3s-serving (ver=225) (count 10): map[listener.cattle.io/cn-10.43.0.1:10.43.0.1 listener.cattle.io/cn-127.0.0.1:127.0.0.1 listener.cattle.io/cn-192.168.56.110:192.168.56.110 listener.cattle.io/cn-__1-f16284:::1 listener.cattle.io/cn-kubernetes:kubernetes listener.cattle.io/cn-kubernetes.default:kubernetes.default listener.cattle.io/cn-kubernetes.default.svc:kubernetes.default.svc listener.cattle.io/cn-kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local listener.cattle.io/cn-localhost:localhost listener.cattle.io/cn-server:server listener.cattle.io/fingerprint:SHA1=3EF84C4C3E5D7F82BBAF806634968BEF8CE80EDA]"
I0322 03:28:26.735497    2280 controllermanager.go:189] "Starting" version="v1.28.7+k3s1"
I0322 03:28:26.735654    2280 controllermanager.go:191] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0322 03:28:26.740244    2280 event.go:307] "Event occurred" object="server" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodePasswordValidationComplete" message="Deferred node password secret validation complete"
I0322 03:28:26.740917    2280 secure_serving.go:213] Serving securely on 127.0.0.1:10257
I0322 03:28:26.741582    2280 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I0322 03:28:26.741693    2280 shared_informer.go:311] Waiting for caches to sync for RequestHeaderAuthRequestController
I0322 03:28:26.741775    2280 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0322 03:28:26.742008    2280 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0322 03:28:26.742094    2280 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0322 03:28:26.742159    2280 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0322 03:28:26.742225    2280 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0322 03:28:26.753260    2280 shared_informer.go:311] Waiting for caches to sync for tokens
I0322 03:28:26.759961    2280 controller.go:624] quota admission added evaluator for: serviceaccounts
I0322 03:28:26.762109    2280 controllermanager.go:642] "Started controller" controller="endpoints-controller"
I0322 03:28:26.762401    2280 endpoints_controller.go:174] "Starting endpoint controller"
I0322 03:28:26.762417    2280 shared_informer.go:311] Waiting for caches to sync for endpoint
I0322 03:28:26.770994    2280 controllermanager.go:642] "Started controller" controller="deployment-controller"
I0322 03:28:26.771281    2280 deployment_controller.go:168] "Starting controller" controller="deployment"
I0322 03:28:26.771296    2280 shared_informer.go:311] Waiting for caches to sync for deployment
I0322 03:28:26.778788    2280 controllermanager.go:642] "Started controller" controller="statefulset-controller"
I0322 03:28:26.778875    2280 stateful_set.go:161] "Starting stateful set controller"
I0322 03:28:26.779027    2280 shared_informer.go:311] Waiting for caches to sync for stateful set
I0322 03:28:26.788099    2280 controllermanager.go:642] "Started controller" controller="disruption-controller"
I0322 03:28:26.788133    2280 controllermanager.go:607] "Warning: controller is disabled" controller="bootstrap-signer-controller"
I0322 03:28:26.788402    2280 disruption.go:433] "Sending events to api server."
I0322 03:28:26.788450    2280 disruption.go:444] "Starting disruption controller"
I0322 03:28:26.788459    2280 shared_informer.go:311] Waiting for caches to sync for disruption
I0322 03:28:26.796373    2280 controllermanager.go:642] "Started controller" controller="persistentvolume-expander-controller"
I0322 03:28:26.796485    2280 expand_controller.go:328] "Starting expand controller"
I0322 03:28:26.796534    2280 shared_informer.go:311] Waiting for caches to sync for expand
I0322 03:28:26.804132    2280 controllermanager.go:642] "Started controller" controller="root-ca-certificate-publisher-controller"
I0322 03:28:26.804520    2280 publisher.go:102] "Starting root CA cert publisher controller"
I0322 03:28:26.804720    2280 shared_informer.go:311] Waiting for caches to sync for crt configmap
I0322 03:28:26.812431    2280 controllermanager.go:642] "Started controller" controller="serviceaccount-controller"
I0322 03:28:26.812602    2280 serviceaccounts_controller.go:111] "Starting service account controller"
I0322 03:28:26.812615    2280 shared_informer.go:311] Waiting for caches to sync for service account
I0322 03:28:26.818774    2280 controllermanager.go:642] "Started controller" controller="daemonset-controller"
I0322 03:28:26.819560    2280 daemon_controller.go:291] "Starting daemon sets controller"
I0322 03:28:26.819628    2280 shared_informer.go:311] Waiting for caches to sync for daemon sets
I0322 03:28:26.828205    2280 controllermanager.go:642] "Started controller" controller="certificatesigningrequest-cleaner-controller"
I0322 03:28:26.828231    2280 cleaner.go:83] "Starting CSR cleaner controller"
I0322 03:28:26.835949    2280 controllermanager.go:642] "Started controller" controller="ttl-controller"
I0322 03:28:26.836010    2280 ttl_controller.go:124] "Starting TTL controller"
I0322 03:28:26.836061    2280 shared_informer.go:311] Waiting for caches to sync for TTL
I0322 03:28:26.842099    2280 shared_informer.go:318] Caches are synced for RequestHeaderAuthRequestController
I0322 03:28:26.842124    2280 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0322 03:28:26.842290    2280 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0322 03:28:26.853355    2280 shared_informer.go:318] Caches are synced for tokens
time="2024-03-22T03:28:26Z" level=info msg="Labels and annotations have been set successfully on node: server"
I0322 03:28:26.958507    2280 node_lifecycle_controller.go:431] "Controller will reconcile labels"
I0322 03:28:26.958670    2280 controllermanager.go:642] "Started controller" controller="node-lifecycle-controller"
I0322 03:28:26.958940    2280 node_lifecycle_controller.go:465] "Sending events to api server"
I0322 03:28:26.959133    2280 node_lifecycle_controller.go:476] "Starting node controller"
I0322 03:28:26.959213    2280 shared_informer.go:311] Waiting for caches to sync for taint
I0322 03:28:27.050670    2280 controller.go:624] quota admission added evaluator for: addons.k3s.cattle.io
I0322 03:28:27.062579    2280 event.go:307] "Event occurred" object="kube-system/ccm" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/ccm.yaml\""
I0322 03:28:27.110732    2280 event.go:307] "Event occurred" object="kube-system/ccm" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/ccm.yaml\""
I0322 03:28:27.149032    2280 event.go:307] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/coredns.yaml\""
I0322 03:28:27.150248    2280 controllermanager.go:642] "Started controller" controller="ttl-after-finished-controller"
I0322 03:28:27.150500    2280 ttlafterfinished_controller.go:109] "Starting TTL after finished controller"
I0322 03:28:27.150525    2280 shared_informer.go:311] Waiting for caches to sync for TTL after finished
time="2024-03-22T03:28:27Z" level=warning msg="Unable to fetch coredns config map: configmaps \"coredns\" not found"
I0322 03:28:27.242640    2280 controller.go:624] quota admission added evaluator for: deployments.apps
I0322 03:28:27.256070    2280 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.43.0.10"}
I0322 03:28:27.257550    2280 event.go:307] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/coredns.yaml\""
I0322 03:28:27.260879    2280 controllermanager.go:642] "Started controller" controller="replicationcontroller-controller"
I0322 03:28:27.261310    2280 replica_set.go:214] "Starting controller" name="replicationcontroller"
I0322 03:28:27.261327    2280 shared_informer.go:311] Waiting for caches to sync for ReplicationController
I0322 03:28:27.270562    2280 event.go:307] "Event occurred" object="kube-system/local-storage" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/local-storage.yaml\""
I0322 03:28:27.317469    2280 event.go:307] "Event occurred" object="kube-system/local-storage" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/local-storage.yaml\""
I0322 03:28:27.341797    2280 event.go:307] "Event occurred" object="kube-system/aggregated-metrics-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/aggregated-metrics-reader.yaml\""
I0322 03:28:27.364930    2280 event.go:307] "Event occurred" object="kube-system/aggregated-metrics-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/aggregated-metrics-reader.yaml\""
I0322 03:28:27.377101    2280 event.go:307] "Event occurred" object="kube-system/auth-delegator" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/auth-delegator.yaml\""
I0322 03:28:27.386452    2280 event.go:307] "Event occurred" object="kube-system/auth-delegator" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/auth-delegator.yaml\""
I0322 03:28:27.397597    2280 event.go:307] "Event occurred" object="kube-system/auth-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/auth-reader.yaml\""
I0322 03:28:27.409767    2280 event.go:307] "Event occurred" object="kube-system/auth-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/auth-reader.yaml\""
I0322 03:28:27.514132    2280 event.go:307] "Event occurred" object="kube-system/metrics-apiservice" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-apiservice.yaml\""
I0322 03:28:27.524542    2280 event.go:307] "Event occurred" object="kube-system/metrics-apiservice" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-apiservice.yaml\""
I0322 03:28:27.525834    2280 handler.go:275] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
W0322 03:28:27.532802    2280 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:28:27.533088    2280 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
E0322 03:28:27.533355    2280 handler_proxy.go:137] error resolving kube-system/metrics-server: service "metrics-server" not found
E0322 03:28:27.575863    2280 resource_quota_controller.go:169] initial discovery check failure, continuing and counting on future sync update: unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I0322 03:28:27.577478    2280 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="addons.k3s.cattle.io"
I0322 03:28:27.577609    2280 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="horizontalpodautoscalers.autoscaling"
I0322 03:28:27.577734    2280 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="cronjobs.batch"
I0322 03:28:27.577850    2280 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="csistoragecapacities.storage.k8s.io"
I0322 03:28:27.577894    2280 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="podtemplates"
I0322 03:28:27.578018    2280 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingresses.networking.k8s.io"
I0322 03:28:27.578128    2280 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="endpointslices.discovery.k8s.io"
I0322 03:28:27.578179    2280 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="helmchartconfigs.helm.cattle.io"
I0322 03:28:27.578292    2280 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="daemonsets.apps"
I0322 03:28:27.578425    2280 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="jobs.batch"
I0322 03:28:27.578524    2280 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="deployments.apps"
I0322 03:28:27.578575    2280 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="limitranges"
I0322 03:28:27.578726    2280 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="rolebindings.rbac.authorization.k8s.io"
W0322 03:28:27.578847    2280 shared_informer.go:593] resyncPeriod 15h55m22.999911872s is smaller than resyncCheckPeriod 17h32m11.650938779s and the informer has already started. Changing it to 17h32m11.650938779s
I0322 03:28:27.579013    2280 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="serviceaccounts"
I0322 03:28:27.579158    2280 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="replicasets.apps"
I0322 03:28:27.579271    2280 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="statefulsets.apps"
I0322 03:28:27.579324    2280 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="roles.rbac.authorization.k8s.io"
I0322 03:28:27.579466    2280 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="leases.coordination.k8s.io"
I0322 03:28:27.579591    2280 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="helmcharts.helm.cattle.io"
I0322 03:28:27.579629    2280 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="endpoints"
I0322 03:28:27.579764    2280 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="controllerrevisions.apps"
I0322 03:28:27.579794    2280 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="networkpolicies.networking.k8s.io"
I0322 03:28:27.579977    2280 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="poddisruptionbudgets.policy"
I0322 03:28:27.580041    2280 controllermanager.go:642] "Started controller" controller="resourcequota-controller"
I0322 03:28:27.580358    2280 resource_quota_controller.go:294] "Starting resource quota controller"
I0322 03:28:27.580375    2280 shared_informer.go:311] Waiting for caches to sync for resource quota
I0322 03:28:27.580390    2280 resource_quota_monitor.go:305] "QuotaMonitor running"
E0322 03:28:27.583193    2280 resource_quota_controller.go:440] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
E0322 03:28:27.812177    2280 namespaced_resources_deleter.go:162] unable to get all supported resources from server: unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I0322 03:28:27.812278    2280 controllermanager.go:642] "Started controller" controller="namespace-controller"
I0322 03:28:27.812291    2280 controllermanager.go:607] "Warning: controller is disabled" controller="node-route-controller"
I0322 03:28:27.812449    2280 namespace_controller.go:197] "Starting namespace controller"
I0322 03:28:27.812462    2280 shared_informer.go:311] Waiting for caches to sync for namespace
I0322 03:28:27.906769    2280 serving.go:355] Generated self-signed cert in-memory
I0322 03:28:27.912033    2280 event.go:307] "Event occurred" object="kube-system/metrics-server-deployment" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-deployment.yaml\""
I0322 03:28:27.943402    2280 event.go:307] "Event occurred" object="kube-system/metrics-server-deployment" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-deployment.yaml\""
I0322 03:28:27.957368    2280 controllermanager.go:642] "Started controller" controller="persistentvolume-protection-controller"
I0322 03:28:27.957429    2280 pv_protection_controller.go:78] "Starting PV protection controller"
I0322 03:28:27.957471    2280 shared_informer.go:311] Waiting for caches to sync for PV protection
I0322 03:28:28.187917    2280 controllermanager.go:642] "Started controller" controller="endpointslice-mirroring-controller"
I0322 03:28:28.188451    2280 endpointslicemirroring_controller.go:223] "Starting EndpointSliceMirroring controller"
I0322 03:28:28.188524    2280 shared_informer.go:311] Waiting for caches to sync for endpoint_slice_mirroring
I0322 03:28:28.225806    2280 controllermanager.go:642] "Started controller" controller="certificatesigningrequest-signing-controller"
I0322 03:28:28.226295    2280 certificate_controller.go:115] "Starting certificate controller" name="csrsigning-kubelet-serving"
I0322 03:28:28.226397    2280 shared_informer.go:311] Waiting for caches to sync for certificate-csrsigning-kubelet-serving
I0322 03:28:28.226519    2280 certificate_controller.go:115] "Starting certificate controller" name="csrsigning-kubelet-client"
I0322 03:28:28.226569    2280 shared_informer.go:311] Waiting for caches to sync for certificate-csrsigning-kubelet-client
I0322 03:28:28.226677    2280 certificate_controller.go:115] "Starting certificate controller" name="csrsigning-kube-apiserver-client"
I0322 03:28:28.226736    2280 shared_informer.go:311] Waiting for caches to sync for certificate-csrsigning-kube-apiserver-client
I0322 03:28:28.226899    2280 certificate_controller.go:115] "Starting certificate controller" name="csrsigning-legacy-unknown"
I0322 03:28:28.226960    2280 shared_informer.go:311] Waiting for caches to sync for certificate-csrsigning-legacy-unknown
I0322 03:28:28.227071    2280 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/var/lib/rancher/k3s/server/tls/server-ca.nochain.crt::/var/lib/rancher/k3s/server/tls/server-ca.key"
I0322 03:28:28.227241    2280 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/var/lib/rancher/k3s/server/tls/client-ca.nochain.crt::/var/lib/rancher/k3s/server/tls/client-ca.key"
I0322 03:28:28.227402    2280 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/var/lib/rancher/k3s/server/tls/client-ca.nochain.crt::/var/lib/rancher/k3s/server/tls/client-ca.key"
I0322 03:28:28.227560    2280 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/var/lib/rancher/k3s/server/tls/server-ca.nochain.crt::/var/lib/rancher/k3s/server/tls/server-ca.key"
time="2024-03-22T03:28:28Z" level=info msg="Running kube-proxy --cluster-cidr=10.42.0.0/16 --conntrack-max-per-core=0 --conntrack-tcp-timeout-close-wait=0s --conntrack-tcp-timeout-established=0s --healthz-bind-address=127.0.0.1 --hostname-override=server --kubeconfig=/var/lib/rancher/k3s/agent/kubeproxy.kubeconfig --proxy-mode=iptables"
I0322 03:28:28.356550    2280 event.go:307] "Event occurred" object="kube-system/metrics-server-service" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-service.yaml\""
I0322 03:28:28.359898    2280 node.go:141] Successfully retrieved node IP: 192.168.56.110
I0322 03:28:28.367719    2280 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0322 03:28:28.369363    2280 server_others.go:152] "Using iptables Proxier"
I0322 03:28:28.369387    2280 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0322 03:28:28.369394    2280 server_others.go:438] "Defaulting to no-op detect-local"
I0322 03:28:28.369406    2280 proxier.go:250] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0322 03:28:28.369536    2280 server.go:846] "Version info" version="v1.28.7+k3s1"
I0322 03:28:28.369544    2280 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0322 03:28:28.378167    2280 config.go:188] "Starting service config controller"
I0322 03:28:28.378188    2280 shared_informer.go:311] Waiting for caches to sync for service config
I0322 03:28:28.378202    2280 config.go:97] "Starting endpoint slice config controller"
I0322 03:28:28.378206    2280 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0322 03:28:28.378560    2280 config.go:315] "Starting node config controller"
I0322 03:28:28.378568    2280 shared_informer.go:311] Waiting for caches to sync for node config
I0322 03:28:28.390395    2280 controllermanager.go:642] "Started controller" controller="clusterrole-aggregation-controller"
I0322 03:28:28.390967    2280 clusterroleaggregation_controller.go:189] "Starting ClusterRoleAggregator controller"
I0322 03:28:28.391028    2280 shared_informer.go:311] Waiting for caches to sync for ClusterRoleAggregator
I0322 03:28:28.401331    2280 alloc.go:330] "allocated clusterIPs" service="kube-system/metrics-server" clusterIPs={"IPv4":"10.43.79.93"}
I0322 03:28:28.402825    2280 event.go:307] "Event occurred" object="kube-system/metrics-server-service" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-service.yaml\""
W0322 03:28:28.409010    2280 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:28:28.409221    2280 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
E0322 03:28:28.409375    2280 handler_proxy.go:137] error resolving kube-system/metrics-server: endpoints "metrics-server" not found
I0322 03:28:28.462856    2280 controllermanager.go:642] "Started controller" controller="replicaset-controller"
I0322 03:28:28.463289    2280 replica_set.go:214] "Starting controller" name="replicaset"
I0322 03:28:28.463383    2280 shared_informer.go:311] Waiting for caches to sync for ReplicaSet
I0322 03:28:28.478875    2280 shared_informer.go:318] Caches are synced for node config
I0322 03:28:28.479027    2280 shared_informer.go:318] Caches are synced for service config
I0322 03:28:28.479121    2280 shared_informer.go:318] Caches are synced for endpoint slice config
I0322 03:28:28.495529    2280 serving.go:355] Generated self-signed cert in-memory
W0322 03:28:28.529093    2280 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:28:28.529157    2280 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 03:28:28.529164    2280 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0322 03:28:28.529194    2280 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:28:28.529205    2280 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I0322 03:28:28.530458    2280 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0322 03:28:28.610487    2280 controllermanager.go:642] "Started controller" controller="token-cleaner-controller"
I0322 03:28:28.611026    2280 tokencleaner.go:112] "Starting token cleaner controller"
I0322 03:28:28.611245    2280 shared_informer.go:311] Waiting for caches to sync for token_cleaner
I0322 03:28:28.611458    2280 shared_informer.go:318] Caches are synced for token_cleaner
I0322 03:28:28.743169    2280 event.go:307] "Event occurred" object="kube-system/resource-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/resource-reader.yaml\""
I0322 03:28:28.765713    2280 controllermanager.go:642] "Started controller" controller="ephemeral-volume-controller"
I0322 03:28:28.765932    2280 controller.go:169] "Starting ephemeral volume controller"
I0322 03:28:28.765950    2280 shared_informer.go:311] Waiting for caches to sync for ephemeral
I0322 03:28:28.767377    2280 event.go:307] "Event occurred" object="kube-system/resource-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/resource-reader.yaml\""
I0322 03:28:28.829107    2280 controllermanager.go:168] Version: v1.28.7+k3s1
I0322 03:28:28.832895    2280 secure_serving.go:213] Serving securely on 127.0.0.1:10258
I0322 03:28:28.833157    2280 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0322 03:28:28.833197    2280 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I0322 03:28:28.833208    2280 shared_informer.go:311] Waiting for caches to sync for RequestHeaderAuthRequestController
I0322 03:28:28.833339    2280 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0322 03:28:28.833348    2280 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0322 03:28:28.833422    2280 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0322 03:28:28.833435    2280 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
E0322 03:28:28.845241    2280 controllermanager.go:524] unable to get all supported resources from server: unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
time="2024-03-22T03:28:28Z" level=info msg="Creating  event broadcaster"
I0322 03:28:28.909099    2280 controllermanager.go:642] "Started controller" controller="job-controller"
I0322 03:28:28.909402    2280 job_controller.go:226] "Starting job controller"
I0322 03:28:28.909441    2280 shared_informer.go:311] Waiting for caches to sync for job
I0322 03:28:28.933370    2280 shared_informer.go:318] Caches are synced for RequestHeaderAuthRequestController
I0322 03:28:28.933575    2280 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0322 03:28:28.933628    2280 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
time="2024-03-22T03:28:28Z" level=info msg="Starting /v1, Kind=Node controller"
time="2024-03-22T03:28:28Z" level=info msg="Starting /v1, Kind=Pod controller"
time="2024-03-22T03:28:28Z" level=info msg="Starting apps/v1, Kind=DaemonSet controller"
I0322 03:28:28.987965    2280 controllermanager.go:337] Started "cloud-node-controller"
time="2024-03-22T03:28:28Z" level=info msg="Starting discovery.k8s.io/v1, Kind=EndpointSlice controller"
I0322 03:28:28.988720    2280 node_controller.go:165] Sending events to api server.
I0322 03:28:28.988939    2280 node_controller.go:174] Waiting for informer caches to sync
I0322 03:28:28.991220    2280 controllermanager.go:337] Started "cloud-node-lifecycle-controller"
I0322 03:28:28.991509    2280 node_lifecycle_controller.go:113] Sending events to api server
I0322 03:28:28.991802    2280 controllermanager.go:337] Started "service-lb-controller"
W0322 03:28:28.991831    2280 controllermanager.go:314] "node-route-controller" is disabled
I0322 03:28:28.992182    2280 controller.go:231] Starting service controller
I0322 03:28:28.992270    2280 shared_informer.go:311] Waiting for caches to sync for service
I0322 03:28:29.090104    2280 node_controller.go:431] Initializing node server with cloud provider
I0322 03:28:29.092540    2280 shared_informer.go:318] Caches are synced for service
I0322 03:28:29.095517    2280 node_controller.go:502] Successfully initialized node server with cloud provider
I0322 03:28:29.095882    2280 event.go:307] "Event occurred" object="server" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="Synced" message="Node synced successfully"
I0322 03:28:29.196335    2280 event.go:307] "Event occurred" object="kube-system/rolebindings" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/rolebindings.yaml\""
time="2024-03-22T03:28:29Z" level=info msg="Updated coredns node hosts entry [192.168.56.110 server]"
I0322 03:28:29.220965    2280 controllermanager.go:642] "Started controller" controller="horizontal-pod-autoscaler-controller"
I0322 03:28:29.245012    2280 controllermanager.go:607] "Warning: controller is disabled" controller="service-lb-controller"
I0322 03:28:29.220980    2280 horizontal.go:200] "Starting HPA controller"
I0322 03:28:29.245080    2280 shared_informer.go:311] Waiting for caches to sync for HPA
I0322 03:28:29.248847    2280 event.go:307] "Event occurred" object="kube-system/rolebindings" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/rolebindings.yaml\""
I0322 03:28:29.360003    2280 controllermanager.go:642] "Started controller" controller="persistentvolume-binder-controller"
I0322 03:28:29.360359    2280 pv_controller_base.go:319] "Starting persistent volume controller"
I0322 03:28:29.360412    2280 shared_informer.go:311] Waiting for caches to sync for persistent volume
I0322 03:28:29.514303    2280 controllermanager.go:642] "Started controller" controller="persistentvolume-attach-detach-controller"
I0322 03:28:29.514683    2280 attach_detach_controller.go:337] "Starting attach detach controller"
I0322 03:28:29.514707    2280 shared_informer.go:311] Waiting for caches to sync for attach detach
I0322 03:28:29.518635    2280 event.go:307] "Event occurred" object="kube-system/runtimes" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/runtimes.yaml\""
I0322 03:28:29.619102    2280 serving.go:355] Generated self-signed cert in-memory
I0322 03:28:29.725667    2280 event.go:307] "Event occurred" object="kube-system/runtimes" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/runtimes.yaml\""
I0322 03:28:29.917221    2280 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/traefik.yaml\""
I0322 03:28:29.926511    2280 controller.go:624] quota admission added evaluator for: helmcharts.helm.cattle.io
time="2024-03-22T03:28:29Z" level=info msg="Tunnel authorizer set Kubelet Port 10250"
I0322 03:28:29.998810    2280 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/traefik.yaml\""
I0322 03:28:30.016103    2280 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
I0322 03:28:30.123884    2280 event.go:307] "Event occurred" object="kube-system/traefik-crd" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik-crd"
I0322 03:28:30.205991    2280 controller.go:624] quota admission added evaluator for: jobs.batch
time="2024-03-22T03:28:30Z" level=error msg="error syncing 'kube-system/traefik': handler helm-controller-chart-registration: helmcharts.helm.cattle.io \"traefik\" not found, requeuing"
I0322 03:28:30.279612    2280 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
I0322 03:28:30.390363    2280 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
I0322 03:28:30.418389    2280 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
time="2024-03-22T03:28:30Z" level=error msg="error syncing 'kube-system/traefik-crd': handler helm-controller-chart-registration: helmcharts.helm.cattle.io \"traefik-crd\" not found, requeuing"
I0322 03:28:30.449349    2280 event.go:307] "Event occurred" object="kube-system/traefik-crd" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik-crd"
I0322 03:28:30.483779    2280 event.go:307] "Event occurred" object="kube-system/traefik-crd" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik-crd"
I0322 03:28:30.508785    2280 event.go:307] "Event occurred" object="kube-system/traefik-crd" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik-crd"
I0322 03:28:30.703217    2280 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.7+k3s1"
I0322 03:28:30.703353    2280 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0322 03:28:30.706660    2280 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0322 03:28:30.706704    2280 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I0322 03:28:30.706711    2280 shared_informer.go:311] Waiting for caches to sync for RequestHeaderAuthRequestController
I0322 03:28:30.706722    2280 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0322 03:28:30.707921    2280 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0322 03:28:30.707929    2280 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0322 03:28:30.707940    2280 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0322 03:28:30.707944    2280 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0322 03:28:30.807542    2280 shared_informer.go:318] Caches are synced for RequestHeaderAuthRequestController
I0322 03:28:30.808047    2280 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0322 03:28:30.808197    2280 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
time="2024-03-22T03:28:30Z" level=info msg="Stopped tunnel to 127.0.0.1:6443"
time="2024-03-22T03:28:30Z" level=info msg="Connecting to proxy" url="wss://192.168.56.110:6443/v1-k3s/connect"
time="2024-03-22T03:28:30Z" level=info msg="Proxy done" err="context canceled" url="wss://127.0.0.1:6443/v1-k3s/connect"
time="2024-03-22T03:28:30Z" level=info msg="error in remotedialer server [400]: websocket: close 1006 (abnormal closure): unexpected EOF"
time="2024-03-22T03:28:30Z" level=info msg="Handling backend connection request [server]"
I0322 03:28:39.593599    2280 range_allocator.go:111] "No Secondary Service CIDR provided. Skipping filtering out secondary service addresses"
I0322 03:28:39.593932    2280 node_ipam_controller.go:162] "Starting ipam controller"
I0322 03:28:39.593949    2280 shared_informer.go:311] Waiting for caches to sync for node
I0322 03:28:39.594022    2280 controllermanager.go:642] "Started controller" controller="node-ipam-controller"
I0322 03:28:39.594124    2280 controllermanager.go:607] "Warning: controller is disabled" controller="cloud-node-lifecycle-controller"
I0322 03:28:39.604510    2280 controllermanager.go:642] "Started controller" controller="persistentvolumeclaim-protection-controller"
I0322 03:28:39.604916    2280 pvc_protection_controller.go:102] "Starting PVC protection controller"
I0322 03:28:39.604932    2280 shared_informer.go:311] Waiting for caches to sync for PVC protection
I0322 03:28:39.615810    2280 controllermanager.go:642] "Started controller" controller="endpointslice-controller"
I0322 03:28:39.616632    2280 endpointslice_controller.go:264] "Starting endpoint slice controller"
I0322 03:28:39.617710    2280 shared_informer.go:311] Waiting for caches to sync for endpoint_slice
I0322 03:28:39.626909    2280 controllermanager.go:642] "Started controller" controller="pod-garbage-collector-controller"
I0322 03:28:39.627580    2280 gc_controller.go:101] "Starting GC controller"
I0322 03:28:39.628937    2280 shared_informer.go:311] Waiting for caches to sync for GC
I0322 03:28:39.678878    2280 garbagecollector.go:155] "Starting controller" controller="garbagecollector"
I0322 03:28:39.678937    2280 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0322 03:28:39.678980    2280 graph_builder.go:294] "Running" component="GraphBuilder"
I0322 03:28:39.679586    2280 controllermanager.go:642] "Started controller" controller="garbage-collector-controller"
I0322 03:28:39.695490    2280 controllermanager.go:642] "Started controller" controller="cronjob-controller"
I0322 03:28:39.696253    2280 cronjob_controllerv2.go:139] "Starting cronjob controller v2"
I0322 03:28:39.696357    2280 shared_informer.go:311] Waiting for caches to sync for cronjob
I0322 03:28:39.700847    2280 controllermanager.go:642] "Started controller" controller="certificatesigningrequest-approving-controller"
I0322 03:28:39.738933    2280 garbagecollector.go:816] "failed to discover some groups" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"
I0322 03:28:39.714955    2280 certificate_controller.go:115] "Starting certificate controller" name="csrapproving"
I0322 03:28:39.739841    2280 shared_informer.go:311] Waiting for caches to sync for certificate-csrapproving
I0322 03:28:39.752928    2280 shared_informer.go:311] Waiting for caches to sync for resource quota
I0322 03:28:39.767910    2280 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0322 03:28:39.796623    2280 shared_informer.go:318] Caches are synced for expand
I0322 03:28:39.799541    2280 shared_informer.go:318] Caches are synced for ReplicationController
I0322 03:28:39.805078    2280 shared_informer.go:318] Caches are synced for PVC protection
I0322 03:28:39.810783    2280 actual_state_of_world.go:547] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"server\" does not exist"
I0322 03:28:39.811741    2280 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:28:39.812048    2280 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:28:39.812698    2280 shared_informer.go:318] Caches are synced for service account
I0322 03:28:39.818462    2280 shared_informer.go:318] Caches are synced for endpoint_slice
I0322 03:28:39.819861    2280 shared_informer.go:318] Caches are synced for daemon sets
I0322 03:28:39.827032    2280 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0322 03:28:39.827230    2280 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I0322 03:28:39.827928    2280 shared_informer.go:318] Caches are synced for namespace
I0322 03:28:39.828128    2280 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I0322 03:28:39.828278    2280 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I0322 03:28:39.829493    2280 shared_informer.go:318] Caches are synced for GC
I0322 03:28:39.838717    2280 shared_informer.go:318] Caches are synced for TTL
I0322 03:28:39.839907    2280 shared_informer.go:318] Caches are synced for certificate-csrapproving
I0322 03:28:39.845752    2280 shared_informer.go:318] Caches are synced for HPA
I0322 03:28:39.850591    2280 shared_informer.go:318] Caches are synced for TTL after finished
I0322 03:28:39.853410    2280 shared_informer.go:318] Caches are synced for resource quota
I0322 03:28:39.859987    2280 shared_informer.go:318] Caches are synced for taint
I0322 03:28:39.860285    2280 node_lifecycle_controller.go:1225] "Initializing eviction metric for zone" zone=""
I0322 03:28:39.860523    2280 node_lifecycle_controller.go:877] "Missing timestamp for Node. Assuming now as a timestamp" node="server"
I0322 03:28:39.860681    2280 node_lifecycle_controller.go:1071] "Controller detected that zone is now in new state" zone="" newState="Normal"
I0322 03:28:39.860910    2280 taint_manager.go:205] "Starting NoExecuteTaintManager"
I0322 03:28:39.861078    2280 taint_manager.go:210] "Sending events to api server"
I0322 03:28:39.861843    2280 event.go:307] "Event occurred" object="server" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node server event: Registered Node server in Controller"
I0322 03:28:39.863458    2280 shared_informer.go:318] Caches are synced for ReplicaSet
I0322 03:28:39.870281    2280 shared_informer.go:318] Caches are synced for ephemeral
I0322 03:28:39.870416    2280 shared_informer.go:318] Caches are synced for endpoint
I0322 03:28:39.887475    2280 shared_informer.go:318] Caches are synced for deployment
I0322 03:28:39.889648    2280 shared_informer.go:318] Caches are synced for disruption
I0322 03:28:39.890208    2280 shared_informer.go:318] Caches are synced for stateful set
I0322 03:28:39.890705    2280 shared_informer.go:318] Caches are synced for resource quota
I0322 03:28:39.904761    2280 shared_informer.go:318] Caches are synced for crt configmap
I0322 03:28:39.894013    2280 shared_informer.go:318] Caches are synced for node
I0322 03:28:39.905889    2280 range_allocator.go:174] "Sending events to api server"
I0322 03:28:39.906085    2280 range_allocator.go:178] "Starting range CIDR allocator"
I0322 03:28:39.906104    2280 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I0322 03:28:39.906184    2280 shared_informer.go:318] Caches are synced for cidrallocator
I0322 03:28:39.904454    2280 shared_informer.go:318] Caches are synced for cronjob
I0322 03:28:39.904544    2280 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I0322 03:28:39.904565    2280 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I0322 03:28:39.909857    2280 shared_informer.go:318] Caches are synced for job
I0322 03:28:39.919872    2280 shared_informer.go:318] Caches are synced for attach detach
I0322 03:28:39.952305    2280 range_allocator.go:380] "Set node PodCIDR" node="server" podCIDRs=["10.42.0.0/24"]
time="2024-03-22T03:28:39Z" level=info msg="Flannel found PodCIDR assigned for node server"
time="2024-03-22T03:28:39Z" level=info msg="The interface enp0s3 with ipv4 address 10.0.2.15 will be used by flannel"
I0322 03:28:39.960705    2280 shared_informer.go:318] Caches are synced for PV protection
I0322 03:28:39.961926    2280 kube.go:139] Waiting 10m0s for node controller to sync
I0322 03:28:39.962039    2280 shared_informer.go:318] Caches are synced for persistent volume
I0322 03:28:39.963879    2280 kube.go:461] Starting kube subnet manager
I0322 03:28:40.011911    2280 controller.go:624] quota admission added evaluator for: replicasets.apps
I0322 03:28:40.024431    2280 event.go:307] "Event occurred" object="kube-system/metrics-server" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set metrics-server-67c658944b to 1"
I0322 03:28:40.024476    2280 event.go:307] "Event occurred" object="kube-system/local-path-provisioner" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set local-path-provisioner-6c86858495 to 1"
I0322 03:28:40.055858    2280 event.go:307] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set coredns-6799fbcd5 to 1"
I0322 03:28:40.081505    2280 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:28:40.086285    2280 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:28:40.086560    2280 event.go:307] "Event occurred" object="kube-system/helm-install-traefik" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: helm-install-traefik-fxsb6"
I0322 03:28:40.086628    2280 event.go:307] "Event occurred" object="kube-system/helm-install-traefik-crd" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: helm-install-traefik-crd-dlc6m"
W0322 03:28:40.181394    2280 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:28:40.182227    2280 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
E0322 03:28:40.187377    2280 handler_proxy.go:137] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
I0322 03:28:40.264623    2280 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:28:40.264655    2280 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:28:40.284884    2280 event.go:307] "Event occurred" object="kube-system/local-path-provisioner-6c86858495" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: local-path-provisioner-6c86858495-lvfsk"
I0322 03:28:40.285247    2280 event.go:307] "Event occurred" object="kube-system/metrics-server-67c658944b" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: metrics-server-67c658944b-p82jk"
I0322 03:28:40.297443    2280 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:28:40.362068    2280 event.go:307] "Event occurred" object="kube-system/coredns-6799fbcd5" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: coredns-6799fbcd5-rx42b"
time="2024-03-22T03:28:40Z" level=info msg="Starting network policy controller version v2.0.1, built on 2024-02-29T20:36:21Z, go1.21.7"
I0322 03:28:40.364975    2280 network_policy_controller.go:164] Starting network policy controller
I0322 03:28:40.513488    2280 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="481.78576ms"
I0322 03:28:40.514051    2280 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="487.493171ms"
I0322 03:28:40.516994    2280 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:28:40.554471    2280 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="500.12195ms"
I0322 03:28:40.568157    2280 shared_informer.go:318] Caches are synced for garbage collector
I0322 03:28:40.579158    2280 shared_informer.go:318] Caches are synced for garbage collector
I0322 03:28:40.584793    2280 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I0322 03:28:40.581144    2280 topology_manager.go:215] "Topology Admit Handler" podUID="1ea7b8bb-866e-4ee0-84fb-ed8098467564" podNamespace="kube-system" podName="metrics-server-67c658944b-p82jk"
I0322 03:28:40.600119    2280 topology_manager.go:215] "Topology Admit Handler" podUID="bd655422-2e30-4b8c-a19a-617f504f2026" podNamespace="kube-system" podName="local-path-provisioner-6c86858495-lvfsk"
I0322 03:28:40.600321    2280 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="45.725717ms"
I0322 03:28:40.600482    2280 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="62.989s"
I0322 03:28:40.600532    2280 topology_manager.go:215] "Topology Admit Handler" podUID="5358d3b8-3fa6-4dcc-8789-ee2721507f22" podNamespace="kube-system" podName="coredns-6799fbcd5-rx42b"
I0322 03:28:40.602629    2280 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="87.749s"
I0322 03:28:40.652624    2280 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="138.454864ms"
I0322 03:28:40.652986    2280 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="139.294739ms"
I0322 03:28:40.680880    2280 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp-dir\" (UniqueName: \"kubernetes.io/empty-dir/1ea7b8bb-866e-4ee0-84fb-ed8098467564-tmp-dir\") pod \"metrics-server-67c658944b-p82jk\" (UID: \"1ea7b8bb-866e-4ee0-84fb-ed8098467564\") " pod="kube-system/metrics-server-67c658944b-p82jk"
I0322 03:28:40.680963    2280 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-wg79r\" (UniqueName: \"kubernetes.io/projected/1ea7b8bb-866e-4ee0-84fb-ed8098467564-kube-api-access-wg79r\") pod \"metrics-server-67c658944b-p82jk\" (UID: \"1ea7b8bb-866e-4ee0-84fb-ed8098467564\") " pod="kube-system/metrics-server-67c658944b-p82jk"
I0322 03:28:40.695034    2280 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="42.344385ms"
I0322 03:28:40.695180    2280 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="87.581s"
I0322 03:28:40.702905    2280 network_policy_controller.go:176] Starting network policy controller full sync goroutine
I0322 03:28:40.705232    2280 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="52.202s"
I0322 03:28:40.706309    2280 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="53.279978ms"
I0322 03:28:40.706691    2280 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="105.571s"
E0322 03:28:40.708566    2280 watcher.go:152] Failed to watch directory "/sys/fs/cgroup/devices/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-podbd655422_2e30_4b8c_a19a_617f504f2026.slice": readdirent /sys/fs/cgroup/devices/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-podbd655422_2e30_4b8c_a19a_617f504f2026.slice: no such file or directory
I0322 03:28:40.772483    2280 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="242.316s"
I0322 03:28:40.786769    2280 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-czff4\" (UniqueName: \"kubernetes.io/projected/bd655422-2e30-4b8c-a19a-617f504f2026-kube-api-access-czff4\") pod \"local-path-provisioner-6c86858495-lvfsk\" (UID: \"bd655422-2e30-4b8c-a19a-617f504f2026\") " pod="kube-system/local-path-provisioner-6c86858495-lvfsk"
I0322 03:28:40.786845    2280 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/5358d3b8-3fa6-4dcc-8789-ee2721507f22-config-volume\") pod \"coredns-6799fbcd5-rx42b\" (UID: \"5358d3b8-3fa6-4dcc-8789-ee2721507f22\") " pod="kube-system/coredns-6799fbcd5-rx42b"
I0322 03:28:40.786915    2280 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"custom-config-volume\" (UniqueName: \"kubernetes.io/configmap/5358d3b8-3fa6-4dcc-8789-ee2721507f22-custom-config-volume\") pod \"coredns-6799fbcd5-rx42b\" (UID: \"5358d3b8-3fa6-4dcc-8789-ee2721507f22\") " pod="kube-system/coredns-6799fbcd5-rx42b"
I0322 03:28:40.786961    2280 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-gx6b7\" (UniqueName: \"kubernetes.io/projected/5358d3b8-3fa6-4dcc-8789-ee2721507f22-kube-api-access-gx6b7\") pod \"coredns-6799fbcd5-rx42b\" (UID: \"5358d3b8-3fa6-4dcc-8789-ee2721507f22\") " pod="kube-system/coredns-6799fbcd5-rx42b"
I0322 03:28:40.787079    2280 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/bd655422-2e30-4b8c-a19a-617f504f2026-config-volume\") pod \"local-path-provisioner-6c86858495-lvfsk\" (UID: \"bd655422-2e30-4b8c-a19a-617f504f2026\") " pod="kube-system/local-path-provisioner-6c86858495-lvfsk"
I0322 03:28:40.812116    2280 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="105.45s"
I0322 03:28:40.966408    2280 kube.go:146] Node controller sync successful
I0322 03:28:40.966885    2280 vxlan.go:141] VXLAN config: VNI=1 Port=0 GBP=false Learning=false DirectRouting=false
I0322 03:28:40.977787    2280 kube.go:621] List of node(server) annotations: map[string]string{"alpha.kubernetes.io/provided-node-ip":"192.168.56.110", "k3s.io/hostname":"server", "k3s.io/internal-ip":"192.168.56.110", "k3s.io/node-args":"[\"server\",\"--node-ip\",\"192.168.56.110\",\"--write-kubeconfig-mode\",\"644\",\"--log\",\"/vagrant/logs/server.logs\"]", "k3s.io/node-config-hash":"OEDLXPLCFN65I5K4IWF2M6JMEKD34L47WZJH7YZKLHVRXJTJG6TQ====", "k3s.io/node-env":"{\"K3S_DATA_DIR\":\"/var/lib/rancher/k3s/data/a3b46c0299091b71bfcc617b1e1fec1845c13bdd848584ceb39d2e700e702a4b\"}", "node.alpha.kubernetes.io/ttl":"0", "volumes.kubernetes.io/controller-managed-attach-detach":"true"}
I0322 03:28:41.058555    2280 kube.go:482] Creating the node lease for IPv4. This is the n.Spec.PodCIDRs: [10.42.0.0/24]
time="2024-03-22T03:28:41Z" level=info msg="Wrote flannel subnet file to /run/flannel/subnet.env"
time="2024-03-22T03:28:41Z" level=info msg="Running flannel backend."
I0322 03:28:41.107186    2280 vxlan_network.go:65] watching for new subnet leases
I0322 03:28:41.107487    2280 iptables.go:290] generated 3 rules
I0322 03:28:41.110421    2280 iptables.go:290] generated 7 rules
I0322 03:28:41.155925    2280 iptables.go:283] bootstrap done
I0322 03:28:41.173206    2280 iptables.go:283] bootstrap done
W0322 03:28:41.183338    2280 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:28:41.183422    2280 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 03:28:41.183432    2280 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0322 03:28:41.187003    2280 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:28:41.187024    2280 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I0322 03:28:41.187031    2280 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0322 03:28:41.352480    2280 topology_manager.go:215] "Topology Admit Handler" podUID="075f8ac2-232b-4d0a-b9ad-8fb5739de94d" podNamespace="kube-system" podName="helm-install-traefik-crd-dlc6m"
I0322 03:28:41.355136    2280 topology_manager.go:215] "Topology Admit Handler" podUID="f5749ede-2796-414e-b0ea-d6e2523ec959" podNamespace="kube-system" podName="helm-install-traefik-fxsb6"
I0322 03:28:41.362265    2280 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:28:41.390905    2280 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
W0322 03:28:41.401537    2280 watcher.go:93] Error while processing event ("/sys/fs/cgroup/devices/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-podf5749ede_2796_414e_b0ea_d6e2523ec959.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/devices/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-podf5749ede_2796_414e_b0ea_d6e2523ec959.slice: no such file or directory
I0322 03:28:41.410998    2280 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:28:41.423217    2280 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:28:41.502834    2280 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/f5749ede-2796-414e-b0ea-d6e2523ec959-klipper-cache\") pod \"helm-install-traefik-fxsb6\" (UID: \"f5749ede-2796-414e-b0ea-d6e2523ec959\") " pod="kube-system/helm-install-traefik-fxsb6"
I0322 03:28:41.502880    2280 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/f5749ede-2796-414e-b0ea-d6e2523ec959-klipper-config\") pod \"helm-install-traefik-fxsb6\" (UID: \"f5749ede-2796-414e-b0ea-d6e2523ec959\") " pod="kube-system/helm-install-traefik-fxsb6"
I0322 03:28:41.502905    2280 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/f5749ede-2796-414e-b0ea-d6e2523ec959-tmp\") pod \"helm-install-traefik-fxsb6\" (UID: \"f5749ede-2796-414e-b0ea-d6e2523ec959\") " pod="kube-system/helm-install-traefik-fxsb6"
I0322 03:28:41.502931    2280 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/075f8ac2-232b-4d0a-b9ad-8fb5739de94d-klipper-cache\") pod \"helm-install-traefik-crd-dlc6m\" (UID: \"075f8ac2-232b-4d0a-b9ad-8fb5739de94d\") " pod="kube-system/helm-install-traefik-crd-dlc6m"
I0322 03:28:41.502962    2280 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"values\" (UniqueName: \"kubernetes.io/secret/075f8ac2-232b-4d0a-b9ad-8fb5739de94d-values\") pod \"helm-install-traefik-crd-dlc6m\" (UID: \"075f8ac2-232b-4d0a-b9ad-8fb5739de94d\") " pod="kube-system/helm-install-traefik-crd-dlc6m"
I0322 03:28:41.502988    2280 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/f5749ede-2796-414e-b0ea-d6e2523ec959-klipper-helm\") pod \"helm-install-traefik-fxsb6\" (UID: \"f5749ede-2796-414e-b0ea-d6e2523ec959\") " pod="kube-system/helm-install-traefik-fxsb6"
I0322 03:28:41.503015    2280 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"content\" (UniqueName: \"kubernetes.io/configmap/f5749ede-2796-414e-b0ea-d6e2523ec959-content\") pod \"helm-install-traefik-fxsb6\" (UID: \"f5749ede-2796-414e-b0ea-d6e2523ec959\") " pod="kube-system/helm-install-traefik-fxsb6"
I0322 03:28:41.503042    2280 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-mk5j8\" (UniqueName: \"kubernetes.io/projected/f5749ede-2796-414e-b0ea-d6e2523ec959-kube-api-access-mk5j8\") pod \"helm-install-traefik-fxsb6\" (UID: \"f5749ede-2796-414e-b0ea-d6e2523ec959\") " pod="kube-system/helm-install-traefik-fxsb6"
I0322 03:28:41.503072    2280 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-p7vhx\" (UniqueName: \"kubernetes.io/projected/075f8ac2-232b-4d0a-b9ad-8fb5739de94d-kube-api-access-p7vhx\") pod \"helm-install-traefik-crd-dlc6m\" (UID: \"075f8ac2-232b-4d0a-b9ad-8fb5739de94d\") " pod="kube-system/helm-install-traefik-crd-dlc6m"
I0322 03:28:41.503095    2280 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/075f8ac2-232b-4d0a-b9ad-8fb5739de94d-klipper-helm\") pod \"helm-install-traefik-crd-dlc6m\" (UID: \"075f8ac2-232b-4d0a-b9ad-8fb5739de94d\") " pod="kube-system/helm-install-traefik-crd-dlc6m"
I0322 03:28:41.503118    2280 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/075f8ac2-232b-4d0a-b9ad-8fb5739de94d-klipper-config\") pod \"helm-install-traefik-crd-dlc6m\" (UID: \"075f8ac2-232b-4d0a-b9ad-8fb5739de94d\") " pod="kube-system/helm-install-traefik-crd-dlc6m"
I0322 03:28:41.503140    2280 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/075f8ac2-232b-4d0a-b9ad-8fb5739de94d-tmp\") pod \"helm-install-traefik-crd-dlc6m\" (UID: \"075f8ac2-232b-4d0a-b9ad-8fb5739de94d\") " pod="kube-system/helm-install-traefik-crd-dlc6m"
I0322 03:28:41.503167    2280 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"content\" (UniqueName: \"kubernetes.io/configmap/075f8ac2-232b-4d0a-b9ad-8fb5739de94d-content\") pod \"helm-install-traefik-crd-dlc6m\" (UID: \"075f8ac2-232b-4d0a-b9ad-8fb5739de94d\") " pod="kube-system/helm-install-traefik-crd-dlc6m"
I0322 03:28:41.503188    2280 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"values\" (UniqueName: \"kubernetes.io/secret/f5749ede-2796-414e-b0ea-d6e2523ec959-values\") pod \"helm-install-traefik-fxsb6\" (UID: \"f5749ede-2796-414e-b0ea-d6e2523ec959\") " pod="kube-system/helm-install-traefik-fxsb6"
I0322 03:28:44.418463    2280 kuberuntime_manager.go:1528] "Updating runtime config through cri with podcidr" CIDR="10.42.0.0/24"
I0322 03:28:44.419991    2280 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.42.0.0/24"
E0322 03:28:44.979856    2280 cadvisor_stats_provider.go:444] "Partial failure issuing cadvisor.ContainerInfoV2" err="partial failures: [\"/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod1ea7b8bb_866e_4ee0_84fb_ed8098467564.slice/cri-containerd-f8d6d2a9ac5584504ac22a785192ee50175f155f64076683cb2e278007acdef4.scope\": RecentStats: unable to find data in memory cache]"
I0322 03:28:52.964054    2280 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/local-path-provisioner-6c86858495-lvfsk" containerName="local-path-provisioner"
I0322 03:28:53.729941    2280 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/local-path-provisioner-6c86858495-lvfsk" podStartSLOduration=5.739240854 podCreationTimestamp="2024-03-22 03:28:40 +0000 UTC" firstStartedPulling="2024-03-22 03:28:44.966309695 +0000 UTC m=+24.957957931" lastFinishedPulling="2024-03-22 03:28:52.954749352 +0000 UTC m=+32.946397583" observedRunningTime="2024-03-22 03:28:53.712893776 +0000 UTC m=+33.704542014" watchObservedRunningTime="2024-03-22 03:28:53.727680506 +0000 UTC m=+33.719328743"
I0322 03:28:53.733712    2280 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="15.89296ms"
I0322 03:28:53.734939    2280 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="37.655s"
I0322 03:28:58.663702    2280 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/coredns-6799fbcd5-rx42b" containerName="coredns"
I0322 03:28:59.886552    2280 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="1.072369ms"
I0322 03:28:59.893140    2280 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/coredns-6799fbcd5-rx42b" podStartSLOduration=6.199135584 podCreationTimestamp="2024-03-22 03:28:40 +0000 UTC" firstStartedPulling="2024-03-22 03:28:44.946311022 +0000 UTC m=+24.937959252" lastFinishedPulling="2024-03-22 03:28:58.635388429 +0000 UTC m=+38.627036669" observedRunningTime="2024-03-22 03:28:59.887728949 +0000 UTC m=+39.879377194" watchObservedRunningTime="2024-03-22 03:28:59.888213001 +0000 UTC m=+39.879861246"
I0322 03:29:00.014840    2280 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/metrics-server-67c658944b-p82jk" containerName="metrics-server"
I0322 03:29:00.879823    2280 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="91.523s"
I0322 03:29:00.994067    2280 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/metrics-server-67c658944b-p82jk" podStartSLOduration=5.9790133260000005 podCreationTimestamp="2024-03-22 03:28:40 +0000 UTC" firstStartedPulling="2024-03-22 03:28:44.994336088 +0000 UTC m=+24.985984314" lastFinishedPulling="2024-03-22 03:29:00.009300066 +0000 UTC m=+40.000948308" observedRunningTime="2024-03-22 03:29:00.868389141 +0000 UTC m=+40.860037407" watchObservedRunningTime="2024-03-22 03:29:00.99397732 +0000 UTC m=+40.985625556"
I0322 03:29:01.081201    2280 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="129.43477ms"
I0322 03:29:01.088626    2280 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="89.629s"
I0322 03:29:03.560806    2280 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/helm-install-traefik-crd-dlc6m" containerName="helm"
I0322 03:29:03.561942    2280 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/helm-install-traefik-fxsb6" containerName="helm"
I0322 03:29:05.071435    2280 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/helm-install-traefik-crd-dlc6m" podStartSLOduration=6.552531801 podCreationTimestamp="2024-03-22 03:28:40 +0000 UTC" firstStartedPulling="2024-03-22 03:28:45.041063925 +0000 UTC m=+25.032712167" lastFinishedPulling="2024-03-22 03:29:03.554397322 +0000 UTC m=+43.546045554" observedRunningTime="2024-03-22 03:29:05.060505861 +0000 UTC m=+45.052154098" watchObservedRunningTime="2024-03-22 03:29:05.065865188 +0000 UTC m=+45.057513450"
I0322 03:29:05.108792    2280 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:29:05.153944    2280 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:29:05.156627    2280 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/helm-install-traefik-fxsb6" podStartSLOduration=6.626980955 podCreationTimestamp="2024-03-22 03:28:40 +0000 UTC" firstStartedPulling="2024-03-22 03:28:45.0286122 +0000 UTC m=+25.020260469" lastFinishedPulling="2024-03-22 03:29:03.558191475 +0000 UTC m=+43.549839700" observedRunningTime="2024-03-22 03:29:05.148140424 +0000 UTC m=+45.139788698" watchObservedRunningTime="2024-03-22 03:29:05.156560186 +0000 UTC m=+45.148208428"
W0322 03:29:05.884291    2280 aggregator.go:166] failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 03:29:05.945112    2280 scope.go:117] "RemoveContainer" containerID="1779798eb7a1d6c3f4036d9c4d5699c079decddf8c345293050d81069b171e58"
I0322 03:29:05.956282    2280 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/helm-install-traefik-fxsb6" containerName="helm"
I0322 03:29:06.029559    2280 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:29:06.172963    2280 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:29:07.051887    2280 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:29:08.068270    2280 scope.go:117] "RemoveContainer" containerID="dd39ba9cb1d7cc7d1a9ff395b58c474adcecd56014f2c94294c76500ea39d1c6"
E0322 03:29:08.071645    2280 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"helm\" with CrashLoopBackOff: \"back-off 10s restarting failed container=helm pod=helm-install-traefik-fxsb6_kube-system(f5749ede-2796-414e-b0ea-d6e2523ec959)\"" pod="kube-system/helm-install-traefik-fxsb6" podUID="f5749ede-2796-414e-b0ea-d6e2523ec959"
I0322 03:29:08.089599    2280 scope.go:117] "RemoveContainer" containerID="1779798eb7a1d6c3f4036d9c4d5699c079decddf8c345293050d81069b171e58"
I0322 03:29:08.208615    2280 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:29:08.210516    2280 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:29:08.294236    2280 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0322 03:29:08.350144    2280 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0322 03:29:08.374727    2280 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0322 03:29:08.441068    2280 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0322 03:29:08.469728    2280 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0322 03:29:08.473120    2280 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0322 03:29:08.481761    2280 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0322 03:29:08.506688    2280 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0322 03:29:08.551137    2280 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0322 03:29:08.598373    2280 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0322 03:29:08.612892    2280 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0322 03:29:08.692852    2280 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0322 03:29:08.842696    2280 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0322 03:29:08.864912    2280 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0322 03:29:08.889844    2280 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0322 03:29:08.899369    2280 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0322 03:29:08.904182    2280 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0322 03:29:08.908365    2280 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0322 03:29:09.110326    2280 scope.go:117] "RemoveContainer" containerID="dd39ba9cb1d7cc7d1a9ff395b58c474adcecd56014f2c94294c76500ea39d1c6"
E0322 03:29:09.110940    2280 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"helm\" with CrashLoopBackOff: \"back-off 10s restarting failed container=helm pod=helm-install-traefik-fxsb6_kube-system(f5749ede-2796-414e-b0ea-d6e2523ec959)\"" pod="kube-system/helm-install-traefik-fxsb6" podUID="f5749ede-2796-414e-b0ea-d6e2523ec959"
I0322 03:29:09.132339    2280 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:29:09.144943    2280 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:29:09.347665    2280 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
E0322 03:29:09.872571    2280 resource_quota_controller.go:440] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I0322 03:29:09.875786    2280 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="tlsoptions.traefik.io"
I0322 03:29:09.876043    2280 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressrouteudps.traefik.containo.us"
I0322 03:29:09.876210    2280 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressroutetcps.traefik.containo.us"
I0322 03:29:09.876645    2280 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressroutetcps.traefik.io"
I0322 03:29:09.876819    2280 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="tlsoptions.traefik.containo.us"
I0322 03:29:09.876975    2280 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="serverstransporttcps.traefik.io"
I0322 03:29:09.877121    2280 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="tlsstores.traefik.containo.us"
I0322 03:29:09.877239    2280 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="middlewaretcps.traefik.io"
I0322 03:29:09.877409    2280 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressroutes.traefik.containo.us"
I0322 03:29:09.877546    2280 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="middlewaretcps.traefik.containo.us"
I0322 03:29:09.877666    2280 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="serverstransports.traefik.io"
I0322 03:29:09.877769    2280 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressroutes.traefik.io"
I0322 03:29:09.877885    2280 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="tlsstores.traefik.io"
I0322 03:29:09.878016    2280 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="traefikservices.traefik.containo.us"
I0322 03:29:09.878156    2280 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="middlewares.traefik.io"
I0322 03:29:09.878256    2280 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="traefikservices.traefik.io"
I0322 03:29:09.878380    2280 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressrouteudps.traefik.io"
I0322 03:29:09.878476    2280 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="serverstransports.traefik.containo.us"
I0322 03:29:09.878579    2280 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="middlewares.traefik.containo.us"
I0322 03:29:09.880196    2280 shared_informer.go:311] Waiting for caches to sync for resource quota
I0322 03:29:10.081306    2280 shared_informer.go:318] Caches are synced for resource quota
I0322 03:29:10.141909    2280 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:29:10.408799    2280 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:29:10.452484    2280 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/075f8ac2-232b-4d0a-b9ad-8fb5739de94d-klipper-cache\") pod \"075f8ac2-232b-4d0a-b9ad-8fb5739de94d\" (UID: \"075f8ac2-232b-4d0a-b9ad-8fb5739de94d\") "
I0322 03:29:10.452589    2280 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-p7vhx\" (UniqueName: \"kubernetes.io/projected/075f8ac2-232b-4d0a-b9ad-8fb5739de94d-kube-api-access-p7vhx\") pod \"075f8ac2-232b-4d0a-b9ad-8fb5739de94d\" (UID: \"075f8ac2-232b-4d0a-b9ad-8fb5739de94d\") "
I0322 03:29:10.452617    2280 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/075f8ac2-232b-4d0a-b9ad-8fb5739de94d-klipper-config\") pod \"075f8ac2-232b-4d0a-b9ad-8fb5739de94d\" (UID: \"075f8ac2-232b-4d0a-b9ad-8fb5739de94d\") "
I0322 03:29:10.452658    2280 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"content\" (UniqueName: \"kubernetes.io/configmap/075f8ac2-232b-4d0a-b9ad-8fb5739de94d-content\") pod \"075f8ac2-232b-4d0a-b9ad-8fb5739de94d\" (UID: \"075f8ac2-232b-4d0a-b9ad-8fb5739de94d\") "
I0322 03:29:10.452687    2280 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/075f8ac2-232b-4d0a-b9ad-8fb5739de94d-tmp\") pod \"075f8ac2-232b-4d0a-b9ad-8fb5739de94d\" (UID: \"075f8ac2-232b-4d0a-b9ad-8fb5739de94d\") "
I0322 03:29:10.452704    2280 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/075f8ac2-232b-4d0a-b9ad-8fb5739de94d-klipper-helm\") pod \"075f8ac2-232b-4d0a-b9ad-8fb5739de94d\" (UID: \"075f8ac2-232b-4d0a-b9ad-8fb5739de94d\") "
I0322 03:29:10.452722    2280 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"values\" (UniqueName: \"kubernetes.io/secret/075f8ac2-232b-4d0a-b9ad-8fb5739de94d-values\") pod \"075f8ac2-232b-4d0a-b9ad-8fb5739de94d\" (UID: \"075f8ac2-232b-4d0a-b9ad-8fb5739de94d\") "
I0322 03:29:10.486487    2280 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/configmap/075f8ac2-232b-4d0a-b9ad-8fb5739de94d-content" (OuterVolumeSpecName: "content") pod "075f8ac2-232b-4d0a-b9ad-8fb5739de94d" (UID: "075f8ac2-232b-4d0a-b9ad-8fb5739de94d"). InnerVolumeSpecName "content". PluginName "kubernetes.io/configmap", VolumeGidValue ""
I0322 03:29:10.521876    2280 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/075f8ac2-232b-4d0a-b9ad-8fb5739de94d-klipper-cache" (OuterVolumeSpecName: "klipper-cache") pod "075f8ac2-232b-4d0a-b9ad-8fb5739de94d" (UID: "075f8ac2-232b-4d0a-b9ad-8fb5739de94d"). InnerVolumeSpecName "klipper-cache". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0322 03:29:10.522508    2280 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/075f8ac2-232b-4d0a-b9ad-8fb5739de94d-klipper-config" (OuterVolumeSpecName: "klipper-config") pod "075f8ac2-232b-4d0a-b9ad-8fb5739de94d" (UID: "075f8ac2-232b-4d0a-b9ad-8fb5739de94d"). InnerVolumeSpecName "klipper-config". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0322 03:29:10.523302    2280 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/075f8ac2-232b-4d0a-b9ad-8fb5739de94d-kube-api-access-p7vhx" (OuterVolumeSpecName: "kube-api-access-p7vhx") pod "075f8ac2-232b-4d0a-b9ad-8fb5739de94d" (UID: "075f8ac2-232b-4d0a-b9ad-8fb5739de94d"). InnerVolumeSpecName "kube-api-access-p7vhx". PluginName "kubernetes.io/projected", VolumeGidValue ""
I0322 03:29:10.526048    2280 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/075f8ac2-232b-4d0a-b9ad-8fb5739de94d-values" (OuterVolumeSpecName: "values") pod "075f8ac2-232b-4d0a-b9ad-8fb5739de94d" (UID: "075f8ac2-232b-4d0a-b9ad-8fb5739de94d"). InnerVolumeSpecName "values". PluginName "kubernetes.io/secret", VolumeGidValue ""
I0322 03:29:10.533917    2280 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/075f8ac2-232b-4d0a-b9ad-8fb5739de94d-klipper-helm" (OuterVolumeSpecName: "klipper-helm") pod "075f8ac2-232b-4d0a-b9ad-8fb5739de94d" (UID: "075f8ac2-232b-4d0a-b9ad-8fb5739de94d"). InnerVolumeSpecName "klipper-helm". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0322 03:29:10.534321    2280 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/075f8ac2-232b-4d0a-b9ad-8fb5739de94d-tmp" (OuterVolumeSpecName: "tmp") pod "075f8ac2-232b-4d0a-b9ad-8fb5739de94d" (UID: "075f8ac2-232b-4d0a-b9ad-8fb5739de94d"). InnerVolumeSpecName "tmp". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0322 03:29:10.557266    2280 reconciler_common.go:300] "Volume detached for volume \"content\" (UniqueName: \"kubernetes.io/configmap/075f8ac2-232b-4d0a-b9ad-8fb5739de94d-content\") on node \"server\" DevicePath \"\""
I0322 03:29:10.557303    2280 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-p7vhx\" (UniqueName: \"kubernetes.io/projected/075f8ac2-232b-4d0a-b9ad-8fb5739de94d-kube-api-access-p7vhx\") on node \"server\" DevicePath \"\""
I0322 03:29:10.557320    2280 reconciler_common.go:300] "Volume detached for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/075f8ac2-232b-4d0a-b9ad-8fb5739de94d-klipper-config\") on node \"server\" DevicePath \"\""
I0322 03:29:10.557334    2280 reconciler_common.go:300] "Volume detached for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/075f8ac2-232b-4d0a-b9ad-8fb5739de94d-tmp\") on node \"server\" DevicePath \"\""
I0322 03:29:10.557346    2280 reconciler_common.go:300] "Volume detached for volume \"values\" (UniqueName: \"kubernetes.io/secret/075f8ac2-232b-4d0a-b9ad-8fb5739de94d-values\") on node \"server\" DevicePath \"\""
I0322 03:29:10.557359    2280 reconciler_common.go:300] "Volume detached for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/075f8ac2-232b-4d0a-b9ad-8fb5739de94d-klipper-helm\") on node \"server\" DevicePath \"\""
I0322 03:29:10.557376    2280 reconciler_common.go:300] "Volume detached for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/075f8ac2-232b-4d0a-b9ad-8fb5739de94d-klipper-cache\") on node \"server\" DevicePath \"\""
I0322 03:29:10.579921    2280 garbagecollector.go:816] "failed to discover some groups" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"
I0322 03:29:10.589943    2280 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0322 03:29:10.590242    2280 shared_informer.go:318] Caches are synced for garbage collector
I0322 03:29:11.167590    2280 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="0439454a99028bf8c698a0f50ca188abf5ca0b4e3b29fdb2d89cd0252d09bb1d"
I0322 03:29:11.170751    2280 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:29:11.192810    2280 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:29:11.206770    2280 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:29:11.210776    2280 event.go:307] "Event occurred" object="kube-system/helm-install-traefik-crd" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="Completed" message="Job completed"
I0322 03:29:11.227494    2280 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:29:19.254552    2280 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="63.57082ms"
I0322 03:29:19.255262    2280 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="359.479s"
I0322 03:29:19.411518    2280 handler.go:275] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0322 03:29:24.112018    2280 scope.go:117] "RemoveContainer" containerID="dd39ba9cb1d7cc7d1a9ff395b58c474adcecd56014f2c94294c76500ea39d1c6"
I0322 03:29:24.303727    2280 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/helm-install-traefik-fxsb6" containerName="helm"
E0322 03:29:41.371672    2280 health_controller.go:162] Metrics Controller heartbeat missed
E0322 03:29:47.542147    2280 health_controller.go:162] Metrics Controller heartbeat missed
W0322 03:29:52.006874    2280 transport.go:301] Unable to cancel request for *otelhttp.Transport
I0322 03:30:02.701321    2280 request.go:697] Waited for 4.889872364s due to client-side throttling, not priority and fairness, request: GET:https://127.0.0.1:6444/api
E0322 03:29:56.544486    2280 health_controller.go:162] Metrics Controller heartbeat missed
W0322 03:33:46.450589    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ServiceAccount ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:46.451545    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PersistentVolume ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:46.452576    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ResourceQuota ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:46.453094    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ClusterRoleBinding ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:46.453732    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ValidatingWebhookConfiguration ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:46.522798    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1beta3.FlowSchema ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:46.523495    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Role ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:46.524035    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.IngressClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:46.524209    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ClusterRole ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:46.524833    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:46.525259    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Lease ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:46.525384    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:46.525478    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Secret ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:46.525709    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.MutatingWebhookConfiguration ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:46.526171    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1beta3.PriorityLevelConfiguration ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:46.526717    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.RuntimeClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:46.526854    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.StorageClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:46.526964    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.APIService ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:46.527047    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.RoleBinding ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:46.527157    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.LimitRange ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:46.527583    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:46.527799    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:46.528025    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CustomResourceDefinition ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:46.528964    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:46.529211    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Endpoints ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:46.529343    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Namespace ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:46.735497    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.VolumeAttachment ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:51.129414    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ServiceAccount ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:33:48.982540    2280 event.go:289] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"helm-install-traefik-fxsb6.17bef882108e61b3", GenerateName:"", Namespace:"kube-system", SelfLink:"", UID:"", ResourceVersion:"515", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"helm-install-traefik-fxsb6", UID:"f5749ede-2796-414e-b0ea-d6e2523ec959", APIVersion:"v1", ResourceVersion:"458", FieldPath:"spec.containers{helm}"}, Reason:"Pulled", Message:"Container image \"rancher/klipper-helm:v0.8.2-build20230815\" already present on machine", Source:v1.EventSource{Component:"kubelet", Host:"server"}, FirstTimestamp:time.Date(2024, time.March, 22, 3, 29, 5, 0, time.Local), LastTimestamp:time.Date(2024, time.March, 22, 3, 29, 24, 223409655, time.Local), Count:2, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"kubelet", ReportingInstance:"server"}': 'Patch "https://127.0.0.1:6443/api/v1/namespaces/kube-system/events/helm-install-traefik-fxsb6.17bef882108e61b3": http2: client connection lost'(may retry after sleeping)
I0322 03:33:51.281223    2280 trace.go:236] Trace[1605154831]: "iptables ChainExists" (22-Mar-2024 03:30:29.173) (total time: 202096ms):
Trace[1605154831]: [3m22.096337398s] [3m22.096337398s] END
W0322 03:33:51.300478    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:33:51.389523    2280 watch.go:287] unable to encode watch object *v1.WatchEvent: client disconnected (&streaming.encoderWithAllocator{writer:(*framer.lengthDelimitedFrameWriter)(0xc0078ebdd0), encoder:(*versioning.codec)(0xc009a126e0), memAllocator:(*runtime.Allocator)(0xc0078ebde8)})
E0322 03:33:51.394598    2280 watch.go:287] unable to encode watch object *v1.WatchEvent: client disconnected (&streaming.encoderWithAllocator{writer:(*framer.lengthDelimitedFrameWriter)(0xc007b910b0), encoder:(*versioning.codec)(0xc009a13180), memAllocator:(*runtime.Allocator)(0xc007b910f8)})
E0322 03:33:51.399008    2280 watch.go:287] unable to encode watch object *v1.WatchEvent: client disconnected (&streaming.encoderWithAllocator{writer:(*framer.lengthDelimitedFrameWriter)(0xc004a2ac18), encoder:(*versioning.codec)(0xc00941fea0), memAllocator:(*runtime.Allocator)(0xc004a2ac30)})
E0322 03:33:51.401348    2280 watch.go:287] unable to encode watch object *v1.WatchEvent: client disconnected (&streaming.encoderWithAllocator{writer:(*framer.lengthDelimitedFrameWriter)(0xc00599b938), encoder:(*versioning.codec)(0xc00af20c80), memAllocator:(*runtime.Allocator)(0xc00599b950)})
E0322 03:33:51.401900    2280 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:33:51.404004    2280 watch.go:287] unable to encode watch object *v1.WatchEvent: client disconnected (&streaming.encoderWithAllocator{writer:(*framer.lengthDelimitedFrameWriter)(0xc00856b998), encoder:(*versioning.codec)(0xc00a6fd680), memAllocator:(*runtime.Allocator)(0xc00856b9b0)})
I0322 03:33:51.569829    2280 trace.go:236] Trace[1444289379]: "DeltaFIFO Pop Process" ID:v1.apps,Depth:25,Reason:slow event handlers blocking the queue (22-Mar-2024 03:30:10.343) (total time: 219857ms):
Trace[1444289379]: [3m39.857524361s] [3m39.857524361s] END
W0322 03:33:51.637356    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:33:51.698481    2280 server.go:310] "Unable to authenticate the request due to an error" err="context canceled"
E0322 03:33:51.734235    2280 server.go:310] "Unable to authenticate the request due to an error" err="context canceled"
E0322 03:33:51.835386    2280 watch.go:287] unable to encode watch object *v1.WatchEvent: client disconnected (&streaming.encoderWithAllocator{writer:(*framer.lengthDelimitedFrameWriter)(0xc01137a738), encoder:(*versioning.codec)(0xc011123540), memAllocator:(*runtime.Allocator)(0xc01137a750)})
time="2024-03-22T03:33:47Z" level=info msg="Slow SQL (started: 2024-03-22 03:29:27.671517118 +0000 UTC m=+67.663165387) (total time: 13.831901754s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/configmaps/% false]]"
W0322 03:33:46.800954    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Lease ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
time="2024-03-22T03:33:48Z" level=info msg="Slow SQL (started: 2024-03-22 03:29:27.555460599 +0000 UTC m=+67.547108867) (total time: 1m19.478605827s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC  : [[/registry/events/kube-system/helm-install-traefik-fxsb6.17bef882108e61b3 false]]"
W0322 03:33:46.811593    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PriorityClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
time="2024-03-22T03:33:48Z" level=info msg="Password verification deferred for node serverworker"
time="2024-03-22T03:33:52Z" level=info msg="certificate CN=serverworker signed by CN=k3s-server-ca@1711078100: notBefore=2024-03-22 03:28:20 +0000 UTC notAfter=2025-03-22 03:33:52 +0000 UTC"
W0322 03:33:47.229558    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
time="2024-03-22T03:33:49Z" level=info msg="Slow SQL (started: 2024-03-22 03:29:34.925673464 +0000 UTC m=+74.917321714) (total time: 14.699058905s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? AND mkv.id <= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC LIMIT 1000 : [[/registry/cronjobs/% 612 false]]"
W0322 03:33:47.230260    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.DaemonSet ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.230429    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.232095    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.232217    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.EndpointSlice ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.232617    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.255495    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:33:47.281294    2280 controller.go:113] loading OpenAPI spec for "k8s_internal_local_delegation_chain_0000000002" failed with: Error, could not get list of group versions for APIService
I0322 03:33:52.172212    2280 controller.go:126] OpenAPI AggregationController: action for item k8s_internal_local_delegation_chain_0000000002: Rate Limited Requeue.
W0322 03:33:47.290231    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Endpoints ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.290315    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.291060    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.296464    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Namespace ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.870368    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ClusterRole ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.871856    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.RoleBinding ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.871987    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ControllerRevision ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.872540    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.StorageClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.872868    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.MutatingWebhookConfiguration ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.873522    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.IngressClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.874120    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.880925    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.DaemonSet ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.882145    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.VolumeAttachment ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.882835    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Lease ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.883726    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.RuntimeClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.884039    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.884423    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.884638    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ReplicationController ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.884873    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.885374    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.885555    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.885804    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Namespace ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.885864    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.885956    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.886061    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.LimitRange ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.886834    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.887175    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.887257    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.EndpointSlice ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.887556    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1beta3.PriorityLevelConfiguration ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.888278    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.888407    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.888465    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1beta3.FlowSchema ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.888736    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.888840    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.889104    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.889374    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Job ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.889502    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Deployment ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.889642    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PersistentVolumeClaim ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.889808    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.889910    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.890100    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ReplicaSet ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.890185    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.890249    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.890384    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Endpoints ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.890456    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Ingress ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.891028    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Role ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.891425    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PriorityClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.891461    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.891525    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PodDisruptionBudget ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.891617    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.892179    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Secret ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.892328    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.StatefulSet ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.892390    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.892469    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.892521    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.892595    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CertificateSigningRequest ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.916376    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.916489    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.916890    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CronJob ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.917006    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CSIStorageCapacity ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.917051    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.NetworkPolicy ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.917283    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CSINode ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.917504    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ValidatingWebhookConfiguration ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.918280    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.918774    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ResourceQuota ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.918868    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CSIDriver ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.920124    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v2.HorizontalPodAutoscaler ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.920700    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.920943    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PodTemplate ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.921946    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ClusterRoleBinding ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:47.922147    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:33:48.280990    2280 health_controller.go:162] Metrics Controller heartbeat missed
E0322 03:33:48.367822    2280 remote_runtime.go:407] "ListContainers with filter from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" filter="&ContainerFilter{Id:,State:nil,PodSandboxId:,LabelSelector:map[string]string{},}"
E0322 03:33:48.487479    2280 remote_runtime.go:294] "ListPodSandbox with filter from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" filter="&PodSandboxFilter{Id:,State:nil,LabelSelector:map[string]string{io.kubernetes.pod.uid: f5749ede-2796-414e-b0ea-d6e2523ec959,},}"
E0322 03:33:52.261238    2280 kuberuntime_sandbox.go:349] "Failed to list sandboxes for pod" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" podUID="f5749ede-2796-414e-b0ea-d6e2523ec959"
E0322 03:33:52.263847    2280 generic.go:453] "PLEG: Write status" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" pod="kube-system/helm-install-traefik-fxsb6"
E0322 03:33:48.499269    2280 remote_image.go:128] "ListImages with filter from image service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" filter="nil"
E0322 03:33:52.266899    2280 kuberuntime_image.go:103] "Failed to list images" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded"
I0322 03:33:52.267855    2280 image_gc_manager.go:210] "Failed to update image list" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded"
E0322 03:33:48.694266    2280 remote_runtime.go:633] "Status from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded"
W0322 03:33:48.790415    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.HelmChartConfig ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:48.791676    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Addon ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:48.792361    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ServiceAccount ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:48.803228    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Secret ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:48.803392    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.HelmChart ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:48.804034    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ClusterRoleBinding ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:48.804411    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:48.815161    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:48.918927    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:48.940252    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:48.962669    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Job ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:48.962831    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.RuntimeClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:48.974683    2280 reflector.go:458] object-"kube-system"/"local-path-config": watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:48.974801    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:48.975093    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:48.975196    2280 reflector.go:458] object-"kube-system"/"coredns": watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:48.975282    2280 reflector.go:458] object-"kube-system"/"chart-content-traefik": watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:48.975359    2280 reflector.go:458] object-"kube-system"/"coredns-custom": watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:48.975429    2280 reflector.go:458] object-"kube-system"/"kube-root-ca.crt": watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:48.975498    2280 reflector.go:458] object-"kube-system"/"chart-values-traefik": watch of *v1.Secret ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:48.975537    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CSIDriver ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I0322 03:33:49.249362    2280 trace.go:236] Trace[1855511751]: "iptables ChainExists" (22-Mar-2024 03:29:32.534) (total time: 256709ms):
Trace[1855511751]: [4m16.709863274s] [4m16.709863274s] END
W0322 03:33:49.430489    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.EndpointSlice ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:49.434817    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:49.449080    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:49.449220    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I0322 03:33:49.464614    2280 log.go:245] http: TLS handshake error from 10.42.0.4:48750: write tcp 192.168.56.110:10250->10.42.0.4:48750: write: broken pipe
W0322 03:33:49.792681    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I0322 03:33:52.285983    2280 log.go:245] http: TLS handshake error from 10.42.0.4:55618: write tcp 192.168.56.110:10250->10.42.0.4:55618: write: broken pipe
I0322 03:33:52.286212    2280 log.go:245] http: TLS handshake error from 10.42.0.4:58482: write tcp 192.168.56.110:10250->10.42.0.4:58482: write: broken pipe
I0322 03:33:52.286320    2280 log.go:245] http: TLS handshake error from 10.42.0.4:53434: write tcp 192.168.56.110:10250->10.42.0.4:53434: write: broken pipe
I0322 03:33:52.286367    2280 log.go:245] http: TLS handshake error from 10.42.0.4:32768: write tcp 192.168.56.110:10250->10.42.0.4:32768: write: broken pipe
I0322 03:33:52.286384    2280 log.go:245] http: TLS handshake error from 10.42.0.4:58338: write tcp 192.168.56.110:10250->10.42.0.4:58338: write: broken pipe
I0322 03:33:52.286440    2280 log.go:245] http: TLS handshake error from 10.42.0.4:41640: write tcp 192.168.56.110:10250->10.42.0.4:41640: write: broken pipe
I0322 03:33:52.286458    2280 log.go:245] http: TLS handshake error from 10.42.0.4:50270: write tcp 192.168.56.110:10250->10.42.0.4:50270: write: broken pipe
I0322 03:33:52.286471    2280 log.go:245] http: TLS handshake error from 10.42.0.4:45988: write tcp 192.168.56.110:10250->10.42.0.4:45988: write: broken pipe
I0322 03:33:52.286522    2280 log.go:245] http: TLS handshake error from 10.42.0.4:58902: EOF
I0322 03:33:52.286661    2280 log.go:245] http: TLS handshake error from 10.42.0.4:47290: EOF
I0322 03:33:52.286678    2280 log.go:245] http: TLS handshake error from 10.42.0.4:39592: EOF
I0322 03:33:52.286689    2280 log.go:245] http: TLS handshake error from 10.42.0.4:37560: EOF
I0322 03:33:52.286703    2280 log.go:245] http: TLS handshake error from 10.42.0.4:47892: EOF
E0322 03:33:49.794436    2280 finisher.go:175] FinishRequest: post-timeout activity - time-elapsed: 3m50.1291427s, panicked: false, err: context deadline exceeded, panic-reason: <nil>
I0322 03:33:49.908631    2280 trace.go:236] Trace[1854136345]: "iptables ChainExists" (22-Mar-2024 03:29:32.503) (total time: 257400ms):
Trace[1854136345]: [4m17.400602211s] [4m17.400602211s] END
W0322 03:33:49.938876    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:49.940162    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PersistentVolumeClaim ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:49.940725    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ReplicationController ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I0322 03:33:49.985286    2280 trace.go:236] Trace[592340848]: "DeltaFIFO Pop Process" ID:ingressroutetcps.traefik.io,Depth:21,Reason:slow event handlers blocking the queue (22-Mar-2024 03:33:49.443) (total time: 539ms):
Trace[592340848]: [539.574004ms] [539.574004ms] END
W0322 03:33:50.001393    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PersistentVolume ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:50.039248    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:50.039508    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CSIStorageCapacity ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:50.039544    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CSIDriver ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:50.039568    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:50.039592    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CSINode ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:50.039953    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ReplicaSet ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:50.040220    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PodDisruptionBudget ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:50.040577    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Namespace ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:50.040774    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.StatefulSet ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:50.040806    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.StorageClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:50.075192    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:50.593618    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:33:50.865737    2280 kubelet_node_status.go:540] "Error updating node status, will retry" err="error getting node \"server\": Get \"https://127.0.0.1:6443/api/v1/nodes/server?resourceVersion=0&timeout=10s\": net/http: request canceled (Client.Timeout exceeded while awaiting headers)"
E0322 03:33:50.899339    2280 controller.go:193] "Failed to update lease" err="Put \"https://127.0.0.1:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/server?timeout=10s\": context deadline exceeded"
E0322 03:33:50.971359    2280 remote_runtime.go:407] "ListContainers with filter from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" filter="&ContainerFilter{Id:,State:nil,PodSandboxId:,LabelSelector:map[string]string{},}"
E0322 03:33:52.437517    2280 container_log_manager.go:185] "Failed to rotate container logs" err="failed to list containers: rpc error: code = DeadlineExceeded desc = context deadline exceeded"
W0322 03:33:51.032268    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PersistentVolume ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:51.037357    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.NetworkPolicy ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:33:51.042631    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I0322 03:33:51.062832    2280 trace.go:236] Trace[306736497]: "iptables ChainExists" (22-Mar-2024 03:30:42.282) (total time: 188777ms):
Trace[306736497]: [3m8.777481459s] [3m8.777481459s] END
E0322 03:33:52.648999    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:33:52.661867    2280 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
time="2024-03-22T03:33:49Z" level=error msg="Error writing ping" error="write tcp 192.168.56.110:35918->192.168.56.110:6443: i/o timeout"
time="2024-03-22T03:33:52Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:51.499907808 +0000 UTC m=+331.491556041) (total time: 1.22501258s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC  : [[/registry/masterleases/192.168.56.110 false]]"
E0322 03:33:52.792958    2280 eviction_manager.go:258] "Eviction manager: failed to get summary stats" err="failed to list pod stats: failed to get pod or container map: failed to list all containers: rpc error: code = DeadlineExceeded desc = context deadline exceeded"
E0322 03:33:52.793863    2280 kubelet.go:2840] "Container runtime sanity check failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded"
E0322 03:33:53.131409    2280 watch.go:287] unable to encode watch object *v1.WatchEvent: client disconnected (&streaming.encoderWithAllocator{writer:(*framer.lengthDelimitedFrameWriter)(0xc004a2bde8), encoder:(*versioning.codec)(0xc00a0a4960), memAllocator:(*runtime.Allocator)(0xc004a2be00)})
time="2024-03-22T03:33:49Z" level=info msg="error in remotedialer server [400]: write tcp 192.168.56.110:6443->192.168.56.110:35918: i/o timeout"
time="2024-03-22T03:33:50Z" level=info msg="Slow SQL (started: 2024-03-22 03:29:39.223712793 +0000 UTC m=+79.215361045) (total time: 1m12.332669461s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC LIMIT 500 : [[% 612]]"
time="2024-03-22T03:33:51Z" level=info msg="Slow SQL (started: 2024-03-22 03:29:50.148734269 +0000 UTC m=+90.140382520) (total time: 4m1.336056973s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC LIMIT 1 : [[/registry/masterleases/192.168.56.110 true]]"
time="2024-03-22T03:33:51Z" level=error msg="Compact failed: failed to get current revision: sql: Rows are closed"
time="2024-03-22T03:33:53Z" level=error msg="Remotedialer proxy error" error="read tcp 192.168.56.110:35918->192.168.56.110:6443: i/o timeout"
I0322 03:33:54.187615    2280 trace.go:236] Trace[2049923595]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:da02a2cf-dd2f-4f60-866f-ac59b5f60530,client:127.0.0.1,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events/helm-install-traefik-fxsb6.17bef882108e61b3,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PATCH (22-Mar-2024 03:29:24.805) (total time: 267856ms):
Trace[2049923595]: ---"limitedReadBody succeeded" len:151 83ms (03:29:24.889)
Trace[2049923595]: ["GuaranteedUpdate etcd3" audit-id:da02a2cf-dd2f-4f60-866f-ac59b5f60530,key:/events/kube-system/helm-install-traefik-fxsb6.17bef882108e61b3,type:*core.Event,resource:events 266640ms (03:29:26.022)]
Trace[2049923595]: [4m27.856283605s] [4m27.856283605s] END
time="2024-03-22T03:33:54Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:51.969228556 +0000 UTC m=+331.960876808) (total time: 1.330449365s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/traefik.containo.us/middlewares/% false]]"
time="2024-03-22T03:33:54Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:52.877844075 +0000 UTC m=+332.869492315) (total time: 1.572556384s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/helm.cattle.io/helmcharts/% false]]"
time="2024-03-22T03:33:54Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:53.637620835 +0000 UTC m=+333.629269080) (total time: 1.202509008s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/namespaces/% false]]"
time="2024-03-22T03:33:55Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:53.926755583 +0000 UTC m=+333.918403820) (total time: 1.186097644s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/deployments/% false]]"
time="2024-03-22T03:33:55Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:53.917303401 +0000 UTC m=+333.908951636) (total time: 1.207984054s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/events/% false]]"
E0322 03:33:55.126322    2280 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:33:55.131250    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:33:55.140058    2280 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 03:33:55.143598    2280 trace.go:236] Trace[1185315963]: "Create" accept:application/json, */*,audit-id:456b158e-c0ac-44ab-9d34-fca5ffbe7166,client:127.0.0.1,protocol:HTTP/2.0,resource:secrets,scope:resource,url:/api/v1/namespaces/kube-system/secrets,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:POST (22-Mar-2024 03:33:51.943) (total time: 3197ms):
Trace[1185315963]: ---"Conversion done" 661ms (03:33:52.622)
Trace[1185315963]: [3.197171736s] [3.197171736s] END
E0322 03:33:55.151917    2280 timeout.go:142] post-timeout activity - time-elapsed: 7.632818935s, PATCH "/api/v1/namespaces/kube-system/events/helm-install-traefik-fxsb6.17bef882108e61b3" result: <nil>
time="2024-03-22T03:33:55Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:54.128763493 +0000 UTC m=+334.120411755) (total time: 1.198428329s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC LIMIT 500 : [[% 613]]"
time="2024-03-22T03:33:55Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:54.354990966 +0000 UTC m=+334.346639218) (total time: 1.126886479s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/statefulsets/% false]]"
time="2024-03-22T03:33:56Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:55.423342493 +0000 UTC m=+335.414990724) (total time: 1.112316858s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/helm.cattle.io/helmchartconfigs/% 610]]"
E0322 03:33:56.869996    2280 timeout.go:142] post-timeout activity - time-elapsed: 9.49889555s, POST "/api/v1/namespaces/kube-system/secrets" result: <nil>
time="2024-03-22T03:33:57Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:55.938618133 +0000 UTC m=+335.930266366) (total time: 1.908733738s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/persistentvolumes/% false]]"
time="2024-03-22T03:33:58Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:55.965255609 +0000 UTC m=+335.956903855) (total time: 2.081289386s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/flowschemas/% 612]]"
time="2024-03-22T03:33:58Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:55.972946574 +0000 UTC m=+335.964594819) (total time: 2.090384367s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/traefik.containo.us/ingressroutes/% false]]"
time="2024-03-22T03:33:58Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:55.945823795 +0000 UTC m=+335.937472040) (total time: 2.12786516s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/volumeattachments/% 612]]"
time="2024-03-22T03:33:58Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:55.973016134 +0000 UTC m=+335.964664369) (total time: 2.109819156s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/traefik.containo.us/ingressroutes/% 612]]"
time="2024-03-22T03:33:58Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:55.984665999 +0000 UTC m=+335.976314245) (total time: 2.098926411s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/traefik.containo.us/middlewaretcps/% false]]"
I0322 03:33:58.092429    2280 trace.go:236] Trace[2029664078]: "Proxy via http_connect protocol over tcp" address:10.42.0.4:10250 (22-Mar-2024 03:33:52.575) (total time: 5506ms):
Trace[2029664078]: [5.506286108s] [5.506286108s] END
time="2024-03-22T03:33:58Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:55.984708149 +0000 UTC m=+335.976356387) (total time: 2.113893462s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/traefik.io/serverstransports/% false]]"
time="2024-03-22T03:33:58Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:55.984774824 +0000 UTC m=+335.976423060) (total time: 2.123593966s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/roles/% false]]"
I0322 03:33:58.113209    2280 trace.go:236] Trace[1534593849]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:c770d21d-b008-4771-9f91-7170616f267b,client:127.0.0.1,protocol:HTTP/2.0,resource:serviceaccounts,scope:cluster,url:/api/v1/serviceaccounts,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:33:57.002) (total time: 1110ms):
Trace[1534593849]: ---"Writing http response done" count:40 1109ms (03:33:58.113)
Trace[1534593849]: [1.110863331s] [1.110863331s] END
time="2024-03-22T03:33:58Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:55.985438311 +0000 UTC m=+335.977086549) (total time: 2.13160535s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/secrets/% 612]]"
time="2024-03-22T03:33:58Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:55.986455206 +0000 UTC m=+335.978103442) (total time: 2.131246391s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/apiextensions.k8s.io/customresourcedefinitions/% 612]]"
time="2024-03-22T03:33:58Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:55.986465843 +0000 UTC m=+335.978114071) (total time: 2.132222956s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/jobs/% 612]]"
time="2024-03-22T03:33:58Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:55.986640081 +0000 UTC m=+335.978288314) (total time: 2.132948047s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/volumeattachments/% false]]"
I0322 03:33:58.120036    2280 trace.go:236] Trace[901589010]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:db4c332b-7dd6-4bdf-9ac2-161f31fcad3f,client:127.0.0.1,protocol:HTTP/2.0,resource:secrets,scope:cluster,url:/api/v1/secrets,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:33:57.008) (total time: 1111ms):
Trace[901589010]: ---"Writing http response done" count:5 1110ms (03:33:58.119)
Trace[901589010]: [1.111057329s] [1.111057329s] END
I0322 03:33:58.120172    2280 trace.go:236] Trace[1858431177]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:0c0918f2-eeda-48cf-a755-eb76810e45a5,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:cluster,url:/api/v1/nodes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:33:57.252) (total time: 867ms):
Trace[1858431177]: ---"Writing http response done" count:1 864ms (03:33:58.120)
Trace[1858431177]: [867.399512ms] [867.399512ms] END
I0322 03:33:58.120241    2280 trace.go:236] Trace[390563757]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:e9605fd6-c1c9-48de-b6ae-31b344030f09,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:namespace,url:/api/v1/namespaces/kube-system/configmaps,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:33:57.299) (total time: 820ms):
Trace[390563757]: ---"Writing http response done" count:8 817ms (03:33:58.120)
Trace[390563757]: [820.685946ms] [820.685946ms] END
time="2024-03-22T03:33:58Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:55.983372293 +0000 UTC m=+335.975020525) (total time: 2.137653401s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/csinodes/% 612]]"
time="2024-03-22T03:33:58Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:55.986839035 +0000 UTC m=+335.978487270) (total time: 2.147505306s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/traefik.containo.us/tlsstores/% 612]]"
time="2024-03-22T03:33:58Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:55.987609705 +0000 UTC m=+335.979257941) (total time: 2.147928586s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/replicasets/% 612]]"
time="2024-03-22T03:33:58Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:55.987640799 +0000 UTC m=+335.979289036) (total time: 2.148475939s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/ingressclasses/% 612]]"
time="2024-03-22T03:33:58Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:55.988215663 +0000 UTC m=+335.979863907) (total time: 2.154032675s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/traefik.containo.us/ingressroutetcps/% 612]]"
time="2024-03-22T03:33:58Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:55.988282362 +0000 UTC m=+335.979930600) (total time: 2.154698458s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/serviceaccounts/% 612]]"
time="2024-03-22T03:33:58Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:56.018657956 +0000 UTC m=+336.010306192) (total time: 2.125065039s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/apiextensions.k8s.io/customresourcedefinitions/% false]]"
time="2024-03-22T03:33:58Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:56.031874093 +0000 UTC m=+336.023522349) (total time: 2.126491825s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/rolebindings/% 612]]"
I0322 03:33:58.674900    2280 handler.go:275] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0322 03:33:58.764480    2280 trace.go:236] Trace[65059275]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:6da26d0d-f92b-47ab-a99a-abeb0d27dc86,client:127.0.0.1,protocol:HTTP/2.0,resource:roles,scope:cluster,url:/apis/rbac.authorization.k8s.io/v1/roles,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:33:57.010) (total time: 1748ms):
Trace[65059275]: ---"Writing http response done" count:7 1741ms (03:33:58.759)
Trace[65059275]: [1.748885606s] [1.748885606s] END
I0322 03:33:58.807059    2280 trace.go:236] Trace[764303274]: "Proxy via http_connect protocol over tcp" address:10.42.0.4:10250 (22-Mar-2024 03:33:52.587) (total time: 6219ms):
Trace[764303274]: [6.219241246s] [6.219241246s] END
E0322 03:33:59.128979    2280 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
I0322 03:33:59.129170    2280 trace.go:236] Trace[715765991]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:ed1fde8f-7a3d-479e-8673-dd2e8123b742,client:127.0.0.1,protocol:HTTP/2.0,resource:priorityclasses,scope:cluster,url:/apis/scheduling.k8s.io/v1/priorityclasses,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:33:57.378) (total time: 1746ms):
Trace[715765991]: ["cacher list" audit-id:ed1fde8f-7a3d-479e-8673-dd2e8123b742,type:priorityclasses.scheduling.k8s.io 1744ms (03:33:57.379)
Trace[715765991]:  ---"watchCache locked acquired" 1741ms (03:33:59.121)]
Trace[715765991]: [1.746431414s] [1.746431414s] END
E0322 03:33:59.130957    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:33:59.131782    2280 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 03:33:59.132706    2280 trace.go:236] Trace[131648729]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:4fb83d6b-8e8d-4852-9241-83a1c6354c8e,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/server,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:33:52.822) (total time: 6310ms):
Trace[131648729]: ---"watchCache locked acquired" 1579ms (03:33:54.404)
Trace[131648729]: ---"Writing http response done" 4725ms (03:33:59.132)
Trace[131648729]: [6.310575349s] [6.310575349s] END
I0322 03:33:59.464256    2280 trace.go:236] Trace[804980972]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:66e66dd0-e527-41d4-91d4-638c7b4c6acc,client:127.0.0.1,protocol:HTTP/2.0,resource:flowschemas,scope:cluster,url:/apis/flowcontrol.apiserver.k8s.io/v1beta3/flowschemas,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:33:56.989) (total time: 2471ms):
Trace[804980972]: ---"Writing http response done" count:13 2467ms (03:33:59.460)
Trace[804980972]: [2.471057329s] [2.471057329s] END
I0322 03:33:59.492937    2280 trace.go:236] Trace[1642389114]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:04a38d24-9f3f-4233-8b6a-55c53f28ef4a,client:127.0.0.1,protocol:HTTP/2.0,resource:prioritylevelconfigurations,scope:cluster,url:/apis/flowcontrol.apiserver.k8s.io/v1beta3/prioritylevelconfigurations,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:33:57.186) (total time: 2306ms):
Trace[1642389114]: ---"Writing http response done" count:8 2296ms (03:33:59.492)
Trace[1642389114]: [2.306494734s] [2.306494734s] END
I0322 03:33:59.493185    2280 trace.go:236] Trace[1692228378]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:c5fa0c4e-83a8-478d-88ce-f79944c6ec95,client:127.0.0.1,protocol:HTTP/2.0,resource:clusterroles,scope:cluster,url:/apis/rbac.authorization.k8s.io/v1/clusterroles,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:33:57.231) (total time: 2261ms):
Trace[1692228378]: ---"Writing http response done" count:69 2251ms (03:33:59.493)
Trace[1692228378]: [2.261735408s] [2.261735408s] END
I0322 03:33:59.493375    2280 trace.go:236] Trace[1499009173]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:0ffbc186-40ba-46ba-a77a-19ba5f7f8d28,client:127.0.0.1,protocol:HTTP/2.0,resource:services,scope:cluster,url:/api/v1/services,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:33:57.319) (total time: 2173ms):
Trace[1499009173]: ---"Writing http response done" count:3 2171ms (03:33:59.493)
Trace[1499009173]: [2.173408108s] [2.173408108s] END
I0322 03:33:59.494185    2280 trace.go:236] Trace[1222321548]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:e4a25cbc-0d59-4e99-bd8c-0521bbeb864f,client:127.0.0.1,protocol:HTTP/2.0,resource:clusterrolebindings,scope:cluster,url:/apis/rbac.authorization.k8s.io/v1/clusterrolebindings,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:33:57.391) (total time: 2101ms):
Trace[1222321548]: ["cacher list" audit-id:e4a25cbc-0d59-4e99-bd8c-0521bbeb864f,type:clusterrolebindings.rbac.authorization.k8s.io 2098ms (03:33:57.394)
Trace[1222321548]:  ---"watchCache locked acquired" 1758ms (03:33:59.153)]
Trace[1222321548]: ---"Writing http response done" count:54 339ms (03:33:59.493)
Trace[1222321548]: [2.101931769s] [2.101931769s] END
I0322 03:33:59.530439    2280 trace.go:236] Trace[2118247030]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:cbb0e94b-4e99-481a-b25a-7c478db5a303,client:127.0.0.1,protocol:HTTP/2.0,resource:storageclasses,scope:cluster,url:/apis/storage.k8s.io/v1/storageclasses,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:33:57.781) (total time: 1748ms):
Trace[2118247030]: ---"About to List from storage" 1742ms (03:33:59.524)
Trace[2118247030]: [1.748746133s] [1.748746133s] END
I0322 03:33:59.541840    2280 trace.go:236] Trace[1498577910]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:ff67191e-2e73-45e5-aa78-e92f074c4672,client:127.0.0.1,protocol:HTTP/2.0,resource:validatingwebhookconfigurations,scope:cluster,url:/apis/admissionregistration.k8s.io/v1/validatingwebhookconfigurations,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:33:57.785) (total time: 1755ms):
Trace[1498577910]: ---"Writing http response done" count:0 1739ms (03:33:59.541)
Trace[1498577910]: [1.755827207s] [1.755827207s] END
I0322 03:33:59.564592    2280 trace.go:236] Trace[186905217]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:4e8bdcbf-e7c1-452f-9388-ccf942ed3c3c,client:127.0.0.1,protocol:HTTP/2.0,resource:resourcequotas,scope:cluster,url:/api/v1/resourcequotas,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:33:57.849) (total time: 1715ms):
Trace[186905217]: ---"Writing http response done" count:0 1712ms (03:33:59.564)
Trace[186905217]: [1.715381374s] [1.715381374s] END
time="2024-03-22T03:33:58Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:56.031976301 +0000 UTC m=+336.023624542) (total time: 2.140559554s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/traefik.containo.us/ingressrouteudps/% false]]"
I0322 03:33:59.800866    2280 trace.go:236] Trace[814340197]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:7d080796-f709-4476-b635-02d3e7e92438,client:127.0.0.1,protocol:HTTP/2.0,resource:runtimeclasses,scope:cluster,url:/apis/node.k8s.io/v1/runtimeclasses,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:33:56.955) (total time: 2841ms):
Trace[814340197]: ---"About to List from storage" 1682ms (03:33:58.637)
Trace[814340197]: [2.841867628s] [2.841867628s] END
I0322 03:33:59.823146    2280 trace.go:236] Trace[1382587123]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:ccc0cfe6-5c20-4d98-aaac-d4978e9ccf6f,client:127.0.0.1,protocol:HTTP/2.0,resource:ingressclasses,scope:cluster,url:/apis/networking.k8s.io/v1/ingressclasses,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:33:56.965) (total time: 2857ms):
Trace[1382587123]: ---"Writing http response done" count:0 2850ms (03:33:59.823)
Trace[1382587123]: [2.857542775s] [2.857542775s] END
I0322 03:33:59.857503    2280 trace.go:236] Trace[167824309]: "Proxy via http_connect protocol over tcp" address:10.42.0.4:10250 (22-Mar-2024 03:33:52.588) (total time: 6149ms):
Trace[167824309]: [6.149932314s] [6.149932314s] END
I0322 03:33:59.903577    2280 trace.go:236] Trace[40257984]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:7366f5f5-c559-4326-b4b9-145a5a382493,client:127.0.0.1,protocol:HTTP/2.0,resource:namespaces,scope:cluster,url:/api/v1/namespaces,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:33:58.794) (total time: 1106ms):
Trace[40257984]: ["cacher list" audit-id:7366f5f5-c559-4326-b4b9-145a5a382493,type:namespaces 1103ms (03:33:58.798)
Trace[40257984]:  ---"watchCache locked acquired" 1090ms (03:33:59.888)]
Trace[40257984]: [1.106233656s] [1.106233656s] END
I0322 03:34:00.054851    2280 trace.go:236] Trace[109305404]: "List" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:bec73109-2f74-4c86-9f7b-f6cbf658d13f,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:33:56.538) (total time: 3510ms):
Trace[109305404]: ---"Writing http response done" count:1 3503ms (03:34:00.048)
Trace[109305404]: [3.510413472s] [3.510413472s] END
I0322 03:34:00.094006    2280 trace.go:236] Trace[2138224551]: "Proxy via http_connect protocol over tcp" address:10.42.0.4:10250 (22-Mar-2024 03:33:51.362) (total time: 8731ms):
Trace[2138224551]: [8.731731556s] [8.731731556s] END
I0322 03:34:00.137553    2280 trace.go:236] Trace[96569801]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:47.332) (total time: 12804ms):
Trace[96569801]: ---"Objects listed" error:<nil> 12801ms (03:34:00.134)
Trace[96569801]: [12.804206564s] [12.804206564s] END
I0322 03:34:00.161603    2280 trace.go:236] Trace[693463753]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:47.650) (total time: 12510ms):
Trace[693463753]: ---"Objects listed" error:<nil> 12510ms (03:34:00.161)
Trace[693463753]: [12.510799772s] [12.510799772s] END
E0322 03:34:00.198072    2280 timeout.go:142] post-timeout activity - time-elapsed: 10.523996955s, GET "/api/v1/nodes/server" result: <nil>
I0322 03:34:00.199955    2280 trace.go:236] Trace[1920323452]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:48.142) (total time: 12057ms):
Trace[1920323452]: ---"Objects listed" error:<nil> 12055ms (03:34:00.197)
Trace[1920323452]: [12.057677638s] [12.057677638s] END
I0322 03:34:00.214283    2280 trace.go:236] Trace[1536686930]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:47.948) (total time: 12253ms):
Trace[1536686930]: ---"Objects listed" error:<nil> 12253ms (03:34:00.201)
Trace[1536686930]: [12.253903322s] [12.253903322s] END
I0322 03:34:00.222859    2280 trace.go:236] Trace[753149684]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:48.088) (total time: 12134ms):
Trace[753149684]: ---"Objects listed" error:<nil> 12133ms (03:34:00.221)
Trace[753149684]: [12.134220909s] [12.134220909s] END
I0322 03:34:00.251923    2280 trace.go:236] Trace[1049148020]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:47.826) (total time: 12424ms):
Trace[1049148020]: ---"Objects listed" error:<nil> 12424ms (03:34:00.250)
Trace[1049148020]: [12.424897222s] [12.424897222s] END
I0322 03:34:00.261452    2280 trace.go:236] Trace[2311699]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:47.357) (total time: 12903ms):
Trace[2311699]: ---"Objects listed" error:<nil> 12903ms (03:34:00.261)
Trace[2311699]: [12.903403653s] [12.903403653s] END
time="2024-03-22T03:33:58Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:55.2416566 +0000 UTC m=+335.233304833) (total time: 1.01611786s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/minions/% 612]]"
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:56.03206225 +0000 UTC m=+336.023710484) (total time: 4.304867414s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/apiregistration.k8s.io/apiservices/% 612]]"
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:56.033229308 +0000 UTC m=+336.024877559) (total time: 4.314090465s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/k3s.cattle.io/etcdsnapshotfiles/% false]]"
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:56.03334382 +0000 UTC m=+336.024992047) (total time: 4.335085071s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/runtimeclasses/% 612]]"
I0322 03:34:00.814050    2280 trace.go:236] Trace[1438052744]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:48.135) (total time: 12668ms):
Trace[1438052744]: ---"Objects listed" error:<nil> 12667ms (03:34:00.802)
Trace[1438052744]: [12.668597567s] [12.668597567s] END
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:56.087462467 +0000 UTC m=+336.079110700) (total time: 4.291848472s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/csistoragecapacities/% 612]]"
I0322 03:34:05.830395    2280 trace.go:236] Trace[32988008]: "DeltaFIFO Pop Process" ID:cluster-admin,Depth:67,Reason:slow event handlers blocking the queue (22-Mar-2024 03:34:01.177) (total time: 361ms):
Trace[32988008]: [361.522376ms] [361.522376ms] END
I0322 03:34:00.828834    2280 trace.go:236] Trace[1272887173]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:47.913) (total time: 12909ms):
Trace[1272887173]: ---"Objects listed" error:<nil> 12905ms (03:34:00.819)
Trace[1272887173]: [12.909542397s] [12.909542397s] END
I0322 03:34:00.835192    2280 trace.go:236] Trace[859564878]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:47.550) (total time: 13284ms):
Trace[859564878]: ---"Objects listed" error:<nil> 13284ms (03:34:00.835)
Trace[859564878]: [13.284707724s] [13.284707724s] END
I0322 03:34:01.164127    2280 trace.go:236] Trace[249888836]: "DeltaFIFO Pop Process" ID:endpoint-controller,Depth:11,Reason:slow event handlers blocking the queue (22-Mar-2024 03:34:01.043) (total time: 114ms):
Trace[249888836]: [114.196723ms] [114.196723ms] END
I0322 03:34:01.175561    2280 trace.go:236] Trace[1632753887]: "DeltaFIFO Pop Process" ID:kube-system/auth-delegator,Depth:11,Reason:slow event handlers blocking the queue (22-Mar-2024 03:34:01.044) (total time: 130ms):
Trace[1632753887]: [130.800114ms] [130.800114ms] END
I0322 03:34:01.212492    2280 trace.go:236] Trace[1040880600]: "DeltaFIFO Pop Process" ID:kube-node-lease/default,Depth:38,Reason:slow event handlers blocking the queue (22-Mar-2024 03:34:00.995) (total time: 213ms):
Trace[1040880600]: [213.311919ms] [213.311919ms] END
I0322 03:34:01.258977    2280 trace.go:236] Trace[809384691]: "List" accept:application/json, */*,audit-id:e4650131-7fec-41b5-b1dd-60b0d15b159d,client:127.0.0.1,protocol:HTTP/2.0,resource:jobs,scope:cluster,url:/apis/batch/v1/jobs,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:LIST (22-Mar-2024 03:33:59.693) (total time: 1563ms):
Trace[809384691]: ---"Writing http response done" count:2 1561ms (03:34:01.257)
Trace[809384691]: [1.563346583s] [1.563346583s] END
I0322 03:34:01.259103    2280 trace.go:236] Trace[1046116173]: "List" accept:application/json, */*,audit-id:ee3ee788-6946-43c8-bca4-1a793bd1580f,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:cluster,url:/api/v1/nodes,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:LIST (22-Mar-2024 03:33:59.711) (total time: 1547ms):
Trace[1046116173]: ---"Writing http response done" count:1 1547ms (03:34:01.259)
Trace[1046116173]: [1.547230185s] [1.547230185s] END
I0322 03:34:01.285814    2280 trace.go:236] Trace[94847786]: "List" accept:application/json, */*,audit-id:b959d40f-a4ae-4f65-81bb-fb8e42a5d898,client:127.0.0.1,protocol:HTTP/2.0,resource:serviceaccounts,scope:cluster,url:/api/v1/serviceaccounts,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:LIST (22-Mar-2024 03:33:59.852) (total time: 1431ms):
Trace[94847786]: ---"Writing http response done" count:40 1428ms (03:34:01.284)
Trace[94847786]: [1.43188339s] [1.43188339s] END
I0322 03:34:01.288478    2280 trace.go:236] Trace[387400329]: "List" accept:application/json, */*,audit-id:47b905f8-82d3-4859-9ab8-34a814457c62,client:127.0.0.1,protocol:HTTP/2.0,resource:clusterrolebindings,scope:cluster,url:/apis/rbac.authorization.k8s.io/v1/clusterrolebindings,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:LIST (22-Mar-2024 03:33:59.755) (total time: 1533ms):
Trace[387400329]: ---"Writing http response done" count:54 1529ms (03:34:01.288)
Trace[387400329]: [1.533405735s] [1.533405735s] END
I0322 03:34:01.290263    2280 trace.go:236] Trace[1306656759]: "List" accept:application/json, */*,audit-id:e492fc34-6779-4a60-ad23-9463833f3e05,client:127.0.0.1,protocol:HTTP/2.0,resource:addons,scope:cluster,url:/apis/k3s.cattle.io/v1/addons,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:LIST (22-Mar-2024 03:33:59.834) (total time: 1455ms):
Trace[1306656759]: ---"Writing http response done" count:13 1455ms (03:34:01.290)
Trace[1306656759]: [1.455616306s] [1.455616306s] END
I0322 03:34:02.149986    2280 trace.go:236] Trace[852281439]: "Proxy via http_connect protocol over tcp" address:10.42.0.4:10250 (22-Mar-2024 03:33:52.587) (total time: 9556ms):
Trace[852281439]: [9.556456056s] [9.556456056s] END
I0322 03:34:02.444040    2280 trace.go:236] Trace[825415369]: "iptables ChainExists" (22-Mar-2024 03:33:58.386) (total time: 4051ms):
Trace[825415369]: [4.051560645s] [4.051560645s] END
I0322 03:34:04.428608    2280 trace.go:236] Trace[1375688543]: "List" accept:application/json, */*,audit-id:3cbc219d-ba04-4270-b081-f5f5cdc629e9,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:cluster,url:/api/v1/configmaps,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:LIST (22-Mar-2024 03:33:59.677) (total time: 1569ms):
Trace[1375688543]: ---"Writing http response done" count:11 1568ms (03:34:01.246)
Trace[1375688543]: [1.569088395s] [1.569088395s] END
I0322 03:34:04.932204    2280 trace.go:236] Trace[728017651]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.406) (total time: 11512ms):
Trace[728017651]: ---"Objects listed" error:<nil> 11506ms (03:34:04.912)
Trace[728017651]: [11.512345724s] [11.512345724s] END
I0322 03:34:05.395217    2280 trace.go:236] Trace[708017022]: "List" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:b778c553-d9db-48dc-be86-cc01ebb2f295,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:33:59.083) (total time: 1996ms):
Trace[708017022]: ---"Writing http response done" count:1 1993ms (03:34:01.080)
Trace[708017022]: [1.996982134s] [1.996982134s] END
I0322 03:34:05.464934    2280 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:34:07.157704    2280 trace.go:236] Trace[1087859421]: "Proxy via http_connect protocol over tcp" address:10.42.0.4:10250 (22-Mar-2024 03:33:51.910) (total time: 10730ms):
Trace[1087859421]: [10.730500371s] [10.730500371s] END
I0322 03:34:07.851438    2280 trace.go:236] Trace[1097028036]: "Proxy via http_connect protocol over tcp" address:10.42.0.4:10250 (22-Mar-2024 03:33:52.584) (total time: 15258ms):
Trace[1097028036]: [15.258566515s] [15.258566515s] END
W0322 03:34:03.388793    2280 transport.go:301] Unable to cancel request for *otelhttp.Transport
W0322 03:34:08.893847    2280 transport.go:301] Unable to cancel request for *otelhttp.Transport
E0322 03:34:09.120899    2280 kubelet_node_status.go:540] "Error updating node status, will retry" err="error getting node \"server\": Get \"https://127.0.0.1:6443/api/v1/nodes/server?timeout=10s\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)"
I0322 03:34:10.445712    2280 trace.go:236] Trace[1543152881]: "SerializeObject" audit-id:ac4bb7b1-a37e-4b9c-bd89-88659baa75f2,method:GET,url:/apis/apiregistration.k8s.io/v1/apiservices,protocol:HTTP/2.0,mediaType:application/vnd.kubernetes.protobuf,encoder:{"encodeGV":"apiregistration.k8s.io/v1","encoder":"protobuf","name":"versioning"} (22-Mar-2024 03:33:57.818) (total time: 7661ms):
Trace[1543152881]: ---"About to start writing response" size:14287 1744ms (03:33:59.563)
Trace[1543152881]: ---"Write call succeeded" writer:responsewriter.outerWithCloseNotifyAndFlush,size:14287,firstWrite:true 5917ms (03:34:05.480)
Trace[1543152881]: [7.66197499s] [7.66197499s] END
I0322 03:34:10.519461    2280 trace.go:236] Trace[549730267]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.637) (total time: 16866ms):
Trace[549730267]: ---"Objects listed" error:<nil> 6966ms (03:34:00.604)
Trace[549730267]: ---"Objects extracted" 3925ms (03:34:04.530)
Trace[549730267]: [16.866913164s] [16.866913164s] END
I0322 03:34:10.859637    2280 trace.go:236] Trace[2115636721]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.383) (total time: 11294ms):
Trace[2115636721]: ---"Objects listed" error:<nil> 11292ms (03:34:04.675)
Trace[2115636721]: [11.294571303s] [11.294571303s] END
I0322 03:34:11.439559    2280 trace.go:236] Trace[264871255]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.525) (total time: 11562ms):
Trace[264871255]: ---"Objects listed" error:<nil> 11545ms (03:34:05.071)
Trace[264871255]: [11.562917318s] [11.562917318s] END
time="2024-03-22T03:33:59Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:57.520711144 +0000 UTC m=+337.512359412) (total time: 1.649547254s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/persistentvolumeclaims/% 612]]"
time="2024-03-22T03:34:10Z" level=info msg="Slow SQL (started: 2024-03-22 03:34:01.03981526 +0000 UTC m=+341.031463494) (total time: 4.209118414s):  SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key' : [[]]"
I0322 03:34:12.502125    2280 trace.go:236] Trace[1290209928]: "DeltaFIFO Pop Process" ID:kube-system/attachdetach-controller,Depth:36,Reason:slow event handlers blocking the queue (22-Mar-2024 03:34:06.311) (total time: 126ms):
Trace[1290209928]: [126.911946ms] [126.911946ms] END
E0322 03:34:12.996419    2280 health_controller.go:162] Metrics Controller heartbeat missed
time="2024-03-22T03:34:13Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:59.974489897 +0000 UTC m=+339.966138128) (total time: 7.162544582s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/traefik.io/serverstransporttcps/% 612]]"
time="2024-03-22T03:33:59Z" level=info msg="Connecting to proxy" url="wss://192.168.56.110:6443/v1-k3s/connect"
time="2024-03-22T03:33:59Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:57.813462005 +0000 UTC m=+337.805110242) (total time: 1.741211844s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/resourcequotas/% 612]]"
time="2024-03-22T03:33:59Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:57.900433423 +0000 UTC m=+337.892081659) (total time: 1.686693514s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/mutatingwebhookconfigurations/% 612]]"
time="2024-03-22T03:33:59Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:57.946109086 +0000 UTC m=+337.937757342) (total time: 1.648734866s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/csidrivers/% 610]]"
time="2024-03-22T03:33:59Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:55.946311996 +0000 UTC m=+335.937960234) (total time: 2.065064512s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/traefik.io/middlewaretcps/% 612]]"
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:56.087566599 +0000 UTC m=+336.079214839) (total time: 4.292625411s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/horizontalpodautoscalers/% false]]"
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:56.08803616 +0000 UTC m=+336.079684399) (total time: 4.292588598s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/clusterrolebindings/% false]]"
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:56.115474779 +0000 UTC m=+336.107123012) (total time: 4.26554033s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/priorityclasses/% false]]"
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:55.73580356 +0000 UTC m=+335.727451805) (total time: 4.646198732s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/csinodes/% false]]"
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:56.165256832 +0000 UTC m=+336.156905067) (total time: 4.216967287s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/traefik.containo.us/serverstransports/% false]]"
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:55.818991698 +0000 UTC m=+335.810639934) (total time: 4.563334823s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/traefik.containo.us/middlewaretcps/% 612]]"
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:56.289420249 +0000 UTC m=+336.281068480) (total time: 4.093169861s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/traefik.io/middlewaretcps/% false]]"
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:55.272857871 +0000 UTC m=+335.264506116) (total time: 5.109883966s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/traefik.containo.us/tlsstores/% false]]"
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:56.321519625 +0000 UTC m=+336.313167858) (total time: 4.061370683s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/traefik.containo.us/tlsoptions/% false]]"
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:56.369153976 +0000 UTC m=+336.360802207) (total time: 4.014699949s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/clusterroles/% false]]"
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:56.406447321 +0000 UTC m=+336.398095554) (total time: 3.977768815s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/podtemplates/% 612]]"
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:56.415409208 +0000 UTC m=+336.407057463) (total time: 3.972560102s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/minions/% false]]"
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:56.787560481 +0000 UTC m=+336.779208711) (total time: 3.603806871s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/traefik.containo.us/traefikservices/% 612]]"
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:56.788123434 +0000 UTC m=+336.779771677) (total time: 3.604143358s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/leases/% false]]"
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:56.796030163 +0000 UTC m=+336.787678398) (total time: 3.602775721s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/traefik.io/middlewares/% false]]"
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:56.796065627 +0000 UTC m=+336.787713863) (total time: 3.604572183s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/traefik.io/middlewares/% 612]]"
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:56.796439742 +0000 UTC m=+336.788087977) (total time: 3.604734459s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/rolebindings/% false]]"
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:56.79651875 +0000 UTC m=+336.788166986) (total time: 3.605403016s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/traefik.containo.us/ingressrouteudps/% 612]]"
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:56.796610573 +0000 UTC m=+336.788258806) (total time: 3.606360183s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/k3s.cattle.io/etcdsnapshotfiles/% 612]]"
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:56.796629423 +0000 UTC m=+336.788277659) (total time: 3.606791987s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/runtimeclasses/% false]]"
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:56.796663152 +0000 UTC m=+336.788311387) (total time: 3.60849607s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/services/specs/% false]]"
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:56.811672586 +0000 UTC m=+336.803320815) (total time: 3.594450585s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/clusterrolebindings/% 612]]"
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:56.820815266 +0000 UTC m=+336.812463502) (total time: 3.586029214s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/mutatingwebhookconfigurations/% false]]"
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:56.834717144 +0000 UTC m=+336.826365380) (total time: 3.573798384s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/horizontalpodautoscalers/% 612]]"
E0322 03:34:14.819135    2280 server.go:310] "Unable to authenticate the request due to an error" err="context canceled"
I0322 03:34:15.265153    2280 trace.go:236] Trace[707787155]: "Reflector ListAndWatch" name:object-"kube-system"/"local-path-config" (22-Mar-2024 03:33:53.431) (total time: 15721ms):
Trace[707787155]: ---"Objects listed" error:<nil> 15716ms (03:34:09.148)
Trace[707787155]: [15.721733944s] [15.721733944s] END
I0322 03:34:15.533753    2280 trace.go:236] Trace[23319611]: "Reflector ListAndWatch" name:object-"kube-system"/"coredns-custom" (22-Mar-2024 03:33:53.529) (total time: 15724ms):
Trace[23319611]: ---"Objects listed" error:<nil> 15720ms (03:34:09.250)
Trace[23319611]: [15.724184799s] [15.724184799s] END
I0322 03:34:17.258552    2280 trace.go:236] Trace[1088645597]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:ac4bb7b1-a37e-4b9c-bd89-88659baa75f2,client:127.0.0.1,protocol:HTTP/2.0,resource:apiservices,scope:cluster,url:/apis/apiregistration.k8s.io/v1/apiservices,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:33:57.813) (total time: 19430ms):
Trace[1088645597]: ---"Writing http response done" count:26 19427ms (03:34:17.243)
Trace[1088645597]: [19.430839649s] [19.430839649s] END
E0322 03:34:18.505489    2280 health_controller.go:162] Metrics Controller heartbeat missed
I0322 03:34:18.902610    2280 trace.go:236] Trace[780565244]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.894) (total time: 16992ms):
Trace[780565244]: ---"Objects listed" error:<nil> 10778ms (03:34:04.673)
Trace[780565244]: ---"Objects extracted" 6201ms (03:34:10.885)
Trace[780565244]: [16.992583566s] [16.992583566s] END
I0322 03:34:19.528333    2280 trace.go:236] Trace[1787660286]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.661) (total time: 12239ms):
Trace[1787660286]: ---"Objects listed" error:<nil> 12233ms (03:34:05.895)
Trace[1787660286]: [12.239649413s] [12.239649413s] END
I0322 03:34:20.460623    2280 trace.go:236] Trace[1307525284]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.622) (total time: 26830ms):
Trace[1307525284]: ---"Objects listed" error:<nil> 18697ms (03:34:12.319)
Trace[1307525284]: ---"SyncWith done" 8113ms (03:34:20.452)
Trace[1307525284]: [26.830793306s] [26.830793306s] END
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:56.834779682 +0000 UTC m=+336.826427917) (total time: 3.574395021s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/services/endpoints/% false]]"
I0322 03:34:22.105227    2280 trace.go:236] Trace[388531840]: "DeltaFIFO Pop Process" ID:edit,Depth:65,Reason:slow event handlers blocking the queue (22-Mar-2024 03:34:05.839) (total time: 3133ms):
Trace[388531840]: [3.1338545s] [3.1338545s] END
I0322 03:34:15.161897    2280 trace.go:236] Trace[1802257185]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.406) (total time: 15681ms):
Trace[1802257185]: ---"Objects listed" error:<nil> 15675ms (03:34:09.082)
Trace[1802257185]: [15.681320229s] [15.681320229s] END
I0322 03:34:23.619739    2280 trace.go:236] Trace[1013817924]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.238) (total time: 21586ms):
Trace[1013817924]: ---"Objects listed" error:<nil> 21581ms (03:34:14.820)
Trace[1013817924]: [21.586077363s] [21.586077363s] END
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:56.861180117 +0000 UTC m=+336.852828352) (total time: 3.548477626s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/traefik.io/serverstransporttcps/% false]]"
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:56.862501509 +0000 UTC m=+336.854149746) (total time: 3.547667809s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/daemonsets/% 612]]"
time="2024-03-22T03:34:24Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:56.885256547 +0000 UTC m=+336.876904794) (total time: 7.346420533s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/horizontalpodautoscalers/% false]]"
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:56.872590627 +0000 UTC m=+336.864238870) (total time: 3.54783896s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/persistentvolumeclaims/% false]]"
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:56.872640852 +0000 UTC m=+336.864289088) (total time: 3.548322515s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/controllerrevisions/% false]]"
I0322 03:34:24.931216    2280 trace.go:236] Trace[46031383]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.425) (total time: 22915ms):
Trace[46031383]: ---"Objects listed" error:<nil> 22903ms (03:34:16.328)
Trace[46031383]: [22.915797716s] [22.915797716s] END
I0322 03:34:25.676216    2280 trace.go:236] Trace[892467457]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:47.408) (total time: 22547ms):
Trace[892467457]: ---"Objects listed" error:<nil> 22538ms (03:34:09.946)
Trace[892467457]: [22.547162836s] [22.547162836s] END
I0322 03:34:26.704742    2280 trace.go:236] Trace[664137412]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:48.140) (total time: 23027ms):
Trace[664137412]: ---"Objects listed" error:<nil> 12625ms (03:34:00.766)
Trace[664137412]: ---"SyncWith done" 10397ms (03:34:11.168)
Trace[664137412]: [23.02787195s] [23.02787195s] END
I0322 03:34:28.382846    2280 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:34:29.894085    2280 trace.go:236] Trace[1850177720]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:48.129) (total time: 34785ms):
Trace[1850177720]: ---"Objects listed" error:<nil> 26861ms (03:34:14.990)
Trace[1850177720]: ---"SyncWith done" 7919ms (03:34:22.913)
Trace[1850177720]: [34.785970784s] [34.785970784s] END
E0322 03:34:30.339532    2280 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
I0322 03:34:30.775742    2280 trace.go:236] Trace[314088457]: "DeltaFIFO Pop Process" ID:local-path-provisioner-role,Depth:63,Reason:slow event handlers blocking the queue (22-Mar-2024 03:34:22.109) (total time: 2257ms):
Trace[314088457]: [2.257608303s] [2.257608303s] END
I0322 03:34:32.469196    2280 trace.go:236] Trace[260526148]: "Reflector ListAndWatch" name:object-"kube-system"/"chart-content-traefik" (22-Mar-2024 03:33:53.334) (total time: 30865ms):
Trace[260526148]: ---"Objects listed" error:<nil> 22644ms (03:34:15.978)
Trace[260526148]: ---"Resource version extracted" 8210ms (03:34:24.188)
Trace[260526148]: [30.865218066s] [30.865218066s] END
I0322 03:34:33.577768    2280 trace.go:236] Trace[710150444]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.436) (total time: 31728ms):
Trace[710150444]: ---"Objects listed" error:<nil> 11784ms (03:34:05.220)
Trace[710150444]: ---"Objects extracted" 4617ms (03:34:09.846)
Trace[710150444]: ---"SyncWith done" 15317ms (03:34:25.164)
Trace[710150444]: [31.728792611s] [31.728792611s] END
I0322 03:34:34.714973    2280 trace.go:236] Trace[535080015]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.860) (total time: 32810ms):
Trace[535080015]: ---"Objects listed" error:<nil> 24543ms (03:34:18.404)
Trace[535080015]: ---"SyncWith done" 8255ms (03:34:26.669)
Trace[535080015]: [32.810462999s] [32.810462999s] END
E0322 03:34:34.900850    2280 health_controller.go:162] Metrics Controller heartbeat missed
I0322 03:34:40.837503    2280 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="21.938516251s"
I0322 03:34:41.977303    2280 trace.go:236] Trace[1100622001]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.770) (total time: 37766ms):
Trace[1100622001]: ---"Objects listed" error:<nil> 37756ms (03:34:31.527)
Trace[1100622001]: [37.76668999s] [37.76668999s] END
I0322 03:34:45.947699    2280 trace.go:236] Trace[1910514348]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.391) (total time: 52547ms):
Trace[1910514348]: ---"Objects listed" error:<nil> 48620ms (03:34:42.012)
Trace[1910514348]: ---"Objects extracted" 3925ms (03:34:45.937)
Trace[1910514348]: [52.547916747s] [52.547916747s] END
I0322 03:34:45.985912    2280 trace.go:236] Trace[939505080]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.841) (total time: 37772ms):
Trace[939505080]: ---"Objects listed" error:<nil> 29841ms (03:34:23.683)
Trace[939505080]: [37.772568501s] [37.772568501s] END
E0322 03:34:45.988017    2280 controller.go:193] "Failed to update lease" err="Put \"https://127.0.0.1:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/server?timeout=10s\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)"
W0322 03:34:46.033977    2280 helpers.go:242] readString: Failed to read "/sys/fs/cgroup/memory/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-podf5749ede_2796_414e_b0ea_d6e2523ec959.slice/cri-containerd-afe4640d3a121c51da6bad0227debc7256b8fec3cc9277d94249ed4d0a986231.scope/memory.soft_limit_in_bytes": read /sys/fs/cgroup/memory/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-podf5749ede_2796_414e_b0ea_d6e2523ec959.slice/cri-containerd-afe4640d3a121c51da6bad0227debc7256b8fec3cc9277d94249ed4d0a986231.scope/memory.soft_limit_in_bytes: no such device
E0322 03:34:46.036196    2280 webhook.go:154] Failed to make webhook authenticator request: Post "https://127.0.0.1:6443/apis/authentication.k8s.io/v1/tokenreviews": context deadline exceeded
I0322 03:34:46.097466    2280 trace.go:236] Trace[1868240604]: "DeltaFIFO Pop Process" ID:system:aggregate-to-admin,Depth:62,Reason:slow event handlers blocking the queue (22-Mar-2024 03:34:30.783) (total time: 11460ms):
Trace[1868240604]: [11.460723709s] [11.460723709s] END
I0322 03:34:46.189866    2280 trace.go:236] Trace[1665089311]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.387) (total time: 48949ms):
Trace[1665089311]: ---"Objects listed" error:<nil> 48946ms (03:34:42.333)
Trace[1665089311]: [48.949594642s] [48.949594642s] END
I0322 03:34:46.220027    2280 trace.go:236] Trace[1259712856]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.723) (total time: 52490ms):
Trace[1259712856]: ---"Objects listed" error:<nil> 36506ms (03:34:30.229)
Trace[1259712856]: ---"Objects extracted" 11079ms (03:34:41.309)
Trace[1259712856]: ---"SyncWith done" 4904ms (03:34:46.213)
Trace[1259712856]: [52.490672432s] [52.490672432s] END
I0322 03:34:46.378881    2280 trace.go:236] Trace[461481951]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.868) (total time: 52496ms):
Trace[461481951]: ---"Objects listed" error:<nil> 52495ms (03:34:46.364)
Trace[461481951]: [52.496370185s] [52.496370185s] END
I0322 03:34:46.451816    2280 trace.go:236] Trace[2120688443]: "iptables ChainExists" (22-Mar-2024 03:34:24.054) (total time: 22392ms):
Trace[2120688443]: [22.392938753s] [22.392938753s] END
I0322 03:34:46.505646    2280 trace.go:236] Trace[2081747333]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:48.105) (total time: 58399ms):
Trace[2081747333]: ---"Objects listed" error:<nil> 54659ms (03:34:42.765)
Trace[2081747333]: ---"SyncWith done" 3735ms (03:34:46.505)
Trace[2081747333]: [58.399681599s] [58.399681599s] END
I0322 03:34:46.692660    2280 trace.go:236] Trace[473269449]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.363) (total time: 53323ms):
Trace[473269449]: ---"Objects listed" error:<nil> 53322ms (03:34:46.685)
Trace[473269449]: [53.323824601s] [53.323824601s] END
E0322 03:34:46.716379    2280 kubelet_node_status.go:540] "Error updating node status, will retry" err="error getting node \"server\": Get \"https://127.0.0.1:6443/api/v1/nodes/server?timeout=10s\": net/http: request canceled (Client.Timeout exceeded while awaiting headers)"
I0322 03:34:46.784041    2280 trace.go:236] Trace[1789987994]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.981) (total time: 38645ms):
Trace[1789987994]: ---"Objects listed" error:<nil> 22278ms (03:34:16.259)
Trace[1789987994]: ---"SyncWith done" 8456ms (03:34:24.719)
Trace[1789987994]: [38.645402752s] [38.645402752s] END
E0322 03:34:46.816625    2280 finisher.go:175] FinishRequest: post-timeout activity - time-elapsed: 50.430234597s, panicked: false, err: Internal error occurred: resource quota evaluation timed out, panic-reason: <nil>
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:56.872702198 +0000 UTC m=+336.864350433) (total time: 3.548785144s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/prioritylevelconfigurations/% false]]"
I0322 03:34:31.862077    2280 trace.go:236] Trace[651034481]: "SerializeObject" audit-id:3ffd83e1-a5e4-4946-9a2f-00e020d512b4,method:GET,url:/api/v1/pods,protocol:HTTP/2.0,mediaType:application/vnd.kubernetes.protobuf,encoder:{"encodeGV":"v1","encoder":"protobuf","name":"versioning"} (22-Mar-2024 03:33:59.611) (total time: 9596ms):
Trace[651034481]: ---"About to start writing response" size:19342 2228ms (03:34:01.839)
Trace[651034481]: ---"Write call succeeded" writer:responsewriter.outerWithCloseNotifyAndFlush,size:19342,firstWrite:true 7364ms (03:34:09.204)
Trace[651034481]: [9.596597645s] [9.596597645s] END
I0322 03:34:46.906858    2280 trace.go:236] Trace[45704233]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:3ffd83e1-a5e4-4946-9a2f-00e020d512b4,client:127.0.0.1,protocol:HTTP/2.0,resource:pods,scope:cluster,url:/api/v1/pods,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/scheduler,verb:LIST (22-Mar-2024 03:33:59.606) (total time: 47289ms):
Trace[45704233]: ---"Writing http response done" count:4 47284ms (03:34:46.895)
Trace[45704233]: [47.289036022s] [47.289036022s] END
E0322 03:34:46.966392    2280 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
I0322 03:34:46.997496    2280 trace.go:236] Trace[213834035]: "Reflector ListAndWatch" name:object-"kube-system"/"kube-root-ca.crt" (22-Mar-2024 03:33:53.429) (total time: 53562ms):
Trace[213834035]: ---"Objects listed" error:<nil> 53559ms (03:34:46.989)
Trace[213834035]: [53.562832793s] [53.562832793s] END
I0322 03:34:47.040988    2280 trace.go:236] Trace[1533119891]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.437) (total time: 53602ms):
Trace[1533119891]: ---"Objects listed" error:<nil> 53600ms (03:34:47.037)
Trace[1533119891]: [53.602910634s] [53.602910634s] END
I0322 03:34:47.077383    2280 trace.go:236] Trace[108418041]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.646) (total time: 49529ms):
Trace[108418041]: ---"Objects listed" error:<nil> 30650ms (03:34:24.296)
Trace[108418041]: ---"Objects extracted" 18876ms (03:34:43.174)
Trace[108418041]: [49.529701252s] [49.529701252s] END
E0322 03:34:47.096797    2280 request.go:1116] Unexpected error when reading response body: http2: client connection lost
I0322 03:34:47.118571    2280 trace.go:236] Trace[718290549]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.500) (total time: 53615ms):
Trace[718290549]: ---"Objects listed" error:<nil> 53607ms (03:34:47.108)
Trace[718290549]: [53.615496781s] [53.615496781s] END
time="2024-03-22T03:34:33Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:56.930151697 +0000 UTC m=+336.921799933) (total time: 28.312280547s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/persistentvolumes/% 612]]"
I0322 03:34:42.864852    2280 trace.go:236] Trace[190255354]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.421) (total time: 38990ms):
Trace[190255354]: ---"Objects listed" error:<nil> 30730ms (03:34:24.152)
Trace[190255354]: ---"SyncWith done" 8247ms (03:34:32.410)
Trace[190255354]: [38.990354523s] [38.990354523s] END
I0322 03:34:47.232643    2280 trace.go:236] Trace[1681103648]: "DeltaFIFO Pop Process" ID:system:aggregate-to-view,Depth:60,Reason:slow event handlers blocking the queue (22-Mar-2024 03:34:46.100) (total time: 1126ms):
Trace[1681103648]: [1.126826186s] [1.126826186s] END
W0322 03:34:47.237734    2280 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Pod: unexpected error when reading response body. Please retry. Original error: http2: client connection lost
I0322 03:34:47.237796    2280 trace.go:236] Trace[1179837818]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.392) (total time: 53844ms):
Trace[1179837818]: ---"Objects listed" error:unexpected error when reading response body. Please retry. Original error: http2: client connection lost 53843ms (03:34:47.236)
Trace[1179837818]: [53.844860933s] [53.844860933s] END
E0322 03:34:47.238500    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Pod: failed to list *v1.Pod: unexpected error when reading response body. Please retry. Original error: http2: client connection lost
W0322 03:34:47.317948    2280 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.StorageClass: Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/storageclasses?resourceVersion=267": http2: client connection lost
I0322 03:34:47.324503    2280 trace.go:236] Trace[548454097]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.429) (total time: 53892ms):
Trace[548454097]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/storageclasses?resourceVersion=267": http2: client connection lost 53887ms (03:34:47.317)
Trace[548454097]: [53.892758084s] [53.892758084s] END
E0322 03:34:47.326649    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/storageclasses?resourceVersion=267": http2: client connection lost
E0322 03:34:47.328759    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Node: Get "https://127.0.0.1:6444/api/v1/nodes?allowWatchBookmarks=true&resourceVersion=612&timeout=6m33s&timeoutSeconds=393&watch=true": http2: client connection lost
E0322 03:34:47.329443    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ConfigMap: Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?allowWatchBookmarks=true&fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=613&timeout=8m23s&timeoutSeconds=503&watch=true": http2: client connection lost
E0322 03:34:47.329974    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ConfigMap: Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?allowWatchBookmarks=true&fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=613&timeout=7m13s&timeoutSeconds=433&watch=true": http2: client connection lost
E0322 03:34:47.331745    2280 reflector.go:147] object-"kube-system"/"coredns-custom": Failed to watch *v1.ConfigMap: Get "https://127.0.0.1:6443/api/v1/namespaces/kube-system/configmaps?allowWatchBookmarks=true&fieldSelector=metadata.name%3Dcoredns-custom&resourceVersion=613&timeout=8m44s&timeoutSeconds=524&watch=true": http2: client connection lost
E0322 03:34:47.332135    2280 reflector.go:147] object-"kube-system"/"local-path-config": Failed to watch *v1.ConfigMap: Get "https://127.0.0.1:6443/api/v1/namespaces/kube-system/configmaps?allowWatchBookmarks=true&fieldSelector=metadata.name%3Dlocal-path-config&resourceVersion=613&timeout=6m11s&timeoutSeconds=371&watch=true": http2: client connection lost
E0322 03:34:47.332890    2280 request.go:1116] Unexpected error when reading response body: http2: client connection lost
W0322 03:34:47.335004    2280 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Pod: Get "https://127.0.0.1:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dserver&resourceVersion=610": http2: client connection lost
I0322 03:34:47.335143    2280 trace.go:236] Trace[797135946]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.842) (total time: 53492ms):
Trace[797135946]: ---"Objects listed" error:Get "https://127.0.0.1:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dserver&resourceVersion=610": http2: client connection lost 53492ms (03:34:47.334)
Trace[797135946]: [53.492101561s] [53.492101561s] END
E0322 03:34:47.335276    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://127.0.0.1:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dserver&resourceVersion=610": http2: client connection lost
W0322 03:34:47.335475    2280 reflector.go:535] object-"kube-system"/"chart-values-traefik": failed to list *v1.Secret: Get "https://127.0.0.1:6443/api/v1/namespaces/kube-system/secrets?fieldSelector=metadata.name%3Dchart-values-traefik&resourceVersion=610": http2: client connection lost
I0322 03:34:47.336115    2280 trace.go:236] Trace[1553116711]: "Reflector ListAndWatch" name:object-"kube-system"/"chart-values-traefik" (22-Mar-2024 03:33:53.573) (total time: 53763ms):
Trace[1553116711]: ---"Objects listed" error:Get "https://127.0.0.1:6443/api/v1/namespaces/kube-system/secrets?fieldSelector=metadata.name%3Dchart-values-traefik&resourceVersion=610": http2: client connection lost 53762ms (03:34:47.335)
Trace[1553116711]: [53.763034637s] [53.763034637s] END
E0322 03:34:47.336189    2280 reflector.go:147] object-"kube-system"/"chart-values-traefik": Failed to watch *v1.Secret: failed to list *v1.Secret: Get "https://127.0.0.1:6443/api/v1/namespaces/kube-system/secrets?fieldSelector=metadata.name%3Dchart-values-traefik&resourceVersion=610": http2: client connection lost
E0322 03:34:47.339387    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ConfigMap: Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?allowWatchBookmarks=true&fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=613&timeout=7m4s&timeoutSeconds=424&watch=true": http2: client connection lost
W0322 03:34:47.352003    2280 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.RuntimeClass: Get "https://127.0.0.1:6443/apis/node.k8s.io/v1/runtimeclasses?resourceVersion=610": http2: client connection lost
I0322 03:34:47.352258    2280 trace.go:236] Trace[1058272193]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.425) (total time: 53926ms):
Trace[1058272193]: ---"Objects listed" error:Get "https://127.0.0.1:6443/apis/node.k8s.io/v1/runtimeclasses?resourceVersion=610": http2: client connection lost 53925ms (03:34:47.351)
Trace[1058272193]: [53.926321846s] [53.926321846s] END
E0322 03:34:47.352342    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: Get "https://127.0.0.1:6443/apis/node.k8s.io/v1/runtimeclasses?resourceVersion=610": http2: client connection lost
E0322 03:34:47.352573    2280 reflector.go:147] object-"kube-system"/"chart-content-traefik": Failed to watch *v1.ConfigMap: Get "https://127.0.0.1:6443/api/v1/namespaces/kube-system/configmaps?allowWatchBookmarks=true&fieldSelector=metadata.name%3Dchart-content-traefik&resourceVersion=612&timeout=7m40s&timeoutSeconds=460&watch=true": http2: client connection lost
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:56.936272802 +0000 UTC m=+336.927921038) (total time: 3.487260297s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/networkpolicies/% 612]]"
I0322 03:34:32.944450    2280 trace.go:236] Trace[1242930533]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:47.395) (total time: 29513ms):
Trace[1242930533]: ---"Objects listed" error:<nil> 22923ms (03:34:10.318)
Trace[1242930533]: ---"Objects extracted" 6587ms (03:34:16.905)
Trace[1242930533]: [29.513293752s] [29.513293752s] END
I0322 03:34:47.374704    2280 trace.go:236] Trace[736389131]: "DeltaFIFO Pop Process" ID:system:aggregated-metrics-reader,Depth:59,Reason:slow event handlers blocking the queue (22-Mar-2024 03:34:47.232) (total time: 141ms):
Trace[736389131]: [141.706547ms] [141.706547ms] END
W0322 03:34:47.376441    2280 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Node: unexpected error when reading response body. Please retry. Original error: http2: client connection lost
I0322 03:34:47.376624    2280 trace.go:236] Trace[1739415761]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.718) (total time: 53658ms):
Trace[1739415761]: ---"Objects listed" error:unexpected error when reading response body. Please retry. Original error: http2: client connection lost 53657ms (03:34:47.376)
Trace[1739415761]: [53.658060988s] [53.658060988s] END
E0322 03:34:47.376725    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Node: failed to list *v1.Node: unexpected error when reading response body. Please retry. Original error: http2: client connection lost
W0322 03:34:47.376747    2280 reflector.go:535] object-"kube-system"/"coredns": failed to list *v1.ConfigMap: Get "https://127.0.0.1:6443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dcoredns&resourceVersion=423": http2: client connection lost
I0322 03:34:47.376882    2280 trace.go:236] Trace[1925346598]: "Reflector ListAndWatch" name:object-"kube-system"/"coredns" (22-Mar-2024 03:33:53.356) (total time: 54020ms):
Trace[1925346598]: ---"Objects listed" error:Get "https://127.0.0.1:6443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dcoredns&resourceVersion=423": http2: client connection lost 54020ms (03:34:47.376)
Trace[1925346598]: [54.020374123s] [54.020374123s] END
E0322 03:34:47.376928    2280 reflector.go:147] object-"kube-system"/"coredns": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://127.0.0.1:6443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dcoredns&resourceVersion=423": http2: client connection lost
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:56.936938319 +0000 UTC m=+336.928586555) (total time: 3.487098812s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/traefik.containo.us/traefikservices/% false]]"
I0322 03:34:45.050392    2280 trace.go:236] Trace[2129912332]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:47.604) (total time: 57434ms):
Trace[2129912332]: ---"Objects listed" error:<nil> 39101ms (03:34:26.705)
Trace[2129912332]: ---"Objects extracted" 8102ms (03:34:34.809)
Trace[2129912332]: ---"SyncWith done" 10227ms (03:34:45.037)
Trace[2129912332]: [57.434607675s] [57.434607675s] END
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:55.987558175 +0000 UTC m=+335.979206408) (total time: 4.455571356s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/traefik.containo.us/tlsoptions/% 612]]"
I0322 03:34:45.078139    2280 trace.go:236] Trace[278273617]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:47.860) (total time: 57214ms):
Trace[278273617]: ---"Objects listed" error:<nil> 46985ms (03:34:34.846)
Trace[278273617]: ---"Objects extracted" 10228ms (03:34:45.074)
Trace[278273617]: [57.214137188s] [57.214137188s] END
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:58.529451369 +0000 UTC m=+338.521099601) (total time: 1.91403188s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/clusterroles/% 612]]"
E0322 03:34:45.136758    2280 health_controller.go:162] Metrics Controller heartbeat missed
E0322 03:34:47.410893    2280 health_controller.go:162] Metrics Controller heartbeat missed
E0322 03:34:47.411032    2280 health_controller.go:162] Metrics Controller heartbeat missed
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:59.107849865 +0000 UTC m=+339.099498120) (total time: 1.336655315s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/flowschemas/% false]]"
I0322 03:34:45.190923    2280 trace.go:236] Trace[1732358026]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:47.753) (total time: 32266ms):
Trace[1732358026]: ---"Objects listed" error:<nil> 24299ms (03:34:12.052)
Trace[1732358026]: ---"Objects extracted" 7956ms (03:34:20.012)
Trace[1732358026]: [32.266293122s] [32.266293122s] END
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:57.634456119 +0000 UTC m=+337.626104370) (total time: 2.8101934s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/prioritylevelconfigurations/% 612]]"
I0322 03:34:45.260290    2280 trace.go:236] Trace[1833857534]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:47.519) (total time: 31221ms):
Trace[1833857534]: ---"Objects listed" error:<nil> 31188ms (03:34:18.708)
Trace[1833857534]: [31.22122332s] [31.22122332s] END
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:59.125010638 +0000 UTC m=+339.116658893) (total time: 1.320027871s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/leases/% 612]]"
E0322 03:34:45.343864    2280 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:34:47.462972    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:59.125017997 +0000 UTC m=+339.116666234) (total time: 1.320535469s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/podtemplates/% false]]"
I0322 03:34:45.386057    2280 trace.go:236] Trace[1945594602]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.649) (total time: 51733ms):
Trace[1945594602]: ---"Objects listed" error:<nil> 51729ms (03:34:45.379)
Trace[1945594602]: [51.733391727s] [51.733391727s] END
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:59.125054172 +0000 UTC m=+339.116702415) (total time: 1.320840041s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/flowschemas/% 612]]"
I0322 03:34:45.386291    2280 trace.go:236] Trace[1820309915]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.374) (total time: 52011ms):
Trace[1820309915]: ---"Objects listed" error:<nil> 52011ms (03:34:45.385)
Trace[1820309915]: [52.011188806s] [52.011188806s] END
E0322 03:34:45.500925    2280 status.go:71] apiserver received an error that is not an metav1.Status: context.deadlineExceededError{}: context deadline exceeded
I0322 03:34:45.666735    2280 trace.go:236] Trace[183161543]: "DeltaFIFO Pop Process" ID:kube-system/clusterrole-aggregation-controller,Depth:34,Reason:slow event handlers blocking the queue (22-Mar-2024 03:34:12.507) (total time: 1331ms):
Trace[183161543]: [1.331456202s] [1.331456202s] END
I0322 03:34:45.723922    2280 trace.go:236] Trace[766270749]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.332) (total time: 52388ms):
Trace[766270749]: ---"Objects listed" error:<nil> 36973ms (03:34:30.305)
Trace[766270749]: ---"Objects extracted" 11245ms (03:34:41.552)
Trace[766270749]: ---"SyncWith done" 4168ms (03:34:45.721)
Trace[766270749]: [52.388447364s] [52.388447364s] END
I0322 03:34:47.560496    2280 trace.go:236] Trace[1112165132]: "SerializeObject" audit-id:3a2da695-6ae7-4728-87e3-cf05cf094bc8,method:GET,url:/apis/traefik.containo.us/v1alpha1/tlsstores,protocol:HTTP/2.0,mediaType:application/vnd.kubernetes.protobuf,encoder:{"encodeGV":"meta.k8s.io/v1","encoder":"protobuf","name":"versioning"} (22-Mar-2024 03:34:19.123) (total time: 24294ms):
Trace[1112165132]: ---"About to start writing response" size:66 24281ms (03:34:43.404)
Trace[1112165132]: [24.294220254s] [24.294220254s] END
I0322 03:34:47.561724    2280 trace.go:236] Trace[414850654]: "List" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json,audit-id:3a2da695-6ae7-4728-87e3-cf05cf094bc8,client:127.0.0.1,protocol:HTTP/2.0,resource:tlsstores,scope:cluster,url:/apis/traefik.containo.us/v1alpha1/tlsstores,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/metadata-informers,verb:LIST (22-Mar-2024 03:33:59.401) (total time: 48159ms):
Trace[414850654]: ---"Writing http response done" count:0 46043ms (03:34:47.561)
Trace[414850654]: [48.159356158s] [48.159356158s] END
E0322 03:34:45.767998    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:34:47.567929    2280 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 03:34:47.574248    2280 trace.go:236] Trace[1924212896]: "SerializeObject" audit-id:4c2ea923-8b17-4fce-8dc2-33eec4f250cc,method:PUT,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/server,protocol:HTTP/2.0,mediaType:application/vnd.kubernetes.protobuf,encoder:{"encodeGV":"coordination.k8s.io/v1","encoder":"protobuf","name":"versioning"} (22-Mar-2024 03:34:02.861) (total time: 44712ms):
Trace[1924212896]: ---"About to start writing response" size:154 11184ms (03:34:14.046)
Trace[1924212896]: ---"Write call failed" writer:responsewriter.outerWithCloseNotifyAndFlush,size:154,firstWrite:true,err:http: Handler timeout 8320ms (03:34:22.367)
Trace[1924212896]: ---"About to start writing response" size:69 25198ms (03:34:47.566)
Trace[1924212896]: [44.712048558s] [44.712048558s] END
I0322 03:34:47.575796    2280 trace.go:236] Trace[400094235]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:4c2ea923-8b17-4fce-8dc2-33eec4f250cc,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/server,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PUT (22-Mar-2024 03:33:56.190) (total time: 51384ms):
Trace[400094235]: ---"About to convert to expected version" 2341ms (03:33:58.537)
Trace[400094235]: ---"Conversion done" 1523ms (03:34:00.060)
Trace[400094235]: [51.384006402s] [51.384006402s] END
I0322 03:34:45.892711    2280 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="40.204709964s"
E0322 03:34:47.669314    2280 server.go:310] "Unable to authenticate the request due to an error" err="context canceled"
I0322 03:34:47.723320    2280 trace.go:236] Trace[419420616]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:48.144) (total time: 45681ms):
Trace[419420616]: ---"Objects listed" error:<nil> 37148ms (03:34:25.293)
Trace[419420616]: ---"Objects extracted" 8532ms (03:34:33.825)
Trace[419420616]: [45.68134709s] [45.68134709s] END
E0322 03:34:47.990268    2280 request.go:1116] Unexpected error when reading response body: http2: client connection lost
E0322 03:34:47.998360    2280 resource_quota_controller.go:440] failed to discover resources: Get "https://127.0.0.1:6444/api": http2: client connection lost
I0322 03:34:48.052523    2280 trace.go:236] Trace[1190788563]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.388) (total time: 40839ms):
Trace[1190788563]: ---"Objects listed" error:<nil> 33182ms (03:34:26.571)
Trace[1190788563]: ---"Resource version extracted" 7649ms (03:34:34.220)
Trace[1190788563]: [40.839026571s] [40.839026571s] END
E0322 03:34:48.116136    2280 node_lifecycle_controller.go:971] "Error updating node" err="Put \"https://127.0.0.1:6444/api/v1/nodes/server/status\": http2: client connection lost" node="server"
I0322 03:34:48.122360    2280 trace.go:236] Trace[395818421]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:55.002) (total time: 53117ms):
Trace[395818421]: ---"Objects listed" error:<nil> 53114ms (03:34:48.117)
Trace[395818421]: [53.117245796s] [53.117245796s] END
E0322 03:34:48.206988    2280 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, context canceled]"
E0322 03:34:48.242682    2280 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:34:48.244574    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/apiregistration.k8s.io/v1/apiservices?allowWatchBookmarks=true&resourceVersion=612&timeout=9m1s&timeoutSeconds=541&watch=true": http2: client connection lost
E0322 03:34:48.244841    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
I0322 03:34:48.254550    2280 trace.go:236] Trace[431083398]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.840) (total time: 54412ms):
Trace[431083398]: ---"Objects listed" error:<nil> 54410ms (03:34:48.250)
Trace[431083398]: [54.412093188s] [54.412093188s] END
I0322 03:34:48.273887    2280 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="34.050910212s"
I0322 03:34:48.342120    2280 trace.go:236] Trace[1114689785]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:48.142) (total time: 60186ms):
Trace[1114689785]: ---"Objects listed" error:<nil> 60182ms (03:34:48.325)
Trace[1114689785]: [1m0.186835541s] [1m0.186835541s] END
E0322 03:34:48.439049    2280 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:34:48.443333    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
W0322 03:34:45.890281    2280 transport.go:301] Unable to cancel request for *otelhttp.Transport
I0322 03:34:48.504532    2280 trace.go:236] Trace[600932446]: "List" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:6390ea64-5389-43fe-b150-f47a41eb0892,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:34:45.998) (total time: 2504ms):
Trace[600932446]: ---"Writing http response done" count:1 2498ms (03:34:48.502)
Trace[600932446]: [2.504371572s] [2.504371572s] END
W0322 03:34:48.684492    2280 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Node: unexpected error when reading response body. Please retry. Original error: http2: client connection lost
I0322 03:34:48.689404    2280 trace.go:236] Trace[516782958]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.659) (total time: 55027ms):
Trace[516782958]: ---"Objects listed" error:unexpected error when reading response body. Please retry. Original error: http2: client connection lost 55020ms (03:34:48.679)
Trace[516782958]: [55.027510577s] [55.027510577s] END
E0322 03:34:48.692171    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ConfigMap: Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?allowWatchBookmarks=true&fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=613&timeout=5m17s&timeoutSeconds=317&watch=true": http2: client connection lost
E0322 03:34:48.692358    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ConfigMap: Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?allowWatchBookmarks=true&fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=613&timeout=7m48s&timeoutSeconds=468&watch=true": http2: client connection lost
I0322 03:34:48.709046    2280 trace.go:236] Trace[38024637]: "SerializeObject" audit-id:b2cb2838-0a17-428d-82e9-631b36d4d7a1,method:GET,url:/api/v1/pods,protocol:HTTP/2.0,mediaType:application/vnd.kubernetes.protobuf,encoder:{"encodeGV":"v1","encoder":"protobuf","name":"versioning"} (22-Mar-2024 03:34:00.117) (total time: 48588ms):
Trace[38024637]: ---"About to start writing response" size:25584 22959ms (03:34:23.076)
Trace[38024637]: ---"Write call succeeded" writer:responsewriter.outerWithCloseNotifyAndFlush,size:25584,firstWrite:true 25628ms (03:34:48.705)
Trace[38024637]: [48.588393777s] [48.588393777s] END
I0322 03:34:48.710984    2280 trace.go:236] Trace[1361447191]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:b2cb2838-0a17-428d-82e9-631b36d4d7a1,client:127.0.0.1,protocol:HTTP/2.0,resource:pods,scope:cluster,url:/api/v1/pods,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:33:59.033) (total time: 49676ms):
Trace[1361447191]: ["cacher list" audit-id:b2cb2838-0a17-428d-82e9-631b36d4d7a1,type:pods 49674ms (03:33:59.035)
Trace[1361447191]:  ---"watchCache locked acquired" 1061ms (03:34:00.097)]
Trace[1361447191]: ---"Writing http response done" count:5 48610ms (03:34:48.710)
Trace[1361447191]: [49.676919733s] [49.676919733s] END
E0322 03:34:48.721778    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Node: Get "https://127.0.0.1:6444/api/v1/nodes?allowWatchBookmarks=true&resourceVersion=612&timeout=7m6s&timeoutSeconds=426&watch=true": http2: client connection lost
E0322 03:34:48.722075    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ConfigMap: Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?allowWatchBookmarks=true&fieldSelector=metadata.name%3Dkube-apiserver-legacy-service-account-token-tracking&resourceVersion=612&timeout=7m38s&timeoutSeconds=458&watch=true": http2: client connection lost
E0322 03:34:48.723643    2280 request.go:1116] Unexpected error when reading response body: http2: client connection lost
E0322 03:34:48.730534    2280 repair.go:125] unable to refresh the service IP block: Get "https://127.0.0.1:6444/api/v1/services": http2: client connection lost
E0322 03:34:48.731058    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Service: Get "https://127.0.0.1:6444/api/v1/services?allowWatchBookmarks=true&resourceVersion=612&timeout=5m23s&timeoutSeconds=323&watch=true": http2: client connection lost
E0322 03:34:48.732304    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ValidatingWebhookConfiguration: Get "https://127.0.0.1:6444/apis/admissionregistration.k8s.io/v1/validatingwebhookconfigurations?allowWatchBookmarks=true&resourceVersion=610&timeout=9m17s&timeoutSeconds=557&watch=true": http2: client connection lost
E0322 03:34:48.735674    2280 available_controller.go:460] v1beta1.metrics.k8s.io failed with: Put "https://127.0.0.1:6444/apis/apiregistration.k8s.io/v1/apiservices/v1beta1.metrics.k8s.io/status": http2: client connection lost
E0322 03:34:48.751504    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PersistentVolume: Get "https://127.0.0.1:6444/api/v1/persistentvolumes?allowWatchBookmarks=true&resourceVersion=612&timeout=5m59s&timeoutSeconds=359&watch=true": http2: client connection lost
E0322 03:34:48.751909    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ConfigMap: Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?allowWatchBookmarks=true&resourceVersion=612&timeout=9m19s&timeoutSeconds=559&watch=true": http2: client connection lost
E0322 03:34:48.754517    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ClusterRoleBinding: Get "https://127.0.0.1:6444/apis/rbac.authorization.k8s.io/v1/clusterrolebindings?allowWatchBookmarks=true&resourceVersion=612&timeout=7m1s&timeoutSeconds=421&watch=true": http2: client connection lost
E0322 03:34:48.755628    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1beta3.FlowSchema: Get "https://127.0.0.1:6444/apis/flowcontrol.apiserver.k8s.io/v1beta3/flowschemas?allowWatchBookmarks=true&resourceVersion=612&timeout=8m5s&timeoutSeconds=485&watch=true": http2: client connection lost
E0322 03:34:48.756122    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ResourceQuota: Get "https://127.0.0.1:6444/api/v1/resourcequotas?allowWatchBookmarks=true&resourceVersion=612&timeout=7m52s&timeoutSeconds=472&watch=true": http2: client connection lost
E0322 03:34:48.766063    2280 controller.go:193] "Failed to update lease" err="Put \"https://127.0.0.1:6444/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-7zoch227uv4owxg75pgtnmv3jq\": http2: client connection lost"
E0322 03:34:48.780798    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Secret: Get "https://127.0.0.1:6444/api/v1/secrets?allowWatchBookmarks=true&resourceVersion=612&timeout=6m18s&timeoutSeconds=378&watch=true": http2: client connection lost
E0322 03:34:48.789431    2280 finisher.go:175] FinishRequest: post-timeout activity - time-elapsed: 8.935284335s, panicked: false, err: context deadline exceeded, panic-reason: <nil>
W0322 03:34:48.787592    2280 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.MutatingWebhookConfiguration: Get "https://127.0.0.1:6444/apis/admissionregistration.k8s.io/v1/mutatingwebhookconfigurations?resourceVersion=603": http2: client connection lost
I0322 03:34:48.795265    2280 trace.go:236] Trace[527696379]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:47.570) (total time: 61223ms):
Trace[527696379]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/admissionregistration.k8s.io/v1/mutatingwebhookconfigurations?resourceVersion=603": http2: client connection lost 61215ms (03:34:48.786)
Trace[527696379]: [1m1.223436138s] [1m1.223436138s] END
E0322 03:34:48.795398    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.MutatingWebhookConfiguration: failed to list *v1.MutatingWebhookConfiguration: Get "https://127.0.0.1:6444/apis/admissionregistration.k8s.io/v1/mutatingwebhookconfigurations?resourceVersion=603": http2: client connection lost
E0322 03:34:48.797607    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1beta3.PriorityLevelConfiguration: Get "https://127.0.0.1:6444/apis/flowcontrol.apiserver.k8s.io/v1beta3/prioritylevelconfigurations?allowWatchBookmarks=true&resourceVersion=612&timeout=6m33s&timeoutSeconds=393&watch=true": http2: client connection lost
E0322 03:34:48.800648    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ServiceAccount: Get "https://127.0.0.1:6444/api/v1/serviceaccounts?allowWatchBookmarks=true&resourceVersion=612&timeout=8m0s&timeoutSeconds=480&watch=true": http2: client connection lost
E0322 03:34:48.801549    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: Get "https://127.0.0.1:6444/api/v1/namespaces?allowWatchBookmarks=true&resourceVersion=612&timeout=6m21s&timeoutSeconds=381&watch=true": http2: client connection lost
E0322 03:34:48.806453    2280 repair.go:83] unable to refresh the port block: Get "https://127.0.0.1:6444/api/v1/services": http2: client connection lost
W0322 03:34:48.807030    2280 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.CustomResourceDefinition: Get "https://127.0.0.1:6444/apis/apiextensions.k8s.io/v1/customresourcedefinitions?resourceVersion=604": http2: client connection lost
I0322 03:34:48.807478    2280 trace.go:236] Trace[301036765]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:47.412) (total time: 61395ms):
Trace[301036765]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/apiextensions.k8s.io/v1/customresourcedefinitions?resourceVersion=604": http2: client connection lost 61394ms (03:34:48.807)
Trace[301036765]: [1m1.395283157s] [1m1.395283157s] END
E0322 03:34:48.807562    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.CustomResourceDefinition: failed to list *v1.CustomResourceDefinition: Get "https://127.0.0.1:6444/apis/apiextensions.k8s.io/v1/customresourcedefinitions?resourceVersion=604": http2: client connection lost
E0322 03:34:48.807756    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ClusterRole: Get "https://127.0.0.1:6444/apis/rbac.authorization.k8s.io/v1/clusterroles?allowWatchBookmarks=true&resourceVersion=612&timeout=9m14s&timeoutSeconds=554&watch=true": http2: client connection lost
E0322 03:34:48.808834    2280 request.go:1116] Unexpected error when reading response body: http2: client connection lost
E0322 03:34:48.811013    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.VolumeAttachment: Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/volumeattachments?allowWatchBookmarks=true&resourceVersion=612&timeout=6m58s&timeoutSeconds=418&watch=true": http2: client connection lost
E0322 03:34:48.811154    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Endpoints: Get "https://127.0.0.1:6444/api/v1/endpoints?allowWatchBookmarks=true&resourceVersion=612&timeout=7m43s&timeoutSeconds=463&watch=true": http2: client connection lost
E0322 03:34:48.811612    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PriorityClass: Get "https://127.0.0.1:6444/apis/scheduling.k8s.io/v1/priorityclasses?allowWatchBookmarks=true&resourceVersion=612&timeout=9m33s&timeoutSeconds=573&watch=true": http2: client connection lost
E0322 03:34:48.828437    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.containo.us/v1alpha1/traefikservices?allowWatchBookmarks=true&resourceVersion=612&timeout=5m43s&timeoutSeconds=343&watch=true": http2: client connection lost
W0322 03:34:48.987726    2280 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.APIService: unexpected error when reading response body. Please retry. Original error: http2: client connection lost
I0322 03:34:48.991608    2280 trace.go:236] Trace[1707660112]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:47.993) (total time: 60996ms):
Trace[1707660112]: ---"Objects listed" error:unexpected error when reading response body. Please retry. Original error: http2: client connection lost 60992ms (03:34:48.985)
Trace[1707660112]: [1m0.99618798s] [1m0.99618798s] END
E0322 03:34:48.992850    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.APIService: failed to list *v1.APIService: unexpected error when reading response body. Please retry. Original error: http2: client connection lost
W0322 03:34:49.014142    2280 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.RoleBinding: Get "https://127.0.0.1:6444/apis/rbac.authorization.k8s.io/v1/rolebindings?resourceVersion=610": http2: client connection lost
I0322 03:34:49.017150    2280 trace.go:236] Trace[158529017]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:47.683) (total time: 61333ms):
Trace[158529017]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/rbac.authorization.k8s.io/v1/rolebindings?resourceVersion=610": http2: client connection lost 61330ms (03:34:49.014)
Trace[158529017]: [1m1.333356184s] [1m1.333356184s] END
E0322 03:34:49.017587    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.RoleBinding: failed to list *v1.RoleBinding: Get "https://127.0.0.1:6444/apis/rbac.authorization.k8s.io/v1/rolebindings?resourceVersion=610": http2: client connection lost
W0322 03:34:49.019502    2280 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Pod: unexpected error when reading response body. Please retry. Original error: http2: client connection lost
I0322 03:34:49.019595    2280 trace.go:236] Trace[497144373]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:47.815) (total time: 61203ms):
Trace[497144373]: ---"Objects listed" error:unexpected error when reading response body. Please retry. Original error: http2: client connection lost 61203ms (03:34:49.019)
Trace[497144373]: [1m1.203598626s] [1m1.203598626s] END
E0322 03:34:49.019646    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Pod: failed to list *v1.Pod: unexpected error when reading response body. Please retry. Original error: http2: client connection lost
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:59.165276751 +0000 UTC m=+339.156924987) (total time: 1.281012843s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/flowschemas/% false]]"
time="2024-03-22T03:34:48Z" level=info msg="Slow SQL (started: 2024-03-22 03:34:37.743813233 +0000 UTC m=+377.735461496) (total time: 4.123934121s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? AND mkv.id <= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC LIMIT 1000 : [[/registry/traefik.containo.us/serverstransports/% 613 false]]"
I0322 03:34:49.061419    2280 trace.go:236] Trace[442975246]: "SerializeObject" audit-id:31c4b7ee-2930-4ee3-9f59-19cead79ddf8,method:GET,url:/api/v1/nodes,protocol:HTTP/2.0,mediaType:application/json,encoder:{"encodeGV":"v1","encoder":"{\"name\":\"json\",\"pretty\":\"false\",\"strict\":\"false\",\"yaml\":\"false\"}","name":"versioning"} (22-Mar-2024 03:33:59.276) (total time: 42193ms):
Trace[442975246]: ---"About to start writing response" size:5376 5621ms (03:34:04.897)
Trace[442975246]: ---"Write call succeeded" writer:responsewriter.outerWithCloseNotifyAndFlush,size:5376,firstWrite:true 36557ms (03:34:41.455)
Trace[442975246]: [42.193467365s] [42.193467365s] END
I0322 03:34:49.063247    2280 trace.go:236] Trace[1220520402]: "List" accept:application/json, */*,audit-id:31c4b7ee-2930-4ee3-9f59-19cead79ddf8,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:cluster,url:/api/v1/nodes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:33:59.263) (total time: 49798ms):
Trace[1220520402]: ---"Writing http response done" count:1 49794ms (03:34:49.062)
Trace[1220520402]: [49.798736775s] [49.798736775s] END
E0322 03:34:49.068787    2280 server.go:310] "Unable to authenticate the request due to an error" err="context canceled"
E0322 03:34:49.106604    2280 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:34:49.114589    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
I0322 03:34:49.120771    2280 trace.go:236] Trace[1118765840]: "iptables ChainExists" (22-Mar-2024 03:34:31.305) (total time: 17812ms):
Trace[1118765840]: [17.812559628s] [17.812559628s] END
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:59.165591059 +0000 UTC m=+339.157239286) (total time: 1.281101667s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/traefik.io/ingressroutes/% false]]"
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:59.202838084 +0000 UTC m=+339.194486319) (total time: 1.252507334s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/controllerrevisions/% 610]]"
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:59.214501645 +0000 UTC m=+339.206149879) (total time: 1.241438534s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/k3s.cattle.io/addons/% 612]]"
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:57.733080376 +0000 UTC m=+337.724728607) (total time: 2.723155719s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/traefik.io/tlsstores/% 612]]"
time="2024-03-22T03:34:49Z" level=error msg="Failed to list /registry/traefik.io/tlsstores/ for revision 612: context canceled"
E0322 03:34:49.194057    2280 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: error trying to reach service: net/http: TLS handshake timeout
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 03:34:49.199420    2280 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:57.768347815 +0000 UTC m=+337.759996073) (total time: 2.688182928s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/traefik.io/tlsstores/% false]]"
time="2024-03-22T03:34:49Z" level=error msg="error while range on /registry/traefik.io/tlsstores/ /registry/traefik.io/tlsstores/: context canceled"
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:55.939187891 +0000 UTC m=+335.930836127) (total time: 4.519674809s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/traefik.io/traefikservices/% 612]]"
time="2024-03-22T03:33:59Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:55.921210892 +0000 UTC m=+335.912859134) (total time: 1.360774241s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/traefik.io/traefikservices/% false]]"
time="2024-03-22T03:34:02Z" level=info msg="Slow SQL (started: 2024-03-22 03:34:00.006050988 +0000 UTC m=+339.997699218) (total time: 2.700732384s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/prioritylevelconfigurations/% false]]"
time="2024-03-22T03:34:09Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:56.821581031 +0000 UTC m=+336.813229267) (total time: 12.615902259s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/validatingwebhookconfigurations/% false]]"
time="2024-03-22T03:34:09Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:56.03349453 +0000 UTC m=+336.025142770) (total time: 13.417649008s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/services/specs/% 612]]"
time="2024-03-22T03:34:14Z" level=info msg="Slow SQL (started: 2024-03-22 03:34:00.388138285 +0000 UTC m=+340.379786517) (total time: 3.588794189s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? AND mkv.id <= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC LIMIT 1000 : [[/registry/ingressclasses/% 613 false]]"
time="2024-03-22T03:34:15Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:56.796548388 +0000 UTC m=+336.788196623) (total time: 12.539299432s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/apiregistration.k8s.io/apiservices/% false]]"
time="2024-03-22T03:34:16Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:56.864957114 +0000 UTC m=+336.856605350) (total time: 7.25090125s): SELECT SUM(pgsize) FROM dbstat : [[]]"
time="2024-03-22T03:34:20Z" level=info msg="Slow SQL (started: 2024-03-22 03:34:02.014394726 +0000 UTC m=+342.006042978) (total time: 4.815432668s):  SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key' : [[]]"
time="2024-03-22T03:34:25Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:56.872480923 +0000 UTC m=+336.864129155) (total time: 12.668868747s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/priorityclasses/% 612]]"
time="2024-03-22T03:34:25Z" level=info msg="Slow SQL (started: 2024-03-22 03:34:01.261135094 +0000 UTC m=+341.252783327) (total time: 8.81985618s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC LIMIT 500 : [[% 613]]"
time="2024-03-22T03:34:28Z" level=info msg="Slow SQL (started: 2024-03-22 03:34:21.307755672 +0000 UTC m=+361.299403923) (total time: 7.044380551s):  SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key' : [[]]"
time="2024-03-22T03:34:29Z" level=info msg="Slow SQL (started: 2024-03-22 03:34:03.938102455 +0000 UTC m=+343.929750706) (total time: 3.51893376s):  SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key' : [[]]"
time="2024-03-22T03:34:31Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:56.821659139 +0000 UTC m=+336.813307376) (total time: 27.601643236s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/validatingwebhookconfigurations/% 610]]"
time="2024-03-22T03:34:35Z" level=info msg="Slow SQL (started: 2024-03-22 03:34:10.911650498 +0000 UTC m=+350.903298748) (total time: 15.0826312s):  SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key' : [[]]"
time="2024-03-22T03:34:00Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:56.865124504 +0000 UTC m=+336.856772740) (total time: 3.55758083s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/traefik.io/ingressroutetcps/% 612]]"
time="2024-03-22T03:34:38Z" level=info msg="Slow SQL (started: 2024-03-22 03:34:12.019153656 +0000 UTC m=+352.010801886) (total time: 7.866887941s):  SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key' : [[]]"
time="2024-03-22T03:34:42Z" level=info msg="Slow SQL (started: 2024-03-22 03:34:24.436692871 +0000 UTC m=+364.428341109) (total time: 17.913010909s):  SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key' : [[]]"
time="2024-03-22T03:34:44Z" level=info msg="Slow SQL (started: 2024-03-22 03:34:21.224716645 +0000 UTC m=+361.216364907) (total time: 18.221862677s):  SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key' : [[]]"
time="2024-03-22T03:34:45Z" level=info msg="Slow SQL (started: 2024-03-22 03:34:11.395519427 +0000 UTC m=+351.387167680) (total time: 25.467300923s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/ingress/% 610]]"
time="2024-03-22T03:34:45Z" level=info msg="Slow SQL (started: 2024-03-22 03:34:20.92046246 +0000 UTC m=+360.912110705) (total time: 17.141333227s):  SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key' : [[]]"
time="2024-03-22T03:34:45Z" level=info msg="Slow SQL (started: 2024-03-22 03:34:21.988158518 +0000 UTC m=+361.979806749) (total time: 7.696533669s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? AND mkv.id <= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC LIMIT 1000 : [[/registry/mutatingwebhookconfigurations/% 613 false]]"
time="2024-03-22T03:34:45Z" level=info msg="Slow SQL (started: 2024-03-22 03:34:22.025477937 +0000 UTC m=+362.017126168) (total time: 7.834116422s):  SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key' : [[]]"
time="2024-03-22T03:34:45Z" level=info msg="Slow SQL (started: 2024-03-22 03:33:56.40777795 +0000 UTC m=+336.399426186) (total time: 49.394484092s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/k3s.cattle.io/addons/% false]]"
time="2024-03-22T03:34:45Z" level=info msg="Slow SQL (started: 2024-03-22 03:34:20.600182396 +0000 UTC m=+360.591830636) (total time: 21.214804228s):  SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key' : [[]]"
time="2024-03-22T03:34:46Z" level=info msg="Slow SQL (started: 2024-03-22 03:34:24.196780066 +0000 UTC m=+364.188428320) (total time: 22.495672607s):  SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key' : [[]]"
time="2024-03-22T03:34:48Z" level=info msg="Slow SQL (started: 2024-03-22 03:34:23.7726318 +0000 UTC m=+363.764280047) (total time: 22.313593889s):  SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key' : [[]]"
time="2024-03-22T03:34:48Z" level=error msg="error while range on /registry/minions/server : context deadline exceeded"
time="2024-03-22T03:34:48Z" level=info msg="Slow SQL (started: 2024-03-22 03:34:41.840667523 +0000 UTC m=+381.832315780) (total time: 6.553325902s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? AND mkv.id <= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC LIMIT 1000 : [[/registry/traefik.containo.us/tlsstores/% 613 false]]"
I0322 03:34:49.266315    2280 trace.go:236] Trace[2015263671]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:52.509) (total time: 54046ms):
Trace[2015263671]: ---"Objects listed" error:<nil> 54042ms (03:34:46.552)
Trace[2015263671]: [54.046038605s] [54.046038605s] END
E0322 03:34:49.287395    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/traefikservices?allowWatchBookmarks=true&resourceVersion=612&timeout=7m18s&timeoutSeconds=438&watch=true": http2: client connection lost
I0322 03:34:49.302793    2280 trace.go:236] Trace[1525316145]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:92688c04-12cf-4e9d-b0f9-469a571ebe09,client:127.0.0.1,protocol:HTTP/2.0,resource:rolebindings,scope:cluster,url:/apis/rbac.authorization.k8s.io/v1/rolebindings,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:33:59.948) (total time: 49352ms):
Trace[1525316145]: ---"About to List from storage" 2525ms (03:34:02.473)
Trace[1525316145]: ["cacher list" audit-id:92688c04-12cf-4e9d-b0f9-469a571ebe09,type:rolebindings.rbac.authorization.k8s.io 41415ms (03:34:07.885)
Trace[1525316145]:  ---"watchCache locked acquired" 15552ms (03:34:23.439)]
Trace[1525316145]: ---"Writing http response done" count:9 18289ms (03:34:49.300)
Trace[1525316145]: [49.352525897s] [49.352525897s] END
time="2024-03-22T03:34:49Z" level=info msg="Slow SQL (started: 2024-03-22 03:34:46.638944494 +0000 UTC m=+386.630592750) (total time: 2.703392427s):  SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key' : [[]]"
W0322 03:34:49.415027    2280 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Pod: Get "https://127.0.0.1:6443/api/v1/pods?resourceVersion=605": http2: client connection lost
E0322 03:34:49.415214    2280 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:34:49.416821    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:34:49.418081    2280 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 03:34:49.418833    2280 trace.go:236] Trace[972698766]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:983bb4de-cc0d-4e92-b82c-3fcdf13f733b,client:127.0.0.1,protocol:HTTP/2.0,resource:statefulsets,scope:cluster,url:/apis/apps/v1/statefulsets,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/scheduler,verb:LIST (22-Mar-2024 03:34:46.784) (total time: 2633ms):
Trace[972698766]: ---"Writing http response done" count:0 2625ms (03:34:49.418)
Trace[972698766]: [2.633579017s] [2.633579017s] END
W0322 03:34:49.420715    2280 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Endpoints: Get "https://127.0.0.1:6443/api/v1/namespaces/default/endpoints?fieldSelector=metadata.name%3Dkubernetes&resourceVersion=225": http2: client connection lost
I0322 03:34:49.421256    2280 trace.go:236] Trace[143035866]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.640) (total time: 55781ms):
Trace[143035866]: ---"Objects listed" error:Get "https://127.0.0.1:6443/api/v1/namespaces/default/endpoints?fieldSelector=metadata.name%3Dkubernetes&resourceVersion=225": http2: client connection lost 55780ms (03:34:49.420)
Trace[143035866]: [55.781064674s] [55.781064674s] END
E0322 03:34:49.422790    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Endpoints: failed to list *v1.Endpoints: Get "https://127.0.0.1:6443/api/v1/namespaces/default/endpoints?fieldSelector=metadata.name%3Dkubernetes&resourceVersion=225": http2: client connection lost
W0322 03:34:49.422927    2280 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://127.0.0.1:6443/api/v1/namespaces?resourceVersion=381": http2: client connection lost
I0322 03:34:49.423041    2280 trace.go:236] Trace[402693638]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.200) (total time: 56222ms):
Trace[402693638]: ---"Objects listed" error:Get "https://127.0.0.1:6443/api/v1/namespaces?resourceVersion=381": http2: client connection lost 56222ms (03:34:49.422)
Trace[402693638]: [56.222539784s] [56.222539784s] END
E0322 03:34:49.423086    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://127.0.0.1:6443/api/v1/namespaces?resourceVersion=381": http2: client connection lost
time="2024-03-22T03:34:49Z" level=info msg="Slow SQL (started: 2024-03-22 03:34:46.804493898 +0000 UTC m=+386.796142131) (total time: 2.618803757s):  SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key' : [[]]"
E0322 03:34:49.424856    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PodTemplate: Get "https://127.0.0.1:6444/api/v1/podtemplates?allowWatchBookmarks=true&resourceVersion=612&timeout=8m6s&timeoutSeconds=486&watch=true": http2: client connection lost
E0322 03:34:49.425876    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.DaemonSet: Get "https://127.0.0.1:6444/apis/apps/v1/daemonsets?allowWatchBookmarks=true&resourceVersion=612&timeout=8m31s&timeoutSeconds=511&watch=true": http2: client connection lost
W0322 03:34:49.438893    2280 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.NetworkPolicy: Get "https://127.0.0.1:6443/apis/networking.k8s.io/v1/networkpolicies?resourceVersion=381": http2: client connection lost
I0322 03:34:49.439360    2280 trace.go:236] Trace[33151873]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.487) (total time: 55951ms):
Trace[33151873]: ---"Objects listed" error:Get "https://127.0.0.1:6443/apis/networking.k8s.io/v1/networkpolicies?resourceVersion=381": http2: client connection lost 55951ms (03:34:49.438)
Trace[33151873]: [55.951428433s] [55.951428433s] END
E0322 03:34:49.439504    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.NetworkPolicy: failed to list *v1.NetworkPolicy: Get "https://127.0.0.1:6443/apis/networking.k8s.io/v1/networkpolicies?resourceVersion=381": http2: client connection lost
E0322 03:34:49.440444    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ClusterRole: Get "https://127.0.0.1:6444/apis/rbac.authorization.k8s.io/v1/clusterroles?allowWatchBookmarks=true&resourceVersion=612&timeout=8m12s&timeoutSeconds=492&watch=true": http2: client connection lost
E0322 03:34:49.446285    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Ingress: Get "https://127.0.0.1:6444/apis/networking.k8s.io/v1/ingresses?allowWatchBookmarks=true&resourceVersion=610&timeout=9m28s&timeoutSeconds=568&watch=true": http2: client connection lost
E0322 03:34:49.446579    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.NetworkPolicy: Get "https://127.0.0.1:6444/apis/networking.k8s.io/v1/networkpolicies?allowWatchBookmarks=true&resourceVersion=612&timeout=9m46s&timeoutSeconds=586&watch=true": http2: client connection lost
E0322 03:34:49.447494    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/k3s.cattle.io/v1/etcdsnapshotfiles?allowWatchBookmarks=true&resourceVersion=612&timeout=7m7s&timeoutSeconds=427&watch=true": http2: client connection lost
E0322 03:34:49.447599    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/ingressroutes?allowWatchBookmarks=true&resourceVersion=612&timeout=9m19s&timeoutSeconds=559&watch=true": http2: client connection lost
E0322 03:34:49.448164    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1beta3.FlowSchema: Get "https://127.0.0.1:6444/apis/flowcontrol.apiserver.k8s.io/v1beta3/flowschemas?allowWatchBookmarks=true&resourceVersion=612&timeout=8m33s&timeoutSeconds=513&watch=true": http2: client connection lost
E0322 03:34:49.448325    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.containo.us/v1alpha1/serverstransports?allowWatchBookmarks=true&resourceVersion=612&timeout=6m8s&timeoutSeconds=368&watch=true": http2: client connection lost
E0322 03:34:49.448457    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ReplicationController: Get "https://127.0.0.1:6444/api/v1/replicationcontrollers?allowWatchBookmarks=true&resourceVersion=613&timeout=8m18s&timeoutSeconds=498&watch=true": http2: client connection lost
E0322 03:34:49.448623    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.containo.us/v1alpha1/ingressrouteudps?allowWatchBookmarks=true&resourceVersion=612&timeout=5m55s&timeoutSeconds=355&watch=true": http2: client connection lost
E0322 03:34:49.449543    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.CSIStorageCapacity: Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/csistoragecapacities?allowWatchBookmarks=true&resourceVersion=612&timeout=7m32s&timeoutSeconds=452&watch=true": http2: client connection lost
E0322 03:34:49.450107    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PodDisruptionBudget: Get "https://127.0.0.1:6444/apis/policy/v1/poddisruptionbudgets?allowWatchBookmarks=true&resourceVersion=612&timeout=6m23s&timeoutSeconds=383&watch=true": http2: client connection lost
E0322 03:34:49.450837    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Service: Get "https://127.0.0.1:6444/api/v1/services?allowWatchBookmarks=true&resourceVersion=612&timeout=7m25s&timeoutSeconds=445&watch=true": http2: client connection lost
E0322 03:34:49.450968    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PersistentVolumeClaim: Get "https://127.0.0.1:6444/api/v1/persistentvolumeclaims?allowWatchBookmarks=true&resourceVersion=612&timeout=7m26s&timeoutSeconds=446&watch=true": http2: client connection lost
E0322 03:34:49.451116    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/serverstransporttcps?allowWatchBookmarks=true&resourceVersion=612&timeout=6m57s&timeoutSeconds=417&watch=true": http2: client connection lost
E0322 03:34:49.451318    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.CSIDriver: Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/csidrivers?allowWatchBookmarks=true&resourceVersion=610&timeout=9m48s&timeoutSeconds=588&watch=true": http2: client connection lost
E0322 03:34:49.451440    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.StatefulSet: Get "https://127.0.0.1:6444/apis/apps/v1/statefulsets?allowWatchBookmarks=true&resourceVersion=612&timeout=7m29s&timeoutSeconds=449&watch=true": http2: client connection lost
E0322 03:34:49.451580    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ReplicaSet: Get "https://127.0.0.1:6444/apis/apps/v1/replicasets?allowWatchBookmarks=true&resourceVersion=612&timeout=5m48s&timeoutSeconds=348&watch=true": http2: client connection lost
E0322 03:34:49.451893    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/middlewares?allowWatchBookmarks=true&resourceVersion=612&timeout=6m9s&timeoutSeconds=369&watch=true": http2: client connection lost
E0322 03:34:49.452301    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Role: Get "https://127.0.0.1:6444/apis/rbac.authorization.k8s.io/v1/roles?allowWatchBookmarks=true&resourceVersion=612&timeout=5m23s&timeoutSeconds=323&watch=true": http2: client connection lost
E0322 03:34:49.452454    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.StorageClass: Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/storageclasses?allowWatchBookmarks=true&resourceVersion=613&timeout=7m0s&timeoutSeconds=420&watch=true": http2: client connection lost
E0322 03:34:49.452650    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/helm.cattle.io/v1/helmchartconfigs?allowWatchBookmarks=true&resourceVersion=610&timeout=7m35s&timeoutSeconds=455&watch=true": http2: client connection lost
E0322 03:34:49.452771    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.containo.us/v1alpha1/middlewaretcps?allowWatchBookmarks=true&resourceVersion=612&timeout=9m51s&timeoutSeconds=591&watch=true": http2: client connection lost
E0322 03:34:49.453483    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.VolumeAttachment: Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/volumeattachments?allowWatchBookmarks=true&resourceVersion=612&timeout=9m5s&timeoutSeconds=545&watch=true": http2: client connection lost
E0322 03:34:49.455385    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.containo.us/v1alpha1/middlewares?allowWatchBookmarks=true&resourceVersion=612&timeout=5m9s&timeoutSeconds=309&watch=true": http2: client connection lost
E0322 03:34:49.455591    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Secret: Get "https://127.0.0.1:6444/api/v1/secrets?allowWatchBookmarks=true&resourceVersion=612&timeout=6m55s&timeoutSeconds=415&watch=true": http2: client connection lost
E0322 03:34:49.455862    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.CSINode: Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/csinodes?allowWatchBookmarks=true&resourceVersion=612&timeout=6m5s&timeoutSeconds=365&watch=true": http2: client connection lost
E0322 03:34:49.456893    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ConfigMap: Get "https://127.0.0.1:6444/api/v1/configmaps?allowWatchBookmarks=true&resourceVersion=613&timeout=5m46s&timeoutSeconds=346&watch=true": http2: client connection lost
E0322 03:34:49.457036    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PersistentVolume: Get "https://127.0.0.1:6444/api/v1/persistentvolumes?allowWatchBookmarks=true&resourceVersion=612&timeout=6m49s&timeoutSeconds=409&watch=true": http2: client connection lost
E0322 03:34:49.457319    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/middlewaretcps?allowWatchBookmarks=true&resourceVersion=612&timeout=6m22s&timeoutSeconds=382&watch=true": http2: client connection lost
E0322 03:34:49.457462    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ResourceQuota: Get "https://127.0.0.1:6444/api/v1/resourcequotas?allowWatchBookmarks=true&resourceVersion=612&timeout=9m5s&timeoutSeconds=545&watch=true": http2: client connection lost
E0322 03:34:49.457653    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.RuntimeClass: Get "https://127.0.0.1:6444/apis/node.k8s.io/v1/runtimeclasses?allowWatchBookmarks=true&resourceVersion=612&timeout=7m29s&timeoutSeconds=449&watch=true": http2: client connection lost
E0322 03:34:49.458539    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.CertificateSigningRequest: Get "https://127.0.0.1:6444/apis/certificates.k8s.io/v1/certificatesigningrequests?allowWatchBookmarks=true&resourceVersion=612&timeout=8m25s&timeoutSeconds=505&watch=true": http2: client connection lost
E0322 03:34:49.458665    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.containo.us/v1alpha1/tlsoptions?allowWatchBookmarks=true&resourceVersion=612&timeout=8m17s&timeoutSeconds=497&watch=true": http2: client connection lost
E0322 03:34:49.459779    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v2.HorizontalPodAutoscaler: Get "https://127.0.0.1:6444/apis/autoscaling/v2/horizontalpodautoscalers?allowWatchBookmarks=true&resourceVersion=612&timeout=7m15s&timeoutSeconds=435&watch=true": http2: client connection lost
E0322 03:34:49.459899    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Endpoints: Get "https://127.0.0.1:6444/api/v1/endpoints?allowWatchBookmarks=true&resourceVersion=612&timeout=9m44s&timeoutSeconds=584&watch=true": http2: client connection lost
W0322 03:34:49.460214    2280 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.containo.us/v1alpha1/tlsstores?resourceVersion=590": http2: client connection lost
I0322 03:34:49.460392    2280 trace.go:236] Trace[822867802]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.230) (total time: 56229ms):
Trace[822867802]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/traefik.containo.us/v1alpha1/tlsstores?resourceVersion=590": http2: client connection lost 56229ms (03:34:49.460)
Trace[822867802]: [56.229735778s] [56.229735778s] END
E0322 03:34:49.460476    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.containo.us/v1alpha1/tlsstores?resourceVersion=590": http2: client connection lost
E0322 03:34:49.460601    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Lease: Get "https://127.0.0.1:6444/apis/coordination.k8s.io/v1/leases?allowWatchBookmarks=true&resourceVersion=612&timeout=6m24s&timeoutSeconds=384&watch=true": http2: client connection lost
E0322 03:34:49.460836    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/k3s.cattle.io/v1/addons?allowWatchBookmarks=true&resourceVersion=612&timeout=6m30s&timeoutSeconds=390&watch=true": http2: client connection lost
E0322 03:34:49.462066    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.MutatingWebhookConfiguration: Get "https://127.0.0.1:6444/apis/admissionregistration.k8s.io/v1/mutatingwebhookconfigurations?allowWatchBookmarks=true&resourceVersion=612&timeout=7m21s&timeoutSeconds=441&watch=true": http2: client connection lost
E0322 03:34:49.462195    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/apiextensions.k8s.io/v1/customresourcedefinitions?allowWatchBookmarks=true&resourceVersion=612&timeout=5m48s&timeoutSeconds=348&watch=true": http2: client connection lost
E0322 03:34:49.462337    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/serverstransports?allowWatchBookmarks=true&resourceVersion=612&timeout=8m34s&timeoutSeconds=514&watch=true": http2: client connection lost
E0322 03:34:49.462973    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PriorityClass: Get "https://127.0.0.1:6444/apis/scheduling.k8s.io/v1/priorityclasses?allowWatchBookmarks=true&resourceVersion=612&timeout=9m38s&timeoutSeconds=578&watch=true": http2: client connection lost
E0322 03:34:49.463078    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Job: Get "https://127.0.0.1:6444/apis/batch/v1/jobs?allowWatchBookmarks=true&resourceVersion=612&timeout=7m34s&timeoutSeconds=454&watch=true": http2: client connection lost
E0322 03:34:49.463261    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.LimitRange: Get "https://127.0.0.1:6444/api/v1/limitranges?allowWatchBookmarks=true&resourceVersion=612&timeout=9m0s&timeoutSeconds=540&watch=true": http2: client connection lost
E0322 03:34:49.463365    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/ingressrouteudps?allowWatchBookmarks=true&resourceVersion=612&timeout=9m8s&timeoutSeconds=548&watch=true": http2: client connection lost
W0322 03:34:49.463625    2280 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/helm.cattle.io/v1/helmcharts?resourceVersion=381": http2: client connection lost
I0322 03:34:49.463847    2280 trace.go:236] Trace[1981984441]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.387) (total time: 56076ms):
Trace[1981984441]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/helm.cattle.io/v1/helmcharts?resourceVersion=381": http2: client connection lost 56076ms (03:34:49.463)
Trace[1981984441]: [56.076198101s] [56.076198101s] END
E0322 03:34:49.463926    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/helm.cattle.io/v1/helmcharts?resourceVersion=381": http2: client connection lost
E0322 03:34:49.464344    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ControllerRevision: Get "https://127.0.0.1:6444/apis/apps/v1/controllerrevisions?allowWatchBookmarks=true&resourceVersion=610&timeout=5m10s&timeoutSeconds=310&watch=true": http2: client connection lost
E0322 03:34:49.464623    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/tlsstores?allowWatchBookmarks=true&resourceVersion=612&timeout=9m46s&timeoutSeconds=586&watch=true": http2: client connection lost
E0322 03:34:49.464757    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/tlsoptions?allowWatchBookmarks=true&resourceVersion=613&timeout=5m53s&timeoutSeconds=353&watch=true": http2: client connection lost
E0322 03:34:49.464988    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: Get "https://127.0.0.1:6444/api/v1/namespaces?allowWatchBookmarks=true&resourceVersion=612&timeout=8m47s&timeoutSeconds=527&watch=true": http2: client connection lost
E0322 03:34:49.465119    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.IngressClass: Get "https://127.0.0.1:6444/apis/networking.k8s.io/v1/ingressclasses?allowWatchBookmarks=true&resourceVersion=612&timeout=7m53s&timeoutSeconds=473&watch=true": http2: client connection lost
E0322 03:34:49.465364    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ServiceAccount: Get "https://127.0.0.1:6444/api/v1/serviceaccounts?allowWatchBookmarks=true&resourceVersion=612&timeout=9m56s&timeoutSeconds=596&watch=true": http2: client connection lost
E0322 03:34:49.465465    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: Get "https://127.0.0.1:6444/apis/discovery.k8s.io/v1/endpointslices?allowWatchBookmarks=true&resourceVersion=613&timeout=5m7s&timeoutSeconds=307&watch=true": http2: client connection lost
I0322 03:34:49.465567    2280 trace.go:236] Trace[737884704]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:049572ed-9512-45e6-a8c0-37c439b1cde0,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-7zoch227uv4owxg75pgtnmv3jq,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PUT (22-Mar-2024 03:33:58.940) (total time: 48609ms):
Trace[737884704]: ---"limitedReadBody succeeded" len:590 468ms (03:33:59.409)
Trace[737884704]: ---"Write to database call failed" len:590,err:Timeout: request did not complete within requested timeout - context deadline exceeded 43984ms (03:34:43.401)
Trace[737884704]: [48.609507302s] [48.609507302s] END
E0322 03:34:49.466233    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.RoleBinding: Get "https://127.0.0.1:6444/apis/rbac.authorization.k8s.io/v1/rolebindings?allowWatchBookmarks=true&resourceVersion=612&timeout=6m48s&timeoutSeconds=408&watch=true": http2: client connection lost
E0322 03:34:49.466464    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ValidatingWebhookConfiguration: Get "https://127.0.0.1:6444/apis/admissionregistration.k8s.io/v1/validatingwebhookconfigurations?allowWatchBookmarks=true&resourceVersion=610&timeout=7m35s&timeoutSeconds=455&watch=true": http2: client connection lost
E0322 03:34:49.466728    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Deployment: Get "https://127.0.0.1:6444/apis/apps/v1/deployments?allowWatchBookmarks=true&resourceVersion=612&timeout=7m8s&timeoutSeconds=428&watch=true": http2: client connection lost
E0322 03:34:49.466832    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.containo.us/v1alpha1/ingressroutes?allowWatchBookmarks=true&resourceVersion=612&timeout=6m35s&timeoutSeconds=395&watch=true": http2: client connection lost
E0322 03:34:49.467040    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/ingressroutetcps?allowWatchBookmarks=true&resourceVersion=612&timeout=9m0s&timeoutSeconds=540&watch=true": http2: client connection lost
E0322 03:34:49.494018    2280 timeout.go:142] post-timeout activity - time-elapsed: 57.718388936s, PUT "/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/server" result: <nil>
E0322 03:34:49.517085    2280 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:34:49.524970    2280 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:34:49.526489    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:34:49.528186    2280 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 03:34:49.530917    2280 trace.go:236] Trace[93824952]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:13540657-ed57-44c3-afc1-02ebe8dc2ef6,client:10.42.0.3,protocol:HTTP/2.0,resource:namespaces,scope:cluster,url:/api/v1/namespaces,user-agent:coredns/v0.0.0 (linux/amd64) kubernetes/$Format,verb:LIST (22-Mar-2024 03:34:46.896) (total time: 2631ms):
Trace[93824952]: ---"Writing http response done" count:4 2630ms (03:34:49.528)
Trace[93824952]: [2.631414456s] [2.631414456s] END
E0322 03:34:49.537726    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:34:49.537920    2280 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 03:34:49.539027    2280 trace.go:236] Trace[1113723871]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:236be78e-3763-4e04-89ce-2a6daf6f3c34,client:10.42.0.3,protocol:HTTP/2.0,resource:services,scope:cluster,url:/api/v1/services,user-agent:coredns/v0.0.0 (linux/amd64) kubernetes/$Format,verb:LIST (22-Mar-2024 03:34:46.957) (total time: 2581ms):
Trace[1113723871]: [2.581771455s] [2.581771455s] END
E0322 03:34:49.578947    2280 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:34:49.579269    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:34:49.611786    2280 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 03:34:49.614761    2280 trace.go:236] Trace[1403064935]: "SerializeObject" audit-id:bab1e3f9-97f4-48bf-9a51-185153c89127,method:GET,url:/api,protocol:HTTP/2.0,mediaType:application/json;g=apidiscovery.k8s.io;v=v2beta1;as=APIGroupDiscoveryList,encoder:{"encodeGV":"apidiscovery.k8s.io/v2beta1","encoder":"{\"name\":\"json\",\"pretty\":\"false\",\"strict\":\"false\",\"yaml\":\"false\"}","name":"versioning"} (22-Mar-2024 03:34:07.224) (total time: 42387ms):
Trace[1403064935]: ---"About to start writing response" size:6969 38638ms (03:34:45.863)
Trace[1403064935]: ---"Write call failed" writer:responsewriter.outerWithCloseNotifyAndFlush,size:6969,firstWrite:true,err:http: Handler timeout 2565ms (03:34:48.429)
Trace[1403064935]: ---"About to start writing response" size:114 1181ms (03:34:49.611)
Trace[1403064935]: [42.387192086s] [42.387192086s] END
I0322 03:34:49.640194    2280 trace.go:236] Trace[445628256]: "iptables ChainExists" (22-Mar-2024 03:34:25.694) (total time: 23946ms):
Trace[445628256]: [23.946078726s] [23.946078726s] END
I0322 03:34:49.675678    2280 garbagecollector.go:818] "failed to discover preferred resources" error="Get \"https://127.0.0.1:6444/api\": http2: client connection lost"
W0322 03:34:49.750078    2280 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.StatefulSet: Get "https://127.0.0.1:6444/apis/apps/v1/statefulsets?resourceVersion=225": http2: client connection lost
I0322 03:34:49.753884    2280 trace.go:236] Trace[1817318105]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.734) (total time: 56017ms):
Trace[1817318105]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/apps/v1/statefulsets?resourceVersion=225": http2: client connection lost 53574ms (03:34:47.308)
Trace[1817318105]: [56.017016553s] [56.017016553s] END
E0322 03:34:49.756675    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://127.0.0.1:6444/apis/apps/v1/statefulsets?resourceVersion=225": http2: client connection lost
E0322 03:34:47.339472    2280 event.go:289] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"helm-install-traefik-fxsb6.17bef882108e61b3", GenerateName:"", Namespace:"kube-system", SelfLink:"", UID:"", ResourceVersion:"515", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"helm-install-traefik-fxsb6", UID:"f5749ede-2796-414e-b0ea-d6e2523ec959", APIVersion:"v1", ResourceVersion:"458", FieldPath:"spec.containers{helm}"}, Reason:"Pulled", Message:"Container image \"rancher/klipper-helm:v0.8.2-build20230815\" already present on machine", Source:v1.EventSource{Component:"kubelet", Host:"server"}, FirstTimestamp:time.Date(2024, time.March, 22, 3, 29, 5, 0, time.Local), LastTimestamp:time.Date(2024, time.March, 22, 3, 29, 24, 223409655, time.Local), Count:2, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"kubelet", ReportingInstance:"server"}': 'Patch "https://127.0.0.1:6443/api/v1/namespaces/kube-system/events/helm-install-traefik-fxsb6.17bef882108e61b3": http2: client connection lost'(may retry after sleeping)
E0322 03:34:49.811102    2280 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:34:49.813009    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:34:49.813856    2280 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 03:34:49.816515    2280 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 03:34:49.818792    2280 trace.go:236] Trace[143853584]: "SerializeObject" audit-id:5894692a-93c0-467b-ad37-9c3009adbb03,method:PUT,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/server,protocol:HTTP/2.0,mediaType:application/vnd.kubernetes.protobuf,encoder:{"encodeGV":"coordination.k8s.io/v1","encoder":"protobuf","name":"versioning"} (22-Mar-2024 03:34:27.494) (total time: 22321ms):
Trace[143853584]: ---"About to start writing response" size:154 17841ms (03:34:45.336)
Trace[143853584]: ---"About to start writing response" size:69 4479ms (03:34:49.816)
Trace[143853584]: [22.321878317s] [22.321878317s] END
I0322 03:34:49.819735    2280 trace.go:236] Trace[298736760]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:5894692a-93c0-467b-ad37-9c3009adbb03,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/server,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PUT (22-Mar-2024 03:33:59.376) (total time: 50442ms):
Trace[298736760]: ---"Write to database call failed" len:465,err:Timeout: request did not complete within requested timeout - context deadline exceeded 6834ms (03:34:12.517)
Trace[298736760]: [50.442742449s] [50.442742449s] END
I0322 03:34:49.858699    2280 trace.go:236] Trace[562813978]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.661) (total time: 54391ms):
Trace[562813978]: ---"Objects listed" error:<nil> 54388ms (03:34:48.049)
Trace[562813978]: [54.391840887s] [54.391840887s] END
time="2024-03-22T03:34:49Z" level=info msg="Slow SQL (started: 2024-03-22 03:34:27.522655611 +0000 UTC m=+367.514303860) (total time: 20.630962214s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? AND mkv.id <= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC LIMIT 1000 : [[/registry/traefik.containo.us/tlsoptions/% 613 false]]"
time="2024-03-22T03:34:49Z" level=info msg="Slow SQL (started: 2024-03-22 03:34:45.58516698 +0000 UTC m=+385.576815232) (total time: 2.701824913s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC  : [[/registry/events/kube-system/helm-install-traefik-fxsb6.17bef882108e61b3 false]]"
I0322 03:34:49.419022    2280 trace.go:236] Trace[409486839]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.305) (total time: 56110ms):
Trace[409486839]: ---"Objects listed" error:Get "https://127.0.0.1:6443/api/v1/pods?resourceVersion=605": http2: client connection lost 56106ms (03:34:49.411)
Trace[409486839]: [56.110506944s] [56.110506944s] END
E0322 03:34:50.001096    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://127.0.0.1:6443/api/v1/pods?resourceVersion=605": http2: client connection lost
E0322 03:34:50.006052    2280 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 03:34:50.006304    2280 trace.go:236] Trace[1120262994]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:c62b3b79-394c-4d75-b2ae-2fbac347322f,client:127.0.0.1,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events/helm-install-traefik-fxsb6.17bef882108e61b3,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PATCH (22-Mar-2024 03:33:59.452) (total time: 48866ms):
Trace[1120262994]: ["GuaranteedUpdate etcd3" audit-id:c62b3b79-394c-4d75-b2ae-2fbac347322f,key:/events/kube-system/helm-install-traefik-fxsb6.17bef882108e61b3,type:*core.Event,resource:events 48857ms (03:33:59.462)]
Trace[1120262994]: [48.866539727s] [48.866539727s] END
I0322 03:34:50.007263    2280 trace.go:236] Trace[697155723]: "List" accept:application/json, */*,audit-id:e1ed841d-9f7c-4007-a5c1-6bfc783580b9,client:10.42.0.5,protocol:HTTP/2.0,resource:storageclasses,scope:cluster,url:/apis/storage.k8s.io/v1/storageclasses,user-agent:local-path-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:LIST (22-Mar-2024 03:34:47.011) (total time: 2995ms):
Trace[697155723]: ---"Writing http response done" count:1 2978ms (03:34:50.006)
Trace[697155723]: [2.995423249s] [2.995423249s] END
I0322 03:34:49.818846    2280 trace.go:236] Trace[1941838378]: "List" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json,audit-id:be8d1cb5-955c-4876-9a34-e94e24f6b31c,client:127.0.0.1,protocol:HTTP/2.0,resource:helmcharts,scope:cluster,url:/apis/helm.cattle.io/v1/helmcharts,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/metadata-informers,verb:LIST (22-Mar-2024 03:33:59.612) (total time: 50203ms):
Trace[1941838378]: ---"Writing http response done" count:2 50203ms (03:34:49.815)
Trace[1941838378]: [50.203451738s] [50.203451738s] END
E0322 03:34:50.043017    2280 timeout.go:142] post-timeout activity - time-elapsed: 29.855201188s, PUT "/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/server" result: <nil>
E0322 03:34:50.047806    2280 writers.go:122] apiserver was unable to write a JSON response: client disconnected
E0322 03:34:50.050020    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"client disconnected"}: client disconnected
E0322 03:34:50.051483    2280 writers.go:135] apiserver was unable to write a fallback JSON response: client disconnected
I0322 03:34:50.051653    2280 trace.go:236] Trace[665351710]: "List" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:301df4c7-cbd4-43fc-824c-e948f771d946,client:127.0.0.1,protocol:HTTP/2.0,resource:pods,scope:cluster,url:/api/v1/pods,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:33:59.258) (total time: 50793ms):
Trace[665351710]: ["cacher list" audit-id:301df4c7-cbd4-43fc-824c-e948f771d946,type:pods 49680ms (03:34:00.371)
Trace[665351710]:  ---"watchCache locked acquired" 3545ms (03:34:03.918)
Trace[665351710]:  ---"Listed items from cache" count:5 5577ms (03:34:09.497)
Trace[665351710]:  ---"Resized result" 6801ms (03:34:16.299)]
Trace[665351710]: ---"Listing from storage done" 8056ms (03:34:32.873)
Trace[665351710]: ---"Writing http response done" count:5 17177ms (03:34:50.051)
Trace[665351710]: [50.793202108s] [50.793202108s] END
E0322 03:34:50.108776    2280 wrap.go:54] timeout or abort while handling: method=PATCH URI="/api/v1/namespaces/kube-system/events/helm-install-traefik-fxsb6.17bef882108e61b3" audit-ID="c62b3b79-394c-4d75-b2ae-2fbac347322f"
E0322 03:34:50.110462    2280 timeout.go:142] post-timeout activity - time-elapsed: 25.846s, PATCH "/api/v1/namespaces/kube-system/events/helm-install-traefik-fxsb6.17bef882108e61b3" result: <nil>
E0322 03:34:50.111800    2280 wrap.go:54] timeout or abort while handling: method=GET URI="/api/v1/pods?fieldSelector=spec.nodeName%3Dserver&resourceVersion=610" audit-ID="301df4c7-cbd4-43fc-824c-e948f771d946"
E0322 03:34:50.112632    2280 wrap.go:54] timeout or abort while handling: method=GET URI="/api/v1/nodes?resourceVersion=481" audit-ID="31c4b7ee-2930-4ee3-9f59-19cead79ddf8"
E0322 03:34:50.113277    2280 wrap.go:54] timeout or abort while handling: method=GET URI="/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dcoredns&resourceVersion=423" audit-ID="6390ea64-5389-43fe-b150-f47a41eb0892"
I0322 03:34:50.253167    2280 trace.go:236] Trace[509846261]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.461) (total time: 56785ms):
Trace[509846261]: ---"Objects listed" error:<nil> 56783ms (03:34:50.245)
Trace[509846261]: [56.785160553s] [56.785160553s] END
E0322 03:34:50.282647    2280 writers.go:122] apiserver was unable to write a JSON response: client disconnected
E0322 03:34:50.284344    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"client disconnected"}: client disconnected
E0322 03:34:50.286822    2280 writers.go:135] apiserver was unable to write a fallback JSON response: client disconnected
I0322 03:34:50.286890    2280 trace.go:236] Trace[1360741974]: "List" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:18a399fd-bcea-43d8-bdc8-c8d0f6e874e8,client:127.0.0.1,protocol:HTTP/2.0,resource:runtimeclasses,scope:cluster,url:/apis/node.k8s.io/v1/runtimeclasses,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:34:45.348) (total time: 4938ms):
Trace[1360741974]: ["cacher list" audit-id:18a399fd-bcea-43d8-bdc8-c8d0f6e874e8,type:runtimeclasses.node.k8s.io 4918ms (03:34:45.368)
Trace[1360741974]:  ---"watchCache locked acquired" 2656ms (03:34:48.024)]
Trace[1360741974]: ---"Writing http response done" count:10 2259ms (03:34:50.286)
Trace[1360741974]: [4.93882971s] [4.93882971s] END
E0322 03:34:50.302968    2280 timeout.go:142] post-timeout activity - time-elapsed: 93.481443ms, GET "/api/v1/namespaces" result: <nil>
E0322 03:34:50.392720    2280 server.go:310] "Unable to authenticate the request due to an error" err="context canceled"
E0322 03:34:50.394971    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Node: failed to list *v1.Node: unexpected error when reading response body. Please retry. Original error: http2: client connection lost
E0322 03:34:50.396495    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
E0322 03:34:50.413384    2280 wrap.go:54] timeout or abort while handling: method=GET URI="/apis/node.k8s.io/v1/runtimeclasses?resourceVersion=610" audit-ID="18a399fd-bcea-43d8-bdc8-c8d0f6e874e8"
E0322 03:34:50.430182    2280 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 03:34:50.431800    2280 trace.go:236] Trace[564425351]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:582f0ecd-9c4d-45b9-af29-4ac66abfb9d1,client:127.0.0.1,protocol:HTTP/2.0,resource:tokenreviews,scope:resource,url:/apis/authentication.k8s.io/v1/tokenreviews,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:POST (22-Mar-2024 03:33:59.415) (total time: 51015ms):
Trace[564425351]: ---"Write to database call failed" len:1152,err:Timeout: request did not complete within requested timeout - context deadline exceeded 39751ms (03:34:39.185)
Trace[564425351]: [51.015641457s] [51.015641457s] END
E0322 03:34:50.432744    2280 timeout.go:142] post-timeout activity - time-elapsed: 251.790217ms, POST "/apis/authentication.k8s.io/v1/tokenreviews" result: <nil>
E0322 03:34:50.435966    2280 wrap.go:54] timeout or abort while handling: method=GET URI="/apis/rbac.authorization.k8s.io/v1/rolebindings?resourceVersion=610" audit-ID="92688c04-12cf-4e9d-b0f9-469a571ebe09"
E0322 03:34:50.446283    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
E0322 03:34:50.448290    2280 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:34:50.448494    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:34:50.450894    2280 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 03:34:50.451260    2280 trace.go:236] Trace[863603732]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:752533e9-44e2-4b31-b01c-0e9113a5290d,client:127.0.0.1,protocol:HTTP/2.0,resource:services,scope:cluster,url:/api/v1/services,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:33:59.437) (total time: 51013ms):
Trace[863603732]: ["List(recursive=true) etcd3" audit-id:752533e9-44e2-4b31-b01c-0e9113a5290d,key:/services/specs,resourceVersion:,resourceVersionMatch:,limit:0,continue: 51010ms (03:33:59.440)]
Trace[863603732]: [51.013855926s] [51.013855926s] END
E0322 03:34:50.454187    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
E0322 03:34:50.454392    2280 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:34:50.454744    2280 wrap.go:54] timeout or abort while handling: method=GET URI="/api/v1/pods?resourceVersion=610" audit-ID="b2cb2838-0a17-428d-82e9-631b36d4d7a1"
E0322 03:34:50.463078    2280 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:34:50.463235    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:34:50.464843    2280 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 03:34:50.464960    2280 trace.go:236] Trace[741485602]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:9d3b1593-6559-4967-9ea7-b994c58a3dd9,client:127.0.0.1,protocol:HTTP/2.0,resource:apiservices,scope:resource,url:/apis/apiregistration.k8s.io/v1/apiservices/v1beta1.metrics.k8s.io/status,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PUT (22-Mar-2024 03:34:48.950) (total time: 1514ms):
Trace[741485602]: ---"limitedReadBody succeeded" len:2079 113ms (03:34:49.063)
Trace[741485602]: ---"Write to database call failed" len:2079,err:Timeout: request did not complete within requested timeout - context canceled 1138ms (03:34:50.456)
Trace[741485602]: [1.514358997s] [1.514358997s] END
E0322 03:34:50.465196    2280 timeout.go:142] post-timeout activity - time-elapsed: 10.213888ms, PUT "/apis/apiregistration.k8s.io/v1/apiservices/v1beta1.metrics.k8s.io/status" result: <nil>
E0322 03:34:50.465401    2280 timeout.go:142] post-timeout activity - time-elapsed: 4.703849072s, GET "/api" result: <nil>
I0322 03:34:50.467729    2280 trace.go:236] Trace[130235424]: "List" accept:application/json, */*,audit-id:12c5f47a-9135-4a39-89dd-1f8dd6baaa32,client:127.0.0.1,protocol:HTTP/2.0,resource:helmcharts,scope:cluster,url:/apis/helm.cattle.io/v1/helmcharts,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:LIST (22-Mar-2024 03:34:11.913) (total time: 38554ms):
Trace[130235424]: ---"About to List from storage" 35739ms (03:34:47.653)
Trace[130235424]: ---"Writing http response done" count:2 2807ms (03:34:50.467)
Trace[130235424]: [38.554017428s] [38.554017428s] END
E0322 03:34:50.468858    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:34:50.468887    2280 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 03:34:50.470195    2280 trace.go:236] Trace[820064263]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:1c6f0a6d-5105-48cf-8f1d-bf85c506cc41,client:127.0.0.1,protocol:HTTP/2.0,resource:resourcequotas,scope:namespace,url:/api/v1/namespaces/kube-system/resourcequotas,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:34:48.557) (total time: 1912ms):
Trace[820064263]: ["List(recursive=true) etcd3" audit-id:1c6f0a6d-5105-48cf-8f1d-bf85c506cc41,key:/resourcequotas/kube-system,resourceVersion:,resourceVersionMatch:,limit:0,continue: 1908ms (03:34:48.562)]
Trace[820064263]: [1.912499796s] [1.912499796s] END
I0322 03:34:50.484919    2280 trace.go:236] Trace[1875719470]: "List" accept:application/json, */*,audit-id:9eec9727-784d-4fa3-8666-73d38198858b,client:127.0.0.1,protocol:HTTP/2.0,resource:secrets,scope:cluster,url:/api/v1/secrets,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:LIST (22-Mar-2024 03:34:47.053) (total time: 3431ms):
Trace[1875719470]: ---"Writing http response done" count:5 3414ms (03:34:50.484)
Trace[1875719470]: [3.431442893s] [3.431442893s] END
E0322 03:34:50.504922    2280 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:34:50.505117    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:34:50.506390    2280 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 03:34:50.507659    2280 trace.go:236] Trace[198943468]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:1ffbf0cd-a5de-4ad7-9795-ab0f9259f0b7,client:127.0.0.1,protocol:HTTP/2.0,resource:customresourcedefinitions,scope:cluster,url:/apis/apiextensions.k8s.io/v1/customresourcedefinitions,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:34:00.116) (total time: 50389ms):
Trace[198943468]: ["cacher list" audit-id:1ffbf0cd-a5de-4ad7-9795-ab0f9259f0b7,type:customresourcedefinitions.apiextensions.k8s.io 47535ms (03:34:02.971)
Trace[198943468]:  ---"watchCache locked acquired" 11660ms (03:34:14.636)
Trace[198943468]:  ---"watchCache fresh enough" 8759ms (03:34:23.395)
Trace[198943468]:  ---"Listed items from cache" count:23 18897ms (03:34:42.293)
Trace[198943468]:  ---"Resized result" 3854ms (03:34:46.147)]
Trace[198943468]: ---"Writing http response done" count:23 4354ms (03:34:50.506)
Trace[198943468]: [50.389942414s] [50.389942414s] END
time="2024-03-22T03:34:50Z" level=info msg="Slow SQL (started: 2024-03-22 03:34:42.402818877 +0000 UTC m=+382.394467126) (total time: 3.845461279s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? AND mkv.id <= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC LIMIT 1000 : [[/registry/persistentvolumeclaims/% 613 false]]"
E0322 03:34:50.628641    2280 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 03:34:50.632423    2280 trace.go:236] Trace[1239010801]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:87b7c24a-5b30-477c-8295-601551a5e906,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/server,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:33:56.496) (total time: 54134ms):
Trace[1239010801]: [54.134109619s] [54.134109619s] END
E0322 03:34:50.751414    2280 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.containo.us/v1alpha1/ingressroutetcps?allowWatchBookmarks=true&resourceVersion=612&timeout=8m10s&timeoutSeconds=490&watch=true": http2: client connection lost
E0322 03:34:49.914837    2280 kubelet.go:2477] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="5m23.273s"
E0322 03:34:50.892881    2280 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:34:50.894643    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:34:50.898358    2280 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 03:34:50.900206    2280 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:34:50.900296    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:34:50.911892    2280 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:34:50.912034    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:34:50.923606    2280 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 03:34:50.925435    2280 trace.go:236] Trace[1599301493]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:7b292a21-ae48-473b-ada3-add08efef820,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/server/status,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:node-controller,verb:PUT (22-Mar-2024 03:34:26.081) (total time: 24842ms):
Trace[1599301493]: ---"limitedReadBody succeeded" len:4071 21485ms (03:34:47.567)
Trace[1599301493]: ---"Conversion done" 1908ms (03:34:49.481)
Trace[1599301493]: ---"Write to database call failed" len:4071,err:Timeout: request did not complete within requested timeout - context canceled 778ms (03:34:50.901)
Trace[1599301493]: [24.84204324s] [24.84204324s] END
E0322 03:34:50.934135    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:34:50.934281    2280 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 03:34:50.936088    2280 trace.go:236] Trace[1374106364]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:95d74788-e3ea-47a2-8775-29810ec829ef,client:10.42.0.3,protocol:HTTP/2.0,resource:endpointslices,scope:cluster,url:/apis/discovery.k8s.io/v1/endpointslices,user-agent:coredns/v0.0.0 (linux/amd64) kubernetes/$Format,verb:LIST (22-Mar-2024 03:34:46.860) (total time: 4075ms):
Trace[1374106364]: ["cacher list" audit-id:95d74788-e3ea-47a2-8775-29810ec829ef,type:endpointslices.discovery.k8s.io 4071ms (03:34:46.863)
Trace[1374106364]:  ---"Filtered items" count:3 2626ms (03:34:49.500)]
Trace[1374106364]: ---"Writing http response done" count:3 1433ms (03:34:50.935)
Trace[1374106364]: [4.075467839s] [4.075467839s] END
E0322 03:34:50.936369    2280 timeout.go:142] post-timeout activity - time-elapsed: 756.15317ms, GET "/apis/discovery.k8s.io/v1/endpointslices" result: <nil>
E0322 03:34:50.970349    2280 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:34:50.970502    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:34:50.983811    2280 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 03:34:50.984056    2280 trace.go:236] Trace[645467067]: "List" accept:application/json, */*,audit-id:11f390ab-0ba6-40c3-a9a4-dcc1183803a6,client:10.42.0.5,protocol:HTTP/2.0,resource:persistentvolumeclaims,scope:cluster,url:/api/v1/persistentvolumeclaims,user-agent:local-path-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:LIST (22-Mar-2024 03:34:46.969) (total time: 4014ms):
Trace[645467067]: ---"Writing http response done" count:0 3993ms (03:34:50.984)
Trace[645467067]: [4.014046055s] [4.014046055s] END
E0322 03:34:50.984671    2280 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
I0322 03:34:51.008841    2280 trace.go:236] Trace[1230036970]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.394) (total time: 57614ms):
Trace[1230036970]: ---"Objects listed" error:<nil> 57612ms (03:34:51.007)
Trace[1230036970]: [57.614292462s] [57.614292462s] END
I0322 03:34:51.087733    2280 trace.go:236] Trace[693770239]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.426) (total time: 57659ms):
Trace[693770239]: ---"Objects listed" error:<nil> 57658ms (03:34:51.085)
Trace[693770239]: [57.659268857s] [57.659268857s] END
E0322 03:34:51.092401    2280 timeout.go:142] post-timeout activity - time-elapsed: 9.46894716s, GET "/api/v1/nodes/server" result: <nil>
time="2024-03-22T03:34:51Z" level=info msg="Slow SQL (started: 2024-03-22 03:34:48.363281871 +0000 UTC m=+388.354930105) (total time: 1.688464926s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC  : [[/registry/services/specs/% false]]"
E0322 03:34:51.191546    2280 wrap.go:54] timeout or abort while handling: method=GET URI="/api/v1/namespaces/default/endpoints?fieldSelector=metadata.name%3Dkubernetes&resourceVersion=225" audit-ID="6857595d-7483-4c2c-ba2f-fe6617b43670"
E0322 03:34:51.201586    2280 timeout.go:142] post-timeout activity - time-elapsed: 967.505229ms, GET "/version" result: <nil>
E0322 03:34:51.299732    2280 timeout.go:142] post-timeout activity - time-elapsed: 1.074479288s, GET "/api/v1/persistentvolumeclaims" result: <nil>
E0322 03:34:51.305246    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:34:51.306460    2280 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 03:34:51.308893    2280 trace.go:236] Trace[1300548826]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:ddc039a6-b8e8-4f75-b0d3-1a403327c206,client:127.0.0.1,protocol:HTTP/2.0,resource:storageclasses,scope:cluster,url:/apis/storage.k8s.io/v1/storageclasses,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/scheduler,verb:LIST (22-Mar-2024 03:34:47.370) (total time: 3935ms):
Trace[1300548826]: ---"Writing http response done" count:1 3918ms (03:34:51.306)
Trace[1300548826]: [3.935870956s] [3.935870956s] END
E0322 03:34:51.429643    2280 timeout.go:142] post-timeout activity - time-elapsed: 1.141572175s, GET "/apis/storage.k8s.io/v1/storageclasses" result: <nil>
I0322 03:34:51.502530    2280 trace.go:236] Trace[1026664552]: "DeltaFIFO Pop Process" ID:kube-node-lease/default,Depth:38,Reason:slow event handlers blocking the queue (22-Mar-2024 03:34:51.336) (total time: 159ms):
Trace[1026664552]: [159.569922ms] [159.569922ms] END
time="2024-03-22T03:34:51Z" level=info msg="Slow SQL (started: 2024-03-22 03:34:47.260265464 +0000 UTC m=+387.251913707) (total time: 4.313467486s):  SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key' : [[]]"
E0322 03:34:51.758240    2280 timeout.go:142] post-timeout activity - time-elapsed: 2.686107842s, GET "/apis/storage.k8s.io/v1/storageclasses" result: <nil>
E0322 03:34:51.758644    2280 timeout.go:142] post-timeout activity - time-elapsed: 1.643574655s, GET "/apis/helm.cattle.io/v1/helmcharts" result: <nil>
E0322 03:34:51.759026    2280 timeout.go:142] post-timeout activity - time-elapsed: 1.646438909s, GET "/api/v1/pods" result: <nil>
E0322 03:34:51.759060    2280 timeout.go:142] post-timeout activity - time-elapsed: 1.579300565s, GET "/api/v1/namespaces/kube-system/configmaps" result: <nil>
E0322 03:34:51.759209    2280 timeout.go:142] post-timeout activity - time-elapsed: 1.304393423s, GET "/api/v1/pods" result: <nil>
E0322 03:34:51.759275    2280 timeout.go:142] post-timeout activity - time-elapsed: 1.646576376s, GET "/api/v1/nodes" result: <nil>
E0322 03:34:51.768869    2280 timeout.go:142] post-timeout activity - time-elapsed: 1.299996478s, GET "/apis/rbac.authorization.k8s.io/v1/rolebindings" result: <nil>
E0322 03:34:51.770499    2280 timeout.go:142] post-timeout activity - time-elapsed: 4.851781568s, GET "/api/v1/services" result: <nil>
E0322 03:34:51.776577    2280 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:34:51.780936    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:34:51.781769    2280 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 03:34:51.789558    2280 timeout.go:142] post-timeout activity - time-elapsed: 1.534301521s, GET "/apis/apps/v1/statefulsets" result: <nil>
E0322 03:34:51.816867    2280 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:34:51.817096    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:34:51.820482    2280 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
time="2024-03-22T03:34:51Z" level=info msg="Slow SQL (started: 2024-03-22 03:34:47.128516077 +0000 UTC m=+387.120164313) (total time: 2.513326898s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? AND mkv.id <= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC LIMIT 1000 : [[/registry/traefik.io/middlewares/% 613 false]]"
E0322 03:34:51.965082    2280 timeout.go:142] post-timeout activity - time-elapsed: 1.544813032s, GET "/apis/node.k8s.io/v1/runtimeclasses" result: <nil>
E0322 03:34:51.999359    2280 timeout.go:142] post-timeout activity - time-elapsed: 1.556287246s, GET "/api/v1/services" result: <nil>
I0322 03:34:52.008547    2280 trace.go:236] Trace[1480372378]: "iptables ChainExists" (22-Mar-2024 03:34:31.292) (total time: 20712ms):
Trace[1480372378]: [20.712802872s] [20.712802872s] END
E0322 03:34:52.010803    2280 timeout.go:142] post-timeout activity - time-elapsed: 1.5570698s, GET "/api/v1/namespaces/kube-system/resourcequotas" result: <nil>
E0322 03:34:52.011271    2280 timeout.go:142] post-timeout activity - time-elapsed: 1.556699322s, GET "/apis/apiextensions.k8s.io/v1/customresourcedefinitions" result: <nil>
E0322 03:34:52.012686    2280 timeout.go:142] post-timeout activity - time-elapsed: 5.239602703s, PUT "/api/v1/nodes/server/status" result: <nil>
I0322 03:34:52.110761    2280 trace.go:236] Trace[770168838]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.041) (total time: 57767ms):
Trace[770168838]: ---"Objects listed" error:<nil> 57765ms (03:34:50.807)
Trace[770168838]: [57.767700259s] [57.767700259s] END
E0322 03:34:52.130827    2280 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:34:52.133493    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:34:52.136317    2280 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 03:34:52.143167    2280 trace.go:236] Trace[561654820]: "List" accept:application/json, */*,audit-id:bd1240ff-7cfe-4d49-a81a-01af3438bef2,client:127.0.0.1,protocol:HTTP/2.0,resource:namespaces,scope:cluster,url:/api/v1/namespaces,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:34:49.378) (total time: 2758ms):
Trace[561654820]: ---"Writing http response done" count:4 2754ms (03:34:52.136)
Trace[561654820]: [2.75805093s] [2.75805093s] END
I0322 03:34:52.159922    2280 trace.go:236] Trace[1606256979]: "List" accept:application/json, */*,audit-id:6857595d-7483-4c2c-ba2f-fe6617b43670,client:127.0.0.1,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/default/endpoints,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:34:49.378) (total time: 1473ms):
Trace[1606256979]: ---"Writing http response done" count:1 1469ms (03:34:50.852)
Trace[1606256979]: [1.473402908s] [1.473402908s] END
I0322 03:34:52.169362    2280 trace.go:236] Trace[1203927980]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.315) (total time: 58853ms):
Trace[1203927980]: ---"Objects listed" error:<nil> 58849ms (03:34:52.164)
Trace[1203927980]: [58.853983864s] [58.853983864s] END
I0322 03:34:52.211999    2280 trace.go:236] Trace[1035957395]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.146) (total time: 59064ms):
Trace[1035957395]: ---"Objects listed" error:<nil> 59060ms (03:34:52.207)
Trace[1035957395]: [59.064400405s] [59.064400405s] END
E0322 03:34:52.323832    2280 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, context canceled]"
E0322 03:34:52.511804    2280 finisher.go:175] FinishRequest: post-timeout activity - time-elapsed: 13.291099351s, panicked: false, err: <nil>, panic-reason: <nil>
E0322 03:34:52.530197    2280 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
time="2024-03-22T03:34:52Z" level=info msg="Handling backend connection request [server]"
I0322 03:34:52.802281    2280 trace.go:236] Trace[666862580]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:967a7f2b-00d0-425e-bcdd-5a20087d1a68,client:127.0.0.1,protocol:HTTP/2.0,resource:services,scope:cluster,url:/api/v1/services,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:33:59.258) (total time: 52535ms):
Trace[666862580]: ["List(recursive=true) etcd3" audit-id:967a7f2b-00d0-425e-bcdd-5a20087d1a68,key:/services/specs,resourceVersion:,resourceVersionMatch:,limit:0,continue: 51368ms (03:34:00.426)]
Trace[666862580]: [52.535804644s] [52.535804644s] END
E0322 03:34:52.806938    2280 timeout.go:142] post-timeout activity - time-elapsed: 2.340216379s, GET "/api/v1/services" result: <nil>
I0322 03:34:52.814302    2280 trace.go:236] Trace[1017714580]: "Proxy via http_connect protocol over tcp" address:10.42.0.4:10250 (22-Mar-2024 03:33:56.077) (total time: 56735ms):
Trace[1017714580]: [56.735582823s] [56.735582823s] END
time="2024-03-22T03:34:52Z" level=error msg="error while range on /registry/resourcequotas/kube-system/ /registry/resourcequotas/kube-system/: context canceled"
I0322 03:34:52.843491    2280 trace.go:236] Trace[619791076]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.851) (total time: 58991ms):
Trace[619791076]: ---"Objects listed" error:<nil> 58987ms (03:34:52.838)
Trace[619791076]: [58.991961701s] [58.991961701s] END
E0322 03:34:53.071988    2280 timeout.go:142] post-timeout activity - time-elapsed: 1.669221771s, GET "/apis/networking.k8s.io/v1/networkpolicies" result: <nil>
E0322 03:34:53.074232    2280 timeout.go:142] post-timeout activity - time-elapsed: 2.2751819s, GET "/api/v1/namespaces" result: <nil>
E0322 03:34:53.074274    2280 timeout.go:142] post-timeout activity - time-elapsed: 1.674541108s, GET "/api/v1/namespaces/default/endpoints" result: <nil>
E0322 03:34:53.076258    2280 timeout.go:142] post-timeout activity - time-elapsed: 7.386648793s, GET "/api" result: <nil>
I0322 03:34:53.297474    2280 trace.go:236] Trace[2109396250]: "DeltaFIFO Pop Process" ID:kube-system/auth-delegator,Depth:11,Reason:slow event handlers blocking the queue (22-Mar-2024 03:34:53.003) (total time: 287ms):
Trace[2109396250]: [287.780265ms] [287.780265ms] END
E0322 03:34:53.433226    2280 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:34:53.434794    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:34:53.440447    2280 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 03:34:53.443971    2280 trace.go:236] Trace[383703743]: "List" accept:application/json, */*,audit-id:77948dc3-077c-44b4-a20a-b40f4eeae999,client:127.0.0.1,protocol:HTTP/2.0,resource:pods,scope:cluster,url:/api/v1/pods,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:34:50.981) (total time: 2459ms):
Trace[383703743]: [2.45927185s] [2.45927185s] END
E0322 03:34:53.683373    2280 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:34:53.689572    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:34:53.697140    2280 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 03:34:54.171915    2280 finisher.go:175] FinishRequest: post-timeout activity - time-elapsed: 47.446665207s, panicked: false, err: context deadline exceeded, panic-reason: <nil>
I0322 03:34:54.200505    2280 trace.go:236] Trace[2110320797]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.550) (total time: 59455ms):
Trace[2110320797]: ---"Objects listed" error:<nil> 59440ms (03:34:52.990)
Trace[2110320797]: [59.455890808s] [59.455890808s] END
time="2024-03-22T03:34:54Z" level=info msg="Slow SQL (started: 2024-03-22 03:34:53.116758325 +0000 UTC m=+393.108406563) (total time: 1.106434973s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC LIMIT 500 : [[% 613]]"
I0322 03:34:54.257271    2280 trace.go:236] Trace[1158183889]: "List" accept:application/json, */*,audit-id:9dd962d4-ba69-4081-94e2-63cb0ec1ccae,client:10.42.0.4,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps,user-agent:metrics-server/v0.6.3 (linux/amd64) kubernetes/a938798,verb:GET (22-Mar-2024 03:34:53.350) (total time: 906ms):
Trace[1158183889]: [906.68655ms] [906.68655ms] END
E0322 03:34:54.273886    2280 timeout.go:142] post-timeout activity - time-elapsed: 3.188925303s, GET "/api/v1/persistentvolumes" result: <nil>
E0322 03:34:54.303813    2280 timeout.go:142] post-timeout activity - time-elapsed: 3.118150883s, GET "/api/v1/pods" result: <nil>
I0322 03:34:54.337578    2280 trace.go:236] Trace[1769395706]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.387) (total time: 60949ms):
Trace[1769395706]: ---"Objects listed" error:<nil> 60947ms (03:34:54.335)
Trace[1769395706]: [1m0.949783415s] [1m0.949783415s] END
E0322 03:34:54.397321    2280 finisher.go:175] FinishRequest: post-timeout activity - time-elapsed: 3.926750352s, panicked: false, err: context canceled, panic-reason: <nil>
I0322 03:34:54.411471    2280 trace.go:236] Trace[1425323708]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.156) (total time: 61255ms):
Trace[1425323708]: ---"Objects listed" error:<nil> 61255ms (03:34:54.411)
Trace[1425323708]: [1m1.255364359s] [1m1.255364359s] END
I0322 03:34:54.458926    2280 trace.go:236] Trace[600070354]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.645) (total time: 60813ms):
Trace[600070354]: ---"Objects listed" error:<nil> 60812ms (03:34:54.458)
Trace[600070354]: [1m0.813023571s] [1m0.813023571s] END
E0322 03:34:54.473718    2280 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: request timed out, Header: map[]
I0322 03:34:54.473749    2280 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0322 03:34:54.507139    2280 trace.go:236] Trace[85512223]: "List" accept:application/json, */*,audit-id:c8c6d45a-a9c9-4bcd-90f6-2c22e1e367c6,client:10.42.0.4,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps,user-agent:metrics-server/v0.6.3 (linux/amd64) kubernetes/a938798,verb:GET (22-Mar-2024 03:34:53.561) (total time: 945ms):
Trace[85512223]: ["cacher list" audit-id:c8c6d45a-a9c9-4bcd-90f6-2c22e1e367c6,type:configmaps 934ms (03:34:53.572)
Trace[85512223]:  ---"watchCache locked acquired" 934ms (03:34:54.506)]
Trace[85512223]: [945.289114ms] [945.289114ms] END
E0322 03:34:54.514223    2280 finisher.go:175] FinishRequest: post-timeout activity - time-elapsed: 21.535188316s, panicked: false, err: context deadline exceeded, panic-reason: <nil>
time="2024-03-22T03:34:54Z" level=info msg="Slow SQL (started: 2024-03-22 03:34:52.342060518 +0000 UTC m=+392.333708756) (total time: 1.390345981s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC LIMIT 1 : [[/registry/masterleases/192.168.56.110 false]]"
I0322 03:34:54.520180    2280 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
I0322 03:34:54.520282    2280 event.go:307] "Event occurred" object="kube-system/traefik-crd" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik-crd"
I0322 03:34:54.575803    2280 trace.go:236] Trace[1005394514]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:33:53.643) (total time: 60932ms):
Trace[1005394514]: ---"Objects listed" error:<nil> 60932ms (03:34:54.575)
Trace[1005394514]: [1m0.932683632s] [1m0.932683632s] END
E0322 03:34:54.650947    2280 finisher.go:175] FinishRequest: post-timeout activity - time-elapsed: 53.935122916s, panicked: false, err: context deadline exceeded, panic-reason: <nil>
I0322 03:34:54.707312    2280 trace.go:236] Trace[893458458]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:f001cc68-3960-4196-8cc0-290bf2ff82a5,client:10.42.0.4,protocol:HTTP/2.0,resource:nodes,scope:cluster,url:/api/v1/nodes,user-agent:metrics-server/v0.6.3 (linux/amd64) kubernetes/a938798,verb:LIST (22-Mar-2024 03:34:52.271) (total time: 2436ms):
Trace[893458458]: ---"Writing http response done" count:1 2429ms (03:34:54.707)
Trace[893458458]: [2.43604936s] [2.43604936s] END
I0322 03:34:54.710256    2280 trace.go:236] Trace[1165904433]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:41f25d2d-0197-4ed4-bc86-eb1a1953d934,client:127.0.0.1,protocol:HTTP/2.0,resource:tokenreviews,scope:resource,url:/apis/authentication.k8s.io/v1/tokenreviews,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:POST (22-Mar-2024 03:34:53.879) (total time: 830ms):
Trace[1165904433]: ---"Write to database call succeeded" len:1152 816ms (03:34:54.708)
Trace[1165904433]: [830.923109ms] [830.923109ms] END
I0322 03:34:54.712435    2280 trace.go:236] Trace[753059501]: "List" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:4fd86d95-8097-4f71-9c8a-74c766b7d62b,client:127.0.0.1,protocol:HTTP/2.0,resource:pods,scope:cluster,url:/api/v1/pods,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:34:54.074) (total time: 638ms):
Trace[753059501]: ---"Writing http response done" count:5 634ms (03:34:54.712)
Trace[753059501]: [638.15766ms] [638.15766ms] END
I0322 03:34:54.721444    2280 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="2.703928ms"
I0322 03:34:54.721830    2280 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="220.977s"
I0322 03:34:54.747865    2280 trace.go:236] Trace[442897195]: "Proxy via http_connect protocol over tcp" address:10.42.0.4:10250 (22-Mar-2024 03:34:49.012) (total time: 5735ms):
Trace[442897195]: [5.735627031s] [5.735627031s] END
I0322 03:34:54.749585    2280 trace.go:236] Trace[1917499070]: "Proxy via http_connect protocol over tcp" address:10.42.0.4:10250 (22-Mar-2024 03:34:49.013) (total time: 5735ms):
Trace[1917499070]: [5.735664835s] [5.735664835s] END
I0322 03:34:54.768615    2280 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="2.209142ms"
W0322 03:34:54.824991    2280 controller.go:134] slow openapi aggregation of "serverstransports.traefik.containo.us": 1m4.989702186s
I0322 03:34:54.872453    2280 trace.go:236] Trace[760184443]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:28038352-1b66-4702-bf9d-8bbd4e313264,client:127.0.0.1,protocol:HTTP/2.0,resource:customresourcedefinitions,scope:cluster,url:/apis/apiextensions.k8s.io/v1/customresourcedefinitions,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:34:54.225) (total time: 646ms):
Trace[760184443]: ---"Writing http response done" count:23 644ms (03:34:54.872)
Trace[760184443]: [646.842197ms] [646.842197ms] END
E0322 03:34:54.925781    2280 finisher.go:175] FinishRequest: post-timeout activity - time-elapsed: 3.678945747s, panicked: false, err: context canceled, panic-reason: <nil>
I0322 03:34:54.947193    2280 trace.go:236] Trace[44496192]: "Proxy via http_connect protocol over tcp" address:10.42.0.4:10250 (22-Mar-2024 03:34:49.014) (total time: 5933ms):
Trace[44496192]: [5.933161024s] [5.933161024s] END
I0322 03:34:54.974344    2280 trace.go:236] Trace[405722912]: "Proxy via http_connect protocol over tcp" address:10.42.0.4:10250 (22-Mar-2024 03:34:49.014) (total time: 5960ms):
Trace[405722912]: [5.960040908s] [5.960040908s] END
I0322 03:34:55.019829    2280 trace.go:236] Trace[1130355825]: "Proxy via http_connect protocol over tcp" address:10.42.0.4:10250 (22-Mar-2024 03:34:48.999) (total time: 6019ms):
Trace[1130355825]: [6.019660275s] [6.019660275s] END
I0322 03:34:55.198228    2280 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:34:55.199573    2280 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:34:55.336010    2280 trace.go:236] Trace[1263604661]: "Get" accept:application/json, */*,audit-id:e548ebaa-b0ec-406e-887c-7b46f9235896,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps/coredns,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:GET (22-Mar-2024 03:34:54.406) (total time: 928ms):
Trace[1263604661]: ---"About to write a response" 928ms (03:34:55.335)
Trace[1263604661]: [928.794916ms] [928.794916ms] END
I0322 03:34:55.450006    2280 trace.go:236] Trace[842202607]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:5c1570bc-4501-4564-801d-055caf51f9b4,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/server,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:34:53.830) (total time: 1618ms):
Trace[842202607]: ---"About to write a response" 1615ms (03:34:55.447)
Trace[842202607]: [1.618973428s] [1.618973428s] END
I0322 03:34:55.478725    2280 trace.go:236] Trace[1458193748]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:cd68b45b-d961-4ee2-a83d-6b44f6a9392e,client:127.0.0.1,protocol:HTTP/2.0,resource:services,scope:resource,url:/api/v1/namespaces/default/services/kubernetes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/,verb:GET (22-Mar-2024 03:34:54.700) (total time: 778ms):
Trace[1458193748]: ---"About to write a response" 778ms (03:34:55.478)
Trace[1458193748]: [778.579843ms] [778.579843ms] END
I0322 03:34:55.488338    2280 trace.go:236] Trace[2080020673]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:b879ca24-411f-47c6-b53b-da8fb7d8a786,client:127.0.0.1,protocol:HTTP/2.0,resource:resourcequotas,scope:namespace,url:/api/v1/namespaces/kube-system/resourcequotas,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:34:54.756) (total time: 731ms):
Trace[2080020673]: ["List(recursive=true) etcd3" audit-id:b879ca24-411f-47c6-b53b-da8fb7d8a786,key:/resourcequotas/kube-system,resourceVersion:,resourceVersionMatch:,limit:0,continue: 730ms (03:34:54.757)]
Trace[2080020673]: [731.745915ms] [731.745915ms] END
I0322 03:34:55.500368    2280 trace.go:236] Trace[166516560]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:4911f569-7f0a-4970-a1e1-32e4374b7351,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/server,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:node-controller,verb:GET (22-Mar-2024 03:34:54.410) (total time: 1089ms):
Trace[166516560]: ---"About to write a response" 1089ms (03:34:55.500)
Trace[166516560]: [1.089464317s] [1.089464317s] END
I0322 03:34:55.517976    2280 trace.go:236] Trace[386400458]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:a6185d86-5414-4ccc-b6d1-0d1ecfdd6587,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-7zoch227uv4owxg75pgtnmv3jq,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PUT (22-Mar-2024 03:34:54.200) (total time: 1317ms):
Trace[386400458]: ---"limitedReadBody succeeded" len:590 279ms (03:34:54.479)
Trace[386400458]: ["GuaranteedUpdate etcd3" audit-id:a6185d86-5414-4ccc-b6d1-0d1ecfdd6587,key:/leases/kube-system/apiserver-7zoch227uv4owxg75pgtnmv3jq,type:*coordination.Lease,resource:leases.coordination.k8s.io 1019ms (03:34:54.498)
Trace[386400458]:  ---"Txn call completed" 1014ms (03:34:55.516)]
Trace[386400458]: [1.317925336s] [1.317925336s] END
I0322 03:34:55.526051    2280 trace.go:236] Trace[1676163906]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:17863adf-aa88-454c-8f38-9a3f6b4e64c0,client:127.0.0.1,protocol:HTTP/2.0,resource:apiservices,scope:resource,url:/apis/apiregistration.k8s.io/v1/apiservices/v1beta1.metrics.k8s.io/status,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PUT (22-Mar-2024 03:34:54.707) (total time: 818ms):
Trace[1676163906]: ---"limitedReadBody succeeded" len:2079 77ms (03:34:54.785)
Trace[1676163906]: ["GuaranteedUpdate etcd3" audit-id:17863adf-aa88-454c-8f38-9a3f6b4e64c0,key:/apiregistration.k8s.io/apiservices/v1beta1.metrics.k8s.io,type:*apiregistration.APIService,resource:apiservices.apiregistration.k8s.io 740ms (03:34:54.785)
Trace[1676163906]:  ---"Txn call completed" 720ms (03:34:55.507)]
Trace[1676163906]: [818.38087ms] [818.38087ms] END
I0322 03:34:55.536415    2280 trace.go:236] Trace[1167238491]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:71d083f8-1a1d-41c5-a09f-b723f1bc4b65,client:127.0.0.1,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/kube-system/pods/metrics-server-67c658944b-p82jk,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:34:54.543) (total time: 992ms):
Trace[1167238491]: ---"About to write a response" 940ms (03:34:55.484)
Trace[1167238491]: [992.936192ms] [992.936192ms] END
E0322 03:34:55.573381    2280 available_controller.go:460] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.42.0.4:10250/apis/metrics.k8s.io/v1beta1: Get "https://10.42.0.4:10250/apis/metrics.k8s.io/v1beta1": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
E0322 03:34:55.592992    2280 webhook.go:223] Failed to make webhook authorizer request: Post "https://127.0.0.1:6443/apis/authorization.k8s.io/v1/subjectaccessreviews": context canceled
E0322 03:34:55.603562    2280 server.go:325] "Authorization error" err="Post \"https://127.0.0.1:6443/apis/authorization.k8s.io/v1/subjectaccessreviews\": context canceled" user="system:serviceaccount:kube-system:metrics-server" verb="get" resource="nodes" subresource="metrics"
W0322 03:34:55.657986    2280 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:34:55.660649    2280 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 03:34:55.662736    2280 trace.go:236] Trace[1622734661]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:e0b1fa1c-88ae-47b1-9aa0-dc54b297ea79,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/server,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PUT (22-Mar-2024 03:34:54.566) (total time: 1096ms):
Trace[1622734661]: ---"About to convert to expected version" 206ms (03:34:54.772)
Trace[1622734661]: ["GuaranteedUpdate etcd3" audit-id:e0b1fa1c-88ae-47b1-9aa0-dc54b297ea79,key:/leases/kube-node-lease/server,type:*coordination.Lease,resource:leases.coordination.k8s.io 886ms (03:34:54.776)
Trace[1622734661]:  ---"Txn call completed" 871ms (03:34:55.650)]
Trace[1622734661]: [1.096606293s] [1.096606293s] END
I0322 03:34:55.668205    2280 trace.go:236] Trace[592730130]: "List" accept:application/json;as=Table;v=v1;g=meta.k8s.io,application/json;as=Table;v=v1beta1;g=meta.k8s.io,application/json,audit-id:23efe9f2-4d35-4d8b-8bda-a87c1e4685e6,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:cluster,url:/api/v1/nodes,user-agent:kubectl/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:34:55.108) (total time: 559ms):
Trace[592730130]: ["List(recursive=true) etcd3" audit-id:23efe9f2-4d35-4d8b-8bda-a87c1e4685e6,key:/minions,resourceVersion:,resourceVersionMatch:,limit:500,continue: 557ms (03:34:55.111)]
Trace[592730130]: [559.566487ms] [559.566487ms] END
I0322 03:34:55.683146    2280 trace.go:236] Trace[771786]: "Get" accept:application/json, */*,audit-id:9dfc4e90-dfc1-4ba6-a218-c9b9cbe6a5c6,client:127.0.0.1,protocol:HTTP/2.0,resource:secrets,scope:resource,url:/api/v1/namespaces/kube-system/secrets/k3s-serving,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:GET (22-Mar-2024 03:34:55.128) (total time: 554ms):
Trace[771786]: ---"About to write a response" 554ms (03:34:55.683)
Trace[771786]: [554.679851ms] [554.679851ms] END
E0322 03:34:55.760649    2280 kubelet.go:2477] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="4.845s"
time="2024-03-22T03:34:55Z" level=info msg="Updating TLS secret for kube-system/k3s-serving (count: 10): map[listener.cattle.io/cn-10.43.0.1:10.43.0.1 listener.cattle.io/cn-127.0.0.1:127.0.0.1 listener.cattle.io/cn-192.168.56.110:192.168.56.110 listener.cattle.io/cn-__1-f16284:::1 listener.cattle.io/cn-kubernetes:kubernetes listener.cattle.io/cn-kubernetes.default:kubernetes.default listener.cattle.io/cn-kubernetes.default.svc:kubernetes.default.svc listener.cattle.io/cn-kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local listener.cattle.io/cn-localhost:localhost listener.cattle.io/cn-server:server listener.cattle.io/fingerprint:SHA1=3EF84C4C3E5D7F82BBAF806634968BEF8CE80EDA]"
I0322 03:34:55.790231    2280 trace.go:236] Trace[611867672]: "Create" accept:application/json, */*,audit-id:b6416182-e0ed-4d46-b4f4-2ef973f4a7c8,client:127.0.0.1,protocol:HTTP/2.0,resource:secrets,scope:resource,url:/api/v1/namespaces/kube-system/secrets,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:POST (22-Mar-2024 03:34:51.747) (total time: 4042ms):
Trace[611867672]: ---"Conversion done" 1042ms (03:34:52.793)
Trace[611867672]: [4.042538867s] [4.042538867s] END
I0322 03:34:55.840405    2280 scope.go:117] "RemoveContainer" containerID="afe4640d3a121c51da6bad0227debc7256b8fec3cc9277d94249ed4d0a986231"
E0322 03:34:55.841765    2280 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"helm\" with CrashLoopBackOff: \"back-off 20s restarting failed container=helm pod=helm-install-traefik-fxsb6_kube-system(f5749ede-2796-414e-b0ea-d6e2523ec959)\"" pod="kube-system/helm-install-traefik-fxsb6" podUID="f5749ede-2796-414e-b0ea-d6e2523ec959"
I0322 03:34:55.842181    2280 event.go:307] "Event occurred" object="serverworker" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodePasswordValidationComplete" message="Deferred node password secret validation complete"
I0322 03:34:55.892245    2280 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/coredns-6799fbcd5-rx42b" containerName="coredns"
I0322 03:34:56.007749    2280 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/metrics-server-67c658944b-p82jk" containerName="metrics-server"
I0322 03:34:56.059357    2280 trace.go:236] Trace[402798530]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.56.110,type:*v1.Endpoints,resource:apiServerIPInfo (22-Mar-2024 03:29:44.989) (total time: 311070ms):
Trace[402798530]: ---"initial value restored" 250808ms (03:33:55.797)
Trace[402798530]: ---"Transaction prepared" 52336ms (03:34:48.136)
Trace[402798530]: ---"Txn call completed" 6914ms (03:34:55.051)
Trace[402798530]: ---"Transaction prepared" 551ms (03:34:55.605)
Trace[402798530]: ---"Txn call completed" 453ms (03:34:56.059)
Trace[402798530]: [5m11.070254939s] [5m11.070254939s] END
I0322 03:34:56.171236    2280 trace.go:236] Trace[945952828]: "Patch" accept:application/json, */*,audit-id:39e6a41f-b6bd-461a-9486-e9e8e9f947dc,client:127.0.0.1,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events/traefik.17bef879b248b2b9,user-agent:helm-controller@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:PATCH (22-Mar-2024 03:34:54.789) (total time: 1381ms):
Trace[945952828]: ["GuaranteedUpdate etcd3" audit-id:39e6a41f-b6bd-461a-9486-e9e8e9f947dc,key:/events/kube-system/traefik.17bef879b248b2b9,type:*core.Event,resource:events 1380ms (03:34:54.790)
Trace[945952828]:  ---"initial value restored" 821ms (03:34:55.612)
Trace[945952828]:  ---"Transaction prepared" 214ms (03:34:55.843)
Trace[945952828]:  ---"Txn call completed" 327ms (03:34:56.171)]
Trace[945952828]: ---"Object stored in database" 545ms (03:34:56.171)
Trace[945952828]: [1.381591112s] [1.381591112s] END
E0322 03:34:56.274504    2280 available_controller.go:460] v1beta1.metrics.k8s.io failed with: Operation cannot be fulfilled on apiservices.apiregistration.k8s.io "v1beta1.metrics.k8s.io": the object has been modified; please apply your changes to the latest version and try again
I0322 03:34:56.396948    2280 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="332.218547ms"
I0322 03:34:56.400215    2280 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="73.667s"
I0322 03:34:56.592215    2280 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="151.422875ms"
I0322 03:34:56.592594    2280 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="89.245s"
E0322 03:34:56.596225    2280 available_controller.go:460] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.42.0.4:10250/apis/metrics.k8s.io/v1beta1: Get "https://10.42.0.4:10250/apis/metrics.k8s.io/v1beta1": proxy error from 127.0.0.1:6443 while dialing 10.42.0.4:10250, code 502: 502 Bad Gateway
W0322 03:34:56.597878    2280 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:34:56.598420    2280 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
E0322 03:34:56.602203    2280 handler_proxy.go:137] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
W0322 03:34:56.664598    2280 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:34:56.665648    2280 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 03:34:56.665813    2280 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0322 03:34:56.665887    2280 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:34:56.666682    2280 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 03:34:56.666839    2280 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E0322 03:34:56.674879    2280 available_controller.go:460] v1beta1.metrics.k8s.io failed with: Operation cannot be fulfilled on apiservices.apiregistration.k8s.io "v1beta1.metrics.k8s.io": the object has been modified; please apply your changes to the latest version and try again
W0322 03:34:56.786311    2280 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:34:56.786347    2280 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
E0322 03:34:56.786402    2280 handler_proxy.go:137] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
I0322 03:34:56.794264    2280 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="97.698522ms"
I0322 03:34:56.796249    2280 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="32.213s"
E0322 03:34:56.849642    2280 available_controller.go:460] v1beta1.metrics.k8s.io failed with: Operation cannot be fulfilled on apiservices.apiregistration.k8s.io "v1beta1.metrics.k8s.io": the object has been modified; please apply your changes to the latest version and try again
I0322 03:34:56.873751    2280 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:34:57.009307    2280 scope.go:117] "RemoveContainer" containerID="dd39ba9cb1d7cc7d1a9ff395b58c474adcecd56014f2c94294c76500ea39d1c6"
I0322 03:34:57.021886    2280 scope.go:117] "RemoveContainer" containerID="afe4640d3a121c51da6bad0227debc7256b8fec3cc9277d94249ed4d0a986231"
E0322 03:34:57.025390    2280 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"helm\" with CrashLoopBackOff: \"back-off 20s restarting failed container=helm pod=helm-install-traefik-fxsb6_kube-system(f5749ede-2796-414e-b0ea-d6e2523ec959)\"" pod="kube-system/helm-install-traefik-fxsb6" podUID="f5749ede-2796-414e-b0ea-d6e2523ec959"
I0322 03:34:57.079172    2280 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="61.817s"
I0322 03:34:57.311388    2280 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="110.358108ms"
I0322 03:34:57.317359    2280 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="38.082s"
time="2024-03-22T03:34:57Z" level=info msg="certificate CN=serverworker signed by CN=k3s-server-ca@1711078100: notBefore=2024-03-22 03:28:20 +0000 UTC notAfter=2025-03-22 03:34:57 +0000 UTC"
I0322 03:34:57.561091    2280 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="157.09622ms"
I0322 03:34:57.561259    2280 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="121.066s"
I0322 03:34:57.737986    2280 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
W0322 03:34:57.798739    2280 handler_proxy.go:93] no RequestInfo found in the context
W0322 03:34:57.805668    2280 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:34:57.807223    2280 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 03:34:57.807262    2280 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E0322 03:34:57.808397    2280 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 03:34:57.808416    2280 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
time="2024-03-22T03:34:58Z" level=info msg="certificate CN=system:node:serverworker,O=system:nodes signed by CN=k3s-client-ca@1711078100: notBefore=2024-03-22 03:28:20 +0000 UTC notAfter=2025-03-22 03:34:58 +0000 UTC"
time="2024-03-22T03:34:59Z" level=info msg="Handling backend connection request [serverworker]"
I0322 03:35:00.134840    2280 actual_state_of_world.go:547] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"serverworker\" does not exist"
I0322 03:35:00.141474    2280 node_controller.go:431] Initializing node serverworker with cloud provider
E0322 03:35:00.144895    2280 node_controller.go:240] error syncing 'serverworker': failed to get instance metadata for node serverworker: address annotations not yet set, requeuing
I0322 03:35:00.149657    2280 node_controller.go:431] Initializing node serverworker with cloud provider
E0322 03:35:00.149708    2280 node_controller.go:240] error syncing 'serverworker': failed to get instance metadata for node serverworker: address annotations not yet set, requeuing
I0322 03:35:00.163156    2280 node_controller.go:431] Initializing node serverworker with cloud provider
E0322 03:35:00.163266    2280 node_controller.go:240] error syncing 'serverworker': failed to get instance metadata for node serverworker: address annotations not yet set, requeuing
I0322 03:35:00.184040    2280 node_controller.go:431] Initializing node serverworker with cloud provider
E0322 03:35:00.184311    2280 node_controller.go:240] error syncing 'serverworker': failed to get instance metadata for node serverworker: address annotations not yet set, requeuing
I0322 03:35:00.224634    2280 node_controller.go:431] Initializing node serverworker with cloud provider
I0322 03:35:00.546499    2280 range_allocator.go:380] "Set node PodCIDR" node="serverworker" podCIDRs=["10.42.1.0/24"]
I0322 03:35:00.986342    2280 event.go:307] "Event occurred" object="serverworker" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node serverworker event: Registered Node serverworker in Controller"
I0322 03:35:01.261460    2280 node_lifecycle_controller.go:877] "Missing timestamp for Node. Assuming now as a timestamp" node="serverworker"
I0322 03:35:01.712804    2280 trace.go:236] Trace[2010138855]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:101a9505-36a3-454d-ab37-53b5ded365cc,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/node-controller,verb:PUT (22-Mar-2024 03:35:00.246) (total time: 1459ms):
Trace[2010138855]: ---"limitedReadBody succeeded" len:2533 35ms (03:35:00.282)
Trace[2010138855]: ["GuaranteedUpdate etcd3" audit-id:101a9505-36a3-454d-ab37-53b5ded365cc,key:/minions/serverworker,type:*core.Node,resource:nodes 1426ms (03:35:00.282)]
Trace[2010138855]: ---"Write to database call failed" len:2533,err:Operation cannot be fulfilled on nodes "serverworker": the object has been modified; please apply your changes to the latest version and try again 98ms (03:35:01.436)
Trace[2010138855]: [1.459640358s] [1.459640358s] END
I0322 03:35:01.843073    2280 trace.go:236] Trace[1164307805]: "Update" accept:application/json, */*,audit-id:faf39515-8495-4d84-b124-023c2eedb8f0,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps/coredns,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:PUT (22-Mar-2024 03:35:00.260) (total time: 1577ms):
Trace[1164307805]: ["GuaranteedUpdate etcd3" audit-id:faf39515-8495-4d84-b124-023c2eedb8f0,key:/configmaps/kube-system/coredns,type:*core.ConfigMap,resource:configmaps 1577ms (03:35:00.260)
Trace[1164307805]:  ---"About to Encode" 61ms (03:35:00.329)
Trace[1164307805]:  ---"Txn call completed" 1013ms (03:35:01.343)]
Trace[1164307805]: ---"Writing http response done" 490ms (03:35:01.837)
Trace[1164307805]: [1.577029628s] [1.577029628s] END
time="2024-03-22T03:35:02Z" level=info msg="Updated coredns node hosts entry [192.168.56.111 serverworker]"
I0322 03:35:02.716704    2280 trace.go:236] Trace[2126763397]: "Patch" accept:application/vnd.kubernetes.protobuf, */*,audit-id:e4075a33-d7e3-44c8-874f-d71d88461fd8,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:ttl-controller,verb:PATCH (22-Mar-2024 03:35:00.189) (total time: 2521ms):
Trace[2126763397]: ["GuaranteedUpdate etcd3" audit-id:e4075a33-d7e3-44c8-874f-d71d88461fd8,key:/minions/serverworker,type:*core.Node,resource:nodes 2522ms (03:35:00.189)
Trace[2126763397]:  ---"Txn call completed" 126ms (03:35:00.317)
Trace[2126763397]:  ---"Txn call completed" 1693ms (03:35:02.022)
Trace[2126763397]:  ---"decode succeeded" len:2657 449ms (03:35:02.472)]
Trace[2126763397]: ---"About to apply patch" 127ms (03:35:00.317)
Trace[2126763397]: ---"Object stored in database" 2148ms (03:35:02.474)
Trace[2126763397]: ---"Writing http response done" 235ms (03:35:02.710)
Trace[2126763397]: [2.521577892s] [2.521577892s] END
I0322 03:35:02.840053    2280 trace.go:236] Trace[331510885]: "List" accept:application/json, */*,audit-id:6a648d78-d60d-4d2f-9959-c322e3c76950,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:cluster,url:/api/v1/nodes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:35:01.923) (total time: 912ms):
Trace[331510885]: ---"Writing http response done" count:2 595ms (03:35:02.835)
Trace[331510885]: [912.281029ms] [912.281029ms] END
I0322 03:35:02.891068    2280 trace.go:236] Trace[1466393744]: "List" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:7b56aa84-519d-4ed8-aca7-bdfe5a7f74d4,client:192.168.56.111,protocol:HTTP/2.0,resource:pods,scope:cluster,url:/api/v1/pods,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:35:02.226) (total time: 657ms):
Trace[1466393744]: ---"Writing http response done" count:0 193ms (03:35:02.883)
Trace[1466393744]: [657.700263ms] [657.700263ms] END
I0322 03:35:03.959668    2280 trace.go:236] Trace[415579878]: "Get" accept:application/json, */*,audit-id:a892c3c6-5b0e-4245-8cbf-19e6af5c600b,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:35:01.980) (total time: 1978ms):
Trace[415579878]: ---"About to write a response" 1873ms (03:35:03.854)
Trace[415579878]: [1.978250884s] [1.978250884s] END
I0322 03:35:03.959756    2280 trace.go:236] Trace[2168788]: "Get" accept:application/json, */*,audit-id:a44aef10-7793-463b-b0e2-cbdaeed64858,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:35:01.757) (total time: 2196ms):
Trace[2168788]: ---"About to write a response" 1949ms (03:35:03.707)
Trace[2168788]: ---"Writing http response done" 245ms (03:35:03.953)
Trace[2168788]: [2.196623061s] [2.196623061s] END
E0322 03:35:04.113375    2280 kubelet.go:2477] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.851s"
I0322 03:35:04.269595    2280 trace.go:236] Trace[1530720404]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:a2d796c9-7543-4a73-b4a4-8cb0bb255979,client:192.168.56.111,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/default/events/serverworker.17bef8d48295943e,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PATCH (22-Mar-2024 03:35:00.254) (total time: 4011ms):
Trace[1530720404]: ["GuaranteedUpdate etcd3" audit-id:a2d796c9-7543-4a73-b4a4-8cb0bb255979,key:/events/default/serverworker.17bef8d48295943e,type:*core.Event,resource:events 4011ms (03:35:00.254)
Trace[1530720404]:  ---"initial value restored" 1428ms (03:35:01.682)
Trace[1530720404]:  ---"About to Encode" 2077ms (03:35:03.760)
Trace[1530720404]:  ---"Txn call completed" 481ms (03:35:04.249)]
Trace[1530720404]: ---"About to check admission control" 627ms (03:35:02.311)
Trace[1530720404]: ---"Object stored in database" 1943ms (03:35:04.254)
Trace[1530720404]: [4.011252243s] [4.011252243s] END
I0322 03:35:04.284979    2280 trace.go:236] Trace[233925978]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:f15d946f-dae9-46db-a103-f7c1d6b758fa,client:127.0.0.1,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events/coredns-6799fbcd5-rx42b.17bef886db19ab2f,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PATCH (22-Mar-2024 03:35:00.192) (total time: 4092ms):
Trace[233925978]: ["GuaranteedUpdate etcd3" audit-id:f15d946f-dae9-46db-a103-f7c1d6b758fa,key:/events/kube-system/coredns-6799fbcd5-rx42b.17bef886db19ab2f,type:*core.Event,resource:events 4092ms (03:35:00.192)
Trace[233925978]:  ---"initial value restored" 880ms (03:35:01.073)
Trace[233925978]:  ---"About to Encode" 1894ms (03:35:02.967)
Trace[233925978]:  ---"Txn call completed" 1312ms (03:35:04.284)]
Trace[233925978]: ---"About to check admission control" 1429ms (03:35:02.504)
Trace[233925978]: ---"Object stored in database" 1780ms (03:35:04.284)
Trace[233925978]: [4.09208523s] [4.09208523s] END
I0322 03:35:04.304616    2280 trace.go:236] Trace[1630167686]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:b5bd5474-adff-4019-86cc-4838c09dfa67,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker/status,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PATCH (22-Mar-2024 03:35:00.150) (total time: 4153ms):
Trace[1630167686]: ["GuaranteedUpdate etcd3" audit-id:b5bd5474-adff-4019-86cc-4838c09dfa67,key:/minions/serverworker,type:*core.Node,resource:nodes 4152ms (03:35:00.151)
Trace[1630167686]:  ---"Txn call completed" 60ms (03:35:00.216)
Trace[1630167686]:  ---"Txn call completed" 142ms (03:35:00.361)
Trace[1630167686]:  ---"Txn call completed" 1540ms (03:35:01.907)
Trace[1630167686]:  ---"Retry value restored" 302ms (03:35:02.210)
Trace[1630167686]:  ---"About to Encode" 1936ms (03:35:04.146)
Trace[1630167686]:  ---"Txn call completed" 157ms (03:35:04.304)]
Trace[1630167686]: ---"About to apply patch" 61ms (03:35:00.216)
Trace[1630167686]: ---"About to apply patch" 143ms (03:35:00.361)
Trace[1630167686]: ---"About to apply patch" 1847ms (03:35:02.212)
Trace[1630167686]: ---"About to check admission control" 1561ms (03:35:03.773)
Trace[1630167686]: ---"Object stored in database" 530ms (03:35:04.304)
Trace[1630167686]: [4.153716534s] [4.153716534s] END
I0322 03:35:04.657024    2280 trace.go:236] Trace[1717221467]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:224b9f5b-987c-4fb4-8f86-17474f938573,client:192.168.56.111,protocol:HTTP/2.0,resource:csinodes,scope:resource,url:/apis/storage.k8s.io/v1/csinodes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:POST (22-Mar-2024 03:35:01.452) (total time: 3195ms):
Trace[1717221467]: ---"Write to database call succeeded" len:332 46ms (03:35:04.444)
Trace[1717221467]: ---"Writing http response done" 203ms (03:35:04.648)
Trace[1717221467]: [3.195384279s] [3.195384279s] END
I0322 03:35:04.670619    2280 trace.go:236] Trace[426903465]: "Update" accept:application/json, */*,audit-id:50747f02-9c73-446b-abe9-e4635347125d,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:PUT (22-Mar-2024 03:35:04.020) (total time: 649ms):
Trace[426903465]: ---"Conversion done" 76ms (03:35:04.101)
Trace[426903465]: ["GuaranteedUpdate etcd3" audit-id:50747f02-9c73-446b-abe9-e4635347125d,key:/minions/serverworker,type:*core.Node,resource:nodes 568ms (03:35:04.101)]
Trace[426903465]: [649.64329ms] [649.64329ms] END
I0322 03:35:04.768652    2280 trace.go:236] Trace[188255049]: "Create" accept:application/vnd.kubernetes.protobuf, */*,audit-id:dacdad36-1010-4981-af1c-bbabdf6d4806,client:127.0.0.1,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/default/events,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:node-controller,verb:POST (22-Mar-2024 03:35:02.868) (total time: 1894ms):
Trace[188255049]: ["Create etcd3" audit-id:dacdad36-1010-4981-af1c-bbabdf6d4806,key:/events/default/serverworker.17bef8d4ab485d06,type:*core.Event,resource:events 554ms (03:35:04.209)
Trace[188255049]:  ---"Txn call succeeded" 547ms (03:35:04.760)]
Trace[188255049]: [1.894334296s] [1.894334296s] END
I0322 03:35:04.771231    2280 trace.go:236] Trace[2030493204]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:a1100b46-a9c9-4164-840b-5d0267fbddb8,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/node-controller,verb:PUT (22-Mar-2024 03:35:04.032) (total time: 738ms):
Trace[2030493204]: ---"limitedReadBody succeeded" len:2620 30ms (03:35:04.062)
Trace[2030493204]: ["GuaranteedUpdate etcd3" audit-id:a1100b46-a9c9-4164-840b-5d0267fbddb8,key:/minions/serverworker,type:*core.Node,resource:nodes 694ms (03:35:04.076)]
Trace[2030493204]: [738.837254ms] [738.837254ms] END
I0322 03:35:05.088764    2280 kube.go:482] Creating the node lease for IPv4. This is the n.Spec.PodCIDRs: [10.42.1.0/24]
I0322 03:35:05.176656    2280 subnet.go:160] Batch elem [0] is { lease.Event{Type:0, Lease:lease.Lease{EnableIPv4:true, EnableIPv6:false, Subnet:ip.IP4Net{IP:0xa2a0100, PrefixLen:0x18}, IPv6Subnet:ip.IP6Net{IP:(*ip.IP6)(nil), PrefixLen:0x0}, Attrs:lease.LeaseAttrs{PublicIP:0xa00020f, PublicIPv6:(*ip.IP6)(nil), BackendType:"vxlan", BackendData:json.RawMessage{0x7b, 0x22, 0x56, 0x4e, 0x49, 0x22, 0x3a, 0x31, 0x2c, 0x22, 0x56, 0x74, 0x65, 0x70, 0x4d, 0x41, 0x43, 0x22, 0x3a, 0x22, 0x36, 0x36, 0x3a, 0x39, 0x63, 0x3a, 0x66, 0x30, 0x3a, 0x61, 0x36, 0x3a, 0x63, 0x32, 0x3a, 0x34, 0x30, 0x22, 0x7d}, BackendV6Data:json.RawMessage(nil)}, Expiration:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Asof:0}} }
I0322 03:35:05.379839    2280 trace.go:236] Trace[1389543613]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:3ada2af7-a4bf-450a-82b1-09553a870fc8,client:127.0.0.1,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:POST (22-Mar-2024 03:35:04.706) (total time: 667ms):
Trace[1389543613]: ---"Writing http response done" 256ms (03:35:05.373)
Trace[1389543613]: [667.610284ms] [667.610284ms] END
I0322 03:35:05.728594    2280 trace.go:236] Trace[1627208121]: "Create" accept:application/vnd.kubernetes.protobuf, */*,audit-id:a434ddbc-5300-46f8-b5b3-a7fc4b98e08f,client:192.168.56.111,protocol:HTTP/2.0,resource:events,scope:resource,url:/apis/events.k8s.io/v1/namespaces/default/events,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:POST (22-Mar-2024 03:35:04.650) (total time: 1071ms):
Trace[1627208121]: ["Create etcd3" audit-id:a434ddbc-5300-46f8-b5b3-a7fc4b98e08f,key:/events/default/serverworker.17bef8d5875bbc24,type:*core.Event,resource:events 975ms (03:35:04.747)
Trace[1627208121]:  ---"TransformToStorage succeeded" 194ms (03:35:04.942)
Trace[1627208121]:  ---"Txn call succeeded" 634ms (03:35:05.577)]
Trace[1627208121]: ---"Write to database call succeeded" len:235 78ms (03:35:05.660)
Trace[1627208121]: ---"Writing http response done" 61ms (03:35:05.721)
Trace[1627208121]: [1.071060047s] [1.071060047s] END
I0322 03:35:05.733146    2280 trace.go:236] Trace[1842129950]: "Get" accept:application/json, */*,audit-id:21371fc1-2cd0-4082-b509-e844939b3f51,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps/coredns,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:GET (22-Mar-2024 03:35:04.850) (total time: 882ms):
Trace[1842129950]: ---"About to write a response" 806ms (03:35:05.658)
Trace[1842129950]: [882.256649ms] [882.256649ms] END
I0322 03:35:06.107892    2280 trace.go:236] Trace[479029001]: "Get" accept:application/json, */*,audit-id:29a8a982-6525-434b-a37f-8c8436178de7,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:35:05.497) (total time: 607ms):
Trace[479029001]: ---"About to write a response" 524ms (03:35:06.023)
Trace[479029001]: [607.909017ms] [607.909017ms] END
I0322 03:35:06.250129    2280 trace.go:236] Trace[1762731657]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:4457898a-0e4b-4757-a33c-f636d7d6a8c9,client:192.168.56.111,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/default/events/serverworker.17bef8d48295c280,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PATCH (22-Mar-2024 03:35:04.611) (total time: 1631ms):
Trace[1762731657]: ["GuaranteedUpdate etcd3" audit-id:4457898a-0e4b-4757-a33c-f636d7d6a8c9,key:/events/default/serverworker.17bef8d48295c280,type:*core.Event,resource:events 1494ms (03:35:04.749)
Trace[1762731657]:  ---"initial value restored" 296ms (03:35:05.046)
Trace[1762731657]:  ---"About to Encode" 776ms (03:35:05.822)
Trace[1762731657]:  ---"Txn call completed" 387ms (03:35:06.212)]
Trace[1762731657]: ---"About to check admission control" 551ms (03:35:05.688)
Trace[1762731657]: ---"Object stored in database" 551ms (03:35:06.239)
Trace[1762731657]: [1.631508225s] [1.631508225s] END
I0322 03:35:06.295179    2280 trace.go:236] Trace[169336027]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:e528a1ee-74d3-424d-8118-1d009b0ede0a,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:node-controller,verb:GET (22-Mar-2024 03:35:05.713) (total time: 581ms):
Trace[169336027]: ---"About to write a response" 504ms (03:35:06.218)
Trace[169336027]: [581.629179ms] [581.629179ms] END
I0322 03:35:06.785748    2280 trace.go:236] Trace[1754833991]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:58661374-a388-4590-951e-e302f2b5600e,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/node-controller,verb:PUT (22-Mar-2024 03:35:05.611) (total time: 1104ms):
Trace[1754833991]: ---"limitedReadBody succeeded" len:2701 73ms (03:35:05.684)
Trace[1754833991]: ["GuaranteedUpdate etcd3" audit-id:58661374-a388-4590-951e-e302f2b5600e,key:/minions/serverworker,type:*core.Node,resource:nodes 1080ms (03:35:05.697)]
Trace[1754833991]: ---"Write to database call failed" len:2701,err:Operation cannot be fulfilled on nodes "serverworker": the object has been modified; please apply your changes to the latest version and try again 93ms (03:35:06.705)
Trace[1754833991]: [1.104530657s] [1.104530657s] END
I0322 03:35:07.122264    2280 trace.go:236] Trace[735285025]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:b0865d0f-9629-4e21-81bc-0092902d0b05,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker/status,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PATCH (22-Mar-2024 03:35:04.868) (total time: 2247ms):
Trace[735285025]: ["GuaranteedUpdate etcd3" audit-id:b0865d0f-9629-4e21-81bc-0092902d0b05,key:/minions/serverworker,type:*core.Node,resource:nodes 2168ms (03:35:04.948)
Trace[735285025]:  ---"initial value restored" 132ms (03:35:05.080)
Trace[735285025]:  ---"About to Encode" 636ms (03:35:05.717)
Trace[735285025]:  ---"Txn call completed" 249ms (03:35:05.966)
Trace[735285025]:  ---"About to Encode" 300ms (03:35:06.278)
Trace[735285025]:  ---"Txn call completed" 796ms (03:35:07.077)]
Trace[735285025]: ---"About to check admission control" 628ms (03:35:05.710)
Trace[735285025]: ---"About to apply patch" 271ms (03:35:05.981)
Trace[735285025]: ---"About to check admission control" 75ms (03:35:06.057)
Trace[735285025]: ---"Object stored in database" 1046ms (03:35:07.103)
Trace[735285025]: ---"Writing http response done" 11ms (03:35:07.115)
Trace[735285025]: [2.247061992s] [2.247061992s] END
E0322 03:35:07.651228    2280 kubelet.go:2477] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.418s"
I0322 03:35:07.680371    2280 trace.go:236] Trace[1040690404]: "Get" accept:application/json, */*,audit-id:be6fe9de-5d32-4a3a-b34a-6db432cb15e0,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:35:06.374) (total time: 1304ms):
Trace[1040690404]: ---"About to write a response" 914ms (03:35:07.289)
Trace[1040690404]: ---"Writing http response done" 389ms (03:35:07.678)
Trace[1040690404]: [1.304566253s] [1.304566253s] END
I0322 03:35:07.716913    2280 trace.go:236] Trace[9909489]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:c52db792-947a-48ac-a226-cfe61fec5d53,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/server,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:35:06.870) (total time: 750ms):
Trace[9909489]: ---"About to Get from storage" 140ms (03:35:07.011)
Trace[9909489]: ---"Writing http response done" 597ms (03:35:07.621)
Trace[9909489]: [750.11256ms] [750.11256ms] END
I0322 03:35:07.521468    2280 topologycache.go:237] "Can't get CPU or zone information for node" node="serverworker"
I0322 03:35:08.110210    2280 trace.go:236] Trace[1195075800]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:4e8f9d74-d0bd-4f5e-b24b-a43a1ef11774,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/server,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PUT (22-Mar-2024 03:35:06.398) (total time: 1563ms):
Trace[1195075800]: ---"Conversion done" 95ms (03:35:06.499)
Trace[1195075800]: ["GuaranteedUpdate etcd3" audit-id:4e8f9d74-d0bd-4f5e-b24b-a43a1ef11774,key:/leases/kube-node-lease/server,type:*coordination.Lease,resource:leases.coordination.k8s.io 1436ms (03:35:06.527)
Trace[1195075800]:  ---"About to Encode" 393ms (03:35:06.927)
Trace[1195075800]:  ---"Txn call completed" 1017ms (03:35:07.949)]
Trace[1195075800]: [1.563781199s] [1.563781199s] END
I0322 03:35:08.184044    2280 trace.go:236] Trace[2030201223]: "Update" accept:application/json, */*,audit-id:591eb5fc-9981-4e34-850b-3e9b19af9691,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:PUT (22-Mar-2024 03:35:06.207) (total time: 1973ms):
Trace[2030201223]: ---"Conversion done" 220ms (03:35:06.440)
Trace[2030201223]: ["GuaranteedUpdate etcd3" audit-id:591eb5fc-9981-4e34-850b-3e9b19af9691,key:/minions/serverworker,type:*core.Node,resource:nodes 1714ms (03:35:06.467)]
Trace[2030201223]: ---"Write to database call failed" len:3827,err:Operation cannot be fulfilled on nodes "serverworker": the object has been modified; please apply your changes to the latest version and try again 69ms (03:35:08.176)
Trace[2030201223]: [1.973447659s] [1.973447659s] END
I0322 03:35:08.233605    2280 trace.go:236] Trace[1437165877]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:20cfb365-8856-4a46-b582-8925aadd34de,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-7zoch227uv4owxg75pgtnmv3jq,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PUT (22-Mar-2024 03:35:06.123) (total time: 2102ms):
Trace[1437165877]: ["GuaranteedUpdate etcd3" audit-id:20cfb365-8856-4a46-b582-8925aadd34de,key:/leases/kube-system/apiserver-7zoch227uv4owxg75pgtnmv3jq,type:*coordination.Lease,resource:leases.coordination.k8s.io 1967ms (03:35:06.257)
Trace[1437165877]:  ---"About to Encode" 777ms (03:35:07.038)
Trace[1437165877]:  ---"Transaction prepared" 132ms (03:35:07.182)
Trace[1437165877]:  ---"Txn call completed" 1025ms (03:35:08.208)]
Trace[1437165877]: ---"Write to database call succeeded" len:590 16ms (03:35:08.225)
Trace[1437165877]: [2.10231613s] [2.10231613s] END
I0322 03:35:08.389466    2280 trace.go:236] Trace[39502681]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:d12200d7-86bf-432d-abed-88658b7085f1,client:127.0.0.1,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events/metrics-server-67c658944b-p82jk.17bef886e4ee4346,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PATCH (22-Mar-2024 03:35:06.408) (total time: 1976ms):
Trace[39502681]: ["GuaranteedUpdate etcd3" audit-id:d12200d7-86bf-432d-abed-88658b7085f1,key:/events/kube-system/metrics-server-67c658944b-p82jk.17bef886e4ee4346,type:*core.Event,resource:events 1938ms (03:35:06.446)
Trace[39502681]:  ---"initial value restored" 494ms (03:35:06.940)
Trace[39502681]:  ---"About to Encode" 1238ms (03:35:08.179)
Trace[39502681]:  ---"Txn call completed" 198ms (03:35:08.377)]
Trace[39502681]: ---"About to check admission control" 1074ms (03:35:08.017)
Trace[39502681]: ---"Object stored in database" 360ms (03:35:08.377)
Trace[39502681]: [1.976712462s] [1.976712462s] END
I0322 03:35:08.396349    2280 trace.go:236] Trace[1319234068]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:f785b5bf-3870-4f3f-bfbc-6bd96d716c2c,client:192.168.56.111,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/default/events/serverworker.17bef8d48295cbd0,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PATCH (22-Mar-2024 03:35:07.085) (total time: 1311ms):
Trace[1319234068]: ["GuaranteedUpdate etcd3" audit-id:f785b5bf-3870-4f3f-bfbc-6bd96d716c2c,key:/events/default/serverworker.17bef8d48295cbd0,type:*core.Event,resource:events 1295ms (03:35:07.100)
Trace[1319234068]:  ---"initial value restored" 781ms (03:35:07.881)
Trace[1319234068]:  ---"About to Encode" 232ms (03:35:08.114)
Trace[1319234068]:  ---"Txn call completed" 242ms (03:35:08.360)]
Trace[1319234068]: ---"About to check admission control" 229ms (03:35:08.113)
Trace[1319234068]: ---"Object stored in database" 282ms (03:35:08.396)
Trace[1319234068]: [1.311244699s] [1.311244699s] END
I0322 03:35:08.409415    2280 trace.go:236] Trace[1285128081]: "Patch" accept:application/vnd.kubernetes.protobuf, */*,audit-id:12ab1bf4-c3e0-41ef-a017-63630f001822,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:node-controller,verb:PATCH (22-Mar-2024 03:35:07.687) (total time: 721ms):
Trace[1285128081]: ---"About to check admission control" 113ms (03:35:08.041)
Trace[1285128081]: [721.681126ms] [721.681126ms] END
I0322 03:35:08.570300    2280 trace.go:236] Trace[82456323]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.56.110,type:*v1.Endpoints,resource:apiServerIPInfo (22-Mar-2024 03:35:06.801) (total time: 1762ms):
Trace[82456323]: ---"initial value restored" 921ms (03:35:07.723)
Trace[82456323]: ---"Transaction prepared" 533ms (03:35:08.258)
Trace[82456323]: ---"Txn call completed" 302ms (03:35:08.561)
Trace[82456323]: [1.762033251s] [1.762033251s] END
I0322 03:35:09.107930    2280 trace.go:236] Trace[897073686]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:7436b2b1-7643-42df-b5ce-26d07c3b5bb1,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/node-controller,verb:PUT (22-Mar-2024 03:35:08.290) (total time: 809ms):
Trace[897073686]: ---"limitedReadBody succeeded" len:3426 23ms (03:35:08.313)
Trace[897073686]: ["GuaranteedUpdate etcd3" audit-id:7436b2b1-7643-42df-b5ce-26d07c3b5bb1,key:/minions/serverworker,type:*core.Node,resource:nodes 773ms (03:35:08.327)
Trace[897073686]:  ---"About to Encode" 190ms (03:35:08.536)
Trace[897073686]:  ---"Txn call completed" 507ms (03:35:09.048)]
Trace[897073686]: ---"Write to database call succeeded" len:3426 34ms (03:35:09.090)
Trace[897073686]: [809.747242ms] [809.747242ms] END
I0322 03:35:09.161415    2280 trace.go:236] Trace[942728755]: "Get" accept:application/json, */*,audit-id:0b6df141-f58c-4636-a7e9-0ad082666265,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps/coredns,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:GET (22-Mar-2024 03:35:08.589) (total time: 572ms):
Trace[942728755]: ---"About to write a response" 329ms (03:35:08.918)
Trace[942728755]: ---"Writing http response done" 242ms (03:35:09.161)
Trace[942728755]: [572.266155ms] [572.266155ms] END
I0322 03:35:09.449662    2280 node_controller.go:502] Successfully initialized node serverworker with cloud provider
I0322 03:35:09.479792    2280 event.go:307] "Event occurred" object="serverworker" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="Synced" message="Node synced successfully"
I0322 03:35:09.510093    2280 trace.go:236] Trace[1379730231]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:85ab5256-13e8-47b2-8ab5-36be2de91931,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:node-controller,verb:GET (22-Mar-2024 03:35:08.951) (total time: 555ms):
Trace[1379730231]: ---"About to write a response" 515ms (03:35:09.467)
Trace[1379730231]: [555.165497ms] [555.165497ms] END
I0322 03:35:09.515022    2280 trace.go:236] Trace[89162441]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:62b7f5ea-03d7-42f6-9ca6-b5deb7191fe7,client:127.0.0.1,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/default/endpoints/kubernetes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:35:08.853) (total time: 661ms):
Trace[89162441]: ---"About to write a response" 660ms (03:35:09.514)
Trace[89162441]: [661.046721ms] [661.046721ms] END
I0322 03:35:09.596822    2280 trace.go:236] Trace[853471791]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:68c93039-efb8-4ae2-a10a-01ae08b1877a,client:127.0.0.1,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:POST (22-Mar-2024 03:35:08.825) (total time: 763ms):
Trace[853471791]: ---"Conversion done" 71ms (03:35:08.901)
Trace[853471791]: ["Create etcd3" audit-id:68c93039-efb8-4ae2-a10a-01ae08b1877a,key:/events/kube-system/coredns-6799fbcd5-rx42b.17bef8c49ba12899,type:*core.Event,resource:events 621ms (03:35:08.970)
Trace[853471791]:  ---"Txn call succeeded" 541ms (03:35:09.515)]
Trace[853471791]: ---"Writing http response done" 72ms (03:35:09.589)
Trace[853471791]: [763.996467ms] [763.996467ms] END
I0322 03:35:09.596885    2280 trace.go:236] Trace[1429300544]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:794d4aad-33e6-4694-94dd-74d79ca83417,client:192.168.56.111,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/default/events,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:POST (22-Mar-2024 03:35:08.561) (total time: 1034ms):
Trace[1429300544]: ---"Conversion done" 88ms (03:35:08.654)
Trace[1429300544]: ["Create etcd3" audit-id:794d4aad-33e6-4694-94dd-74d79ca83417,key:/events/default/serverworker.17bef8d4859bcf43,type:*core.Event,resource:events 563ms (03:35:09.032)
Trace[1429300544]:  ---"Txn call succeeded" 551ms (03:35:09.586)]
Trace[1429300544]: [1.034886023s] [1.034886023s] END
I0322 03:35:09.746327    2280 trace.go:236] Trace[1037520958]: "Patch" accept:application/vnd.kubernetes.protobuf, */*,audit-id:81ae6cc9-4518-4607-9080-262e9c5f644e,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:node-controller,verb:PATCH (22-Mar-2024 03:35:08.637) (total time: 1103ms):
Trace[1037520958]: ["GuaranteedUpdate etcd3" audit-id:81ae6cc9-4518-4607-9080-262e9c5f644e,key:/minions/serverworker,type:*core.Node,resource:nodes 1025ms (03:35:08.715)
Trace[1037520958]:  ---"About to Encode" 372ms (03:35:09.096)
Trace[1037520958]:  ---"Txn call completed" 376ms (03:35:09.475)]
Trace[1037520958]: ---"About to check admission control" 218ms (03:35:08.943)
Trace[1037520958]: ---"About to apply patch" 532ms (03:35:09.476)
Trace[1037520958]: ---"About to check admission control" 225ms (03:35:09.702)
Trace[1037520958]: [1.103790647s] [1.103790647s] END
I0322 03:35:10.198839    2280 scope.go:117] "RemoveContainer" containerID="afe4640d3a121c51da6bad0227debc7256b8fec3cc9277d94249ed4d0a986231"
I0322 03:35:10.397217    2280 trace.go:236] Trace[2140307920]: "Patch" accept:application/vnd.kubernetes.protobuf, */*,audit-id:7a111ef1-aad5-45ca-9894-57553bdc9fa3,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:node-controller,verb:PATCH (22-Mar-2024 03:35:09.863) (total time: 530ms):
Trace[2140307920]: ---"About to check admission control" 82ms (03:35:09.997)
Trace[2140307920]: ---"Object stored in database" 396ms (03:35:10.393)
Trace[2140307920]: [530.009645ms] [530.009645ms] END
I0322 03:35:11.187527    2280 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/helm-install-traefik-fxsb6" containerName="helm"
I0322 03:35:11.968324    2280 trace.go:236] Trace[1551670874]: "List" accept:application/json, */*,audit-id:8c86b7c7-95e4-4934-ab29-3d2f73633a55,client:192.168.56.111,protocol:HTTP/2.0,resource:pods,scope:cluster,url:/api/v1/pods,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:35:11.081) (total time: 879ms):
Trace[1551670874]: ---"About to List from storage" 174ms (03:35:11.256)
Trace[1551670874]: ---"Writing http response done" count:5 538ms (03:35:11.961)
Trace[1551670874]: [879.586352ms] [879.586352ms] END
I0322 03:35:11.976496    2280 trace.go:236] Trace[2049842796]: "List" accept:application/json, */*,audit-id:061aa2be-1135-46f7-ad33-edb09d26ba37,client:192.168.56.111,protocol:HTTP/2.0,resource:namespaces,scope:cluster,url:/api/v1/namespaces,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:35:10.934) (total time: 1041ms):
Trace[2049842796]: ---"Writing http response done" count:4 1036ms (03:35:11.976)
Trace[2049842796]: [1.041855258s] [1.041855258s] END
I0322 03:35:12.575880    2280 trace.go:236] Trace[2011692761]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:324551c8-1e7d-4fb3-a28c-04853a417142,client:192.168.56.111,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:35:10.381) (total time: 1994ms):
Trace[2011692761]: [1.994447969s] [1.994447969s] END
I0322 03:35:12.621587    2280 trace.go:236] Trace[809168681]: "Get" accept:application/json, */*,audit-id:07bfe81e-9d36-46b8-8e7c-b79c8cadca0c,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps/coredns,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:GET (22-Mar-2024 03:35:11.177) (total time: 1150ms):
Trace[809168681]: ---"About to write a response" 814ms (03:35:11.992)
Trace[809168681]: ---"Writing http response done" 335ms (03:35:12.327)
Trace[809168681]: [1.150390693s] [1.150390693s] END
I0322 03:35:15.360564    2280 trace.go:236] Trace[2054216727]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:161fe2d8-1674-4e79-8bbb-e8b1760aa5e4,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:node-controller,verb:GET (22-Mar-2024 03:35:12.725) (total time: 2089ms):
Trace[2054216727]: ---"Writing http response done" 2086ms (03:35:14.814)
Trace[2054216727]: [2.089541127s] [2.089541127s] END
I0322 03:35:44.578861    2280 request.go:697] Waited for 9.375690799s due to client-side throttling, not priority and fairness, request: GET:https://127.0.0.1:6444/api
W0322 03:36:21.466955    2280 transport.go:301] Unable to cancel request for *otelhttp.Transport
W0322 03:36:22.339956    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.RuntimeClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:36:22.375589    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Lease ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:36:22.376956    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PersistentVolume ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:36:22.377440    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:36:22.377916    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.VolumeAttachment ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:36:22.378025    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.RoleBinding ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:36:22.378510    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Endpoints ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:36:22.378533    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Secret ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:36:22.378583    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.IngressClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:36:22.378912    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:36:22.378942    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1beta3.PriorityLevelConfiguration ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:36:22.385586    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:36:22.385702    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1beta3.FlowSchema ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:36:22.386304    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.APIService ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:36:22.386808    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Role ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:36:22.386864    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ValidatingWebhookConfiguration ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:36:22.386907    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.MutatingWebhookConfiguration ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:36:22.387446    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:36:22.388234    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PriorityClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:36:22.388626    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Namespace ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:36:22.388684    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Lease ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:36:22.388714    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.LimitRange ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:36:22.388741    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ServiceAccount ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:36:22.388763    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:36:23.189061    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.EndpointSlice ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:36:23.200394    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:36:23.761125    2280 handler_proxy.go:137] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
W0322 03:36:23.889389    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:36:23.891304    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PersistentVolume ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:36:23.892210    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PodDisruptionBudget ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:36:23.908443    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.StatefulSet ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:36:23.913564    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.StorageClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:36:23.914294    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ReplicationController ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:36:23.914851    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CSIDriver ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:36:23.915726    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:36:23.916392    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CSINode ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:36:23.917278    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:36:23.918016    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CSIStorageCapacity ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:36:24.382893    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:35:52.960039    2280 health_controller.go:162] Metrics Controller heartbeat missed
W0322 03:36:39.991280    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:36:21.214138    2280 handler_proxy.go:93] no RequestInfo found in the context
W0322 03:36:43.511817    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ClusterRoleBinding ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:36:43.593409    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:36:43.760028    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.DaemonSet ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:36:45.292294    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:36:25.347652    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:36:47.048421    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:36:48.502861    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.EndpointSlice ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I0322 03:36:49.099124    2280 trace.go:236] Trace[832651997]: "Calculate volume metrics of kube-api-access-mk5j8 for pod kube-system/helm-install-traefik-fxsb6" (22-Mar-2024 03:35:52.513) (total time: 56572ms):
Trace[832651997]: [56.572549938s] [56.572549938s] END
E0322 03:36:51.865697    2280 health_controller.go:162] Metrics Controller heartbeat missed
W0322 03:36:53.095481    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.HelmChartConfig ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:36:53.130626    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.HelmChart ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:36:53.132949    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Job ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:36:39.595823    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.VolumeAttachment ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:37:07.026734    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:37:10.020406    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ReplicaSet ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:36:45.852989    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Namespace ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding



W0322 03:36:45.879525    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PersistentVolumeClaim ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:38:11.246868    2280 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 03:39:01.089478    2280 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0322 03:39:01.218264    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.252710    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ResourceQuota ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.273925    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PriorityClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.274299    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1beta3.FlowSchema ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.274828    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Job ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.275063    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ValidatingWebhookConfiguration ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.275698    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Endpoints ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.276325    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Lease ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.276561    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.276764    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v2.HorizontalPodAutoscaler ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.277408    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.277768    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.278053    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Deployment ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.286449    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ResourceQuota ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.286587    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.286780    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.NetworkPolicy ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.287013    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.StorageClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.287183    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Namespace ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.287335    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.287490    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.287652    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PodDisruptionBudget ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.288146    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1beta3.PriorityLevelConfiguration ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.288439    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ReplicationController ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.298509    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.298963    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Ingress ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.299514    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Secret ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.299637    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ServiceAccount ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.300092    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PodTemplate ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.300205    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.IngressClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.300292    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CSINode ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.300379    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.300477    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.300553    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CronJob ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.300652    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.300741    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.300828    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.300925    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.301032    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.RuntimeClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.301132    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ClusterRoleBinding ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.301217    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ControllerRevision ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.301357    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.301476    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.RoleBinding ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.301996    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.LimitRange ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.302122    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.302253    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PersistentVolumeClaim ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.302391    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Role ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.302519    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.EndpointSlice ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.302632    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PersistentVolume ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.302724    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ReplicaSet ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.302796    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:01.309179    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:39:01.494661    2280 remote_runtime.go:407] "ListContainers with filter from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" filter="&ContainerFilter{Id:,State:nil,PodSandboxId:,LabelSelector:map[string]string{},}"
E0322 03:39:01.497335    2280 kuberuntime_container.go:477] "ListContainers failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded"
E0322 03:39:01.499378    2280 generic.go:238] "GenericPLEG: Unable to retrieve pods" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded"
time="2024-03-22T03:37:09Z" level=info msg="Slow SQL (started: 2024-03-22 03:35:14.12519908 +0000 UTC m=+414.116847314) (total time: 48.64932891s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC LIMIT 500 : [[% 691]]"
E0322 03:37:23.329727    2280 handler_proxy.go:137] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
W0322 03:37:34.271579    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ServiceAccount ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:37:49.523387    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CertificateSigningRequest ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I0322 03:39:02.322370    2280 trace.go:236] Trace[514068412]: "Calculate volume metrics of tmp-dir for pod kube-system/metrics-server-67c658944b-p82jk" (22-Mar-2024 03:36:10.246) (total time: 68210ms):
Trace[514068412]: [1m8.210155963s] [1m8.210155963s] END
W0322 03:36:57.568603    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:02.422227    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:39:02.497198    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
E0322 03:39:02.536182    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
E0322 03:39:02.549098    2280 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:39:02.549202    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:39:02.550480    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
E0322 03:39:02.550535    2280 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:39:02.550989    2280 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 03:39:02.551929    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
E0322 03:39:02.554352    2280 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:39:02.554366    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
W0322 03:37:50.796891    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:39:02.555900    2280 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 03:39:02.558416    2280 timeout.go:142] post-timeout activity - time-elapsed: 185.025744ms, GET "/api/v1/nodes/serverworker" result: <nil>
E0322 03:39:02.572509    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:39:02.572608    2280 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 03:39:02.572704    2280 timeout.go:142] post-timeout activity - time-elapsed: 200.345819ms, GET "/api/v1/nodes/serverworker" result: <nil>
W0322 03:36:41.614365    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.StorageClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:38:32.191161    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.NetworkPolicy ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:39:02.601293    2280 timeout.go:142] post-timeout activity - time-elapsed: 228.895805ms, GET "/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/serverworker" result: <nil>
W0322 03:37:30.603211    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Secret ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I0322 03:37:50.977065    2280 trace.go:236] Trace[949712173]: "Calculate volume metrics of kube-api-access-czff4 for pod kube-system/local-path-provisioner-6c86858495-lvfsk" (22-Mar-2024 03:36:21.908) (total time: 40084ms):
Trace[949712173]: [40.08437467s] [40.08437467s] END
W0322 03:38:02.336735    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CustomResourceDefinition ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I0322 03:38:59.505513    2280 trace.go:236] Trace[109819938]: "iptables ChainExists" (22-Mar-2024 03:35:28.390) (total time: 156682ms):
Trace[109819938]: [2m36.682847841s] [2m36.682847841s] END
W0322 03:38:59.525026    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:38:59.789081    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:00.556079    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ClusterRole ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:00.931373    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:00.931797    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:00.932860    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:00.932897    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:00.932916    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CSIStorageCapacity ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:00.932926    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:37:10.096291    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:39:02.733651    2280 controller_utils.go:215] unable to remove [&Taint{Key:node.kubernetes.io/not-ready,Value:,Effect:NoSchedule,TimeAdded:<nil>,}] unneeded taint from unresponsive Node "serverworker": Patch "https://127.0.0.1:6444/api/v1/nodes/serverworker": http2: client connection lost
E0322 03:39:02.772725    2280 node_lifecycle_controller.go:1257] "Failed to remove taint from node" err="Get \"https://127.0.0.1:6444/api/v1/nodes/serverworker?resourceVersion=0\": http2: client connection lost" node="serverworker"
E0322 03:39:02.773227    2280 node_lifecycle_controller.go:800] "Failed to remove taints from node. Will retry in next iteration" node="serverworker"
E0322 03:39:02.821413    2280 wrap.go:54] timeout or abort while handling: method=GET URI="/api/v1/nodes/serverworker?resourceVersion=0" audit-ID="161fe2d8-1674-4e79-8bbb-e8b1760aa5e4"
W0322 03:39:02.822193    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Namespace ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:39:02.823691    2280 timeout.go:142] post-timeout activity - time-elapsed: 36.176778593s, GET "/api/v1/nodes/serverworker" result: <nil>
E0322 03:39:02.844238    2280 remote_runtime.go:407] "ListContainers with filter from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" filter="&ContainerFilter{Id:,State:nil,PodSandboxId:,LabelSelector:map[string]string{},}"
E0322 03:39:02.845380    2280 kuberuntime_container.go:477] "ListContainers failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded"
E0322 03:39:02.864775    2280 watch.go:287] unable to encode watch object *v1.WatchEvent: client disconnected (&streaming.encoderWithAllocator{writer:(*framer.lengthDelimitedFrameWriter)(0xc01118bf38), encoder:(*versioning.codec)(0xc00e570b40), memAllocator:(*runtime.Allocator)(0xc00711b128)})
time="2024-03-22T03:39:02Z" level=info msg="error in remotedialer server [400]: write tcp 192.168.56.110:6443->192.168.56.111:53030: write: broken pipe"
E0322 03:39:02.904921    2280 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:39:02.906181    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:39:02.906354    2280 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
W0322 03:39:02.918443    2280 reflector.go:458] object-"kube-system"/"coredns-custom": watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:39:02.922385    2280 kubelet_node_status.go:540] "Error updating node status, will retry" err="error getting node \"server\": Get \"https://127.0.0.1:6443/api/v1/nodes/server?resourceVersion=0&timeout=10s\": net/http: request canceled (Client.Timeout exceeded while awaiting headers)"
W0322 03:39:02.935431    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:02.936618    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:02.997361    2280 reflector.go:458] object-"kube-system"/"kube-root-ca.crt": watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:39:02.999404    2280 remote_image.go:128] "ListImages with filter from image service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" filter="nil"
E0322 03:39:02.999456    2280 kuberuntime_image.go:103] "Failed to list images" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded"
I0322 03:39:02.999784    2280 image_gc_manager.go:210] "Failed to update image list" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded"
W0322 03:39:03.003310    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CSIDriver ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:39:03.045745    2280 handler_proxy.go:137] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
I0322 03:39:03.120011    2280 trace.go:236] Trace[1856483553]: "SerializeObject" audit-id:6495b897-817f-4a16-aebc-bb9bc3944cfe,method:PATCH,url:/api/v1/nodes/serverworker,protocol:HTTP/2.0,mediaType:application/vnd.kubernetes.protobuf,encoder:{"encodeGV":"v1","encoder":"protobuf","name":"versioning"} (22-Mar-2024 03:37:24.361) (total time: 98757ms):
Trace[1856483553]: ---"About to start writing response" size:154 98542ms (03:39:02.903)
Trace[1856483553]: [1m38.75719033s] [1m38.75719033s] END
I0322 03:39:03.120403    2280 trace.go:236] Trace[1227755979]: "Patch" accept:application/vnd.kubernetes.protobuf, */*,audit-id:6495b897-817f-4a16-aebc-bb9bc3944cfe,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:node-controller,verb:PATCH (22-Mar-2024 03:35:12.924) (total time: 230196ms):
Trace[1227755979]: ---"About to apply patch" 292ms (03:35:13.713)
Trace[1227755979]: [3m50.196289337s] [3m50.196289337s] END
W0322 03:39:03.162030    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I0322 03:39:03.167916    2280 trace.go:236] Trace[1399812597]: "iptables ChainExists" (22-Mar-2024 03:35:37.760) (total time: 205407ms):
Trace[1399812597]: [3m25.407269611s] [3m25.407269611s] END
W0322 03:39:03.169013    2280 reflector.go:458] object-"kube-system"/"chart-content-traefik": watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:39:03.174579    2280 event.go:289] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"metrics-server-67c658944b-p82jk.17bef8c556ebe6b3", GenerateName:"", Namespace:"kube-system", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"metrics-server-67c658944b-p82jk", UID:"1ea7b8bb-866e-4ee0-84fb-ed8098467564", APIVersion:"v1", ResourceVersion:"440", FieldPath:"spec.containers{metrics-server}"}, Reason:"Unhealthy", Message:"Readiness probe failed: Get \"https://10.42.0.4:10250/readyz\": context deadline exceeded", Source:v1.EventSource{Component:"kubelet", Host:"server"}, FirstTimestamp:time.Date(2024, time.March, 22, 3, 33, 54, 897913523, time.Local), LastTimestamp:time.Date(2024, time.March, 22, 3, 33, 54, 897913523, time.Local), Count:1, Type:"Warning", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"kubelet", ReportingInstance:"server"}': 'Post "https://127.0.0.1:6443/api/v1/namespaces/kube-system/events": http2: client connection lost'(may retry after sleeping)
I0322 03:39:03.180911    2280 trace.go:236] Trace[1897581811]: "iptables ChainExists" (22-Mar-2024 03:35:26.655) (total time: 86807ms):
Trace[1897581811]: [1m26.807444467s] [1m26.807444467s] END
E0322 03:39:03.182487    2280 timeout.go:142] post-timeout activity - time-elapsed: 208.205789ms, PATCH "/api/v1/nodes/serverworker" result: <nil>
W0322 03:39:03.182541    2280 reflector.go:458] object-"kube-system"/"coredns": watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:03.182685    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.RuntimeClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:03.182718    2280 reflector.go:458] object-"kube-system"/"local-path-config": watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:03.182738    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:03.182796    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:03.223262    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Addon ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:39:03.244384    2280 watch.go:287] unable to encode watch object *v1.WatchEvent: client disconnected (&streaming.encoderWithAllocator{writer:(*framer.lengthDelimitedFrameWriter)(0xc01122dc50), encoder:(*versioning.codec)(0xc00c074820), memAllocator:(*runtime.Allocator)(0xc0072d5a88)})
W0322 03:39:03.245499    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ClusterRoleBinding ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I0322 03:39:03.264586    2280 trace.go:236] Trace[23062412]: "iptables ChainExists" (22-Mar-2024 03:35:27.408) (total time: 215855ms):
Trace[23062412]: [3m35.855827946s] [3m35.855827946s] END
W0322 03:39:03.274819    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I0322 03:39:03.276108    2280 trace.go:236] Trace[614214614]: "DeltaFIFO Pop Process" ID:v2.autoscaling,Depth:25,Reason:slow event handlers blocking the queue (22-Mar-2024 03:36:51.508) (total time: 131767ms):
Trace[614214614]: [2m11.767094167s] [2m11.767094167s] END
I0322 03:39:03.279890    2280 trace.go:236] Trace[154044102]: "Calculate volume metrics of config-volume for pod kube-system/coredns-6799fbcd5-rx42b" (22-Mar-2024 03:36:40.026) (total time: 143253ms):
Trace[154044102]: [2m23.253120987s] [2m23.253120987s] END
W0322 03:38:50.958838    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
time="2024-03-22T03:39:03Z" level=info msg="error in remotedialer server [400]: write tcp 192.168.56.110:6443->192.168.56.110:52460: i/o timeout"
W0322 03:37:36.837528    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Endpoints ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:38:46.572774    2280 health_controller.go:162] Metrics Controller heartbeat missed
E0322 03:39:03.315466    2280 health_controller.go:131] Network Policy Controller heartbeat missed
E0322 03:39:03.315558    2280 health_controller.go:162] Metrics Controller heartbeat missed
E0322 03:39:03.315573    2280 health_controller.go:162] Metrics Controller heartbeat missed
E0322 03:39:03.315679    2280 health_controller.go:162] Metrics Controller heartbeat missed
W0322 03:39:03.325757    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
time="2024-03-22T03:39:03Z" level=error msg="Error writing ping" error="write tcp 192.168.56.110:52460->192.168.56.110:6443: i/o timeout"
time="2024-03-22T03:39:03Z" level=error msg="Error writing ping" error="write tcp 192.168.56.110:52460->192.168.56.110:6443: i/o timeout"
time="2024-03-22T03:39:03Z" level=error msg="Remotedialer proxy error" error="read tcp 192.168.56.110:52460->192.168.56.110:6443: i/o timeout"
W0322 03:39:03.339226    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.MutatingWebhookConfiguration ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:03.356478    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:39:03.367454    2280 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:39:03.368165    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:39:03.388345    2280 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 03:39:03.388716    2280 trace.go:236] Trace[70694152]: "SerializeObject" audit-id:a75de4b8-d5ef-4451-9179-12c4e49501d0,method:POST,url:/apis/authentication.k8s.io/v1/tokenreviews,protocol:HTTP/2.0,mediaType:application/vnd.kubernetes.protobuf,encoder:{"encodeGV":"authentication.k8s.io/v1","encoder":"protobuf","name":"versioning"} (22-Mar-2024 03:38:27.428) (total time: 35960ms):
Trace[70694152]: ---"About to start writing response" size:154 35445ms (03:39:02.873)
Trace[70694152]: [35.960058712s] [35.960058712s] END
I0322 03:39:03.388919    2280 trace.go:236] Trace[1343073428]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:a75de4b8-d5ef-4451-9179-12c4e49501d0,client:192.168.56.111,protocol:HTTP/2.0,resource:tokenreviews,scope:resource,url:/apis/authentication.k8s.io/v1/tokenreviews,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:POST (22-Mar-2024 03:35:12.879) (total time: 230509ms):
Trace[1343073428]: ---"Conversion done" 477ms (03:35:13.379)
Trace[1343073428]: ---"Write to database call failed" len:1152,err:Timeout: request did not complete within requested timeout - context deadline exceeded 53677ms (03:36:07.059)
Trace[1343073428]: [3m50.509732971s] [3m50.509732971s] END
E0322 03:39:03.390174    2280 timeout.go:142] post-timeout activity - time-elapsed: 4.51s, POST "/apis/authentication.k8s.io/v1/tokenreviews" result: <nil>
E0322 03:39:03.391864    2280 finisher.go:175] FinishRequest: post-timeout activity - time-elapsed: 2m38.669274506s, panicked: false, err: context deadline exceeded, panic-reason: <nil>
E0322 03:39:03.396846    2280 status.go:71] apiserver received an error that is not an metav1.Status: context.deadlineExceededError{}: context deadline exceeded
W0322 03:38:53.968949    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.StatefulSet ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:39:03.401881    2280 watch.go:287] unable to encode watch object *v1.WatchEvent: client disconnected (&streaming.encoderWithAllocator{writer:(*framer.lengthDelimitedFrameWriter)(0xc0119e51d0), encoder:(*versioning.codec)(0xc00c593400), memAllocator:(*runtime.Allocator)(0xc00fd0dde8)})
W0322 03:39:03.402203    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:38:54.361817    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.DaemonSet ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:39:03.402628    2280 watch.go:287] unable to encode watch object *v1.WatchEvent: client disconnected (&streaming.encoderWithAllocator{writer:(*framer.lengthDelimitedFrameWriter)(0xc0119e5878), encoder:(*versioning.codec)(0xc00c593a40), memAllocator:(*runtime.Allocator)(0xc000bdf668)})
time="2024-03-22T03:39:03Z" level=error msg="error syncing 'serverworker': handler node: Put \"https://127.0.0.1:6444/api/v1/nodes/serverworker\": http2: client connection lost, requeuing"
E0322 03:39:03.482508    2280 controller.go:193] "Failed to update lease" err="context deadline exceeded"
I0322 03:39:03.524072    2280 trace.go:236] Trace[2074452594]: "DeltaFIFO Pop Process" ID:v1.networking.k8s.io,Depth:24,Reason:slow event handlers blocking the queue (22-Mar-2024 03:39:03.276) (total time: 247ms):
Trace[2074452594]: [247.476036ms] [247.476036ms] END
E0322 03:39:03.545993    2280 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:39:03.547532    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:39:03.549487    2280 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 03:39:03.549549    2280 trace.go:236] Trace[1008341132]: "Get" accept:application/json, */*,audit-id:be1d3bc9-c793-4349-84ec-da172bba8c74,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:35:12.838) (total time: 230710ms):
Trace[1008341132]: [3m50.710722368s] [3m50.710722368s] END
E0322 03:39:03.554074    2280 timeout.go:142] post-timeout activity - time-elapsed: 154.750259ms, GET "/api/v1/nodes/serverworker" result: <nil>
E0322 03:39:03.588564    2280 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:39:03.588727    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
W0322 03:39:03.546807    2280 handler_proxy.go:93] no RequestInfo found in the context
W0322 03:38:59.522809    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:39:03.619793    2280 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
E0322 03:39:03.620193    2280 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 03:39:03.695832    2280 trace.go:236] Trace[1509563748]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:0b998cbd-f828-487c-84cf-f8c8a6bf3429,client:127.0.0.1,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:POST (22-Mar-2024 03:35:10.992) (total time: 232703ms):
Trace[1509563748]: ["Create etcd3" audit-id:0b998cbd-f828-487c-84cf-f8c8a6bf3429,key:/events/kube-system/metrics-server-67c658944b-p82jk.17bef8c556ebe6b3,type:*core.Event,resource:events 230147ms (03:35:13.548)
Trace[1509563748]:  ---"About to Encode" 215ms (03:35:13.763)
Trace[1509563748]:  ---"Txn call failed" err:context deadline exceeded 229577ms (03:39:03.346)]
Trace[1509563748]: [3m52.703366862s] [3m52.703366862s] END
E0322 03:39:03.724073    2280 watch.go:287] unable to encode watch object *v1.WatchEvent: client disconnected (&streaming.encoderWithAllocator{writer:(*framer.lengthDelimitedFrameWriter)(0xc011a51d40), encoder:(*versioning.codec)(0xc00c602e60), memAllocator:(*runtime.Allocator)(0xc0076a6b70)})
E0322 03:39:03.728216    2280 timeout.go:142] post-timeout activity - time-elapsed: 307.594429ms, POST "/api/v1/namespaces/kube-system/events" result: <nil>
W0322 03:39:00.141412    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:39:03.754541    2280 remote_runtime.go:407] "ListContainers with filter from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" filter="&ContainerFilter{Id:,State:nil,PodSandboxId:,LabelSelector:map[string]string{},}"
E0322 03:39:03.759604    2280 resource_metrics.go:152] "Error getting summary for resourceMetric prometheus endpoint" err="failed to list pod stats: failed to get pod or container map: failed to list all containers: rpc error: code = DeadlineExceeded desc = context deadline exceeded"
E0322 03:39:03.778091    2280 watch.go:287] unable to encode watch object *v1.WatchEvent: client disconnected (&streaming.encoderWithAllocator{writer:(*framer.lengthDelimitedFrameWriter)(0xc0115481c8), encoder:(*versioning.codec)(0xc00c074c80), memAllocator:(*runtime.Allocator)(0xc008e812c0)})
W0322 03:39:03.780137    2280 reflector.go:458] object-"kube-system"/"chart-values-traefik": watch of *v1.Secret ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:03.824361    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:39:03.838772    2280 remote_runtime.go:407] "ListContainers with filter from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" filter="&ContainerFilter{Id:,State:nil,PodSandboxId:,LabelSelector:map[string]string{},}"
E0322 03:39:03.840527    2280 container_log_manager.go:185] "Failed to rotate container logs" err="failed to list containers: rpc error: code = DeadlineExceeded desc = context deadline exceeded"
time="2024-03-22T03:39:03Z" level=info msg="Slow SQL (started: 2024-03-22 03:37:09.589775781 +0000 UTC m=+529.581424026) (total time: 1m51.220862598s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? AND mkv.id <= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC LIMIT 1000 : [[/registry/traefik.containo.us/ingressroutes/% 691 false]]"
E0322 03:39:03.853091    2280 remote_runtime.go:633] "Status from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded"
time="2024-03-22T03:39:03Z" level=error msg="error while range on /registry/traefik.containo.us/ingressroutes/ /registry/traefik.containo.us/ingressroutes/: context canceled"
E0322 03:39:03.859394    2280 kubelet.go:2840] "Container runtime sanity check failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded"
W0322 03:39:00.898164    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CSIDriver ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I0322 03:39:03.879919    2280 log.go:245] http: TLS handshake error from 10.42.0.4:60500: write tcp 192.168.56.110:10250->10.42.0.4:60500: write: broken pipe
I0322 03:39:03.886291    2280 log.go:245] http: TLS handshake error from 10.42.0.4:55868: write tcp 192.168.56.110:10250->10.42.0.4:55868: write: broken pipe
I0322 03:39:03.910102    2280 request.go:697] Waited for 1.841951082s due to client-side throttling, not priority and fairness, request: GET:https://127.0.0.1:6444/api/v1/namespaces?resourceVersion=612
I0322 03:39:03.947218    2280 trace.go:236] Trace[290725516]: "Calculate volume metrics of kube-api-access-wg79r for pod kube-system/metrics-server-67c658944b-p82jk" (22-Mar-2024 03:39:02.329) (total time: 1616ms):
Trace[290725516]: [1.616038491s] [1.616038491s] END
W0322 03:39:03.968849    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:39:03.992897    2280 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ClusterRole ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I0322 03:39:03.994818    2280 log.go:245] http: TLS handshake error from 10.42.0.4:41262: write tcp 192.168.56.110:10250->10.42.0.4:41262: write: broken pipe
I0322 03:39:03.994848    2280 log.go:245] http: TLS handshake error from 10.42.0.4:41498: write tcp 192.168.56.110:10250->10.42.0.4:41498: write: broken pipe
I0322 03:39:03.994856    2280 log.go:245] http: TLS handshake error from 10.42.0.4:36832: write tcp 192.168.56.110:10250->10.42.0.4:36832: write: broken pipe
I0322 03:39:03.994865    2280 log.go:245] http: TLS handshake error from 10.42.0.4:40204: write tcp 192.168.56.110:10250->10.42.0.4:40204: write: broken pipe
I0322 03:39:03.994872    2280 log.go:245] http: TLS handshake error from 10.42.0.4:41340: EOF
I0322 03:39:03.994882    2280 log.go:245] http: TLS handshake error from 10.42.0.4:47766: EOF
I0322 03:39:03.994892    2280 log.go:245] http: TLS handshake error from 10.42.0.4:48280: EOF
I0322 03:39:03.994904    2280 log.go:245] http: TLS handshake error from 10.42.0.4:45638: EOF
E0322 03:39:04.023038    2280 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:39:04.023958    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:39:04.024440    2280 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 03:39:04.024485    2280 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
I0322 03:39:04.025167    2280 trace.go:236] Trace[992676576]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:1cc797c8-a777-4b32-8f78-ac6bbaf6e8f2,client:192.168.56.111,protocol:HTTP/2.0,resource:tokenreviews,scope:resource,url:/apis/authentication.k8s.io/v1/tokenreviews,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:POST (22-Mar-2024 03:39:02.473) (total time: 1552ms):
Trace[992676576]: ---"About to convert to expected version" 1533ms (03:39:04.008)
Trace[992676576]: [1.552036163s] [1.552036163s] END
E0322 03:39:04.027288    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:39:04.027357    2280 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 03:39:04.030702    2280 trace.go:236] Trace[55705872]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:f7eb89e3-a46c-4ed6-a64f-63ce7aeb247d,client:192.168.56.111,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:39:02.482) (total time: 1548ms):
Trace[55705872]: [1.548115846s] [1.548115846s] END
I0322 03:39:04.031311    2280 trace.go:236] Trace[1172189316]: "Calculate volume metrics of config-volume for pod kube-system/local-path-provisioner-6c86858495-lvfsk" (22-Mar-2024 03:39:02.607) (total time: 1424ms):
Trace[1172189316]: [1.424152118s] [1.424152118s] END
time="2024-03-22T03:39:04Z" level=info msg="Slow SQL (started: 2024-03-22 03:35:15.23186016 +0000 UTC m=+415.223508416) (total time: 3m48.805272775s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC  : [[/registry/minions/serverworker false]]"
E0322 03:39:04.046290    2280 node_lifecycle_controller.go:523] "Failed to taint NoSchedule on node, requeue it" err="failed to swap taints of node &Node{ObjectMeta:{serverworker    4db81d8d-2f54-4067-a296-9c608cf1f3d9 679 0 2024-03-22 03:34:59 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:linux kubernetes.io/arch:amd64 kubernetes.io/hostname:serverworker kubernetes.io/os:linux] map[alpha.kubernetes.io/provided-node-ip:192.168.56.111 flannel.alpha.coreos.com/backend-data:{\"VNI\":1,\"VtepMAC\":\"66:9c:f0:a6:c2:40\"} flannel.alpha.coreos.com/backend-type:vxlan flannel.alpha.coreos.com/kube-subnet-manager:true flannel.alpha.coreos.com/public-ip:10.0.2.15 k3s.io/hostname:serverworker k3s.io/internal-ip:192.168.56.111 k3s.io/node-args:[\"agent\",\"--server\",\"https://192.168.56.110:6443\",\"--token-file\",\"/vagrant/provision/token\",\"--node-ip\",\"192.168.56.111\",\"--log\",\"/vagrant/logs/agent.logs\"] k3s.io/node-config-hash:YEJVE25BLELMUPKU6DR5NCCNF5THAXBIIU55LWYRIMCEB5QXNKOA==== k3s.io/node-env:{\"K3S_DATA_DIR\":\"/var/lib/rancher/k3s/data/a3b46c0299091b71bfcc617b1e1fec1845c13bdd848584ceb39d2e700e702a4b\"} node.alpha.kubernetes.io/ttl:0 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [] [{k3s Update v1 2024-03-22 03:35:00 +0000 UTC FieldsV1 {\"f:metadata\":{\"f:annotations\":{\".\":{},\"f:alpha.kubernetes.io/provided-node-ip\":{},\"f:k3s.io/hostname\":{},\"f:k3s.io/internal-ip\":{},\"f:k3s.io/node-args\":{},\"f:k3s.io/node-config-hash\":{},\"f:k3s.io/node-env\":{},\"f:node.alpha.kubernetes.io/ttl\":{},\"f:volumes.kubernetes.io/controller-managed-attach-detach\":{}},\"f:labels\":{\".\":{},\"f:beta.kubernetes.io/arch\":{},\"f:beta.kubernetes.io/os\":{},\"f:kubernetes.io/arch\":{},\"f:kubernetes.io/hostname\":{},\"f:kubernetes.io/os\":{}}},\"f:spec\":{\"f:podCIDR\":{},\"f:podCIDRs\":{\".\":{},\"v:\\\"10.42.1.0/24\\\"\":{}},\"f:taints\":{}}} } {k3s Update v1 2024-03-22 03:35:06 +0000 UTC FieldsV1 {\"f:metadata\":{\"f:annotations\":{\"f:flannel.alpha.coreos.com/backend-data\":{},\"f:flannel.alpha.coreos.com/backend-type\":{},\"f:flannel.alpha.coreos.com/kube-subnet-manager\":{},\"f:flannel.alpha.coreos.com/public-ip\":{}}},\"f:status\":{\"f:conditions\":{\"k:{\\\"type\\\":\\\"DiskPressure\\\"}\":{\"f:lastHeartbeatTime\":{}},\"k:{\\\"type\\\":\\\"MemoryPressure\\\"}\":{\"f:lastHeartbeatTime\":{}},\"k:{\\\"type\\\":\\\"PIDPressure\\\"}\":{\"f:lastHeartbeatTime\":{}},\"k:{\\\"type\\\":\\\"Ready\\\"}\":{\"f:lastHeartbeatTime\":{},\"f:lastTransitionTime\":{},\"f:message\":{},\"f:reason\":{},\"f:status\":{}}}}} status}]},Spec:NodeSpec{PodCIDR:10.42.1.0/24,DoNotUseExternalID:,ProviderID:,Unschedulable:false,Taints:[]Taint{Taint{Key:node.cloudprovider.kubernetes.io/uninitialized,Value:true,Effect:NoSchedule,TimeAdded:<nil>,},Taint{Key:node.kubernetes.io/not-ready,Value:,Effect:NoSchedule,TimeAdded:<nil>,},},ConfigSource:nil,PodCIDRs:[10.42.1.0/24],},Status:NodeStatus{Capacity:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{41555521536 0} {<nil>} 40581564Ki BinarySI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1011728384 0} {<nil>}  BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{39477745429 0} {<nil>} 39477745429 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1011728384 0} {<nil>}  BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2024-03-22 03:35:04 +0000 UTC,LastTransitionTime:2024-03-22 03:35:00 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2024-03-22 03:35:04 +0000 UTC,LastTransitionTime:2024-03-22 03:35:00 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2024-03-22 03:35:04 +0000 UTC,LastTransitionTime:2024-03-22 03:35:00 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2024-03-22 03:35:04 +0000 UTC,LastTransitionTime:2024-03-22 03:35:04 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:192.168.56.111,},NodeAddress{Type:Hostname,Address:serverworker,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:91fde03401b34b6b98a63d39246ade6d,SystemUUID:7ff1ef8d-1d14-4747-bc65-a961234fbb64,BootID:e1649a1a-7823-42a5-8da8-cff6881d1809,KernelVersion:5.4.0-173-generic,OSImage:Ubuntu 20.04.6 LTS,ContainerRuntimeVersion:containerd://1.7.11-k3s2,KubeletVersion:v1.28.7+k3s1,KubeProxyVersion:v1.28.7+k3s1,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}" node="serverworker"
E0322 03:39:04.068460    2280 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:39:04.068828    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:39:04.070009    2280 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 03:39:04.070328    2280 trace.go:236] Trace[1477661573]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:8742ee6b-d472-4f3a-b6f8-0753ce15253c,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:39:03.098) (total time: 971ms):
Trace[1477661573]: ---"About to Get from storage" 963ms (03:39:04.062)
Trace[1477661573]: [971.463235ms] [971.463235ms] END
time="2024-03-22T03:39:04Z" level=info msg="Slow SQL (started: 2024-03-22 03:35:27.528055447 +0000 UTC m=+427.519703699) (total time: 3m36.620461407s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC LIMIT 1 : [[/registry/masterleases/192.168.56.110 true]]"
E0322 03:39:04.149175    2280 timeout.go:142] post-timeout activity - time-elapsed: 1.778607443s, POST "/apis/authentication.k8s.io/v1/tokenreviews" result: <nil>
I0322 03:39:04.150937    2280 log.go:245] http: TLS handshake error from 10.42.0.4:47668: write tcp 192.168.56.110:10250->10.42.0.4:47668: write: broken pipe
E0322 03:39:04.175943    2280 handler_proxy.go:137] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
W0322 03:39:04.181272    2280 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:39:04.185613    2280 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 03:39:04.185776    2280 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0322 03:39:04.190515    2280 log.go:245] http: TLS handshake error from 10.42.0.4:56088: write tcp 192.168.56.110:10250->10.42.0.4:56088: write: broken pipe
time="2024-03-22T03:39:04Z" level=error msg="Compact failed: failed to get current revision: sql: Rows are closed"
E0322 03:39:04.207172    2280 watch.go:287] unable to encode watch object *v1.WatchEvent: client disconnected (&streaming.encoderWithAllocator{writer:(*framer.lengthDelimitedFrameWriter)(0xc01122c318), encoder:(*versioning.codec)(0xc00c3b4aa0), memAllocator:(*runtime.Allocator)(0xc00711b8d8)})
E0322 03:39:04.230423    2280 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:39:04.231034    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:39:04.238716    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
E0322 03:39:04.239499    2280 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:39:04.239944    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:39:04.241068    2280 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 03:39:04.241516    2280 timeout.go:142] post-timeout activity - time-elapsed: 102.681904ms, GET "/api/v1/nodes/serverworker" result: <nil>
E0322 03:39:04.290328    2280 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
time="2024-03-22T03:39:04Z" level=error msg="error in txn compare:<target:MOD key:\"/registry/events/kube-system/metrics-server-67c658944b-p82jk.17bef8c556ebe6b3\" mod_revision:0 > success:<request_put:<key:\"/registry/events/kube-system/metrics-server-67c658944b-p82jk.17bef8c556ebe6b3\" value:\"k8s\\000\\n\\013\\n\\002v1\\022\\005Event\\022\\261\\005\\n\\375\\002\\n0metrics-server-67c658944b-p82jk.17bef8c556ebe6b3\\022\\000\\032\\013kube-system\\\"\\000*$f7e0ed3c-5d8e-420d-b639-e1129d4d90902\\0008\\000B\\010\\010\\360\\370\\363\\257\\006\\020\\000\\212\\001\\202\\002\\n\\003k3s\\022\\006Update\\032\\002v1\\\"\\010\\010\\357\\370\\363\\257\\006\\020\\0002\\010FieldsV1:\\330\\001\\n\\325\\001{\\\"f:count\\\":{},\\\"f:firstTimestamp\\\":{},\\\"f:involvedObject\\\":{},\\\"f:lastTimestamp\\\":{},\\\"f:message\\\":{},\\\"f:reason\\\":{},\\\"f:reportingComponent\\\":{},\\\"f:reportingInstance\\\":{},\\\"f:source\\\":{\\\"f:component\\\":{},\\\"f:host\\\":{}},\\\"f:type\\\":{}}B\\000\\022\\203\\001\\n\\003Pod\\022\\013kube-system\\032\\037metrics-server-67c658944b-p82jk\\\"$1ea7b8bb-866e-4ee0-84fb-ed8098467564*\\002v12\\003440:\\037spec.containers{metrics-server}\\032\\tUnhealthy\\\"WReadiness probe failed: Get \\\"https://10.42.0.4:10250/readyz\\\": context deadline exceeded*\\021\\n\\007kubelet\\022\\006server2\\010\\010\\242\\370\\363\\257\\006\\020\\000:\\010\\010\\242\\370\\363\\257\\006\\020\\000@\\001J\\007WarningR\\000b\\000r\\007kubeletz\\006server\\032\\000\\\"\\000\" lease:3660 > > : context deadline exceeded"
E0322 03:39:04.326065    2280 timeout.go:142] post-timeout activity - time-elapsed: 169.433296ms, PUT "/api/v1/nodes/serverworker" result: <nil>
E0322 03:39:04.352502    2280 timeout.go:142] post-timeout activity - time-elapsed: 1.980336707s, GET "/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/serverworker" result: <nil>
E0322 03:39:04.380609    2280 finisher.go:175] FinishRequest: post-timeout activity - time-elapsed: 3m14.585983731s, panicked: false, err: context deadline exceeded, panic-reason: <nil>
E0322 03:39:04.417826    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
E0322 03:39:04.421746    2280 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:39:04.421786    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:39:04.421866    2280 timeout.go:142] post-timeout activity - time-elapsed: 1.603645425s, GET "/api/v1/nodes/serverworker" result: <nil>
E0322 03:39:04.430351    2280 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 03:39:04.430447    2280 timeout.go:142] post-timeout activity - time-elapsed: 659.1758ms, GET "/api/v1/nodes/serverworker" result: <nil>
E0322 03:39:04.495386    2280 status.go:71] apiserver received an error that is not an metav1.Status: context.deadlineExceededError{}: context deadline exceeded
E0322 03:39:04.496734    2280 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:39:04.497849    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:39:04.497974    2280 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 03:39:04.499183    2280 timeout.go:142] post-timeout activity - time-elapsed: 2m39.362768394s, GET "/api/v1/nodes/serverworker" result: <nil>
E0322 03:39:04.506706    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
E0322 03:39:04.506897    2280 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:39:04.509720    2280 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:39:04.509810    2280 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 03:39:04.513934    2280 timeout.go:142] post-timeout activity - time-elapsed: 285.982323ms, GET "/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/serverworker" result: <nil>
W0322 03:39:04.625978    2280 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:39:04.796966    2280 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 03:39:04.796995    2280 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E0322 03:39:04.956367    2280 kubelet.go:1402] "Container garbage collection failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded"
W0322 03:39:05.186421    2280 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:39:05.190292    2280 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 03:39:05.190406    2280 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0322 03:39:05.199663    2280 trace.go:236] Trace[1228633067]: "List" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json,audit-id:68820a68-97c8-4d35-965f-4c77510ff7f5,client:127.0.0.1,protocol:HTTP/2.0,resource:addons,scope:cluster,url:/apis/k3s.cattle.io/v1/addons,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/metadata-informers,verb:LIST (22-Mar-2024 03:39:04.654) (total time: 543ms):
Trace[1228633067]: ---"Writing http response done" count:13 543ms (03:39:05.198)
Trace[1228633067]: [543.544148ms] [543.544148ms] END
I0322 03:39:05.199927    2280 trace.go:236] Trace[1062373836]: "List" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json,audit-id:ca09f8c3-e8ff-4cb7-ae18-e3ae41166d87,client:127.0.0.1,protocol:HTTP/2.0,resource:apiservices,scope:cluster,url:/apis/apiregistration.k8s.io/v1/apiservices,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/metadata-informers,verb:LIST (22-Mar-2024 03:39:04.663) (total time: 536ms):
Trace[1062373836]: ---"Writing http response done" count:26 534ms (03:39:05.199)
Trace[1062373836]: [536.165397ms] [536.165397ms] END
I0322 03:39:05.200118    2280 trace.go:236] Trace[1442293318]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:747255ba-25b0-429f-b185-436bbce03750,client:127.0.0.1,protocol:HTTP/2.0,resource:flowschemas,scope:cluster,url:/apis/flowcontrol.apiserver.k8s.io/v1beta3/flowschemas,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:39:04.688) (total time: 511ms):
Trace[1442293318]: ---"Writing http response done" count:13 511ms (03:39:05.200)
Trace[1442293318]: [511.876214ms] [511.876214ms] END
I0322 03:39:05.201314    2280 trace.go:236] Trace[1028354810]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:b74dd90e-76fa-4ad1-abae-a0e3444a0682,client:127.0.0.1,protocol:HTTP/2.0,resource:runtimeclasses,scope:cluster,url:/apis/node.k8s.io/v1/runtimeclasses,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:39:04.691) (total time: 509ms):
Trace[1028354810]: ---"Writing http response done" count:10 508ms (03:39:05.201)
Trace[1028354810]: [509.289958ms] [509.289958ms] END
E0322 03:39:05.202777    2280 finisher.go:175] FinishRequest: post-timeout activity - time-elapsed: 3m17.367593941s, panicked: false, err: <nil>, panic-reason: <nil>
E0322 03:39:05.202929    2280 finisher.go:175] FinishRequest: post-timeout activity - time-elapsed: 1.056135835s, panicked: false, err: <nil>, panic-reason: <nil>
I0322 03:39:05.749054    2280 trace.go:236] Trace[1005025905]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:1d7a0fd3-df79-49bb-b4d0-ad8284219683,client:127.0.0.1,protocol:HTTP/2.0,resource:clusterroles,scope:cluster,url:/apis/rbac.authorization.k8s.io/v1/clusterroles,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:39:05.210) (total time: 535ms):
Trace[1005025905]: ---"Writing http response done" count:69 533ms (03:39:05.746)
Trace[1005025905]: [535.962248ms] [535.962248ms] END
I0322 03:39:05.789827    2280 trace.go:236] Trace[188191850]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:44f9bc8b-8417-4756-9fd7-bebd3f5a0a3d,client:127.0.0.1,protocol:HTTP/2.0,resource:runtimeclasses,scope:cluster,url:/apis/node.k8s.io/v1/runtimeclasses,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:39:05.287) (total time: 502ms):
Trace[188191850]: ---"Writing http response done" count:10 502ms (03:39:05.789)
Trace[188191850]: [502.446477ms] [502.446477ms] END
I0322 03:39:05.790016    2280 trace.go:236] Trace[923679630]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:e3d758bb-e776-4a48-8485-775100101cd9,client:127.0.0.1,protocol:HTTP/2.0,resource:flowschemas,scope:cluster,url:/apis/flowcontrol.apiserver.k8s.io/v1beta3/flowschemas,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:39:05.287) (total time: 502ms):
Trace[923679630]: ---"Writing http response done" count:13 502ms (03:39:05.789)
Trace[923679630]: [502.375145ms] [502.375145ms] END
I0322 03:39:05.791286    2280 trace.go:236] Trace[880127172]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:6870a92f-370a-4cd1-b580-8067f4d367d9,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:cluster,url:/api/v1/configmaps,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:39:04.706) (total time: 1084ms):
Trace[880127172]: ---"Writing http response done" count:11 1084ms (03:39:05.791)
Trace[880127172]: [1.084570329s] [1.084570329s] END
I0322 03:39:05.791439    2280 trace.go:236] Trace[643231406]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:1aac69bd-479f-4d97-93a3-6788c6c8e53c,client:127.0.0.1,protocol:HTTP/2.0,resource:prioritylevelconfigurations,scope:cluster,url:/apis/flowcontrol.apiserver.k8s.io/v1beta3/prioritylevelconfigurations,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:39:04.707) (total time: 1083ms):
Trace[643231406]: ---"Writing http response done" count:8 1082ms (03:39:05.791)
Trace[643231406]: [1.083528093s] [1.083528093s] END
I0322 03:39:05.791545    2280 trace.go:236] Trace[532172586]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:4e1c7538-6194-4cfa-991f-6d2a76857a63,client:127.0.0.1,protocol:HTTP/2.0,resource:replicasets,scope:cluster,url:/apis/apps/v1/replicasets,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:39:04.711) (total time: 1080ms):
Trace[532172586]: ---"Writing http response done" count:3 1080ms (03:39:05.791)
Trace[532172586]: [1.080491948s] [1.080491948s] END
I0322 03:39:05.791634    2280 trace.go:236] Trace[1379116332]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:e3583ac1-b6f6-4b4a-b936-11eb6dfbe95f,client:127.0.0.1,protocol:HTTP/2.0,resource:jobs,scope:cluster,url:/apis/batch/v1/jobs,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:39:04.715) (total time: 1076ms):
Trace[1379116332]: ---"Writing http response done" count:2 1076ms (03:39:05.791)
Trace[1379116332]: [1.076148429s] [1.076148429s] END
I0322 03:39:05.873153    2280 trace.go:236] Trace[1087287361]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:36:51.643) (total time: 134229ms):
Trace[1087287361]: ---"Objects listed" error:<nil> 134229ms (03:39:05.873)
Trace[1087287361]: [2m14.229535395s] [2m14.229535395s] END
I0322 03:39:05.876118    2280 trace.go:236] Trace[1825742564]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:36:24.881) (total time: 160994ms):
Trace[1825742564]: ---"Objects listed" error:<nil> 160994ms (03:39:05.876)
Trace[1825742564]: [2m40.994907983s] [2m40.994907983s] END
I0322 03:39:05.876599    2280 trace.go:236] Trace[357333988]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:36:24.754) (total time: 161121ms):
Trace[357333988]: ---"Objects listed" error:<nil> 161121ms (03:39:05.876)
Trace[357333988]: [2m41.121809864s] [2m41.121809864s] END
I0322 03:39:05.876733    2280 trace.go:236] Trace[1470899804]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:36:25.096) (total time: 160780ms):
Trace[1470899804]: ---"Objects listed" error:<nil> 160780ms (03:39:05.876)
Trace[1470899804]: [2m40.780037288s] [2m40.780037288s] END
I0322 03:39:05.877432    2280 trace.go:236] Trace[1678210615]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:36:27.650) (total time: 158227ms):
Trace[1678210615]: ---"Objects listed" error:<nil> 158227ms (03:39:05.877)
Trace[1678210615]: [2m38.227295715s] [2m38.227295715s] END
I0322 03:39:05.878932    2280 trace.go:236] Trace[1844592227]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:36:25.237) (total time: 160641ms):
Trace[1844592227]: ---"Objects listed" error:<nil> 160641ms (03:39:05.878)
Trace[1844592227]: [2m40.641038758s] [2m40.641038758s] END
I0322 03:39:05.879078    2280 trace.go:236] Trace[1064448585]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:36:25.045) (total time: 160833ms):
Trace[1064448585]: ---"Objects listed" error:<nil> 160833ms (03:39:05.879)
Trace[1064448585]: [2m40.833274391s] [2m40.833274391s] END
I0322 03:39:05.880598    2280 trace.go:236] Trace[1112663694]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:36:27.509) (total time: 158370ms):
Trace[1112663694]: ---"Objects listed" error:<nil> 158370ms (03:39:05.880)
Trace[1112663694]: [2m38.370710129s] [2m38.370710129s] END
I0322 03:39:05.880973    2280 trace.go:236] Trace[583085220]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:36:25.173) (total time: 160707ms):
Trace[583085220]: ---"Objects listed" error:<nil> 160707ms (03:39:05.880)
Trace[583085220]: [2m40.707133537s] [2m40.707133537s] END
I0322 03:39:05.881409    2280 trace.go:236] Trace[745740719]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:38:11.209) (total time: 54671ms):
Trace[745740719]: ---"Objects listed" error:<nil> 54671ms (03:39:05.881)
Trace[745740719]: [54.671648751s] [54.671648751s] END
I0322 03:39:05.881553    2280 trace.go:236] Trace[334477462]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:36:24.858) (total time: 161023ms):
Trace[334477462]: ---"Objects listed" error:<nil> 161023ms (03:39:05.881)
Trace[334477462]: [2m41.023359919s] [2m41.023359919s] END
I0322 03:39:05.881756    2280 trace.go:236] Trace[92950292]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:36:30.042) (total time: 155839ms):
Trace[92950292]: ---"Objects listed" error:<nil> 155839ms (03:39:05.881)
Trace[92950292]: [2m35.839561483s] [2m35.839561483s] END
I0322 03:39:05.891522    2280 trace.go:236] Trace[1248113022]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:36:47.541) (total time: 138350ms):
Trace[1248113022]: ---"Objects listed" error:<nil> 138350ms (03:39:05.891)
Trace[1248113022]: [2m18.350022811s] [2m18.350022811s] END
I0322 03:39:05.893636    2280 trace.go:236] Trace[1272635807]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:36:44.924) (total time: 140968ms):
Trace[1272635807]: ---"Objects listed" error:<nil> 140968ms (03:39:05.893)
Trace[1272635807]: [2m20.968650931s] [2m20.968650931s] END
I0322 03:39:05.895101    2280 trace.go:236] Trace[39436784]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:36:23.724) (total time: 162170ms):
Trace[39436784]: ---"Objects listed" error:<nil> 162170ms (03:39:05.894)
Trace[39436784]: [2m42.170191833s] [2m42.170191833s] END
I0322 03:39:05.895280    2280 trace.go:236] Trace[195514795]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:36:44.722) (total time: 141172ms):
Trace[195514795]: ---"Objects listed" error:<nil> 141172ms (03:39:05.895)
Trace[195514795]: [2m21.172895779s] [2m21.172895779s] END
I0322 03:39:05.896442    2280 trace.go:236] Trace[1877872596]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:36:25.457) (total time: 160438ms):
Trace[1877872596]: ---"Objects listed" error:<nil> 160438ms (03:39:05.896)
Trace[1877872596]: [2m40.438595337s] [2m40.438595337s] END
I0322 03:39:05.939754    2280 trace.go:236] Trace[606590780]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:2bcba13d-6f4b-4555-8b9d-344432e2de2a,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:node-controller,verb:GET (22-Mar-2024 03:39:05.125) (total time: 814ms):
Trace[606590780]: ---"Writing http response done" 813ms (03:39:05.939)
Trace[606590780]: [814.205528ms] [814.205528ms] END
I0322 03:39:06.025351    2280 trace.go:236] Trace[275660514]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:37:02.691) (total time: 123331ms):
Trace[275660514]: ---"Objects listed" error:<nil> 123330ms (03:39:06.021)
Trace[275660514]: [2m3.331070676s] [2m3.331070676s] END
I0322 03:39:06.029733    2280 trace.go:236] Trace[1350091140]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:37:28.463) (total time: 97565ms):
Trace[1350091140]: ---"Objects listed" error:<nil> 97564ms (03:39:06.028)
Trace[1350091140]: [1m37.565018539s] [1m37.565018539s] END
I0322 03:39:06.032986    2280 trace.go:236] Trace[521978338]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:36:26.013) (total time: 160019ms):
Trace[521978338]: ---"Objects listed" error:<nil> 160019ms (03:39:06.032)
Trace[521978338]: [2m40.019261852s] [2m40.019261852s] END
I0322 03:39:06.034228    2280 trace.go:236] Trace[494595755]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:37:24.277) (total time: 101757ms):
Trace[494595755]: ---"Objects listed" error:<nil> 101757ms (03:39:06.034)
Trace[494595755]: [1m41.757038855s] [1m41.757038855s] END
I0322 03:39:06.111758    2280 trace.go:236] Trace[1593299566]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:36:23.930) (total time: 162176ms):
Trace[1593299566]: ---"Objects listed" error:<nil> 162176ms (03:39:06.106)
Trace[1593299566]: [2m42.176467416s] [2m42.176467416s] END
I0322 03:39:06.113526    2280 trace.go:236] Trace[1862843391]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:36:23.765) (total time: 162348ms):
Trace[1862843391]: ---"Objects listed" error:<nil> 162348ms (03:39:06.113)
Trace[1862843391]: [2m42.348143644s] [2m42.348143644s] END
I0322 03:39:06.114368    2280 trace.go:236] Trace[1250536896]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:36:23.268) (total time: 162845ms):
Trace[1250536896]: ---"Objects listed" error:<nil> 162845ms (03:39:06.114)
Trace[1250536896]: [2m42.845978274s] [2m42.845978274s] END
I0322 03:39:06.116891    2280 trace.go:236] Trace[2035152245]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:36:25.269) (total time: 160847ms):
Trace[2035152245]: ---"Objects listed" error:<nil> 160847ms (03:39:06.116)
Trace[2035152245]: [2m40.847480679s] [2m40.847480679s] END
I0322 03:39:06.120430    2280 trace.go:236] Trace[290665407]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:36:47.260) (total time: 138860ms):
Trace[290665407]: ---"Objects listed" error:<nil> 138860ms (03:39:06.120)
Trace[290665407]: [2m18.860121585s] [2m18.860121585s] END
I0322 03:39:06.122692    2280 trace.go:236] Trace[1976979963]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:36:26.306) (total time: 159816ms):
Trace[1976979963]: ---"Objects listed" error:<nil> 159816ms (03:39:06.122)
Trace[1976979963]: [2m39.816638152s] [2m39.816638152s] END
I0322 03:39:06.123081    2280 trace.go:236] Trace[1134969755]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:36:51.198) (total time: 134924ms):
Trace[1134969755]: ---"Objects listed" error:<nil> 134924ms (03:39:06.123)
Trace[1134969755]: [2m14.924812421s] [2m14.924812421s] END
I0322 03:39:06.123838    2280 trace.go:236] Trace[317052579]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:36:49.920) (total time: 136203ms):
Trace[317052579]: ---"Objects listed" error:<nil> 136203ms (03:39:06.123)
Trace[317052579]: [2m16.203258597s] [2m16.203258597s] END
I0322 03:39:06.124735    2280 trace.go:236] Trace[1273457743]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:36:33.821) (total time: 152303ms):
Trace[1273457743]: ---"Objects listed" error:<nil> 152303ms (03:39:06.124)
Trace[1273457743]: [2m32.30335264s] [2m32.30335264s] END
I0322 03:39:06.125039    2280 trace.go:236] Trace[1074196487]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:36:50.801) (total time: 135323ms):
Trace[1074196487]: ---"Objects listed" error:<nil> 135323ms (03:39:06.125)
Trace[1074196487]: [2m15.323243235s] [2m15.323243235s] END
I0322 03:39:06.125572    2280 trace.go:236] Trace[273259091]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:36:33.942) (total time: 152182ms):
Trace[273259091]: ---"Objects listed" error:<nil> 152182ms (03:39:06.125)
Trace[273259091]: [2m32.182733986s] [2m32.182733986s] END
I0322 03:39:06.126074    2280 trace.go:236] Trace[1401730610]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:36:23.318) (total time: 162807ms):
Trace[1401730610]: ---"Objects listed" error:<nil> 162807ms (03:39:06.126)
Trace[1401730610]: [2m42.807106144s] [2m42.807106144s] END
I0322 03:39:06.130285    2280 trace.go:236] Trace[2142247949]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:36:27.695) (total time: 158435ms):
Trace[2142247949]: ---"Objects listed" error:<nil> 158435ms (03:39:06.130)
Trace[2142247949]: [2m38.435115581s] [2m38.435115581s] END
I0322 03:39:06.131265    2280 trace.go:236] Trace[2102035298]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:36:49.513) (total time: 136617ms):
Trace[2102035298]: ---"Objects listed" error:<nil> 136617ms (03:39:06.131)
Trace[2102035298]: [2m16.617385504s] [2m16.617385504s] END
I0322 03:39:06.136914    2280 trace.go:236] Trace[1663252606]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:36:26.309) (total time: 159827ms):
Trace[1663252606]: ---"Objects listed" error:<nil> 159827ms (03:39:06.136)
Trace[1663252606]: [2m39.827470143s] [2m39.827470143s] END
I0322 03:39:06.138045    2280 trace.go:236] Trace[669549340]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:36:50.444) (total time: 135693ms):
Trace[669549340]: ---"Objects listed" error:<nil> 135693ms (03:39:06.138)
Trace[669549340]: [2m15.693593883s] [2m15.693593883s] END
I0322 03:39:06.139474    2280 trace.go:236] Trace[1253421973]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:36:28.700) (total time: 157438ms):
Trace[1253421973]: ---"Objects listed" error:<nil> 157438ms (03:39:06.139)
Trace[1253421973]: [2m37.438617402s] [2m37.438617402s] END
I0322 03:39:06.147645    2280 trace.go:236] Trace[342994808]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:36:33.528) (total time: 152619ms):
Trace[342994808]: ---"Objects listed" error:<nil> 152611ms (03:39:06.139)
Trace[342994808]: [2m32.619316213s] [2m32.619316213s] END
I0322 03:39:06.150476    2280 trace.go:236] Trace[326948302]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:36:33.208) (total time: 152941ms):
Trace[326948302]: ---"Objects listed" error:<nil> 152941ms (03:39:06.150)
Trace[326948302]: [2m32.941648796s] [2m32.941648796s] END
I0322 03:39:06.151272    2280 trace.go:236] Trace[870464889]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:36:32.424) (total time: 153726ms):
Trace[870464889]: ---"Objects listed" error:<nil> 153726ms (03:39:06.151)
Trace[870464889]: [2m33.726661222s] [2m33.726661222s] END
I0322 03:39:06.153409    2280 trace.go:236] Trace[145064593]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:36:33.985) (total time: 152167ms):
Trace[145064593]: ---"Objects listed" error:<nil> 152167ms (03:39:06.152)
Trace[145064593]: [2m32.167893653s] [2m32.167893653s] END
I0322 03:39:06.159317    2280 trace.go:236] Trace[80064568]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:36:33.832) (total time: 152326ms):
Trace[80064568]: ---"Objects listed" error:<nil> 152326ms (03:39:06.159)
Trace[80064568]: [2m32.326200618s] [2m32.326200618s] END
I0322 03:39:06.173764    2280 trace.go:236] Trace[46640412]: "DeltaFIFO Pop Process" ID:kube-node-lease/default,Depth:38,Reason:slow event handlers blocking the queue (22-Mar-2024 03:39:06.027) (total time: 145ms):
Trace[46640412]: [145.782056ms] [145.782056ms] END
I0322 03:39:06.175176    2280 trace.go:236] Trace[1938745202]: "DeltaFIFO Pop Process" ID:kube-system/auth-delegator,Depth:11,Reason:slow event handlers blocking the queue (22-Mar-2024 03:39:06.027) (total time: 147ms):
Trace[1938745202]: [147.189067ms] [147.189067ms] END
I0322 03:39:06.220125    2280 trace.go:236] Trace[170553249]: "DeltaFIFO Pop Process" ID:clustercidrs-node,Depth:52,Reason:slow event handlers blocking the queue (22-Mar-2024 03:39:06.027) (total time: 192ms):
Trace[170553249]: [192.788374ms] [192.788374ms] END
I0322 03:39:06.282825    2280 trace.go:236] Trace[1811422414]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:36:28.086) (total time: 158196ms):
Trace[1811422414]: ---"Objects listed" error:<nil> 158195ms (03:39:06.282)
Trace[1811422414]: [2m38.196178932s] [2m38.196178932s] END
I0322 03:39:06.287445    2280 trace.go:236] Trace[652701167]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:36:23.705) (total time: 162582ms):
Trace[652701167]: ---"Objects listed" error:<nil> 162581ms (03:39:06.286)
Trace[652701167]: [2m42.582330997s] [2m42.582330997s] END
I0322 03:39:06.288857    2280 trace.go:236] Trace[1739044077]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:89c0c6d2-a5dd-4e4d-bd52-a0ff08cdfb66,client:127.0.0.1,protocol:HTTP/2.0,resource:customresourcedefinitions,scope:cluster,url:/apis/apiextensions.k8s.io/v1/customresourcedefinitions,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:39:05.242) (total time: 1046ms):
Trace[1739044077]: ---"Writing http response done" count:23 1044ms (03:39:06.288)
Trace[1739044077]: [1.046354397s] [1.046354397s] END
W0322 03:39:06.376481    2280 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:39:06.378895    2280 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
E0322 03:39:06.380070    2280 handler_proxy.go:137] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
E0322 03:39:06.387129    2280 kubelet.go:2477] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="3m54.324s"
E0322 03:39:06.389628    2280 kubelet.go:2327] "Skipping pod synchronization" err="container runtime is down"
time="2024-03-22T03:39:06Z" level=info msg="Slow SQL (started: 2024-03-22 03:39:05.051736694 +0000 UTC m=+645.043384929) (total time: 1.386062448s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/deployments/% 691]]"
time="2024-03-22T03:39:06Z" level=info msg="Slow SQL (started: 2024-03-22 03:39:05.057854673 +0000 UTC m=+645.049502916) (total time: 1.386537216s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/clusterrolebindings/% false]]"
E0322 03:39:06.492899    2280 kubelet.go:2327] "Skipping pod synchronization" err="container runtime is down"
I0322 03:39:06.838415    2280 trace.go:236] Trace[816526440]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:38:19.288) (total time: 47546ms):
Trace[816526440]: ---"Objects listed" error:<nil> 47546ms (03:39:06.834)
Trace[816526440]: [47.546085064s] [47.546085064s] END
I0322 03:39:06.868149    2280 trace.go:236] Trace[584588446]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:38:16.717) (total time: 50150ms):
Trace[584588446]: ---"Objects listed" error:<nil> 50150ms (03:39:06.867)
Trace[584588446]: [50.150776082s] [50.150776082s] END
I0322 03:39:06.870740    2280 trace.go:236] Trace[733563615]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:38:37.455) (total time: 29414ms):
Trace[733563615]: ---"Objects listed" error:<nil> 29414ms (03:39:06.870)
Trace[733563615]: [29.414934543s] [29.414934543s] END
I0322 03:39:06.974679    2280 trace.go:236] Trace[1328885398]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:36:47.120) (total time: 139850ms):
Trace[1328885398]: ---"Objects listed" error:<nil> 139850ms (03:39:06.970)
Trace[1328885398]: [2m19.850549361s] [2m19.850549361s] END
I0322 03:39:06.992555    2280 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:39:07.010950    2280 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="393.468s"
I0322 03:39:07.011125    2280 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:39:07.034237    2280 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="725.541s"
I0322 03:39:07.062871    2280 trace.go:236] Trace[727307181]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:8d2d2cc3-ce3b-457b-9c3b-b9ce5037d4e0,client:127.0.0.1,protocol:HTTP/2.0,resource:tokenreviews,scope:resource,url:/apis/authentication.k8s.io/v1/tokenreviews,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:POST (22-Mar-2024 03:39:05.141) (total time: 1920ms):
Trace[727307181]: ---"limitedReadBody succeeded" len:1152 278ms (03:39:05.420)
Trace[727307181]: ---"Write to database call succeeded" len:1152 1001ms (03:39:06.424)
Trace[727307181]: ---"Writing http response done" 637ms (03:39:07.061)
Trace[727307181]: [1.920802627s] [1.920802627s] END
E0322 03:39:07.271421    2280 kubelet.go:2327] "Skipping pod synchronization" err="container runtime is down"
E0322 03:39:07.347730    2280 finisher.go:175] FinishRequest: post-timeout activity - time-elapsed: 3.052764097s, panicked: false, err: context canceled, panic-reason: <nil>
I0322 03:39:07.412643    2280 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="458.534499ms"
W0322 03:39:07.842258    2280 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:39:07.856869    2280 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 03:39:07.856980    2280 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E0322 03:39:07.864327    2280 kubelet.go:2327] "Skipping pod synchronization" err="container runtime is down"
I0322 03:39:08.105403    2280 trace.go:236] Trace[71855771]: "List" accept:application/json, */*,audit-id:818de4c0-17dd-40dc-87a2-ea03dc9cc2c7,client:127.0.0.1,protocol:HTTP/2.0,resource:pods,scope:cluster,url:/api/v1/pods,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:39:06.058) (total time: 2043ms):
Trace[71855771]: ---"Writing http response done" count:5 2042ms (03:39:08.102)
Trace[71855771]: [2.043922582s] [2.043922582s] END
I0322 03:39:08.139540    2280 trace.go:236] Trace[1994200106]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:36:54.338) (total time: 132945ms):
Trace[1994200106]: ---"Objects listed" error:<nil> 132942ms (03:39:07.280)
Trace[1994200106]: [2m12.945145663s] [2m12.945145663s] END
I0322 03:39:08.176414    2280 trace.go:236] Trace[558587300]: "Get" accept:application/json, */*,audit-id:e12d72f1-6089-4b69-be16-ffae45172a61,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps/coredns,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:GET (22-Mar-2024 03:39:04.780) (total time: 3395ms):
Trace[558587300]: ---"About to write a response" 3387ms (03:39:08.167)
Trace[558587300]: [3.395832938s] [3.395832938s] END
I0322 03:39:08.295154    2280 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
time="2024-03-22T03:39:08Z" level=info msg="Connecting to proxy" url="wss://192.168.56.110:6443/v1-k3s/connect"
E0322 03:39:08.674489    2280 kubelet.go:2327] "Skipping pod synchronization" err="container runtime is down"
W0322 03:39:08.746349    2280 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:39:09.359824    2280 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 03:39:09.360623    2280 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0322 03:39:09.500924    2280 event.go:307] "Event occurred" object="kube-system/traefik-crd" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik-crd"
I0322 03:39:09.506376    2280 trace.go:236] Trace[594917130]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:37:14.771) (total time: 114732ms):
Trace[594917130]: ---"Objects listed" error:<nil> 114730ms (03:39:09.501)
Trace[594917130]: [1m54.73203581s] [1m54.73203581s] END
I0322 03:39:10.055301    2280 trace.go:236] Trace[2028178410]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:23ddd02c-34bf-46cc-be67-06fb302d4ba7,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/server,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:39:05.136) (total time: 4914ms):
Trace[2028178410]: ---"About to write a response" 3008ms (03:39:08.145)
Trace[2028178410]: ---"Writing http response done" 1906ms (03:39:10.051)
Trace[2028178410]: [4.91447037s] [4.91447037s] END
I0322 03:39:10.075197    2280 trace.go:236] Trace[282976675]: "List" accept:application/json, */*,audit-id:04cf26ed-d953-4c26-9a55-821ab22920bd,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:39:08.944) (total time: 749ms):
Trace[282976675]: ---"Writing http response done" count:1 736ms (03:39:09.694)
Trace[282976675]: [749.284094ms] [749.284094ms] END
E0322 03:39:10.571228    2280 kubelet.go:2327] "Skipping pod synchronization" err="container runtime is down"
E0322 03:39:11.562563    2280 resource_quota_controller.go:440] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I0322 03:39:11.811157    2280 trace.go:236] Trace[1105447391]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:5a8f4cea-7263-4cbd-822a-17ed2aa734f0,client:127.0.0.1,protocol:HTTP/2.0,resource:mutatingwebhookconfigurations,scope:cluster,url:/apis/admissionregistration.k8s.io/v1/mutatingwebhookconfigurations,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:39:09.135) (total time: 2088ms):
Trace[1105447391]: ["cacher list" audit-id:5a8f4cea-7263-4cbd-822a-17ed2aa734f0,type:mutatingwebhookconfigurations.admissionregistration.k8s.io 2089ms (03:39:09.138)]
Trace[1105447391]: ---"Writing http response done" count:0 1576ms (03:39:11.224)
Trace[1105447391]: [2.088602603s] [2.088602603s] END
I0322 03:39:11.881176    2280 trace.go:236] Trace[1058496072]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:64cb6dfe-68db-4f25-abf1-8c09cab1a563,client:127.0.0.1,protocol:HTTP/2.0,resource:services,scope:cluster,url:/api/v1/services,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:39:06.618) (total time: 5261ms):
Trace[1058496072]: ["List(recursive=true) etcd3" audit-id:64cb6dfe-68db-4f25-abf1-8c09cab1a563,key:/services/specs,resourceVersion:,resourceVersionMatch:,limit:0,continue: 5255ms (03:39:06.624)]
Trace[1058496072]: ---"Writing http response done" count:3 1739ms (03:39:11.879)
Trace[1058496072]: [5.261275488s] [5.261275488s] END
I0322 03:39:11.957670    2280 trace.go:236] Trace[651537592]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:78638589-0612-4535-9cd1-dc1bfd66a068,client:127.0.0.1,protocol:HTTP/2.0,resource:services,scope:resource,url:/api/v1/namespaces/default/services/kubernetes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/,verb:GET (22-Mar-2024 03:39:07.388) (total time: 4569ms):
Trace[651537592]: ---"About to write a response" 4568ms (03:39:11.957)
Trace[651537592]: [4.569387675s] [4.569387675s] END
W0322 03:39:11.993544    2280 controller.go:134] slow openapi aggregation of "addons.k3s.cattle.io": 5.113690323s
I0322 03:39:12.098239    2280 trace.go:236] Trace[1747727211]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:6ee30981-0388-451f-91aa-ec5f44aa8712,client:127.0.0.1,protocol:HTTP/2.0,resource:services,scope:cluster,url:/api/v1/services,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:39:08.065) (total time: 4032ms):
Trace[1747727211]: ["List(recursive=true) etcd3" audit-id:6ee30981-0388-451f-91aa-ec5f44aa8712,key:/services/specs,resourceVersion:,resourceVersionMatch:,limit:0,continue: 4023ms (03:39:08.074)]
Trace[1747727211]: [4.032636566s] [4.032636566s] END
I0322 03:39:12.124750    2280 trace.go:236] Trace[2026228023]: "Get" accept:application/json, */*,audit-id:f5a24f34-6d28-4abd-b991-76f6774b52e9,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps/coredns,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:GET (22-Mar-2024 03:39:11.489) (total time: 635ms):
Trace[2026228023]: ---"About to write a response" 634ms (03:39:12.124)
Trace[2026228023]: [635.232819ms] [635.232819ms] END
time="2024-03-22T03:39:12Z" level=info msg="Handling backend connection request [server]"
I0322 03:39:12.192292    2280 trace.go:236] Trace[497039663]: "Get" accept:application/json, */*,audit-id:3515ca29-a1d3-4ecb-9899-39cc3283ab6a,client:127.0.0.1,protocol:HTTP/2.0,resource:secrets,scope:resource,url:/api/v1/namespaces/kube-system/secrets/k3s-serving,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:GET (22-Mar-2024 03:39:09.990) (total time: 2201ms):
Trace[497039663]: ---"About to write a response" 2200ms (03:39:12.191)
Trace[497039663]: [2.201449775s] [2.201449775s] END
I0322 03:39:12.194952    2280 trace.go:236] Trace[1522586918]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:d41e084d-af10-4e71-b625-9ff3232b40dc,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/server,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PUT (22-Mar-2024 03:39:05.149) (total time: 7045ms):
Trace[1522586918]: ---"limitedReadBody succeeded" len:465 244ms (03:39:05.393)
Trace[1522586918]: ["GuaranteedUpdate etcd3" audit-id:d41e084d-af10-4e71-b625-9ff3232b40dc,key:/leases/kube-node-lease/server,type:*coordination.Lease,resource:leases.coordination.k8s.io 6793ms (03:39:05.401)
Trace[1522586918]:  ---"About to Encode" 2439ms (03:39:07.842)
Trace[1522586918]:  ---"Txn call completed" 4340ms (03:39:12.194)]
Trace[1522586918]: [7.045771486s] [7.045771486s] END
I0322 03:39:12.205954    2280 trace.go:236] Trace[1254465967]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:1c30e49f-80eb-4b96-8c69-7970c7e354c7,client:127.0.0.1,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:POST (22-Mar-2024 03:39:05.128) (total time: 7077ms):
Trace[1254465967]: ["Create etcd3" audit-id:1c30e49f-80eb-4b96-8c69-7970c7e354c7,key:/events/kube-system/metrics-server-67c658944b-p82jk.17bef8c556ebe6b3,type:*core.Event,resource:events 5195ms (03:39:07.010)
Trace[1254465967]:  ---"TransformToStorage succeeded" 3419ms (03:39:10.433)
Trace[1254465967]:  ---"Txn call succeeded" 1770ms (03:39:12.203)]
Trace[1254465967]: [7.077398251s] [7.077398251s] END
time="2024-03-22T03:39:12Z" level=info msg="Updating TLS secret for kube-system/k3s-serving (count: 10): map[listener.cattle.io/cn-10.43.0.1:10.43.0.1 listener.cattle.io/cn-127.0.0.1:127.0.0.1 listener.cattle.io/cn-192.168.56.110:192.168.56.110 listener.cattle.io/cn-__1-f16284:::1 listener.cattle.io/cn-kubernetes:kubernetes listener.cattle.io/cn-kubernetes.default:kubernetes.default listener.cattle.io/cn-kubernetes.default.svc:kubernetes.default.svc listener.cattle.io/cn-kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local listener.cattle.io/cn-localhost:localhost listener.cattle.io/cn-server:server listener.cattle.io/fingerprint:SHA1=3EF84C4C3E5D7F82BBAF806634968BEF8CE80EDA]"
I0322 03:39:12.249116    2280 garbagecollector.go:816] "failed to discover some groups" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"
I0322 03:39:12.256817    2280 trace.go:236] Trace[224327840]: "Patch" accept:application/vnd.kubernetes.protobuf, */*,audit-id:ed0c6d5a-2386-4c30-8a3a-95c4f14b01e8,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:node-controller,verb:PATCH (22-Mar-2024 03:39:09.099) (total time: 3157ms):
Trace[224327840]: ["GuaranteedUpdate etcd3" audit-id:ed0c6d5a-2386-4c30-8a3a-95c4f14b01e8,key:/minions/serverworker,type:*core.Node,resource:nodes 2410ms (03:39:09.846)
Trace[224327840]:  ---"initial value restored" 766ms (03:39:10.613)
Trace[224327840]:  ---"About to Encode" 1378ms (03:39:11.992)
Trace[224327840]:  ---"Txn call completed" 263ms (03:39:12.255)]
Trace[224327840]: ---"About to check admission control" 1375ms (03:39:11.990)
Trace[224327840]: ---"About to apply patch" 264ms (03:39:12.255)
Trace[224327840]: [3.157049238s] [3.157049238s] END
I0322 03:39:12.256962    2280 trace.go:236] Trace[1630739301]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:48c37b02-4ecf-4332-bac4-344a78692317,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker/status,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:node-controller,verb:PUT (22-Mar-2024 03:39:11.585) (total time: 671ms):
Trace[1630739301]: ---"limitedReadBody succeeded" len:3564 236ms (03:39:11.822)
Trace[1630739301]: ---"Write to database call failed" len:3564,err:Operation cannot be fulfilled on nodes "serverworker": the object has been modified; please apply your changes to the latest version and try again 25ms (03:39:12.256)
Trace[1630739301]: [671.107729ms] [671.107729ms] END
I0322 03:39:12.263221    2280 topologycache.go:237] "Can't get CPU or zone information for node" node="serverworker"
I0322 03:39:12.264374    2280 trace.go:236] Trace[1531291569]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:eb80dcdf-fb68-4376-90e3-d164f7b3e4c8,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-7zoch227uv4owxg75pgtnmv3jq,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PUT (22-Mar-2024 03:39:05.287) (total time: 6977ms):
Trace[1531291569]: ---"limitedReadBody succeeded" len:590 1287ms (03:39:06.574)
Trace[1531291569]: ---"About to convert to expected version" 14ms (03:39:06.588)
Trace[1531291569]: ---"Conversion done" 614ms (03:39:07.203)
Trace[1531291569]: ["GuaranteedUpdate etcd3" audit-id:eb80dcdf-fb68-4376-90e3-d164f7b3e4c8,key:/leases/kube-system/apiserver-7zoch227uv4owxg75pgtnmv3jq,type:*coordination.Lease,resource:leases.coordination.k8s.io 4769ms (03:39:07.494)
Trace[1531291569]:  ---"About to Encode" 4457ms (03:39:11.958)
Trace[1531291569]:  ---"Txn call completed" 304ms (03:39:12.264)]
Trace[1531291569]: [6.977194459s] [6.977194459s] END
I0322 03:39:12.300742    2280 trace.go:236] Trace[352075450]: "Patch" accept:application/json, */*,audit-id:3b7b839c-6547-46f2-bf8f-6720f1e87706,client:127.0.0.1,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events/traefik.17bef879b248b2b9,user-agent:helm-controller@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:PATCH (22-Mar-2024 03:39:11.574) (total time: 726ms):
Trace[352075450]: ["GuaranteedUpdate etcd3" audit-id:3b7b839c-6547-46f2-bf8f-6720f1e87706,key:/events/kube-system/traefik.17bef879b248b2b9,type:*core.Event,resource:events 724ms (03:39:11.576)
Trace[352075450]:  ---"initial value restored" 626ms (03:39:12.202)
Trace[352075450]:  ---"Txn call completed" 97ms (03:39:12.300)]
Trace[352075450]: ---"Object stored in database" 97ms (03:39:12.300)
Trace[352075450]: [726.551324ms] [726.551324ms] END
I0322 03:39:12.307011    2280 trace.go:236] Trace[30285360]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:5df2ab17-da3c-448d-bd96-5431c446872b,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/server/status,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:node-controller,verb:PUT (22-Mar-2024 03:39:11.585) (total time: 721ms):
Trace[30285360]: ---"limitedReadBody succeeded" len:4584 297ms (03:39:11.882)
Trace[30285360]: ---"Writing http response done" 52ms (03:39:12.306)
Trace[30285360]: [721.397661ms] [721.397661ms] END
I0322 03:39:12.310197    2280 trace.go:236] Trace[1531613512]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.56.110,type:*v1.Endpoints,resource:apiServerIPInfo (22-Mar-2024 03:35:34.783) (total time: 217526ms):
Trace[1531613512]: ---"initial value restored" 210384ms (03:39:05.168)
Trace[1531613512]: ---"Transaction prepared" 3296ms (03:39:08.465)
Trace[1531613512]: ---"Txn call completed" 2985ms (03:39:11.450)
Trace[1531613512]: ---"Transaction prepared" 746ms (03:39:12.201)
Trace[1531613512]: ---"Txn call completed" 108ms (03:39:12.309)
Trace[1531613512]: [3m37.526496946s] [3m37.526496946s] END
E0322 03:39:12.362690    2280 node_lifecycle_controller.go:971] "Error updating node" err="Operation cannot be fulfilled on nodes \"serverworker\": the object has been modified; please apply your changes to the latest version and try again" node="serverworker"
I0322 03:39:12.364615    2280 event.go:307] "Event occurred" object="server" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="NodeNotReady" message="Node server status is now: NodeNotReady"
I0322 03:39:12.446420    2280 event.go:307] "Event occurred" object="kube-system/local-path-provisioner-6c86858495-lvfsk" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0322 03:39:12.605462    2280 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="142.994745ms"
I0322 03:39:12.679655    2280 event.go:307] "Event occurred" object="serverworker" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="NodeNotReady" message="Node serverworker status is now: NodeNotReady"
I0322 03:39:12.687205    2280 node_lifecycle_controller.go:1029] "Controller detected that all Nodes are not-Ready. Entering master disruption mode"
I0322 03:39:12.694859    2280 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="51.005s"
I0322 03:39:12.695015    2280 event.go:307] "Event occurred" object="kube-system/coredns-6799fbcd5-rx42b" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0322 03:39:12.781669    2280 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="98.011832ms"
I0322 03:39:12.781802    2280 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="90.663s"
W0322 03:39:13.174355    2280 aggregator.go:166] failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
E0322 03:39:46.115842    2280 health_controller.go:162] Metrics Controller heartbeat missed
E0322 03:39:46.123206    2280 health_controller.go:162] Metrics Controller heartbeat missed
E0322 03:39:46.123301    2280 health_controller.go:162] Metrics Controller heartbeat missed
time="2024-03-22T03:39:47Z" level=info msg="Slow SQL (started: 2024-03-22 03:39:22.06492357 +0000 UTC m=+662.056571819) (total time: 25.913089592s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC LIMIT 500 : [[% 728]]"
I0322 03:39:48.950425    2280 trace.go:236] Trace[494361465]: "iptables ChainExists" (22-Mar-2024 03:39:32.722) (total time: 16224ms):
Trace[494361465]: [16.224260499s] [16.224260499s] END
I0322 03:39:48.992618    2280 request.go:697] Waited for 1.150428238s due to client-side throttling, not priority and fairness, request: GET:https://127.0.0.1:6444/api
I0322 03:39:49.035329    2280 trace.go:236] Trace[1764700810]: "iptables ChainExists" (22-Mar-2024 03:39:30.934) (total time: 18097ms):
Trace[1764700810]: [18.097330962s] [18.097330962s] END
E0322 03:39:49.039198    2280 handler_proxy.go:137] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
I0322 03:39:49.266278    2280 trace.go:236] Trace[287614738]: "iptables ChainExists" (22-Mar-2024 03:39:32.694) (total time: 16568ms):
Trace[287614738]: [16.568975401s] [16.568975401s] END
I0322 03:39:49.661454    2280 trace.go:236] Trace[1175545405]: "iptables ChainExists" (22-Mar-2024 03:39:24.264) (total time: 24311ms):
Trace[1175545405]: [24.311468339s] [24.311468339s] END
time="2024-03-22T03:39:49Z" level=info msg="Slow SQL (started: 2024-03-22 03:39:47.594853848 +0000 UTC m=+687.586502085) (total time: 2.24895033s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC LIMIT 1 : [[/registry/masterleases/192.168.56.110 true]]"
I0322 03:39:50.371991    2280 trace.go:236] Trace[1735529636]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:a23c18b8-fa26-4df4-87f1-8b5339b735d7,client:127.0.0.1,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events/coredns-6799fbcd5-rx42b.17bef886db19ab2f,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PATCH (22-Mar-2024 03:39:13.484) (total time: 36879ms):
Trace[1735529636]: ---"About to apply patch" 237ms (03:39:13.722)
Trace[1735529636]: ---"About to check admission control" 308ms (03:39:14.030)
Trace[1735529636]: [36.879233401s] [36.879233401s] END
E0322 03:39:50.735259    2280 finisher.go:175] FinishRequest: post-timeout activity - time-elapsed: 1.886969282s, panicked: false, err: context deadline exceeded, panic-reason: <nil>
E0322 03:39:51.782057    2280 event.go:280] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"coredns-6799fbcd5-rx42b.17bef886db19ab2f", GenerateName:"", Namespace:"kube-system", SelfLink:"", UID:"", ResourceVersion:"725", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"coredns-6799fbcd5-rx42b", UID:"5358d3b8-3fa6-4dcc-8789-ee2721507f22", APIVersion:"v1", ResourceVersion:"442", FieldPath:"spec.containers{coredns}"}, Reason:"Unhealthy", Message:"Readiness probe failed: Get \"http://10.42.0.3:8181/ready\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)", Source:v1.EventSource{Component:"kubelet", Host:"server"}, FirstTimestamp:time.Date(2024, time.March, 22, 3, 29, 26, 0, time.Local), LastTimestamp:time.Date(2024, time.March, 22, 3, 34, 23, 90636777, time.Local), Count:7, Type:"Warning", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"kubelet", ReportingInstance:"server"}': 'Timeout: request did not complete within requested timeout - context deadline exceeded' (will not retry!)
E0322 03:39:53.961715    2280 kubelet.go:2477] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="39.221s"
I0322 03:39:54.439930    2280 trace.go:236] Trace[1774421597]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:397e4569-9923-412f-b4d2-f4a5d36967e3,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/server,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:39:53.377) (total time: 1055ms):
Trace[1774421597]: ---"Writing http response done" 1053ms (03:39:54.432)
Trace[1774421597]: [1.055083284s] [1.055083284s] END
I0322 03:39:54.683665    2280 trace.go:236] Trace[609980237]: "List" accept:application/json, */*,audit-id:bc14cb9b-b6da-40aa-9930-3995298d1aa1,client:10.42.0.4,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps,user-agent:metrics-server/v0.6.3 (linux/amd64) kubernetes/a938798,verb:GET (22-Mar-2024 03:39:52.604) (total time: 1658ms):
Trace[609980237]: ---"Writing http response done" count:1 1325ms (03:39:54.263)
Trace[609980237]: [1.658676067s] [1.658676067s] END
time="2024-03-22T03:39:54Z" level=info msg="certificate CN=serverworker signed by CN=k3s-server-ca@1711078100: notBefore=2024-03-22 03:28:20 +0000 UTC notAfter=2025-03-22 03:39:54 +0000 UTC"
time="2024-03-22T03:39:56Z" level=info msg="Starting k3s v1.28.7+k3s1 (051b14b2)"
time="2024-03-22T03:39:56Z" level=info msg="Configuring sqlite3 database connection pooling: maxIdleConns=2, maxOpenConns=0, connMaxLifetime=0s"
time="2024-03-22T03:39:56Z" level=info msg="Configuring database table schema and indexes, this may take a moment..."
time="2024-03-22T03:39:56Z" level=info msg="Database tables and indexes are up to date"
time="2024-03-22T03:39:56Z" level=info msg="Kine available at unix://kine.sock"
time="2024-03-22T03:39:56Z" level=info msg="Reconciling bootstrap data between datastore and disk"
time="2024-03-22T03:39:57Z" level=info msg="Running kube-apiserver --advertise-address=192.168.56.110 --advertise-port=6443 --allow-privileged=true --anonymous-auth=false --api-audiences=https://kubernetes.default.svc.cluster.local,k3s --authorization-mode=Node,RBAC --bind-address=127.0.0.1 --cert-dir=/var/lib/rancher/k3s/server/tls/temporary-certs --client-ca-file=/var/lib/rancher/k3s/server/tls/client-ca.crt --egress-selector-config-file=/var/lib/rancher/k3s/server/etc/egress-selector-config.yaml --enable-admission-plugins=NodeRestriction --enable-aggregator-routing=true --enable-bootstrap-token-auth=true --etcd-servers=unix://kine.sock --kubelet-certificate-authority=/var/lib/rancher/k3s/server/tls/server-ca.crt --kubelet-client-certificate=/var/lib/rancher/k3s/server/tls/client-kube-apiserver.crt --kubelet-client-key=/var/lib/rancher/k3s/server/tls/client-kube-apiserver.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --profiling=false --proxy-client-cert-file=/var/lib/rancher/k3s/server/tls/client-auth-proxy.crt --proxy-client-key-file=/var/lib/rancher/k3s/server/tls/client-auth-proxy.key --requestheader-allowed-names=system:auth-proxy --requestheader-client-ca-file=/var/lib/rancher/k3s/server/tls/request-header-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6444 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/var/lib/rancher/k3s/server/tls/service.key --service-account-signing-key-file=/var/lib/rancher/k3s/server/tls/service.current.key --service-cluster-ip-range=10.43.0.0/16 --service-node-port-range=30000-32767 --storage-backend=etcd3 --tls-cert-file=/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.crt --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 --tls-private-key-file=/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.key"
time="2024-03-22T03:39:57Z" level=info msg="Running kube-scheduler --authentication-kubeconfig=/var/lib/rancher/k3s/server/cred/scheduler.kubeconfig --authorization-kubeconfig=/var/lib/rancher/k3s/server/cred/scheduler.kubeconfig --bind-address=127.0.0.1 --kubeconfig=/var/lib/rancher/k3s/server/cred/scheduler.kubeconfig --leader-elect=false --profiling=false --secure-port=10259"
time="2024-03-22T03:39:57Z" level=info msg="Running kube-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/var/lib/rancher/k3s/server/cred/controller.kubeconfig --authorization-kubeconfig=/var/lib/rancher/k3s/server/cred/controller.kubeconfig --bind-address=127.0.0.1 --cluster-cidr=10.42.0.0/16 --cluster-signing-kube-apiserver-client-cert-file=/var/lib/rancher/k3s/server/tls/client-ca.nochain.crt --cluster-signing-kube-apiserver-client-key-file=/var/lib/rancher/k3s/server/tls/client-ca.key --cluster-signing-kubelet-client-cert-file=/var/lib/rancher/k3s/server/tls/client-ca.nochain.crt --cluster-signing-kubelet-client-key-file=/var/lib/rancher/k3s/server/tls/client-ca.key --cluster-signing-kubelet-serving-cert-file=/var/lib/rancher/k3s/server/tls/server-ca.nochain.crt --cluster-signing-kubelet-serving-key-file=/var/lib/rancher/k3s/server/tls/server-ca.key --cluster-signing-legacy-unknown-cert-file=/var/lib/rancher/k3s/server/tls/server-ca.nochain.crt --cluster-signing-legacy-unknown-key-file=/var/lib/rancher/k3s/server/tls/server-ca.key --configure-cloud-routes=false --controllers=*,tokencleaner,-service,-route,-cloud-node-lifecycle --kubeconfig=/var/lib/rancher/k3s/server/cred/controller.kubeconfig --leader-elect=false --profiling=false --root-ca-file=/var/lib/rancher/k3s/server/tls/server-ca.crt --secure-port=10257 --service-account-private-key-file=/var/lib/rancher/k3s/server/tls/service.current.key --service-cluster-ip-range=10.43.0.0/16 --use-service-account-credentials=true"
time="2024-03-22T03:39:57Z" level=info msg="Running cloud-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/var/lib/rancher/k3s/server/cred/cloud-controller.kubeconfig --authorization-kubeconfig=/var/lib/rancher/k3s/server/cred/cloud-controller.kubeconfig --bind-address=127.0.0.1 --cloud-config=/var/lib/rancher/k3s/server/etc/cloud-config.yaml --cloud-provider=k3s --cluster-cidr=10.42.0.0/16 --configure-cloud-routes=false --controllers=*,-route --feature-gates=CloudDualStackNodeIPs=true --kubeconfig=/var/lib/rancher/k3s/server/cred/cloud-controller.kubeconfig --leader-elect=false --leader-elect-resource-name=k3s-cloud-controller-manager --node-status-update-frequency=1m0s --profiling=false"
time="2024-03-22T03:39:57Z" level=info msg="Server node token is available at /var/lib/rancher/k3s/server/token"
time="2024-03-22T03:39:57Z" level=info msg="To join server node to cluster: k3s server -s https://10.0.2.15:6443 -t ${SERVER_NODE_TOKEN}"
time="2024-03-22T03:39:57Z" level=info msg="Agent node token is available at /var/lib/rancher/k3s/server/agent-token"
time="2024-03-22T03:39:57Z" level=info msg="To join agent node to cluster: k3s agent -s https://10.0.2.15:6443 -t ${AGENT_NODE_TOKEN}"
I0322 03:39:57.228154    5292 options.go:220] external host was not specified, using 192.168.56.110
time="2024-03-22T03:39:57Z" level=info msg="Wrote kubeconfig /etc/rancher/k3s/k3s.yaml"
time="2024-03-22T03:39:57Z" level=info msg="Run: k3s kubectl"
I0322 03:39:57.230624    5292 server.go:156] Version: v1.28.7+k3s1
I0322 03:39:57.230652    5292 server.go:158] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
time="2024-03-22T03:39:57Z" level=info msg="Waiting for API server to become available"
I0322 03:39:57.279186    5292 shared_informer.go:311] Waiting for caches to sync for node_authorizer
I0322 03:39:57.289742    5292 plugins.go:158] Loaded 12 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,TaintNodesByCondition,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,MutatingAdmissionWebhook.
I0322 03:39:57.289766    5292 plugins.go:161] Loaded 13 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,ClusterTrustBundleAttest,CertificateSubjectRestriction,ValidatingAdmissionPolicy,ValidatingAdmissionWebhook,ResourceQuota.
I0322 03:39:57.293310    5292 instance.go:298] Using reconciler: lease
I0322 03:39:57.303890    5292 handler.go:275] Adding GroupVersion apiextensions.k8s.io v1 to ResourceManager
W0322 03:39:57.303918    5292 genericapiserver.go:744] Skipping API apiextensions.k8s.io/v1beta1 because it has no resources.
I0322 03:39:57.437339    5292 handler.go:275] Adding GroupVersion  v1 to ResourceManager
I0322 03:39:57.437553    5292 instance.go:709] API group "internal.apiserver.k8s.io" is not enabled, skipping.
I0322 03:39:57.559810    5292 instance.go:709] API group "resource.k8s.io" is not enabled, skipping.
I0322 03:39:57.592801    5292 handler.go:275] Adding GroupVersion authentication.k8s.io v1 to ResourceManager
W0322 03:39:57.592929    5292 genericapiserver.go:744] Skipping API authentication.k8s.io/v1beta1 because it has no resources.
W0322 03:39:57.592936    5292 genericapiserver.go:744] Skipping API authentication.k8s.io/v1alpha1 because it has no resources.
I0322 03:39:57.593905    5292 handler.go:275] Adding GroupVersion authorization.k8s.io v1 to ResourceManager
W0322 03:39:57.593975    5292 genericapiserver.go:744] Skipping API authorization.k8s.io/v1beta1 because it has no resources.
I0322 03:39:57.595449    5292 handler.go:275] Adding GroupVersion autoscaling v2 to ResourceManager
I0322 03:39:57.596672    5292 handler.go:275] Adding GroupVersion autoscaling v1 to ResourceManager
W0322 03:39:57.596735    5292 genericapiserver.go:744] Skipping API autoscaling/v2beta1 because it has no resources.
W0322 03:39:57.596740    5292 genericapiserver.go:744] Skipping API autoscaling/v2beta2 because it has no resources.
I0322 03:39:57.599196    5292 handler.go:275] Adding GroupVersion batch v1 to ResourceManager
W0322 03:39:57.599266    5292 genericapiserver.go:744] Skipping API batch/v1beta1 because it has no resources.
I0322 03:39:57.600644    5292 handler.go:275] Adding GroupVersion certificates.k8s.io v1 to ResourceManager
W0322 03:39:57.600711    5292 genericapiserver.go:744] Skipping API certificates.k8s.io/v1beta1 because it has no resources.
W0322 03:39:57.600716    5292 genericapiserver.go:744] Skipping API certificates.k8s.io/v1alpha1 because it has no resources.
I0322 03:39:57.601642    5292 handler.go:275] Adding GroupVersion coordination.k8s.io v1 to ResourceManager
W0322 03:39:57.601708    5292 genericapiserver.go:744] Skipping API coordination.k8s.io/v1beta1 because it has no resources.
W0322 03:39:57.601830    5292 genericapiserver.go:744] Skipping API discovery.k8s.io/v1beta1 because it has no resources.
I0322 03:39:57.602863    5292 handler.go:275] Adding GroupVersion discovery.k8s.io v1 to ResourceManager
I0322 03:39:57.613451    5292 handler.go:275] Adding GroupVersion networking.k8s.io v1 to ResourceManager
W0322 03:39:57.613566    5292 genericapiserver.go:744] Skipping API networking.k8s.io/v1beta1 because it has no resources.
W0322 03:39:57.613574    5292 genericapiserver.go:744] Skipping API networking.k8s.io/v1alpha1 because it has no resources.
I0322 03:39:57.614234    5292 handler.go:275] Adding GroupVersion node.k8s.io v1 to ResourceManager
W0322 03:39:57.614282    5292 genericapiserver.go:744] Skipping API node.k8s.io/v1beta1 because it has no resources.
W0322 03:39:57.614287    5292 genericapiserver.go:744] Skipping API node.k8s.io/v1alpha1 because it has no resources.
I0322 03:39:57.615096    5292 handler.go:275] Adding GroupVersion policy v1 to ResourceManager
W0322 03:39:57.615161    5292 genericapiserver.go:744] Skipping API policy/v1beta1 because it has no resources.
I0322 03:39:57.616603    5292 handler.go:275] Adding GroupVersion rbac.authorization.k8s.io v1 to ResourceManager
W0322 03:39:57.616653    5292 genericapiserver.go:744] Skipping API rbac.authorization.k8s.io/v1beta1 because it has no resources.
W0322 03:39:57.616658    5292 genericapiserver.go:744] Skipping API rbac.authorization.k8s.io/v1alpha1 because it has no resources.
I0322 03:39:57.617057    5292 handler.go:275] Adding GroupVersion scheduling.k8s.io v1 to ResourceManager
W0322 03:39:57.617098    5292 genericapiserver.go:744] Skipping API scheduling.k8s.io/v1beta1 because it has no resources.
W0322 03:39:57.617104    5292 genericapiserver.go:744] Skipping API scheduling.k8s.io/v1alpha1 because it has no resources.
I0322 03:39:57.618696    5292 handler.go:275] Adding GroupVersion storage.k8s.io v1 to ResourceManager
W0322 03:39:57.618750    5292 genericapiserver.go:744] Skipping API storage.k8s.io/v1beta1 because it has no resources.
W0322 03:39:57.618756    5292 genericapiserver.go:744] Skipping API storage.k8s.io/v1alpha1 because it has no resources.
I0322 03:39:57.619621    5292 handler.go:275] Adding GroupVersion flowcontrol.apiserver.k8s.io v1beta3 to ResourceManager
I0322 03:39:57.620450    5292 handler.go:275] Adding GroupVersion flowcontrol.apiserver.k8s.io v1beta2 to ResourceManager
W0322 03:39:57.620494    5292 genericapiserver.go:744] Skipping API flowcontrol.apiserver.k8s.io/v1beta1 because it has no resources.
W0322 03:39:57.620499    5292 genericapiserver.go:744] Skipping API flowcontrol.apiserver.k8s.io/v1alpha1 because it has no resources.
I0322 03:39:57.622991    5292 handler.go:275] Adding GroupVersion apps v1 to ResourceManager
W0322 03:39:57.623042    5292 genericapiserver.go:744] Skipping API apps/v1beta2 because it has no resources.
W0322 03:39:57.623048    5292 genericapiserver.go:744] Skipping API apps/v1beta1 because it has no resources.
I0322 03:39:57.623665    5292 handler.go:275] Adding GroupVersion admissionregistration.k8s.io v1 to ResourceManager
W0322 03:39:57.623707    5292 genericapiserver.go:744] Skipping API admissionregistration.k8s.io/v1beta1 because it has no resources.
W0322 03:39:57.623712    5292 genericapiserver.go:744] Skipping API admissionregistration.k8s.io/v1alpha1 because it has no resources.
I0322 03:39:57.624181    5292 handler.go:275] Adding GroupVersion events.k8s.io v1 to ResourceManager
W0322 03:39:57.624222    5292 genericapiserver.go:744] Skipping API events.k8s.io/v1beta1 because it has no resources.
I0322 03:39:57.627850    5292 handler.go:275] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
W0322 03:39:57.627914    5292 genericapiserver.go:744] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I0322 03:39:58.089098    5292 secure_serving.go:213] Serving securely on 127.0.0.1:6444
I0322 03:39:58.089225    5292 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/rancher/k3s/server/tls/request-header-ca.crt"
I0322 03:39:58.089530    5292 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.crt::/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.key"
I0322 03:39:58.089604    5292 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0322 03:39:58.091532    5292 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/rancher/k3s/server/tls/client-auth-proxy.crt::/var/lib/rancher/k3s/server/tls/client-auth-proxy.key"
I0322 03:39:58.091959    5292 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0322 03:39:58.091985    5292 shared_informer.go:311] Waiting for caches to sync for cluster_authentication_trust_controller
I0322 03:39:58.092236    5292 gc_controller.go:78] Starting apiserver lease garbage collector
I0322 03:39:58.092265    5292 system_namespaces_controller.go:67] Starting system namespaces controller
I0322 03:39:58.092295    5292 available_controller.go:423] Starting AvailableConditionController
I0322 03:39:58.092300    5292 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0322 03:39:58.092379    5292 apf_controller.go:374] Starting API Priority and Fairness config controller
I0322 03:39:58.092709    5292 controller.go:116] Starting legacy_token_tracking_controller
I0322 03:39:58.092721    5292 shared_informer.go:311] Waiting for caches to sync for configmaps
I0322 03:39:58.099895    5292 gc_controller.go:78] Starting apiserver lease garbage collector
I0322 03:39:58.099987    5292 handler_discovery.go:412] Starting ResourceDiscoveryManager
I0322 03:39:58.101260    5292 aggregator.go:164] waiting for initial CRD sync...
I0322 03:39:58.101297    5292 controller.go:78] Starting OpenAPI AggregationController
I0322 03:39:58.102222    5292 controller.go:80] Starting OpenAPI V3 AggregationController
I0322 03:39:58.102447    5292 customresource_discovery_controller.go:289] Starting DiscoveryController
I0322 03:39:58.102523    5292 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0322 03:39:58.102529    5292 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0322 03:39:58.102651    5292 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/rancher/k3s/server/tls/client-ca.crt"
I0322 03:39:58.105991    5292 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/rancher/k3s/server/tls/client-ca.crt"
I0322 03:39:58.106119    5292 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/rancher/k3s/server/tls/request-header-ca.crt"
I0322 03:39:58.107568    5292 crdregistration_controller.go:111] Starting crd-autoregister controller
I0322 03:39:58.107582    5292 shared_informer.go:311] Waiting for caches to sync for crd-autoregister
I0322 03:39:58.108114    5292 controller.go:134] Starting OpenAPI controller
I0322 03:39:58.108418    5292 controller.go:85] Starting OpenAPI V3 controller
I0322 03:39:58.108454    5292 naming_controller.go:291] Starting NamingConditionController
I0322 03:39:58.108498    5292 establishing_controller.go:76] Starting EstablishingController
I0322 03:39:58.108530    5292 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0322 03:39:58.108539    5292 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0322 03:39:58.108548    5292 crd_finalizer.go:266] Starting CRDFinalizer
time="2024-03-22T03:39:58Z" level=info msg="Password verified locally for node server"
time="2024-03-22T03:39:58Z" level=info msg="certificate CN=server signed by CN=k3s-server-ca@1711078100: notBefore=2024-03-22 03:28:20 +0000 UTC notAfter=2025-03-22 03:39:58 +0000 UTC"
I0322 03:39:58.279980    5292 shared_informer.go:318] Caches are synced for node_authorizer
I0322 03:39:58.293062    5292 shared_informer.go:318] Caches are synced for configmaps
I0322 03:39:58.293165    5292 shared_informer.go:318] Caches are synced for cluster_authentication_trust_controller
I0322 03:39:58.293433    5292 cache.go:39] Caches are synced for AvailableConditionController controller
I0322 03:39:58.294171    5292 apf_controller.go:379] Running API Priority and Fairness config worker
I0322 03:39:58.294189    5292 apf_controller.go:382] Running API Priority and Fairness periodic rebalancing process
I0322 03:39:58.310894    5292 handler.go:275] Adding GroupVersion k3s.cattle.io v1 to ResourceManager
I0322 03:39:58.311032    5292 handler.go:275] Adding GroupVersion helm.cattle.io v1 to ResourceManager
I0322 03:39:58.311066    5292 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0322 03:39:58.311169    5292 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0322 03:39:58.311801    5292 cache.go:39] Caches are synced for APIServiceRegistrationController controller
W0322 03:39:58.318058    5292 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:39:58.318199    5292 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 03:39:58.318272    5292 shared_informer.go:318] Caches are synced for crd-autoregister
E0322 03:39:58.335094    5292 handler_proxy.go:137] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
I0322 03:39:58.335250    5292 handler.go:275] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0322 03:39:58.335269    5292 aggregator.go:166] initial CRD sync complete...
I0322 03:39:58.335289    5292 autoregister_controller.go:141] Starting autoregister controller
I0322 03:39:58.335294    5292 cache.go:32] Waiting for caches to sync for autoregister controller
I0322 03:39:58.335356    5292 cache.go:39] Caches are synced for autoregister controller
I0322 03:39:58.364500    5292 controller.go:624] quota admission added evaluator for: leases.coordination.k8s.io
E0322 03:39:58.421719    5292 controller.go:102] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
time="2024-03-22T03:39:58Z" level=info msg="certificate CN=system:node:server,O=system:nodes signed by CN=k3s-client-ca@1711078100: notBefore=2024-03-22 03:28:20 +0000 UTC notAfter=2025-03-22 03:39:58 +0000 UTC"
time="2024-03-22T03:39:58Z" level=info msg="Module overlay was already loaded"
time="2024-03-22T03:39:58Z" level=info msg="Module nf_conntrack was already loaded"
time="2024-03-22T03:39:58Z" level=info msg="Module br_netfilter was already loaded"
time="2024-03-22T03:39:58Z" level=info msg="Module iptable_nat was already loaded"
time="2024-03-22T03:39:58Z" level=info msg="Module iptable_filter was already loaded"
time="2024-03-22T03:39:58Z" level=info msg="Logging containerd to /var/lib/rancher/k3s/agent/containerd/containerd.log"
time="2024-03-22T03:39:58Z" level=info msg="Running containerd -c /var/lib/rancher/k3s/agent/etc/containerd/config.toml -a /run/k3s/containerd/containerd.sock --state /run/k3s/containerd --root /var/lib/rancher/k3s/agent/containerd"
I0322 03:39:59.136619    5292 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
W0322 03:39:59.322424    5292 handler_proxy.go:93] no RequestInfo found in the context
W0322 03:39:59.322883    5292 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:39:59.323013    5292 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I0322 03:39:59.323034    5292 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E0322 03:39:59.323303    5292 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 03:39:59.324570    5292 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
time="2024-03-22T03:39:59Z" level=info msg="containerd is now running"
time="2024-03-22T03:39:59Z" level=info msg="Running kubelet --address=0.0.0.0 --allowed-unsafe-sysctls=net.ipv4.ip_forward,net.ipv6.conf.all.forwarding --anonymous-auth=false --authentication-token-webhook=true --authorization-mode=Webhook --cgroup-driver=systemd --client-ca-file=/var/lib/rancher/k3s/agent/client-ca.crt --cloud-provider=external --cluster-dns=10.43.0.10 --cluster-domain=cluster.local --container-runtime-endpoint=unix:///run/k3s/containerd/containerd.sock --containerd=/run/k3s/containerd/containerd.sock --eviction-hard=imagefs.available<5%,nodefs.available<5% --eviction-minimum-reclaim=imagefs.available=10%,nodefs.available=10% --fail-swap-on=false --feature-gates=CloudDualStackNodeIPs=true --healthz-bind-address=127.0.0.1 --hostname-override=server --kubeconfig=/var/lib/rancher/k3s/agent/kubelet.kubeconfig --node-ip=192.168.56.110 --node-labels= --pod-infra-container-image=rancher/mirrored-pause:3.6 --pod-manifest-path=/var/lib/rancher/k3s/agent/pod-manifests --read-only-port=0 --resolv-conf=/run/systemd/resolve/resolv.conf --serialize-image-pulls=false --tls-cert-file=/var/lib/rancher/k3s/agent/serving-kubelet.crt --tls-private-key-file=/var/lib/rancher/k3s/agent/serving-kubelet.key"
time="2024-03-22T03:39:59Z" level=info msg="Connecting to proxy" url="wss://127.0.0.1:6443/v1-k3s/connect"
time="2024-03-22T03:39:59Z" level=info msg="Handling backend connection request [server]"
Flag --cloud-provider has been deprecated, will be removed in 1.25 or later, in favor of removing cloud provider code from Kubelet.
Flag --containerd has been deprecated, This is a cadvisor flag that was mistakenly registered with the Kubelet. Due to legacy concerns, it will follow the standard CLI deprecation timeline before being removed.
Flag --pod-infra-container-image has been deprecated, will be removed in a future release. Image garbage collector will get sandbox image information from CRI.
I0322 03:39:59.772264    5292 server.go:202] "--pod-infra-container-image will not be pruned by the image garbage collector in kubelet and should also be set in the remote runtime"
I0322 03:39:59.778352    5292 server.go:462] "Kubelet version" kubeletVersion="v1.28.7+k3s1"
I0322 03:39:59.778386    5292 server.go:464] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0322 03:39:59.780214    5292 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/rancher/k3s/agent/client-ca.crt"
time="2024-03-22T03:39:59Z" level=info msg="Annotations and labels have already set on node: server"
time="2024-03-22T03:39:59Z" level=info msg="Starting flannel with backend vxlan"
time="2024-03-22T03:39:59Z" level=info msg="Flannel found PodCIDR assigned for node server"
time="2024-03-22T03:39:59Z" level=info msg="The interface enp0s3 with ipv4 address 10.0.2.15 will be used by flannel"
I0322 03:39:59.805858    5292 kube.go:139] Waiting 10m0s for node controller to sync
I0322 03:39:59.809161    5292 kube.go:461] Starting kube subnet manager
I0322 03:39:59.828053    5292 server.go:720] "--cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /"
I0322 03:39:59.829175    5292 container_manager_linux.go:265] "Container manager verified user specified cgroup-root exists" cgroupRoot=[]
I0322 03:39:59.829528    5292 container_manager_linux.go:270] "Creating Container Manager object based on Node Config" nodeConfig={"RuntimeCgroupsName":"","SystemCgroupsName":"","KubeletCgroupsName":"","KubeletOOMScoreAdj":-999,"ContainerRuntime":"","CgroupsPerQOS":true,"CgroupRoot":"/","CgroupDriver":"systemd","KubeletRootDir":"/var/lib/kubelet","ProtectKernelDefaults":false,"KubeReservedCgroupName":"","SystemReservedCgroupName":"","ReservedSystemCPUs":{},"EnforceNodeAllocatable":{"pods":{}},"KubeReserved":null,"SystemReserved":null,"HardEvictionThresholds":[{"Signal":"imagefs.available","Operator":"LessThan","Value":{"Quantity":null,"Percentage":0.05},"GracePeriod":0,"MinReclaim":null},{"Signal":"nodefs.available","Operator":"LessThan","Value":{"Quantity":null,"Percentage":0.05},"GracePeriod":0,"MinReclaim":null}],"QOSReserved":{},"CPUManagerPolicy":"none","CPUManagerPolicyOptions":null,"TopologyManagerScope":"container","CPUManagerReconcilePeriod":10000000000,"ExperimentalMemoryManagerPolicy":"None","ExperimentalMemoryManagerReservedMemory":null,"PodPidsLimit":-1,"EnforceCPULimits":true,"CPUCFSQuotaPeriod":100000000,"TopologyManagerPolicy":"none","TopologyManagerPolicyOptions":null}
I0322 03:39:59.829618    5292 topology_manager.go:138] "Creating topology manager with none policy"
I0322 03:39:59.829658    5292 container_manager_linux.go:301] "Creating device plugin manager"
I0322 03:39:59.830082    5292 state_mem.go:36] "Initialized new in-memory state store"
I0322 03:39:59.830693    5292 kubelet.go:393] "Attempting to sync node with API server"
I0322 03:39:59.830760    5292 kubelet.go:298] "Adding static pod path" path="/var/lib/rancher/k3s/agent/pod-manifests"
I0322 03:39:59.830833    5292 kubelet.go:309] "Adding apiserver pod source"
I0322 03:39:59.830865    5292 apiserver.go:42] "Waiting for node sync before watching apiserver pods"
I0322 03:39:59.833620    5292 kuberuntime_manager.go:257] "Container runtime initialized" containerRuntime="containerd" version="v1.7.11-k3s2" apiVersion="v1"
I0322 03:39:59.836742    5292 server.go:1227] "Started kubelet"
I0322 03:39:59.847616    5292 fs_resource_analyzer.go:67] "Starting FS ResourceAnalyzer"
I0322 03:39:59.853373    5292 server.go:162] "Starting to listen" address="0.0.0.0" port=10250
I0322 03:39:59.854276    5292 server.go:462] "Adding debug handlers to kubelet server"
I0322 03:39:59.859363    5292 ratelimit.go:65] "Setting rate limiting for podresources endpoint" qps=100 burstTokens=10
I0322 03:39:59.860008    5292 server.go:233] "Starting to serve the podresources API" endpoint="unix:/var/lib/kubelet/pod-resources/kubelet.sock"
I0322 03:39:59.864580    5292 volume_manager.go:291] "Starting Kubelet Volume Manager"
I0322 03:39:59.865007    5292 scope.go:117] "RemoveContainer" containerID="afe4640d3a121c51da6bad0227debc7256b8fec3cc9277d94249ed4d0a986231"
I0322 03:39:59.868538    5292 kube.go:482] Creating the node lease for IPv4. This is the n.Spec.PodCIDRs: [10.42.0.0/24]
I0322 03:39:59.868560    5292 kube.go:482] Creating the node lease for IPv4. This is the n.Spec.PodCIDRs: [10.42.1.0/24]
I0322 03:39:59.869959    5292 desired_state_of_world_populator.go:151] "Desired state populator starts to run"
I0322 03:39:59.875832    5292 reconciler_new.go:29] "Reconciler: start to sync state"
I0322 03:39:59.884555    5292 kubelet_network_linux.go:50] "Initialized iptables rules." protocol="IPv4"
I0322 03:39:59.886414    5292 kubelet_network_linux.go:50] "Initialized iptables rules." protocol="IPv6"
I0322 03:39:59.886437    5292 status_manager.go:217] "Starting to sync pod status with apiserver"
I0322 03:39:59.886457    5292 kubelet.go:2303] "Starting kubelet main sync loop"
E0322 03:39:59.886496    5292 kubelet.go:2327] "Skipping pod synchronization" err="[container runtime status check may not have completed yet, PLEG is not healthy: pleg has yet to be successful]"
E0322 03:39:59.917272    5292 cri_stats_provider.go:448] "Failed to get the info of the filesystem with mountpoint" err="unable to find data in memory cache" mountpoint="/var/lib/rancher/k3s/agent/containerd/io.containerd.snapshotter.v1.overlayfs"
E0322 03:39:59.917307    5292 kubelet.go:1431] "Image garbage collection failed once. Stats initialization may not have completed yet" err="invalid capacity 0 on image filesystem"
time="2024-03-22T03:39:59Z" level=info msg="Waiting to retrieve kube-proxy configuration; server is not ready: https://127.0.0.1:6443/v1-k3s/readyz: 500 Internal Server Error"
I0322 03:39:59.970414    5292 kubelet_node_status.go:70] "Attempting to register node" node="server"
time="2024-03-22T03:39:59Z" level=info msg="Starting network policy controller version v2.0.1, built on 2024-02-29T20:36:21Z, go1.21.7"
I0322 03:39:59.972029    5292 network_policy_controller.go:164] Starting network policy controller
I0322 03:39:59.985486    5292 kubelet_node_status.go:108] "Node was previously registered" node="server"
I0322 03:39:59.986028    5292 kubelet_node_status.go:73] "Successfully registered node" node="server"
E0322 03:39:59.986748    5292 kubelet.go:2327] "Skipping pod synchronization" err="container runtime status check may not have completed yet"
I0322 03:39:59.987224    5292 kuberuntime_manager.go:1528] "Updating runtime config through cri with podcidr" CIDR="10.42.0.0/24"
I0322 03:39:59.987657    5292 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.42.0.0/24"
I0322 03:39:59.991618    5292 setters.go:552] "Node became not ready" node="server" condition={"type":"Ready","status":"False","lastHeartbeatTime":"2024-03-22T03:39:59Z","lastTransitionTime":"2024-03-22T03:39:59Z","reason":"KubeletNotReady","message":"container runtime status check may not have completed yet"}
I0322 03:40:00.080338    5292 cpu_manager.go:214] "Starting CPU manager" policy="none"
I0322 03:40:00.080373    5292 cpu_manager.go:215] "Reconciling" reconcilePeriod="10s"
I0322 03:40:00.080390    5292 state_mem.go:36] "Initialized new in-memory state store"
I0322 03:40:00.080807    5292 state_mem.go:88] "Updated default CPUSet" cpuSet=""
I0322 03:40:00.080843    5292 state_mem.go:96] "Updated CPUSet assignments" assignments={}
I0322 03:40:00.080850    5292 policy_none.go:49] "None policy: Start"
I0322 03:40:00.081458    5292 memory_manager.go:169] "Starting memorymanager" policy="None"
I0322 03:40:00.081531    5292 state_mem.go:35] "Initializing new in-memory state store"
I0322 03:40:00.082000    5292 state_mem.go:75] "Updated machine memory state"
I0322 03:40:00.082659    5292 network_policy_controller.go:176] Starting network policy controller full sync goroutine
I0322 03:40:00.089839    5292 manager.go:471] "Failed to read data from checkpoint" checkpoint="kubelet_internal_checkpoint" err="checkpoint is not found"
I0322 03:40:00.092290    5292 plugin_manager.go:118] "Starting Kubelet Plugin Manager"
I0322 03:40:00.102001    5292 kubelet_node_status.go:493] "Fast updating node status as it just became ready"
I0322 03:40:00.187700    5292 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="afe4640d3a121c51da6bad0227debc7256b8fec3cc9277d94249ed4d0a986231"
I0322 03:40:00.187753    5292 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="0439454a99028bf8c698a0f50ca188abf5ca0b4e3b29fdb2d89cd0252d09bb1d"
time="2024-03-22T03:40:00Z" level=info msg="Kube API server is now running"
time="2024-03-22T03:40:00Z" level=info msg="ETCD server is now running"
time="2024-03-22T03:40:00Z" level=info msg="k3s is up and running"
time="2024-03-22T03:40:00Z" level=info msg="Creating k3s-supervisor event broadcaster"
time="2024-03-22T03:40:00Z" level=info msg="Waiting for cloud-controller-manager privileges to become available"
time="2024-03-22T03:40:00Z" level=info msg="Applying CRD addons.k3s.cattle.io"
I0322 03:40:00.812949    5292 serving.go:355] Generated self-signed cert in-memory
time="2024-03-22T03:40:00Z" level=info msg="Stopped tunnel to 127.0.0.1:6443"
time="2024-03-22T03:40:00Z" level=info msg="Connecting to proxy" url="wss://192.168.56.110:6443/v1-k3s/connect"
time="2024-03-22T03:40:00Z" level=info msg="Proxy done" err="context canceled" url="wss://127.0.0.1:6443/v1-k3s/connect"
I0322 03:40:00.869126    5292 kube.go:146] Node controller sync successful
I0322 03:40:00.869210    5292 vxlan.go:141] VXLAN config: VNI=1 Port=0 GBP=false Learning=false DirectRouting=false
time="2024-03-22T03:40:00Z" level=info msg="error in remotedialer server [400]: websocket: close 1006 (abnormal closure): unexpected EOF"
I0322 03:40:00.886954    5292 serving.go:355] Generated self-signed cert in-memory
I0322 03:40:00.905361    5292 apiserver.go:52] "Watching apiserver"
I0322 03:40:00.907754    5292 serving.go:355] Generated self-signed cert in-memory
time="2024-03-22T03:40:00Z" level=info msg="Handling backend connection request [server]"
I0322 03:40:01.043153    5292 topology_manager.go:215] "Topology Admit Handler" podUID="f5749ede-2796-414e-b0ea-d6e2523ec959" podNamespace="kube-system" podName="helm-install-traefik-fxsb6"
I0322 03:40:01.044210    5292 topology_manager.go:215] "Topology Admit Handler" podUID="bd655422-2e30-4b8c-a19a-617f504f2026" podNamespace="kube-system" podName="local-path-provisioner-6c86858495-lvfsk"
I0322 03:40:01.044256    5292 topology_manager.go:215] "Topology Admit Handler" podUID="1ea7b8bb-866e-4ee0-84fb-ed8098467564" podNamespace="kube-system" podName="metrics-server-67c658944b-p82jk"
I0322 03:40:01.044283    5292 topology_manager.go:215] "Topology Admit Handler" podUID="5358d3b8-3fa6-4dcc-8789-ee2721507f22" podNamespace="kube-system" podName="coredns-6799fbcd5-rx42b"
I0322 03:40:01.044314    5292 topology_manager.go:215] "Topology Admit Handler" podUID="075f8ac2-232b-4d0a-b9ad-8fb5739de94d" podNamespace="kube-system" podName="helm-install-traefik-crd-dlc6m"
I0322 03:40:01.102829    5292 desired_state_of_world_populator.go:159] "Finished populating initial desired state of world"
time="2024-03-22T03:40:01Z" level=info msg="Tunnel authorizer set Kubelet Port 10250"
I0322 03:40:01.251960    5292 kube.go:621] List of node(server) annotations: map[string]string{"alpha.kubernetes.io/provided-node-ip":"192.168.56.110", "flannel.alpha.coreos.com/backend-data":"{\"VNI\":1,\"VtepMAC\":\"1a:3d:0b:c6:5e:e3\"}", "flannel.alpha.coreos.com/backend-type":"vxlan", "flannel.alpha.coreos.com/kube-subnet-manager":"true", "flannel.alpha.coreos.com/public-ip":"10.0.2.15", "k3s.io/hostname":"server", "k3s.io/internal-ip":"192.168.56.110", "k3s.io/node-args":"[\"server\",\"--node-ip\",\"192.168.56.110\",\"--write-kubeconfig-mode\",\"644\",\"--log\",\"/vagrant/logs/server.logs\"]", "k3s.io/node-config-hash":"OEDLXPLCFN65I5K4IWF2M6JMEKD34L47WZJH7YZKLHVRXJTJG6TQ====", "k3s.io/node-env":"{\"K3S_DATA_DIR\":\"/var/lib/rancher/k3s/data/a3b46c0299091b71bfcc617b1e1fec1845c13bdd848584ceb39d2e700e702a4b\"}", "node.alpha.kubernetes.io/ttl":"0", "volumes.kubernetes.io/controller-managed-attach-detach":"true"}
I0322 03:40:01.252014    5292 vxlan.go:155] Setup flannel.1 mac address to 1a:3d:0b:c6:5e:e3 when flannel restarts
time="2024-03-22T03:40:01Z" level=info msg="Wrote flannel subnet file to /run/flannel/subnet.env"
time="2024-03-22T03:40:01Z" level=info msg="Running flannel backend."
I0322 03:40:01.277587    5292 vxlan_network.go:65] watching for new subnet leases
I0322 03:40:01.277610    5292 subnet.go:160] Batch elem [0] is { lease.Event{Type:0, Lease:lease.Lease{EnableIPv4:true, EnableIPv6:false, Subnet:ip.IP4Net{IP:0xa2a0100, PrefixLen:0x18}, IPv6Subnet:ip.IP6Net{IP:(*ip.IP6)(nil), PrefixLen:0x0}, Attrs:lease.LeaseAttrs{PublicIP:0xa00020f, PublicIPv6:(*ip.IP6)(nil), BackendType:"vxlan", BackendData:json.RawMessage{0x7b, 0x22, 0x56, 0x4e, 0x49, 0x22, 0x3a, 0x31, 0x2c, 0x22, 0x56, 0x74, 0x65, 0x70, 0x4d, 0x41, 0x43, 0x22, 0x3a, 0x22, 0x36, 0x36, 0x3a, 0x39, 0x63, 0x3a, 0x66, 0x30, 0x3a, 0x61, 0x36, 0x3a, 0x63, 0x32, 0x3a, 0x34, 0x30, 0x22, 0x7d}, BackendV6Data:json.RawMessage(nil)}, Expiration:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Asof:0}} }
I0322 03:40:01.279139    5292 iptables.go:290] generated 7 rules
I0322 03:40:01.281009    5292 iptables.go:290] generated 3 rules
I0322 03:40:01.313800    5292 iptables.go:283] bootstrap done
time="2024-03-22T03:40:01Z" level=info msg="Applying CRD etcdsnapshotfiles.k3s.cattle.io"
I0322 03:40:01.373199    5292 iptables.go:283] bootstrap done
I0322 03:40:01.456029    5292 serving.go:355] Generated self-signed cert in-memory
I0322 03:40:01.498556    5292 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.7+k3s1"
I0322 03:40:01.498577    5292 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0322 03:40:01.581624    5292 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0322 03:40:01.582531    5292 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I0322 03:40:01.582547    5292 shared_informer.go:311] Waiting for caches to sync for RequestHeaderAuthRequestController
I0322 03:40:01.582560    5292 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0322 03:40:01.584674    5292 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0322 03:40:01.584688    5292 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0322 03:40:01.584701    5292 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0322 03:40:01.584705    5292 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
time="2024-03-22T03:40:01Z" level=info msg="Applying CRD helmcharts.helm.cattle.io"
I0322 03:40:01.639278    5292 controllermanager.go:189] "Starting" version="v1.28.7+k3s1"
I0322 03:40:01.639386    5292 controllermanager.go:191] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0322 03:40:01.645965    5292 scope.go:117] "RemoveContainer" containerID="ffe0f55cf85690948d03fdc4788fc1ac84b6bab89bbb81e00591e6de1946c7ef"
I0322 03:40:01.657764    5292 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/helm-install-traefik-fxsb6" containerName="helm"
I0322 03:40:01.677921    5292 secure_serving.go:213] Serving securely on 127.0.0.1:10257
I0322 03:40:01.678556    5292 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I0322 03:40:01.678657    5292 shared_informer.go:311] Waiting for caches to sync for RequestHeaderAuthRequestController
I0322 03:40:01.678780    5292 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0322 03:40:01.678902    5292 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0322 03:40:01.678955    5292 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0322 03:40:01.679028    5292 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0322 03:40:01.679034    5292 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
time="2024-03-22T03:40:01Z" level=info msg="Applying CRD helmchartconfigs.helm.cattle.io"
I0322 03:40:01.716805    5292 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0322 03:40:01.716997    5292 shared_informer.go:318] Caches are synced for RequestHeaderAuthRequestController
I0322 03:40:01.717098    5292 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0322 03:40:01.782091    5292 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0322 03:40:01.782180    5292 shared_informer.go:318] Caches are synced for RequestHeaderAuthRequestController
I0322 03:40:01.782280    5292 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
E0322 03:40:01.824323    5292 controllermanager.go:503] unable to get all supported resources from server: unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I0322 03:40:01.844514    5292 shared_informer.go:311] Waiting for caches to sync for tokens
I0322 03:40:01.860931    5292 controllermanager.go:642] "Started controller" controller="cronjob-controller"
I0322 03:40:01.861292    5292 controllermanager.go:607] "Warning: controller is disabled" controller="cloud-node-lifecycle-controller"
I0322 03:40:01.863076    5292 cronjob_controllerv2.go:139] "Starting cronjob controller v2"
I0322 03:40:01.863155    5292 shared_informer.go:311] Waiting for caches to sync for cronjob
I0322 03:40:01.890103    5292 controllermanager.go:642] "Started controller" controller="statefulset-controller"
I0322 03:40:01.890615    5292 stateful_set.go:161] "Starting stateful set controller"
I0322 03:40:01.890632    5292 shared_informer.go:311] Waiting for caches to sync for stateful set
I0322 03:40:01.892567    5292 controllermanager.go:642] "Started controller" controller="certificatesigningrequest-approving-controller"
I0322 03:40:01.894164    5292 certificate_controller.go:115] "Starting certificate controller" name="csrapproving"
I0322 03:40:01.894182    5292 shared_informer.go:311] Waiting for caches to sync for certificate-csrapproving
I0322 03:40:01.896291    5292 controllermanager.go:642] "Started controller" controller="token-cleaner-controller"
I0322 03:40:01.896396    5292 tokencleaner.go:112] "Starting token cleaner controller"
I0322 03:40:01.896402    5292 shared_informer.go:311] Waiting for caches to sync for token_cleaner
I0322 03:40:01.896407    5292 shared_informer.go:318] Caches are synced for token_cleaner
time="2024-03-22T03:40:01Z" level=info msg="Writing static file: /var/lib/rancher/k3s/server/static/charts/traefik-25.0.2+up25.0.0.tgz"
time="2024-03-22T03:40:01Z" level=info msg="Writing static file: /var/lib/rancher/k3s/server/static/charts/traefik-crd-25.0.2+up25.0.0.tgz"
time="2024-03-22T03:40:01Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-service.yaml"
time="2024-03-22T03:40:01Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/resource-reader.yaml"
time="2024-03-22T03:40:01Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/rolebindings.yaml"
time="2024-03-22T03:40:01Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/auth-delegator.yaml"
time="2024-03-22T03:40:01Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/auth-reader.yaml"
time="2024-03-22T03:40:01Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-deployment.yaml"
time="2024-03-22T03:40:01Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/metrics-apiservice.yaml"
time="2024-03-22T03:40:01Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/runtimes.yaml"
time="2024-03-22T03:40:01Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/coredns.yaml"
time="2024-03-22T03:40:01Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/local-storage.yaml"
time="2024-03-22T03:40:01Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/aggregated-metrics-reader.yaml"
time="2024-03-22T03:40:01Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/ccm.yaml"
time="2024-03-22T03:40:01Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/traefik.yaml"
I0322 03:40:01.950153    5292 shared_informer.go:318] Caches are synced for tokens
time="2024-03-22T03:40:01Z" level=info msg="Starting dynamiclistener CN filter node controller"
time="2024-03-22T03:40:01Z" level=info msg="Tunnel server egress proxy mode: agent"
I0322 03:40:01.958617    5292 controllermanager.go:168] Version: v1.28.7+k3s1
I0322 03:40:01.967627    5292 secure_serving.go:213] Serving securely on 127.0.0.1:10258
I0322 03:40:01.968512    5292 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I0322 03:40:01.968635    5292 shared_informer.go:311] Waiting for caches to sync for RequestHeaderAuthRequestController
I0322 03:40:01.968716    5292 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0322 03:40:01.968846    5292 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0322 03:40:01.968906    5292 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0322 03:40:01.969285    5292 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0322 03:40:01.969388    5292 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
E0322 03:40:01.990239    5292 controllermanager.go:524] unable to get all supported resources from server: unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
time="2024-03-22T03:40:01Z" level=info msg="Creating  event broadcaster"
I0322 03:40:02.111616    5292 shared_informer.go:318] Caches are synced for RequestHeaderAuthRequestController
I0322 03:40:02.111790    5292 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0322 03:40:02.111846    5292 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
time="2024-03-22T03:40:02Z" level=info msg="Starting k3s.cattle.io/v1, Kind=Addon controller"
time="2024-03-22T03:40:02Z" level=info msg="Creating deploy event broadcaster"
time="2024-03-22T03:40:02Z" level=info msg="Starting /v1, Kind=Node controller"
I0322 03:40:02.212379    5292 event.go:307] "Event occurred" object="kube-system/ccm" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/ccm.yaml\""
time="2024-03-22T03:40:02Z" level=info msg="Starting /v1, Kind=Secret controller"
time="2024-03-22T03:40:02Z" level=info msg="Starting /v1, Kind=Node controller"
time="2024-03-22T03:40:02Z" level=info msg="Creating helm-controller event broadcaster"
time="2024-03-22T03:40:02Z" level=info msg="Starting /v1, Kind=Pod controller"
time="2024-03-22T03:40:02Z" level=info msg="Starting apps/v1, Kind=DaemonSet controller"
I0322 03:40:02.320815    5292 controllermanager.go:337] Started "cloud-node-controller"
I0322 03:40:02.321100    5292 controllermanager.go:337] Started "cloud-node-lifecycle-controller"
I0322 03:40:02.321532    5292 controllermanager.go:337] Started "service-lb-controller"
W0322 03:40:02.321581    5292 controllermanager.go:314] "node-route-controller" is disabled
time="2024-03-22T03:40:02Z" level=info msg="Starting discovery.k8s.io/v1, Kind=EndpointSlice controller"
I0322 03:40:02.322502    5292 node_controller.go:165] Sending events to api server.
I0322 03:40:02.322623    5292 node_controller.go:174] Waiting for informer caches to sync
I0322 03:40:02.322683    5292 node_lifecycle_controller.go:113] Sending events to api server
I0322 03:40:02.322841    5292 controller.go:231] Starting service controller
I0322 03:40:02.322882    5292 shared_informer.go:311] Waiting for caches to sync for service
time="2024-03-22T03:40:02Z" level=info msg="Updating TLS secret for kube-system/k3s-serving (count: 10): map[listener.cattle.io/cn-10.43.0.1:10.43.0.1 listener.cattle.io/cn-127.0.0.1:127.0.0.1 listener.cattle.io/cn-192.168.56.110:192.168.56.110 listener.cattle.io/cn-__1-f16284:::1 listener.cattle.io/cn-kubernetes:kubernetes listener.cattle.io/cn-kubernetes.default:kubernetes.default listener.cattle.io/cn-kubernetes.default.svc:kubernetes.default.svc listener.cattle.io/cn-kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local listener.cattle.io/cn-localhost:localhost listener.cattle.io/cn-server:server listener.cattle.io/fingerprint:SHA1=3EF84C4C3E5D7F82BBAF806634968BEF8CE80EDA]"
time="2024-03-22T03:40:02Z" level=info msg="Cluster dns configmap already exists"
time="2024-03-22T03:40:02Z" level=info msg="Labels and annotations have been set successfully on node: server"
I0322 03:40:02.420014    5292 controller.go:624] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0322 03:40:02.422074    5292 event.go:307] "Event occurred" object="kube-system/ccm" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/ccm.yaml\""
I0322 03:40:02.425669    5292 controller.go:624] quota admission added evaluator for: addons.k3s.cattle.io
time="2024-03-22T03:40:02Z" level=info msg="Starting helm.cattle.io/v1, Kind=HelmChart controller"
I0322 03:40:02.429618    5292 shared_informer.go:318] Caches are synced for service
time="2024-03-22T03:40:02Z" level=info msg="Starting helm.cattle.io/v1, Kind=HelmChartConfig controller"
I0322 03:40:02.436329    5292 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
I0322 03:40:02.436389    5292 event.go:307] "Event occurred" object="kube-system/traefik-crd" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik-crd"
time="2024-03-22T03:40:02Z" level=info msg="Starting rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding controller"
time="2024-03-22T03:40:02Z" level=info msg="Starting batch/v1, Kind=Job controller"
time="2024-03-22T03:40:02Z" level=info msg="Starting /v1, Kind=ConfigMap controller"
time="2024-03-22T03:40:02Z" level=info msg="Starting /v1, Kind=ServiceAccount controller"
I0322 03:40:02.503356    5292 event.go:307] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/coredns.yaml\""
I0322 03:40:02.839187    5292 controller.go:624] quota admission added evaluator for: deployments.apps
I0322 03:40:02.895344    5292 event.go:307] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/coredns.yaml\""
I0322 03:40:02.921352    5292 event.go:307] "Event occurred" object="kube-system/local-storage" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/local-storage.yaml\""
I0322 03:40:02.981728    5292 event.go:307] "Event occurred" object="kube-system/local-storage" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/local-storage.yaml\""
time="2024-03-22T03:40:02Z" level=info msg="certificate CN=serverworker signed by CN=k3s-server-ca@1711078100: notBefore=2024-03-22 03:28:20 +0000 UTC notAfter=2025-03-22 03:40:02 +0000 UTC"
I0322 03:40:02.998462    5292 event.go:307] "Event occurred" object="kube-system/aggregated-metrics-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/aggregated-metrics-reader.yaml\""
I0322 03:40:03.185330    5292 event.go:307] "Event occurred" object="kube-system/aggregated-metrics-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/aggregated-metrics-reader.yaml\""
W0322 03:40:03.312585    5292 aggregator.go:166] failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 03:40:03.341365    5292 event.go:307] "Event occurred" object="server" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodePasswordValidationComplete" message="Deferred node password secret validation complete"
time="2024-03-22T03:40:03Z" level=info msg="certificate CN=system:node:serverworker,O=system:nodes signed by CN=k3s-client-ca@1711078100: notBefore=2024-03-22 03:28:20 +0000 UTC notAfter=2025-03-22 03:40:03 +0000 UTC"
I0322 03:40:03.366122    5292 event.go:307] "Event occurred" object="kube-system/auth-delegator" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/auth-delegator.yaml\""
I0322 03:40:03.485550    5292 event.go:307] "Event occurred" object="kube-system/auth-delegator" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/auth-delegator.yaml\""
I0322 03:40:03.506798    5292 event.go:307] "Event occurred" object="kube-system/auth-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/auth-reader.yaml\""
I0322 03:40:03.648063    5292 event.go:307] "Event occurred" object="kube-system/auth-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/auth-reader.yaml\""
I0322 03:40:03.666671    5292 event.go:307] "Event occurred" object="kube-system/metrics-apiservice" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-apiservice.yaml\""
I0322 03:40:03.696732    5292 event.go:307] "Event occurred" object="kube-system/metrics-apiservice" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-apiservice.yaml\""
I0322 03:40:03.717685    5292 event.go:307] "Event occurred" object="kube-system/metrics-server-deployment" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-deployment.yaml\""
I0322 03:40:03.748582    5292 event.go:307] "Event occurred" object="kube-system/metrics-server-deployment" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-deployment.yaml\""
I0322 03:40:03.756513    5292 event.go:307] "Event occurred" object="kube-system/metrics-server-service" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-service.yaml\""
I0322 03:40:03.765778    5292 event.go:307] "Event occurred" object="kube-system/metrics-server-service" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-service.yaml\""
I0322 03:40:03.772541    5292 event.go:307] "Event occurred" object="kube-system/resource-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/resource-reader.yaml\""
I0322 03:40:03.822962    5292 event.go:307] "Event occurred" object="kube-system/resource-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/resource-reader.yaml\""
I0322 03:40:03.830079    5292 event.go:307] "Event occurred" object="kube-system/rolebindings" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/rolebindings.yaml\""
I0322 03:40:03.854705    5292 event.go:307] "Event occurred" object="kube-system/rolebindings" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/rolebindings.yaml\""
I0322 03:40:03.862270    5292 event.go:307] "Event occurred" object="kube-system/runtimes" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/runtimes.yaml\""
I0322 03:40:03.882783    5292 event.go:307] "Event occurred" object="kube-system/runtimes" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/runtimes.yaml\""
I0322 03:40:03.901422    5292 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/traefik.yaml\""
I0322 03:40:03.912730    5292 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/traefik.yaml\""
I0322 03:40:04.542693    5292 controller.go:624] quota admission added evaluator for: serviceaccounts
I0322 03:40:04.612043    5292 alloc.go:330] "allocated clusterIPs" service="kube-system/traefik" clusterIPs={"IPv4":"10.43.239.204"}
I0322 03:40:04.616572    5292 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Service" apiVersion="v1" type="Normal" reason="EnsuringLoadBalancer" message="Ensuring load balancer"
I0322 03:40:04.685897    5292 controller.go:624] quota admission added evaluator for: daemonsets.apps
I0322 03:40:04.698293    5292 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Service" apiVersion="v1" type="Normal" reason="AppliedDaemonSet" message="Applied LoadBalancer DaemonSet kube-system/svclb-traefik-ae8f6f60"
I0322 03:40:04.698496    5292 controller.go:624] quota admission added evaluator for: ingressroutes.traefik.io
time="2024-03-22T03:40:04Z" level=info msg="Handling backend connection request [serverworker]"
time="2024-03-22T03:40:05Z" level=info msg="Running kube-proxy --cluster-cidr=10.42.0.0/16 --conntrack-max-per-core=0 --conntrack-tcp-timeout-close-wait=0s --conntrack-tcp-timeout-established=0s --healthz-bind-address=127.0.0.1 --hostname-override=server --kubeconfig=/var/lib/rancher/k3s/agent/kubeproxy.kubeconfig --proxy-mode=iptables"
I0322 03:40:05.588825    5292 node.go:141] Successfully retrieved node IP: 192.168.56.110
I0322 03:40:05.611542    5292 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0322 03:40:05.613487    5292 server_others.go:152] "Using iptables Proxier"
I0322 03:40:05.614009    5292 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0322 03:40:05.614102    5292 server_others.go:438] "Defaulting to no-op detect-local"
I0322 03:40:05.615495    5292 proxier.go:250] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0322 03:40:05.617509    5292 server.go:846] "Version info" version="v1.28.7+k3s1"
I0322 03:40:05.617605    5292 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0322 03:40:05.633000    5292 config.go:188] "Starting service config controller"
I0322 03:40:05.633587    5292 shared_informer.go:311] Waiting for caches to sync for service config
I0322 03:40:05.633731    5292 config.go:97] "Starting endpoint slice config controller"
I0322 03:40:05.633797    5292 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0322 03:40:05.633813    5292 config.go:315] "Starting node config controller"
I0322 03:40:05.633824    5292 shared_informer.go:311] Waiting for caches to sync for node config
I0322 03:40:05.734022    5292 shared_informer.go:318] Caches are synced for service config
I0322 03:40:05.735126    5292 shared_informer.go:318] Caches are synced for endpoint slice config
I0322 03:40:05.736035    5292 shared_informer.go:318] Caches are synced for node config
I0322 03:40:05.816051    5292 scope.go:117] "RemoveContainer" containerID="ffe0f55cf85690948d03fdc4788fc1ac84b6bab89bbb81e00591e6de1946c7ef"
I0322 03:40:07.263869    5292 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"content\" (UniqueName: \"kubernetes.io/configmap/f5749ede-2796-414e-b0ea-d6e2523ec959-content\") pod \"f5749ede-2796-414e-b0ea-d6e2523ec959\" (UID: \"f5749ede-2796-414e-b0ea-d6e2523ec959\") "
I0322 03:40:07.264014    5292 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-mk5j8\" (UniqueName: \"kubernetes.io/projected/f5749ede-2796-414e-b0ea-d6e2523ec959-kube-api-access-mk5j8\") pod \"f5749ede-2796-414e-b0ea-d6e2523ec959\" (UID: \"f5749ede-2796-414e-b0ea-d6e2523ec959\") "
I0322 03:40:07.264072    5292 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"values\" (UniqueName: \"kubernetes.io/secret/f5749ede-2796-414e-b0ea-d6e2523ec959-values\") pod \"f5749ede-2796-414e-b0ea-d6e2523ec959\" (UID: \"f5749ede-2796-414e-b0ea-d6e2523ec959\") "
I0322 03:40:07.264123    5292 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/f5749ede-2796-414e-b0ea-d6e2523ec959-klipper-cache\") pod \"f5749ede-2796-414e-b0ea-d6e2523ec959\" (UID: \"f5749ede-2796-414e-b0ea-d6e2523ec959\") "
I0322 03:40:07.264141    5292 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/f5749ede-2796-414e-b0ea-d6e2523ec959-klipper-config\") pod \"f5749ede-2796-414e-b0ea-d6e2523ec959\" (UID: \"f5749ede-2796-414e-b0ea-d6e2523ec959\") "
I0322 03:40:07.264162    5292 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/f5749ede-2796-414e-b0ea-d6e2523ec959-klipper-helm\") pod \"f5749ede-2796-414e-b0ea-d6e2523ec959\" (UID: \"f5749ede-2796-414e-b0ea-d6e2523ec959\") "
I0322 03:40:07.264255    5292 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/f5749ede-2796-414e-b0ea-d6e2523ec959-tmp\") pod \"f5749ede-2796-414e-b0ea-d6e2523ec959\" (UID: \"f5749ede-2796-414e-b0ea-d6e2523ec959\") "
I0322 03:40:07.273086    5292 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/configmap/f5749ede-2796-414e-b0ea-d6e2523ec959-content" (OuterVolumeSpecName: "content") pod "f5749ede-2796-414e-b0ea-d6e2523ec959" (UID: "f5749ede-2796-414e-b0ea-d6e2523ec959"). InnerVolumeSpecName "content". PluginName "kubernetes.io/configmap", VolumeGidValue ""
I0322 03:40:07.308253    5292 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/f5749ede-2796-414e-b0ea-d6e2523ec959-values" (OuterVolumeSpecName: "values") pod "f5749ede-2796-414e-b0ea-d6e2523ec959" (UID: "f5749ede-2796-414e-b0ea-d6e2523ec959"). InnerVolumeSpecName "values". PluginName "kubernetes.io/secret", VolumeGidValue ""
I0322 03:40:07.309925    5292 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/f5749ede-2796-414e-b0ea-d6e2523ec959-klipper-cache" (OuterVolumeSpecName: "klipper-cache") pod "f5749ede-2796-414e-b0ea-d6e2523ec959" (UID: "f5749ede-2796-414e-b0ea-d6e2523ec959"). InnerVolumeSpecName "klipper-cache". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0322 03:40:07.310479    5292 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/f5749ede-2796-414e-b0ea-d6e2523ec959-tmp" (OuterVolumeSpecName: "tmp") pod "f5749ede-2796-414e-b0ea-d6e2523ec959" (UID: "f5749ede-2796-414e-b0ea-d6e2523ec959"). InnerVolumeSpecName "tmp". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0322 03:40:07.313362    5292 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/f5749ede-2796-414e-b0ea-d6e2523ec959-kube-api-access-mk5j8" (OuterVolumeSpecName: "kube-api-access-mk5j8") pod "f5749ede-2796-414e-b0ea-d6e2523ec959" (UID: "f5749ede-2796-414e-b0ea-d6e2523ec959"). InnerVolumeSpecName "kube-api-access-mk5j8". PluginName "kubernetes.io/projected", VolumeGidValue ""
I0322 03:40:07.313734    5292 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/f5749ede-2796-414e-b0ea-d6e2523ec959-klipper-helm" (OuterVolumeSpecName: "klipper-helm") pod "f5749ede-2796-414e-b0ea-d6e2523ec959" (UID: "f5749ede-2796-414e-b0ea-d6e2523ec959"). InnerVolumeSpecName "klipper-helm". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0322 03:40:07.315921    5292 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/f5749ede-2796-414e-b0ea-d6e2523ec959-klipper-config" (OuterVolumeSpecName: "klipper-config") pod "f5749ede-2796-414e-b0ea-d6e2523ec959" (UID: "f5749ede-2796-414e-b0ea-d6e2523ec959"). InnerVolumeSpecName "klipper-config". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0322 03:40:07.365337    5292 reconciler_common.go:300] "Volume detached for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/f5749ede-2796-414e-b0ea-d6e2523ec959-klipper-config\") on node \"server\" DevicePath \"\""
I0322 03:40:07.365371    5292 reconciler_common.go:300] "Volume detached for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/f5749ede-2796-414e-b0ea-d6e2523ec959-klipper-helm\") on node \"server\" DevicePath \"\""
I0322 03:40:07.365381    5292 reconciler_common.go:300] "Volume detached for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/f5749ede-2796-414e-b0ea-d6e2523ec959-tmp\") on node \"server\" DevicePath \"\""
I0322 03:40:07.365390    5292 reconciler_common.go:300] "Volume detached for volume \"content\" (UniqueName: \"kubernetes.io/configmap/f5749ede-2796-414e-b0ea-d6e2523ec959-content\") on node \"server\" DevicePath \"\""
I0322 03:40:07.365400    5292 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-mk5j8\" (UniqueName: \"kubernetes.io/projected/f5749ede-2796-414e-b0ea-d6e2523ec959-kube-api-access-mk5j8\") on node \"server\" DevicePath \"\""
I0322 03:40:07.365407    5292 reconciler_common.go:300] "Volume detached for volume \"values\" (UniqueName: \"kubernetes.io/secret/f5749ede-2796-414e-b0ea-d6e2523ec959-values\") on node \"server\" DevicePath \"\""
I0322 03:40:07.365415    5292 reconciler_common.go:300] "Volume detached for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/f5749ede-2796-414e-b0ea-d6e2523ec959-klipper-cache\") on node \"server\" DevicePath \"\""
I0322 03:40:07.846331    5292 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="b194ae96fe2d09a99234c23f2af670628ad2982956c8338ae3bbf7ac043ccad1"
I0322 03:40:12.001061    5292 range_allocator.go:111] "No Secondary Service CIDR provided. Skipping filtering out secondary service addresses"
I0322 03:40:12.002974    5292 controllermanager.go:642] "Started controller" controller="node-ipam-controller"
I0322 03:40:12.003628    5292 node_ipam_controller.go:162] "Starting ipam controller"
I0322 03:40:12.005227    5292 shared_informer.go:311] Waiting for caches to sync for node
I0322 03:40:12.018898    5292 controllermanager.go:642] "Started controller" controller="persistentvolume-attach-detach-controller"
I0322 03:40:12.020342    5292 attach_detach_controller.go:337] "Starting attach detach controller"
I0322 03:40:12.020365    5292 shared_informer.go:311] Waiting for caches to sync for attach detach
I0322 03:40:12.022286    5292 controllermanager.go:642] "Started controller" controller="ttl-after-finished-controller"
I0322 03:40:12.024458    5292 ttlafterfinished_controller.go:109] "Starting TTL after finished controller"
I0322 03:40:12.024483    5292 shared_informer.go:311] Waiting for caches to sync for TTL after finished
I0322 03:40:12.025857    5292 controllermanager.go:642] "Started controller" controller="root-ca-certificate-publisher-controller"
I0322 03:40:12.027327    5292 publisher.go:102] "Starting root CA cert publisher controller"
I0322 03:40:12.027347    5292 shared_informer.go:311] Waiting for caches to sync for crt configmap
I0322 03:40:12.028920    5292 controllermanager.go:642] "Started controller" controller="replicationcontroller-controller"
I0322 03:40:12.031727    5292 replica_set.go:214] "Starting controller" name="replicationcontroller"
I0322 03:40:12.031831    5292 shared_informer.go:311] Waiting for caches to sync for ReplicationController
I0322 03:40:12.032607    5292 controllermanager.go:642] "Started controller" controller="pod-garbage-collector-controller"
I0322 03:40:12.033068    5292 gc_controller.go:101] "Starting GC controller"
I0322 03:40:12.033117    5292 shared_informer.go:311] Waiting for caches to sync for GC
E0322 03:40:12.048573    5292 namespaced_resources_deleter.go:162] unable to get all supported resources from server: unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I0322 03:40:12.048691    5292 controllermanager.go:642] "Started controller" controller="namespace-controller"
I0322 03:40:12.050741    5292 namespace_controller.go:197] "Starting namespace controller"
I0322 03:40:12.050757    5292 shared_informer.go:311] Waiting for caches to sync for namespace
I0322 03:40:12.052036    5292 certificate_controller.go:115] "Starting certificate controller" name="csrsigning-kubelet-serving"
I0322 03:40:12.052148    5292 shared_informer.go:311] Waiting for caches to sync for certificate-csrsigning-kubelet-serving
I0322 03:40:12.052703    5292 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/var/lib/rancher/k3s/server/tls/server-ca.nochain.crt::/var/lib/rancher/k3s/server/tls/server-ca.key"
I0322 03:40:12.053175    5292 controllermanager.go:642] "Started controller" controller="certificatesigningrequest-signing-controller"
I0322 03:40:12.053195    5292 controllermanager.go:607] "Warning: controller is disabled" controller="bootstrap-signer-controller"
I0322 03:40:12.053402    5292 certificate_controller.go:115] "Starting certificate controller" name="csrsigning-kubelet-client"
I0322 03:40:12.053429    5292 shared_informer.go:311] Waiting for caches to sync for certificate-csrsigning-kubelet-client
I0322 03:40:12.053473    5292 certificate_controller.go:115] "Starting certificate controller" name="csrsigning-kube-apiserver-client"
I0322 03:40:12.053482    5292 shared_informer.go:311] Waiting for caches to sync for certificate-csrsigning-kube-apiserver-client
I0322 03:40:12.053497    5292 certificate_controller.go:115] "Starting certificate controller" name="csrsigning-legacy-unknown"
I0322 03:40:12.053500    5292 shared_informer.go:311] Waiting for caches to sync for certificate-csrsigning-legacy-unknown
I0322 03:40:12.053514    5292 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/var/lib/rancher/k3s/server/tls/client-ca.nochain.crt::/var/lib/rancher/k3s/server/tls/client-ca.key"
I0322 03:40:12.053601    5292 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/var/lib/rancher/k3s/server/tls/client-ca.nochain.crt::/var/lib/rancher/k3s/server/tls/client-ca.key"
I0322 03:40:12.053643    5292 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/var/lib/rancher/k3s/server/tls/server-ca.nochain.crt::/var/lib/rancher/k3s/server/tls/server-ca.key"
I0322 03:40:12.055882    5292 controllermanager.go:642] "Started controller" controller="persistentvolume-expander-controller"
I0322 03:40:12.056182    5292 expand_controller.go:328] "Starting expand controller"
I0322 03:40:12.056196    5292 shared_informer.go:311] Waiting for caches to sync for expand
I0322 03:40:12.060487    5292 controllermanager.go:642] "Started controller" controller="persistentvolume-protection-controller"
I0322 03:40:12.060811    5292 pv_protection_controller.go:78] "Starting PV protection controller"
I0322 03:40:12.060910    5292 shared_informer.go:311] Waiting for caches to sync for PV protection
I0322 03:40:12.063216    5292 controllermanager.go:642] "Started controller" controller="ephemeral-volume-controller"
I0322 03:40:12.063344    5292 controller.go:169] "Starting ephemeral volume controller"
I0322 03:40:12.063358    5292 shared_informer.go:311] Waiting for caches to sync for ephemeral
I0322 03:40:12.066913    5292 controllermanager.go:642] "Started controller" controller="endpoints-controller"
I0322 03:40:12.067119    5292 endpoints_controller.go:174] "Starting endpoint controller"
I0322 03:40:12.067168    5292 shared_informer.go:311] Waiting for caches to sync for endpoint
I0322 03:40:12.073855    5292 garbagecollector.go:155] "Starting controller" controller="garbagecollector"
I0322 03:40:12.073967    5292 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0322 03:40:12.074074    5292 graph_builder.go:294] "Running" component="GraphBuilder"
I0322 03:40:12.079118    5292 controllermanager.go:642] "Started controller" controller="garbage-collector-controller"
I0322 03:40:12.091447    5292 controllermanager.go:642] "Started controller" controller="horizontal-pod-autoscaler-controller"
I0322 03:40:12.091662    5292 horizontal.go:200] "Starting HPA controller"
I0322 03:40:12.091672    5292 shared_informer.go:311] Waiting for caches to sync for HPA
I0322 03:40:12.094072    5292 controllermanager.go:642] "Started controller" controller="certificatesigningrequest-cleaner-controller"
I0322 03:40:12.094239    5292 cleaner.go:83] "Starting CSR cleaner controller"
I0322 03:40:12.096760    5292 node_lifecycle_controller.go:431] "Controller will reconcile labels"
I0322 03:40:12.097341    5292 controllermanager.go:642] "Started controller" controller="node-lifecycle-controller"
I0322 03:40:12.098167    5292 node_lifecycle_controller.go:465] "Sending events to api server"
I0322 03:40:12.098199    5292 node_lifecycle_controller.go:476] "Starting node controller"
I0322 03:40:12.098231    5292 shared_informer.go:311] Waiting for caches to sync for taint
I0322 03:40:12.101134    5292 controllermanager.go:642] "Started controller" controller="clusterrole-aggregation-controller"
I0322 03:40:12.101728    5292 clusterroleaggregation_controller.go:189] "Starting ClusterRoleAggregator controller"
I0322 03:40:12.101747    5292 shared_informer.go:311] Waiting for caches to sync for ClusterRoleAggregator
I0322 03:40:12.102596    5292 garbagecollector.go:816] "failed to discover some groups" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"
I0322 03:40:12.107076    5292 controllermanager.go:642] "Started controller" controller="endpointslice-controller"
I0322 03:40:12.107397    5292 endpointslice_controller.go:264] "Starting endpoint slice controller"
I0322 03:40:12.107414    5292 shared_informer.go:311] Waiting for caches to sync for endpoint_slice
E0322 03:40:12.134429    5292 resource_quota_controller.go:169] initial discovery check failure, continuing and counting on future sync update: unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I0322 03:40:12.139776    5292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="deployments.apps"
I0322 03:40:12.139971    5292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="jobs.batch"
I0322 03:40:12.140273    5292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="networkpolicies.networking.k8s.io"
I0322 03:40:12.140435    5292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="helmchartconfigs.helm.cattle.io"
I0322 03:40:12.140604    5292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="tlsoptions.traefik.containo.us"
I0322 03:40:12.140736    5292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="limitranges"
I0322 03:40:12.140846    5292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingresses.networking.k8s.io"
I0322 03:40:12.140947    5292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="endpoints"
I0322 03:40:12.141442    5292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="daemonsets.apps"
I0322 03:40:12.141572    5292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="tlsstores.traefik.containo.us"
I0322 03:40:12.141695    5292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressroutes.traefik.containo.us"
I0322 03:40:12.142010    5292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="middlewaretcps.traefik.containo.us"
I0322 03:40:12.142108    5292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="replicasets.apps"
I0322 03:40:12.142214    5292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="endpointslices.discovery.k8s.io"
I0322 03:40:12.142373    5292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressroutetcps.traefik.containo.us"
I0322 03:40:12.142478    5292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="tlsstores.traefik.io"
I0322 03:40:12.142673    5292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="serviceaccounts"
I0322 03:40:12.142772    5292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="poddisruptionbudgets.policy"
I0322 03:40:12.143030    5292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="helmcharts.helm.cattle.io"
I0322 03:40:12.143160    5292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="addons.k3s.cattle.io"
I0322 03:40:12.143415    5292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressroutetcps.traefik.io"
I0322 03:40:12.143591    5292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="cronjobs.batch"
I0322 03:40:12.143718    5292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="roles.rbac.authorization.k8s.io"
I0322 03:40:12.144228    5292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="csistoragecapacities.storage.k8s.io"
I0322 03:40:12.144371    5292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="leases.coordination.k8s.io"
I0322 03:40:12.144459    5292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="traefikservices.traefik.containo.us"
I0322 03:40:12.144602    5292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressroutes.traefik.io"
I0322 03:40:12.144695    5292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="horizontalpodautoscalers.autoscaling"
I0322 03:40:12.144984    5292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressrouteudps.traefik.containo.us"
I0322 03:40:12.145262    5292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="statefulsets.apps"
I0322 03:40:12.145372    5292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="tlsoptions.traefik.io"
I0322 03:40:12.145500    5292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="middlewares.traefik.io"
I0322 03:40:12.145703    5292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="traefikservices.traefik.io"
I0322 03:40:12.145818    5292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressrouteudps.traefik.io"
I0322 03:40:12.146057    5292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="middlewaretcps.traefik.io"
I0322 03:40:12.146292    5292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="serverstransports.traefik.io"
I0322 03:40:12.146490    5292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="controllerrevisions.apps"
I0322 03:40:12.146626    5292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="rolebindings.rbac.authorization.k8s.io"
I0322 03:40:12.146816    5292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="middlewares.traefik.containo.us"
I0322 03:40:12.146946    5292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="podtemplates"
I0322 03:40:12.147184    5292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="serverstransports.traefik.containo.us"
I0322 03:40:12.147434    5292 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="serverstransporttcps.traefik.io"
I0322 03:40:12.147555    5292 controllermanager.go:642] "Started controller" controller="resourcequota-controller"
I0322 03:40:12.153357    5292 resource_quota_controller.go:294] "Starting resource quota controller"
I0322 03:40:12.153467    5292 shared_informer.go:311] Waiting for caches to sync for resource quota
I0322 03:40:12.154233    5292 resource_quota_monitor.go:305] "QuotaMonitor running"
E0322 03:40:12.159022    5292 resource_quota_controller.go:440] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I0322 03:40:12.160569    5292 controllermanager.go:642] "Started controller" controller="daemonset-controller"
I0322 03:40:12.160898    5292 daemon_controller.go:291] "Starting daemon sets controller"
I0322 03:40:12.160983    5292 shared_informer.go:311] Waiting for caches to sync for daemon sets
I0322 03:40:12.163272    5292 controllermanager.go:642] "Started controller" controller="deployment-controller"
I0322 03:40:12.163509    5292 deployment_controller.go:168] "Starting controller" controller="deployment"
I0322 03:40:12.163530    5292 shared_informer.go:311] Waiting for caches to sync for deployment
I0322 03:40:12.172209    5292 controllermanager.go:642] "Started controller" controller="job-controller"
I0322 03:40:12.172436    5292 job_controller.go:226] "Starting job controller"
I0322 03:40:12.172509    5292 shared_informer.go:311] Waiting for caches to sync for job
I0322 03:40:12.220877    5292 controllermanager.go:642] "Started controller" controller="replicaset-controller"
I0322 03:40:12.220962    5292 controllermanager.go:607] "Warning: controller is disabled" controller="service-lb-controller"
I0322 03:40:12.220970    5292 controllermanager.go:607] "Warning: controller is disabled" controller="node-route-controller"
I0322 03:40:12.221490    5292 replica_set.go:214] "Starting controller" name="replicaset"
I0322 03:40:12.221557    5292 shared_informer.go:311] Waiting for caches to sync for ReplicaSet
I0322 03:40:12.262011    5292 controllermanager.go:642] "Started controller" controller="endpointslice-mirroring-controller"
I0322 03:40:12.262221    5292 endpointslicemirroring_controller.go:223] "Starting EndpointSliceMirroring controller"
I0322 03:40:12.262266    5292 shared_informer.go:311] Waiting for caches to sync for endpoint_slice_mirroring
I0322 03:40:12.309910    5292 controllermanager.go:642] "Started controller" controller="serviceaccount-controller"
I0322 03:40:12.312315    5292 serviceaccounts_controller.go:111] "Starting service account controller"
I0322 03:40:12.312415    5292 shared_informer.go:311] Waiting for caches to sync for service account
I0322 03:40:12.491155    5292 controllermanager.go:642] "Started controller" controller="disruption-controller"
I0322 03:40:12.492410    5292 disruption.go:433] "Sending events to api server."
I0322 03:40:12.493209    5292 disruption.go:444] "Starting disruption controller"
I0322 03:40:12.493755    5292 shared_informer.go:311] Waiting for caches to sync for disruption
I0322 03:40:12.507440    5292 controllermanager.go:642] "Started controller" controller="ttl-controller"
I0322 03:40:12.507726    5292 ttl_controller.go:124] "Starting TTL controller"
I0322 03:40:12.508086    5292 shared_informer.go:311] Waiting for caches to sync for TTL
I0322 03:40:12.519226    5292 controllermanager.go:642] "Started controller" controller="persistentvolume-binder-controller"
I0322 03:40:12.519584    5292 pv_controller_base.go:319] "Starting persistent volume controller"
I0322 03:40:12.520155    5292 shared_informer.go:311] Waiting for caches to sync for persistent volume
I0322 03:40:12.557255    5292 controllermanager.go:642] "Started controller" controller="persistentvolumeclaim-protection-controller"
I0322 03:40:12.557598    5292 pvc_protection_controller.go:102] "Starting PVC protection controller"
I0322 03:40:12.557825    5292 shared_informer.go:311] Waiting for caches to sync for PVC protection
I0322 03:40:12.575744    5292 shared_informer.go:311] Waiting for caches to sync for resource quota
I0322 03:40:12.637463    5292 topologycache.go:237] "Can't get CPU or zone information for node" node="serverworker"
I0322 03:40:12.638571    5292 actual_state_of_world.go:547] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"server\" does not exist"
I0322 03:40:12.638604    5292 actual_state_of_world.go:547] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"serverworker\" does not exist"
I0322 03:40:12.652278    5292 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I0322 03:40:12.653458    5292 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I0322 03:40:12.653974    5292 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I0322 03:40:12.657772    5292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:40:12.657824    5292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:40:12.661582    5292 shared_informer.go:318] Caches are synced for PV protection
I0322 03:40:12.663077    5292 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0322 03:40:12.663432    5292 shared_informer.go:318] Caches are synced for cronjob
I0322 03:40:12.663689    5292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:40:12.663922    5292 topologycache.go:237] "Can't get CPU or zone information for node" node="serverworker"
I0322 03:40:12.663990    5292 shared_informer.go:318] Caches are synced for namespace
I0322 03:40:12.664717    5292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:40:12.664812    5292 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I0322 03:40:12.667529    5292 shared_informer.go:318] Caches are synced for endpoint
I0322 03:40:12.674554    5292 shared_informer.go:318] Caches are synced for job
I0322 03:40:12.678783    5292 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0322 03:40:12.691789    5292 shared_informer.go:318] Caches are synced for HPA
I0322 03:40:12.694099    5292 shared_informer.go:318] Caches are synced for disruption
I0322 03:40:12.694210    5292 shared_informer.go:318] Caches are synced for certificate-csrapproving
I0322 03:40:12.698468    5292 shared_informer.go:318] Caches are synced for taint
I0322 03:40:12.699037    5292 node_lifecycle_controller.go:1225] "Initializing eviction metric for zone" zone=""
I0322 03:40:12.699218    5292 node_lifecycle_controller.go:877] "Missing timestamp for Node. Assuming now as a timestamp" node="server"
I0322 03:40:12.699344    5292 node_lifecycle_controller.go:877] "Missing timestamp for Node. Assuming now as a timestamp" node="serverworker"
I0322 03:40:12.699611    5292 taint_manager.go:205] "Starting NoExecuteTaintManager"
I0322 03:40:12.699714    5292 taint_manager.go:210] "Sending events to api server"
I0322 03:40:12.701333    5292 node_lifecycle_controller.go:1071] "Controller detected that zone is now in new state" zone="" newState="Normal"
I0322 03:40:12.702630    5292 event.go:307] "Event occurred" object="server" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node server event: Registered Node server in Controller"
I0322 03:40:12.702724    5292 event.go:307] "Event occurred" object="serverworker" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node serverworker event: Registered Node serverworker in Controller"
I0322 03:40:12.702905    5292 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I0322 03:40:12.705941    5292 shared_informer.go:318] Caches are synced for node
I0322 03:40:12.706001    5292 range_allocator.go:174] "Sending events to api server"
I0322 03:40:12.706030    5292 range_allocator.go:178] "Starting range CIDR allocator"
I0322 03:40:12.706034    5292 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I0322 03:40:12.706040    5292 shared_informer.go:318] Caches are synced for cidrallocator
I0322 03:40:12.707572    5292 shared_informer.go:318] Caches are synced for endpoint_slice
I0322 03:40:12.710192    5292 shared_informer.go:318] Caches are synced for TTL
I0322 03:40:12.713334    5292 shared_informer.go:318] Caches are synced for service account
I0322 03:40:12.724907    5292 shared_informer.go:318] Caches are synced for TTL after finished
I0322 03:40:12.726862    5292 shared_informer.go:318] Caches are synced for ReplicaSet
I0322 03:40:12.727402    5292 shared_informer.go:318] Caches are synced for crt configmap
I0322 03:40:12.727935    5292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="30.366s"
I0322 03:40:12.732015    5292 shared_informer.go:318] Caches are synced for ReplicationController
I0322 03:40:12.733487    5292 shared_informer.go:318] Caches are synced for GC
I0322 03:40:12.756198    5292 controller.go:624] quota admission added evaluator for: endpoints
I0322 03:40:12.761296    5292 shared_informer.go:318] Caches are synced for daemon sets
I0322 03:40:12.764181    5292 shared_informer.go:318] Caches are synced for deployment
I0322 03:40:12.790754    5292 shared_informer.go:318] Caches are synced for stateful set
I0322 03:40:12.804941    5292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:40:12.820254    5292 shared_informer.go:318] Caches are synced for persistent volume
I0322 03:40:12.820506    5292 shared_informer.go:318] Caches are synced for attach detach
I0322 03:40:12.829022    5292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:40:12.836532    5292 event.go:307] "Event occurred" object="kube-system/helm-install-traefik" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="Completed" message="Job completed"
I0322 03:40:12.837562    5292 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:40:12.855248    5292 shared_informer.go:318] Caches are synced for resource quota
I0322 03:40:12.857347    5292 shared_informer.go:318] Caches are synced for expand
I0322 03:40:12.858075    5292 shared_informer.go:318] Caches are synced for PVC protection
I0322 03:40:12.863527    5292 shared_informer.go:318] Caches are synced for ephemeral
I0322 03:40:12.876471    5292 shared_informer.go:318] Caches are synced for resource quota
I0322 03:40:13.022352    5292 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0322 03:40:13.073896    5292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="346.95927ms"
I0322 03:40:13.074157    5292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="346.397689ms"
I0322 03:40:13.077537    5292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="55.459s"
I0322 03:40:13.077717    5292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="34.469s"
I0322 03:40:13.110678    5292 controller.go:624] quota admission added evaluator for: controllerrevisions.apps
I0322 03:40:13.124629    5292 event.go:307] "Event occurred" object="kube-system/svclb-traefik-ae8f6f60" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: svclb-traefik-ae8f6f60-mrdt2"
I0322 03:40:13.134928    5292 topology_manager.go:215] "Topology Admit Handler" podUID="81fa3a4a-137e-4852-b6c0-684a9e9ea9ee" podNamespace="kube-system" podName="svclb-traefik-ae8f6f60-mrdt2"
E0322 03:40:13.136081    5292 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="f5749ede-2796-414e-b0ea-d6e2523ec959" containerName="helm"
E0322 03:40:13.136190    5292 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="f5749ede-2796-414e-b0ea-d6e2523ec959" containerName="helm"
E0322 03:40:13.136239    5292 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="075f8ac2-232b-4d0a-b9ad-8fb5739de94d" containerName="helm"
I0322 03:40:13.136759    5292 memory_manager.go:346] "RemoveStaleState removing state" podUID="075f8ac2-232b-4d0a-b9ad-8fb5739de94d" containerName="helm"
I0322 03:40:13.136835    5292 memory_manager.go:346] "RemoveStaleState removing state" podUID="f5749ede-2796-414e-b0ea-d6e2523ec959" containerName="helm"
I0322 03:40:13.136891    5292 memory_manager.go:346] "RemoveStaleState removing state" podUID="f5749ede-2796-414e-b0ea-d6e2523ec959" containerName="helm"
I0322 03:40:13.136297    5292 event.go:307] "Event occurred" object="kube-system/svclb-traefik-ae8f6f60" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: svclb-traefik-ae8f6f60-8p2ds"
I0322 03:40:13.179116    5292 shared_informer.go:318] Caches are synced for garbage collector
I0322 03:40:13.182976    5292 shared_informer.go:318] Caches are synced for garbage collector
I0322 03:40:13.183000    5292 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I0322 03:40:13.188097    5292 controller.go:624] quota admission added evaluator for: replicasets.apps
I0322 03:40:13.220049    5292 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set traefik-f4564c4f4 to 1"
I0322 03:40:13.261881    5292 event.go:307] "Event occurred" object="kube-system/traefik-f4564c4f4" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: traefik-f4564c4f4-vcw4k"
I0322 03:40:13.282311    5292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="62.556442ms"
I0322 03:40:13.340165    5292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="57.794007ms"
I0322 03:40:13.342746    5292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="183.878s"
I0322 03:40:13.343336    5292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="267.25s"
I0322 03:40:18.736901    5292 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/svclb-traefik-ae8f6f60-mrdt2" containerName="lb-tcp-80"
I0322 03:40:18.892085    5292 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/svclb-traefik-ae8f6f60-mrdt2" containerName="lb-tcp-443"
I0322 03:40:19.944713    5292 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/svclb-traefik-ae8f6f60-mrdt2" podStartSLOduration=1.974271012 podCreationTimestamp="2024-03-22 03:40:13 +0000 UTC" firstStartedPulling="2024-03-22 03:40:13.764618423 +0000 UTC m=+16.939829602" lastFinishedPulling="2024-03-22 03:40:18.734977669 +0000 UTC m=+21.910188860" observedRunningTime="2024-03-22 03:40:19.943645122 +0000 UTC m=+23.118856345" watchObservedRunningTime="2024-03-22 03:40:19.94463027 +0000 UTC m=+23.119841456"
I0322 03:40:19.982198    5292 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Service" apiVersion="v1" type="Normal" reason="UpdatedLoadBalancer" message="Updated LoadBalancer with new IPs: [] -> [192.168.56.110]"
I0322 03:40:24.233697    5292 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Service" apiVersion="v1" type="Normal" reason="UpdatedLoadBalancer" message="Updated LoadBalancer with new IPs: [192.168.56.110] -> [192.168.56.110 192.168.56.111]"
I0322 03:40:29.135541    5292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="35.731508ms"
I0322 03:40:29.135642    5292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="65.914s"
I0322 03:40:29.191571    5292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="54.49s"
I0322 03:40:29.228171    5292 handler.go:275] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0322 03:40:30.255071    5292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="20.597987ms"
I0322 03:40:30.256174    5292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="73.45s"
time="2024-03-22T03:44:57Z" level=info msg="COMPACT compactRev=0 targetCompactRev=41 currentRev=1041"
time="2024-03-22T03:44:57Z" level=info msg="COMPACT deleted 1 rows from 41 revisions in 8.219311ms - compacted to 41/1041"
time="2024-03-22T03:49:57Z" level=info msg="COMPACT compactRev=41 targetCompactRev=160 currentRev=1160"
time="2024-03-22T03:49:57Z" level=info msg="COMPACT deleted 13 rows from 119 revisions in 16.707087ms - compacted to 160/1160"
I0322 03:51:34.182597    5292 event.go:307] "Event occurred" object="serverworker" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="NodeNotReady" message="Node serverworker status is now: NodeNotReady"
I0322 03:51:34.202691    5292 event.go:307] "Event occurred" object="kube-system/svclb-traefik-ae8f6f60-8p2ds" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0322 03:51:34.214575    5292 event.go:307] "Event occurred" object="kube-system/traefik-f4564c4f4-vcw4k" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0322 03:51:34.242682    5292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="24.052013ms"
I0322 03:51:34.243735    5292 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="45.868s"
I0322 03:51:34.252215    5292 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Endpoints" apiVersion="v1" type="Warning" reason="FailedToUpdateEndpoint" message="Failed to update endpoint kube-system/traefik: Operation cannot be fulfilled on endpoints \"traefik\": the object has been modified; please apply your changes to the latest version and try again"
I0322 03:51:34.252912    5292 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Service" apiVersion="v1" type="Normal" reason="UpdatedLoadBalancer" message="Updated LoadBalancer with new IPs: [192.168.56.110 192.168.56.111] -> [192.168.56.110]"
time="2024-03-22T03:51:50Z" level=info msg="error in remotedialer server [400]: read tcp 192.168.56.110:6443->192.168.56.111:46880: i/o timeout"
time="2024-03-22T03:53:44Z" level=info msg="Starting k3s v1.28.7+k3s1 (051b14b2)"
time="2024-03-22T03:53:44Z" level=info msg="Configuring sqlite3 database connection pooling: maxIdleConns=2, maxOpenConns=0, connMaxLifetime=0s"
time="2024-03-22T03:53:44Z" level=info msg="Configuring database table schema and indexes, this may take a moment..."
time="2024-03-22T03:53:44Z" level=info msg="Database tables and indexes are up to date"
time="2024-03-22T03:53:44Z" level=info msg="Kine available at unix://kine.sock"
time="2024-03-22T03:53:44Z" level=info msg="generated self-signed CA certificate CN=k3s-client-ca@1711079624: notBefore=2024-03-22 03:53:44.36487047 +0000 UTC notAfter=2034-03-20 03:53:44.36487047 +0000 UTC"
time="2024-03-22T03:53:44Z" level=info msg="certificate CN=system:admin,O=system:masters signed by CN=k3s-client-ca@1711079624: notBefore=2024-03-22 03:53:44 +0000 UTC notAfter=2025-03-22 03:53:44 +0000 UTC"
time="2024-03-22T03:53:44Z" level=info msg="certificate CN=system:k3s-supervisor,O=system:masters signed by CN=k3s-client-ca@1711079624: notBefore=2024-03-22 03:53:44 +0000 UTC notAfter=2025-03-22 03:53:44 +0000 UTC"
time="2024-03-22T03:53:44Z" level=info msg="certificate CN=system:kube-controller-manager signed by CN=k3s-client-ca@1711079624: notBefore=2024-03-22 03:53:44 +0000 UTC notAfter=2025-03-22 03:53:44 +0000 UTC"
time="2024-03-22T03:53:44Z" level=info msg="certificate CN=system:kube-scheduler signed by CN=k3s-client-ca@1711079624: notBefore=2024-03-22 03:53:44 +0000 UTC notAfter=2025-03-22 03:53:44 +0000 UTC"
time="2024-03-22T03:53:44Z" level=info msg="certificate CN=system:apiserver,O=system:masters signed by CN=k3s-client-ca@1711079624: notBefore=2024-03-22 03:53:44 +0000 UTC notAfter=2025-03-22 03:53:44 +0000 UTC"
time="2024-03-22T03:53:44Z" level=info msg="certificate CN=system:kube-proxy signed by CN=k3s-client-ca@1711079624: notBefore=2024-03-22 03:53:44 +0000 UTC notAfter=2025-03-22 03:53:44 +0000 UTC"
time="2024-03-22T03:53:44Z" level=info msg="certificate CN=system:k3s-controller signed by CN=k3s-client-ca@1711079624: notBefore=2024-03-22 03:53:44 +0000 UTC notAfter=2025-03-22 03:53:44 +0000 UTC"
time="2024-03-22T03:53:44Z" level=info msg="certificate CN=k3s-cloud-controller-manager signed by CN=k3s-client-ca@1711079624: notBefore=2024-03-22 03:53:44 +0000 UTC notAfter=2025-03-22 03:53:44 +0000 UTC"
time="2024-03-22T03:53:44Z" level=info msg="generated self-signed CA certificate CN=k3s-server-ca@1711079624: notBefore=2024-03-22 03:53:44.368665011 +0000 UTC notAfter=2034-03-20 03:53:44.368665011 +0000 UTC"
time="2024-03-22T03:53:44Z" level=info msg="certificate CN=kube-apiserver signed by CN=k3s-server-ca@1711079624: notBefore=2024-03-22 03:53:44 +0000 UTC notAfter=2025-03-22 03:53:44 +0000 UTC"
time="2024-03-22T03:53:44Z" level=info msg="generated self-signed CA certificate CN=k3s-request-header-ca@1711079624: notBefore=2024-03-22 03:53:44.369442134 +0000 UTC notAfter=2034-03-20 03:53:44.369442134 +0000 UTC"
time="2024-03-22T03:53:44Z" level=info msg="certificate CN=system:auth-proxy signed by CN=k3s-request-header-ca@1711079624: notBefore=2024-03-22 03:53:44 +0000 UTC notAfter=2025-03-22 03:53:44 +0000 UTC"
time="2024-03-22T03:53:44Z" level=info msg="generated self-signed CA certificate CN=etcd-server-ca@1711079624: notBefore=2024-03-22 03:53:44.370101045 +0000 UTC notAfter=2034-03-20 03:53:44.370101045 +0000 UTC"
time="2024-03-22T03:53:44Z" level=info msg="certificate CN=etcd-client signed by CN=etcd-server-ca@1711079624: notBefore=2024-03-22 03:53:44 +0000 UTC notAfter=2025-03-22 03:53:44 +0000 UTC"
time="2024-03-22T03:53:44Z" level=info msg="generated self-signed CA certificate CN=etcd-peer-ca@1711079624: notBefore=2024-03-22 03:53:44.370871106 +0000 UTC notAfter=2034-03-20 03:53:44.370871106 +0000 UTC"
time="2024-03-22T03:53:44Z" level=info msg="certificate CN=etcd-peer signed by CN=etcd-peer-ca@1711079624: notBefore=2024-03-22 03:53:44 +0000 UTC notAfter=2025-03-22 03:53:44 +0000 UTC"
time="2024-03-22T03:53:44Z" level=info msg="certificate CN=etcd-server signed by CN=etcd-server-ca@1711079624: notBefore=2024-03-22 03:53:44 +0000 UTC notAfter=2025-03-22 03:53:44 +0000 UTC"
time="2024-03-22T03:53:44Z" level=info msg="Saving cluster bootstrap data to datastore"
time="2024-03-22T03:53:44Z" level=info msg="certificate CN=k3s,O=k3s signed by CN=k3s-server-ca@1711079624: notBefore=2024-03-22 03:53:44 +0000 UTC notAfter=2025-03-22 03:53:44 +0000 UTC"
time="2024-03-22T03:53:44Z" level=warning msg="dynamiclistener [::]:6443: no cached certificate available for preload - deferring certificate load until storage initialization or first client request"
time="2024-03-22T03:53:44Z" level=info msg="Active TLS secret / (ver=) (count 10): map[listener.cattle.io/cn-10.43.0.1:10.43.0.1 listener.cattle.io/cn-127.0.0.1:127.0.0.1 listener.cattle.io/cn-192.168.56.110:192.168.56.110 listener.cattle.io/cn-__1-f16284:::1 listener.cattle.io/cn-kubernetes:kubernetes listener.cattle.io/cn-kubernetes.default:kubernetes.default listener.cattle.io/cn-kubernetes.default.svc:kubernetes.default.svc listener.cattle.io/cn-kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local listener.cattle.io/cn-localhost:localhost listener.cattle.io/cn-server:server listener.cattle.io/fingerprint:SHA1=57817D08CEF13EAC5FA6EA63398833E2B858E1D0]"
time="2024-03-22T03:53:44Z" level=info msg="Running kube-apiserver --advertise-address=192.168.56.110 --advertise-port=6443 --allow-privileged=true --anonymous-auth=false --api-audiences=https://kubernetes.default.svc.cluster.local,k3s --authorization-mode=Node,RBAC --bind-address=127.0.0.1 --cert-dir=/var/lib/rancher/k3s/server/tls/temporary-certs --client-ca-file=/var/lib/rancher/k3s/server/tls/client-ca.crt --egress-selector-config-file=/var/lib/rancher/k3s/server/etc/egress-selector-config.yaml --enable-admission-plugins=NodeRestriction --enable-aggregator-routing=true --enable-bootstrap-token-auth=true --etcd-servers=unix://kine.sock --kubelet-certificate-authority=/var/lib/rancher/k3s/server/tls/server-ca.crt --kubelet-client-certificate=/var/lib/rancher/k3s/server/tls/client-kube-apiserver.crt --kubelet-client-key=/var/lib/rancher/k3s/server/tls/client-kube-apiserver.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --profiling=false --proxy-client-cert-file=/var/lib/rancher/k3s/server/tls/client-auth-proxy.crt --proxy-client-key-file=/var/lib/rancher/k3s/server/tls/client-auth-proxy.key --requestheader-allowed-names=system:auth-proxy --requestheader-client-ca-file=/var/lib/rancher/k3s/server/tls/request-header-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6444 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/var/lib/rancher/k3s/server/tls/service.key --service-account-signing-key-file=/var/lib/rancher/k3s/server/tls/service.current.key --service-cluster-ip-range=10.43.0.0/16 --service-node-port-range=30000-32767 --storage-backend=etcd3 --tls-cert-file=/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.crt --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 --tls-private-key-file=/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.key"
time="2024-03-22T03:53:44Z" level=info msg="Running kube-scheduler --authentication-kubeconfig=/var/lib/rancher/k3s/server/cred/scheduler.kubeconfig --authorization-kubeconfig=/var/lib/rancher/k3s/server/cred/scheduler.kubeconfig --bind-address=127.0.0.1 --kubeconfig=/var/lib/rancher/k3s/server/cred/scheduler.kubeconfig --leader-elect=false --profiling=false --secure-port=10259"
time="2024-03-22T03:53:44Z" level=info msg="Running kube-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/var/lib/rancher/k3s/server/cred/controller.kubeconfig --authorization-kubeconfig=/var/lib/rancher/k3s/server/cred/controller.kubeconfig --bind-address=127.0.0.1 --cluster-cidr=10.42.0.0/16 --cluster-signing-kube-apiserver-client-cert-file=/var/lib/rancher/k3s/server/tls/client-ca.nochain.crt --cluster-signing-kube-apiserver-client-key-file=/var/lib/rancher/k3s/server/tls/client-ca.key --cluster-signing-kubelet-client-cert-file=/var/lib/rancher/k3s/server/tls/client-ca.nochain.crt --cluster-signing-kubelet-client-key-file=/var/lib/rancher/k3s/server/tls/client-ca.key --cluster-signing-kubelet-serving-cert-file=/var/lib/rancher/k3s/server/tls/server-ca.nochain.crt --cluster-signing-kubelet-serving-key-file=/var/lib/rancher/k3s/server/tls/server-ca.key --cluster-signing-legacy-unknown-cert-file=/var/lib/rancher/k3s/server/tls/server-ca.nochain.crt --cluster-signing-legacy-unknown-key-file=/var/lib/rancher/k3s/server/tls/server-ca.key --configure-cloud-routes=false --controllers=*,tokencleaner,-service,-route,-cloud-node-lifecycle --kubeconfig=/var/lib/rancher/k3s/server/cred/controller.kubeconfig --leader-elect=false --profiling=false --root-ca-file=/var/lib/rancher/k3s/server/tls/server-ca.crt --secure-port=10257 --service-account-private-key-file=/var/lib/rancher/k3s/server/tls/service.current.key --service-cluster-ip-range=10.43.0.0/16 --use-service-account-credentials=true"
I0322 03:53:44.684917    2297 options.go:220] external host was not specified, using 192.168.56.110
time="2024-03-22T03:53:44Z" level=info msg="Running cloud-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/var/lib/rancher/k3s/server/cred/cloud-controller.kubeconfig --authorization-kubeconfig=/var/lib/rancher/k3s/server/cred/cloud-controller.kubeconfig --bind-address=127.0.0.1 --cloud-config=/var/lib/rancher/k3s/server/etc/cloud-config.yaml --cloud-provider=k3s --cluster-cidr=10.42.0.0/16 --configure-cloud-routes=false --controllers=*,-route --feature-gates=CloudDualStackNodeIPs=true --kubeconfig=/var/lib/rancher/k3s/server/cred/cloud-controller.kubeconfig --leader-elect=false --leader-elect-resource-name=k3s-cloud-controller-manager --node-status-update-frequency=1m0s --profiling=false"
I0322 03:53:44.685477    2297 server.go:156] Version: v1.28.7+k3s1
I0322 03:53:44.685491    2297 server.go:158] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
time="2024-03-22T03:53:44Z" level=info msg="Server node token is available at /var/lib/rancher/k3s/server/token"
time="2024-03-22T03:53:44Z" level=info msg="To join server node to cluster: k3s server -s https://10.0.2.15:6443 -t ${SERVER_NODE_TOKEN}"
time="2024-03-22T03:53:44Z" level=info msg="Agent node token is available at /var/lib/rancher/k3s/server/agent-token"
time="2024-03-22T03:53:44Z" level=info msg="To join agent node to cluster: k3s agent -s https://10.0.2.15:6443 -t ${AGENT_NODE_TOKEN}"
time="2024-03-22T03:53:44Z" level=info msg="Wrote kubeconfig /etc/rancher/k3s/k3s.yaml"
time="2024-03-22T03:53:44Z" level=info msg="Run: k3s kubectl"
time="2024-03-22T03:53:44Z" level=info msg="Waiting for API server to become available"
time="2024-03-22T03:53:45Z" level=info msg="Password verified locally for node server"
time="2024-03-22T03:53:45Z" level=info msg="certificate CN=server signed by CN=k3s-server-ca@1711079624: notBefore=2024-03-22 03:53:44 +0000 UTC notAfter=2025-03-22 03:53:45 +0000 UTC"
time="2024-03-22T03:53:45Z" level=info msg="certificate CN=system:node:server,O=system:nodes signed by CN=k3s-client-ca@1711079624: notBefore=2024-03-22 03:53:44 +0000 UTC notAfter=2025-03-22 03:53:45 +0000 UTC"
I0322 03:53:45.518680    2297 shared_informer.go:311] Waiting for caches to sync for node_authorizer
I0322 03:53:45.520611    2297 plugins.go:158] Loaded 12 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,TaintNodesByCondition,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,MutatingAdmissionWebhook.
I0322 03:53:45.520735    2297 plugins.go:161] Loaded 13 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,ClusterTrustBundleAttest,CertificateSubjectRestriction,ValidatingAdmissionPolicy,ValidatingAdmissionWebhook,ResourceQuota.
I0322 03:53:45.521238    2297 instance.go:298] Using reconciler: lease
I0322 03:53:45.527804    2297 handler.go:275] Adding GroupVersion apiextensions.k8s.io v1 to ResourceManager
W0322 03:53:45.527907    2297 genericapiserver.go:744] Skipping API apiextensions.k8s.io/v1beta1 because it has no resources.
I0322 03:53:45.799379    2297 handler.go:275] Adding GroupVersion  v1 to ResourceManager
I0322 03:53:45.799603    2297 instance.go:709] API group "internal.apiserver.k8s.io" is not enabled, skipping.
time="2024-03-22T03:53:45Z" level=info msg="Module overlay was already loaded"
time="2024-03-22T03:53:45Z" level=info msg="Module br_netfilter was already loaded"
I0322 03:53:45.937558    2297 instance.go:709] API group "resource.k8s.io" is not enabled, skipping.
I0322 03:53:45.947790    2297 handler.go:275] Adding GroupVersion authentication.k8s.io v1 to ResourceManager
W0322 03:53:45.949478    2297 genericapiserver.go:744] Skipping API authentication.k8s.io/v1beta1 because it has no resources.
W0322 03:53:45.955251    2297 genericapiserver.go:744] Skipping API authentication.k8s.io/v1alpha1 because it has no resources.
I0322 03:53:45.955911    2297 handler.go:275] Adding GroupVersion authorization.k8s.io v1 to ResourceManager
W0322 03:53:45.955973    2297 genericapiserver.go:744] Skipping API authorization.k8s.io/v1beta1 because it has no resources.
I0322 03:53:45.956873    2297 handler.go:275] Adding GroupVersion autoscaling v2 to ResourceManager
time="2024-03-22T03:53:45Z" level=info msg="Set sysctl 'net/ipv4/conf/default/forwarding' to 1"
time="2024-03-22T03:53:45Z" level=info msg="Set sysctl 'net/netfilter/nf_conntrack_max' to 131072"
time="2024-03-22T03:53:45Z" level=info msg="Set sysctl 'net/netfilter/nf_conntrack_tcp_timeout_established' to 86400"
time="2024-03-22T03:53:45Z" level=info msg="Set sysctl 'net/netfilter/nf_conntrack_tcp_timeout_close_wait' to 3600"
time="2024-03-22T03:53:45Z" level=info msg="Set sysctl 'net/ipv4/conf/all/forwarding' to 1"
I0322 03:53:45.957705    2297 handler.go:275] Adding GroupVersion autoscaling v1 to ResourceManager
W0322 03:53:45.957757    2297 genericapiserver.go:744] Skipping API autoscaling/v2beta1 because it has no resources.
W0322 03:53:45.957763    2297 genericapiserver.go:744] Skipping API autoscaling/v2beta2 because it has no resources.
I0322 03:53:45.958872    2297 handler.go:275] Adding GroupVersion batch v1 to ResourceManager
W0322 03:53:45.958942    2297 genericapiserver.go:744] Skipping API batch/v1beta1 because it has no resources.
time="2024-03-22T03:53:45Z" level=info msg="Logging containerd to /var/lib/rancher/k3s/agent/containerd/containerd.log"
I0322 03:53:45.959821    2297 handler.go:275] Adding GroupVersion certificates.k8s.io v1 to ResourceManager
W0322 03:53:45.959960    2297 genericapiserver.go:744] Skipping API certificates.k8s.io/v1beta1 because it has no resources.
W0322 03:53:45.960093    2297 genericapiserver.go:744] Skipping API certificates.k8s.io/v1alpha1 because it has no resources.
I0322 03:53:45.960560    2297 handler.go:275] Adding GroupVersion coordination.k8s.io v1 to ResourceManager
W0322 03:53:45.960616    2297 genericapiserver.go:744] Skipping API coordination.k8s.io/v1beta1 because it has no resources.
W0322 03:53:45.960649    2297 genericapiserver.go:744] Skipping API discovery.k8s.io/v1beta1 because it has no resources.
time="2024-03-22T03:53:45Z" level=info msg="Running containerd -c /var/lib/rancher/k3s/agent/etc/containerd/config.toml -a /run/k3s/containerd/containerd.sock --state /run/k3s/containerd --root /var/lib/rancher/k3s/agent/containerd"
I0322 03:53:45.961127    2297 handler.go:275] Adding GroupVersion discovery.k8s.io v1 to ResourceManager
I0322 03:53:45.965083    2297 handler.go:275] Adding GroupVersion networking.k8s.io v1 to ResourceManager
W0322 03:53:45.965148    2297 genericapiserver.go:744] Skipping API networking.k8s.io/v1beta1 because it has no resources.
W0322 03:53:45.965154    2297 genericapiserver.go:744] Skipping API networking.k8s.io/v1alpha1 because it has no resources.
I0322 03:53:45.965652    2297 handler.go:275] Adding GroupVersion node.k8s.io v1 to ResourceManager
W0322 03:53:45.965731    2297 genericapiserver.go:744] Skipping API node.k8s.io/v1beta1 because it has no resources.
W0322 03:53:45.965765    2297 genericapiserver.go:744] Skipping API node.k8s.io/v1alpha1 because it has no resources.
I0322 03:53:45.966393    2297 handler.go:275] Adding GroupVersion policy v1 to ResourceManager
W0322 03:53:45.966521    2297 genericapiserver.go:744] Skipping API policy/v1beta1 because it has no resources.
I0322 03:53:45.967758    2297 handler.go:275] Adding GroupVersion rbac.authorization.k8s.io v1 to ResourceManager
W0322 03:53:45.967849    2297 genericapiserver.go:744] Skipping API rbac.authorization.k8s.io/v1beta1 because it has no resources.
W0322 03:53:45.967915    2297 genericapiserver.go:744] Skipping API rbac.authorization.k8s.io/v1alpha1 because it has no resources.
I0322 03:53:45.968344    2297 handler.go:275] Adding GroupVersion scheduling.k8s.io v1 to ResourceManager
W0322 03:53:45.968422    2297 genericapiserver.go:744] Skipping API scheduling.k8s.io/v1beta1 because it has no resources.
W0322 03:53:45.968489    2297 genericapiserver.go:744] Skipping API scheduling.k8s.io/v1alpha1 because it has no resources.
I0322 03:53:45.969865    2297 handler.go:275] Adding GroupVersion storage.k8s.io v1 to ResourceManager
W0322 03:53:45.970020    2297 genericapiserver.go:744] Skipping API storage.k8s.io/v1beta1 because it has no resources.
W0322 03:53:45.970089    2297 genericapiserver.go:744] Skipping API storage.k8s.io/v1alpha1 because it has no resources.
I0322 03:53:45.970921    2297 handler.go:275] Adding GroupVersion flowcontrol.apiserver.k8s.io v1beta3 to ResourceManager
I0322 03:53:45.973869    2297 handler.go:275] Adding GroupVersion flowcontrol.apiserver.k8s.io v1beta2 to ResourceManager
W0322 03:53:45.973893    2297 genericapiserver.go:744] Skipping API flowcontrol.apiserver.k8s.io/v1beta1 because it has no resources.
W0322 03:53:45.973898    2297 genericapiserver.go:744] Skipping API flowcontrol.apiserver.k8s.io/v1alpha1 because it has no resources.
I0322 03:53:45.977077    2297 handler.go:275] Adding GroupVersion apps v1 to ResourceManager
W0322 03:53:45.977832    2297 genericapiserver.go:744] Skipping API apps/v1beta2 because it has no resources.
W0322 03:53:45.977846    2297 genericapiserver.go:744] Skipping API apps/v1beta1 because it has no resources.
I0322 03:53:45.980609    2297 handler.go:275] Adding GroupVersion admissionregistration.k8s.io v1 to ResourceManager
W0322 03:53:45.980638    2297 genericapiserver.go:744] Skipping API admissionregistration.k8s.io/v1beta1 because it has no resources.
W0322 03:53:45.980642    2297 genericapiserver.go:744] Skipping API admissionregistration.k8s.io/v1alpha1 because it has no resources.
I0322 03:53:45.983495    2297 handler.go:275] Adding GroupVersion events.k8s.io v1 to ResourceManager
W0322 03:53:45.983599    2297 genericapiserver.go:744] Skipping API events.k8s.io/v1beta1 because it has no resources.
I0322 03:53:45.985981    2297 handler.go:275] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
W0322 03:53:45.986078    2297 genericapiserver.go:744] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I0322 03:53:46.409509    2297 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/rancher/k3s/server/tls/request-header-ca.crt"
I0322 03:53:46.409641    2297 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/rancher/k3s/server/tls/client-ca.crt"
I0322 03:53:46.410051    2297 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.crt::/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.key"
I0322 03:53:46.410474    2297 secure_serving.go:213] Serving securely on 127.0.0.1:6444
I0322 03:53:46.410634    2297 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0322 03:53:46.411590    2297 customresource_discovery_controller.go:289] Starting DiscoveryController
I0322 03:53:46.411628    2297 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0322 03:53:46.411634    2297 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0322 03:53:46.411648    2297 controller.go:80] Starting OpenAPI V3 AggregationController
I0322 03:53:46.412224    2297 system_namespaces_controller.go:67] Starting system namespaces controller
I0322 03:53:46.412248    2297 available_controller.go:423] Starting AvailableConditionController
I0322 03:53:46.412253    2297 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0322 03:53:46.412270    2297 handler_discovery.go:412] Starting ResourceDiscoveryManager
I0322 03:53:46.412692    2297 gc_controller.go:78] Starting apiserver lease garbage collector
I0322 03:53:46.412710    2297 apf_controller.go:374] Starting API Priority and Fairness config controller
I0322 03:53:46.413090    2297 gc_controller.go:78] Starting apiserver lease garbage collector
I0322 03:53:46.413234    2297 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0322 03:53:46.413240    2297 shared_informer.go:311] Waiting for caches to sync for cluster_authentication_trust_controller
I0322 03:53:46.413895    2297 aggregator.go:164] waiting for initial CRD sync...
I0322 03:53:46.413907    2297 controller.go:78] Starting OpenAPI AggregationController
I0322 03:53:46.414308    2297 controller.go:116] Starting legacy_token_tracking_controller
I0322 03:53:46.414315    2297 shared_informer.go:311] Waiting for caches to sync for configmaps
I0322 03:53:46.414444    2297 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/rancher/k3s/server/tls/client-auth-proxy.crt::/var/lib/rancher/k3s/server/tls/client-auth-proxy.key"
I0322 03:53:46.415117    2297 controller.go:134] Starting OpenAPI controller
I0322 03:53:46.415192    2297 controller.go:85] Starting OpenAPI V3 controller
I0322 03:53:46.415207    2297 naming_controller.go:291] Starting NamingConditionController
I0322 03:53:46.415228    2297 establishing_controller.go:76] Starting EstablishingController
I0322 03:53:46.415271    2297 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0322 03:53:46.415337    2297 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0322 03:53:46.415415    2297 crd_finalizer.go:266] Starting CRDFinalizer
I0322 03:53:46.429581    2297 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/rancher/k3s/server/tls/client-ca.crt"
I0322 03:53:46.429863    2297 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/rancher/k3s/server/tls/request-header-ca.crt"
I0322 03:53:46.431796    2297 crdregistration_controller.go:111] Starting crd-autoregister controller
I0322 03:53:46.431965    2297 shared_informer.go:311] Waiting for caches to sync for crd-autoregister
I0322 03:53:46.512385    2297 cache.go:39] Caches are synced for AvailableConditionController controller
I0322 03:53:46.512981    2297 apf_controller.go:379] Running API Priority and Fairness config worker
I0322 03:53:46.513088    2297 apf_controller.go:382] Running API Priority and Fairness periodic rebalancing process
I0322 03:53:46.513257    2297 shared_informer.go:318] Caches are synced for cluster_authentication_trust_controller
I0322 03:53:46.514376    2297 shared_informer.go:318] Caches are synced for configmaps
I0322 03:53:46.514957    2297 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0322 03:53:46.521019    2297 shared_informer.go:318] Caches are synced for node_authorizer
I0322 03:53:46.531881    2297 controller.go:624] quota admission added evaluator for: namespaces
I0322 03:53:46.532070    2297 shared_informer.go:318] Caches are synced for crd-autoregister
I0322 03:53:46.532375    2297 aggregator.go:166] initial CRD sync complete...
I0322 03:53:46.532476    2297 autoregister_controller.go:141] Starting autoregister controller
I0322 03:53:46.532560    2297 cache.go:32] Waiting for caches to sync for autoregister controller
I0322 03:53:46.532652    2297 cache.go:39] Caches are synced for autoregister controller
E0322 03:53:46.588619    2297 controller.go:95] Unable to perform initial Kubernetes service initialization: namespaces "default" not found
I0322 03:53:46.589896    2297 controller.go:624] quota admission added evaluator for: leases.coordination.k8s.io
time="2024-03-22T03:53:46Z" level=info msg="containerd is now running"
time="2024-03-22T03:53:46Z" level=info msg="Running kubelet --address=0.0.0.0 --allowed-unsafe-sysctls=net.ipv4.ip_forward,net.ipv6.conf.all.forwarding --anonymous-auth=false --authentication-token-webhook=true --authorization-mode=Webhook --cgroup-driver=systemd --client-ca-file=/var/lib/rancher/k3s/agent/client-ca.crt --cloud-provider=external --cluster-dns=10.43.0.10 --cluster-domain=cluster.local --container-runtime-endpoint=unix:///run/k3s/containerd/containerd.sock --containerd=/run/k3s/containerd/containerd.sock --eviction-hard=imagefs.available<5%,nodefs.available<5% --eviction-minimum-reclaim=imagefs.available=10%,nodefs.available=10% --fail-swap-on=false --feature-gates=CloudDualStackNodeIPs=true --healthz-bind-address=127.0.0.1 --hostname-override=server --kubeconfig=/var/lib/rancher/k3s/agent/kubelet.kubeconfig --node-ip=192.168.56.110 --node-labels= --pod-infra-container-image=rancher/mirrored-pause:3.6 --pod-manifest-path=/var/lib/rancher/k3s/agent/pod-manifests --read-only-port=0 --resolv-conf=/run/systemd/resolve/resolv.conf --serialize-image-pulls=false --tls-cert-file=/var/lib/rancher/k3s/agent/serving-kubelet.crt --tls-private-key-file=/var/lib/rancher/k3s/agent/serving-kubelet.key"
time="2024-03-22T03:53:46Z" level=info msg="Connecting to proxy" url="wss://127.0.0.1:6443/v1-k3s/connect"
time="2024-03-22T03:53:46Z" level=info msg="Handling backend connection request [server]"
time="2024-03-22T03:53:47Z" level=info msg="Waiting to retrieve kube-proxy configuration; server is not ready: https://127.0.0.1:6443/v1-k3s/readyz: 500 Internal Server Error"
I0322 03:53:47.429314    2297 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0322 03:53:47.431607    2297 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0322 03:53:47.431615    2297 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0322 03:53:47.764463    2297 controller.go:624] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0322 03:53:47.789787    2297 controller.go:624] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0322 03:53:47.902536    2297 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.43.0.1"}
W0322 03:53:47.906849    2297 lease.go:263] Resetting endpoints for master service "kubernetes" to [192.168.56.110]
I0322 03:53:47.907587    2297 controller.go:624] quota admission added evaluator for: endpoints
I0322 03:53:47.911920    2297 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
Flag --cloud-provider has been deprecated, will be removed in 1.25 or later, in favor of removing cloud provider code from Kubelet.
Flag --containerd has been deprecated, This is a cadvisor flag that was mistakenly registered with the Kubelet. Due to legacy concerns, it will follow the standard CLI deprecation timeline before being removed.
Flag --pod-infra-container-image has been deprecated, will be removed in a future release. Image garbage collector will get sandbox image information from CRI.
I0322 03:53:47.995322    2297 server.go:202] "--pod-infra-container-image will not be pruned by the image garbage collector in kubelet and should also be set in the remote runtime"
I0322 03:53:47.997281    2297 server.go:462] "Kubelet version" kubeletVersion="v1.28.7+k3s1"
I0322 03:53:47.997301    2297 server.go:464] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0322 03:53:47.999541    2297 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/rancher/k3s/agent/client-ca.crt"
I0322 03:53:48.004852    2297 server.go:720] "--cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /"
I0322 03:53:48.005793    2297 container_manager_linux.go:265] "Container manager verified user specified cgroup-root exists" cgroupRoot=[]
I0322 03:53:48.006064    2297 container_manager_linux.go:270] "Creating Container Manager object based on Node Config" nodeConfig={"RuntimeCgroupsName":"","SystemCgroupsName":"","KubeletCgroupsName":"","KubeletOOMScoreAdj":-999,"ContainerRuntime":"","CgroupsPerQOS":true,"CgroupRoot":"/","CgroupDriver":"systemd","KubeletRootDir":"/var/lib/kubelet","ProtectKernelDefaults":false,"KubeReservedCgroupName":"","SystemReservedCgroupName":"","ReservedSystemCPUs":{},"EnforceNodeAllocatable":{"pods":{}},"KubeReserved":null,"SystemReserved":null,"HardEvictionThresholds":[{"Signal":"imagefs.available","Operator":"LessThan","Value":{"Quantity":null,"Percentage":0.05},"GracePeriod":0,"MinReclaim":null},{"Signal":"nodefs.available","Operator":"LessThan","Value":{"Quantity":null,"Percentage":0.05},"GracePeriod":0,"MinReclaim":null}],"QOSReserved":{},"CPUManagerPolicy":"none","CPUManagerPolicyOptions":null,"TopologyManagerScope":"container","CPUManagerReconcilePeriod":10000000000,"ExperimentalMemoryManagerPolicy":"None","ExperimentalMemoryManagerReservedMemory":null,"PodPidsLimit":-1,"EnforceCPULimits":true,"CPUCFSQuotaPeriod":100000000,"TopologyManagerPolicy":"none","TopologyManagerPolicyOptions":null}
I0322 03:53:48.006107    2297 topology_manager.go:138] "Creating topology manager with none policy"
I0322 03:53:48.006116    2297 container_manager_linux.go:301] "Creating device plugin manager"
I0322 03:53:48.006564    2297 state_mem.go:36] "Initialized new in-memory state store"
I0322 03:53:48.006759    2297 kubelet.go:393] "Attempting to sync node with API server"
I0322 03:53:48.006780    2297 kubelet.go:298] "Adding static pod path" path="/var/lib/rancher/k3s/agent/pod-manifests"
I0322 03:53:48.006804    2297 kubelet.go:309] "Adding apiserver pod source"
I0322 03:53:48.006819    2297 apiserver.go:42] "Waiting for node sync before watching apiserver pods"
I0322 03:53:48.008373    2297 kuberuntime_manager.go:257] "Container runtime initialized" containerRuntime="containerd" version="v1.7.11-k3s2" apiVersion="v1"
W0322 03:53:48.009834    2297 probe.go:268] Flexvolume plugin directory at /usr/libexec/kubernetes/kubelet-plugins/volume/exec/ does not exist. Recreating.
I0322 03:53:48.011184    2297 server.go:1227] "Started kubelet"
I0322 03:53:48.013966    2297 fs_resource_analyzer.go:67] "Starting FS ResourceAnalyzer"
E0322 03:53:48.015806    2297 cri_stats_provider.go:448] "Failed to get the info of the filesystem with mountpoint" err="unable to find data in memory cache" mountpoint="/var/lib/rancher/k3s/agent/containerd/io.containerd.snapshotter.v1.overlayfs"
E0322 03:53:48.015963    2297 kubelet.go:1431] "Image garbage collection failed once. Stats initialization may not have completed yet" err="invalid capacity 0 on image filesystem"
I0322 03:53:48.023847    2297 server.go:162] "Starting to listen" address="0.0.0.0" port=10250
I0322 03:53:48.025327    2297 server.go:462] "Adding debug handlers to kubelet server"
I0322 03:53:48.026217    2297 ratelimit.go:65] "Setting rate limiting for podresources endpoint" qps=100 burstTokens=10
I0322 03:53:48.029180    2297 server.go:233] "Starting to serve the podresources API" endpoint="unix:/var/lib/kubelet/pod-resources/kubelet.sock"
I0322 03:53:48.026385    2297 volume_manager.go:291] "Starting Kubelet Volume Manager"
I0322 03:53:48.026397    2297 desired_state_of_world_populator.go:151] "Desired state populator starts to run"
I0322 03:53:48.029892    2297 reconciler_new.go:29] "Reconciler: start to sync state"
E0322 03:53:48.033505    2297 nodelease.go:49] "Failed to get node when trying to set owner ref to the node lease" err="nodes \"server\" not found" node="server"
I0322 03:53:48.089922    2297 kubelet_network_linux.go:50] "Initialized iptables rules." protocol="IPv4"
I0322 03:53:48.106981    2297 cpu_manager.go:214] "Starting CPU manager" policy="none"
I0322 03:53:48.107146    2297 cpu_manager.go:215] "Reconciling" reconcilePeriod="10s"
I0322 03:53:48.107238    2297 state_mem.go:36] "Initialized new in-memory state store"
I0322 03:53:48.108243    2297 policy_none.go:49] "None policy: Start"
I0322 03:53:48.108798    2297 memory_manager.go:169] "Starting memorymanager" policy="None"
I0322 03:53:48.108952    2297 state_mem.go:35] "Initializing new in-memory state store"
I0322 03:53:48.111601    2297 kubelet_network_linux.go:50] "Initialized iptables rules." protocol="IPv6"
I0322 03:53:48.112187    2297 status_manager.go:217] "Starting to sync pod status with apiserver"
I0322 03:53:48.112247    2297 kubelet.go:2303] "Starting kubelet main sync loop"
E0322 03:53:48.112301    2297 kubelet.go:2327] "Skipping pod synchronization" err="[container runtime status check may not have completed yet, PLEG is not healthy: pleg has yet to be successful]"
I0322 03:53:48.129279    2297 kubelet_node_status.go:70] "Attempting to register node" node="server"
I0322 03:53:48.142846    2297 kubelet_node_status.go:73] "Successfully registered node" node="server"
time="2024-03-22T03:53:48Z" level=info msg="Annotations and labels have been set successfully on node: server"
time="2024-03-22T03:53:48Z" level=info msg="Starting flannel with backend vxlan"
I0322 03:53:48.159747    2297 manager.go:471] "Failed to read data from checkpoint" checkpoint="kubelet_internal_checkpoint" err="checkpoint is not found"
I0322 03:53:48.160351    2297 plugin_manager.go:118] "Starting Kubelet Plugin Manager"
I0322 03:53:48.402310    2297 kubelet_node_status.go:493] "Fast updating node status as it just became ready"
time="2024-03-22T03:53:48Z" level=info msg="Kube API server is now running"
time="2024-03-22T03:53:48Z" level=info msg="ETCD server is now running"
time="2024-03-22T03:53:48Z" level=info msg="k3s is up and running"
time="2024-03-22T03:53:48Z" level=info msg="Creating k3s-supervisor event broadcaster"
time="2024-03-22T03:53:48Z" level=info msg="Applying CRD addons.k3s.cattle.io"
time="2024-03-22T03:53:48Z" level=info msg="Waiting for cloud-controller-manager privileges to become available"
time="2024-03-22T03:53:48Z" level=info msg="Applying CRD etcdsnapshotfiles.k3s.cattle.io"
time="2024-03-22T03:53:48Z" level=info msg="Applying CRD helmcharts.helm.cattle.io"
time="2024-03-22T03:53:48Z" level=info msg="Applying CRD helmchartconfigs.helm.cattle.io"
I0322 03:53:48.610623    2297 handler.go:275] Adding GroupVersion k3s.cattle.io v1 to ResourceManager
I0322 03:53:48.664126    2297 handler.go:275] Adding GroupVersion k3s.cattle.io v1 to ResourceManager
time="2024-03-22T03:53:48Z" level=info msg="Waiting for CRD helmchartconfigs.helm.cattle.io to become available"
I0322 03:53:48.688382    2297 handler.go:275] Adding GroupVersion helm.cattle.io v1 to ResourceManager
I0322 03:53:48.709994    2297 handler.go:275] Adding GroupVersion helm.cattle.io v1 to ResourceManager
I0322 03:53:48.933873    2297 serving.go:355] Generated self-signed cert in-memory
I0322 03:53:49.015752    2297 apiserver.go:52] "Watching apiserver"
I0322 03:53:49.031608    2297 desired_state_of_world_populator.go:159] "Finished populating initial desired state of world"
time="2024-03-22T03:53:49Z" level=info msg="Done waiting for CRD helmchartconfigs.helm.cattle.io to become available"
time="2024-03-22T03:53:49Z" level=info msg="Writing static file: /var/lib/rancher/k3s/server/static/charts/traefik-25.0.2+up25.0.0.tgz"
time="2024-03-22T03:53:49Z" level=info msg="Writing static file: /var/lib/rancher/k3s/server/static/charts/traefik-crd-25.0.2+up25.0.0.tgz"
time="2024-03-22T03:53:49Z" level=info msg="Failed to get existing traefik HelmChart" error="helmcharts.helm.cattle.io \"traefik\" not found"
time="2024-03-22T03:53:49Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/local-storage.yaml"
time="2024-03-22T03:53:49Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/resource-reader.yaml"
time="2024-03-22T03:53:49Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/traefik.yaml"
time="2024-03-22T03:53:49Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-deployment.yaml"
time="2024-03-22T03:53:49Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-service.yaml"
time="2024-03-22T03:53:49Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/rolebindings.yaml"
time="2024-03-22T03:53:49Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/ccm.yaml"
time="2024-03-22T03:53:49Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/coredns.yaml"
time="2024-03-22T03:53:49Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/aggregated-metrics-reader.yaml"
time="2024-03-22T03:53:49Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/auth-reader.yaml"
time="2024-03-22T03:53:49Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/auth-delegator.yaml"
time="2024-03-22T03:53:49Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/metrics-apiservice.yaml"
time="2024-03-22T03:53:49Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/runtimes.yaml"
time="2024-03-22T03:53:49Z" level=info msg="Starting dynamiclistener CN filter node controller"
time="2024-03-22T03:53:49Z" level=info msg="Tunnel server egress proxy mode: agent"
time="2024-03-22T03:53:49Z" level=info msg="Starting k3s.cattle.io/v1, Kind=Addon controller"
time="2024-03-22T03:53:49Z" level=info msg="Creating deploy event broadcaster"
time="2024-03-22T03:53:49Z" level=info msg="Creating helm-controller event broadcaster"
time="2024-03-22T03:53:49Z" level=info msg="Starting /v1, Kind=Node controller"
time="2024-03-22T03:53:49Z" level=warning msg="Unable to fetch coredns config map: configmaps \"coredns\" not found"
time="2024-03-22T03:53:49Z" level=info msg="Starting /v1, Kind=ConfigMap controller"
time="2024-03-22T03:53:49Z" level=info msg="Starting /v1, Kind=ServiceAccount controller"
time="2024-03-22T03:53:49Z" level=info msg="Starting /v1, Kind=Secret controller"
time="2024-03-22T03:53:49Z" level=info msg="Cluster dns configmap has been set successfully"
time="2024-03-22T03:53:49Z" level=info msg="Labels and annotations have been set successfully on node: server"
I0322 03:53:49.514257    2297 controllermanager.go:189] "Starting" version="v1.28.7+k3s1"
I0322 03:53:49.514382    2297 controllermanager.go:191] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0322 03:53:49.517696    2297 secure_serving.go:213] Serving securely on 127.0.0.1:10257
I0322 03:53:49.517845    2297 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0322 03:53:49.517872    2297 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I0322 03:53:49.517881    2297 shared_informer.go:311] Waiting for caches to sync for RequestHeaderAuthRequestController
I0322 03:53:49.517943    2297 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0322 03:53:49.517950    2297 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0322 03:53:49.517959    2297 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0322 03:53:49.517963    2297 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0322 03:53:49.525653    2297 shared_informer.go:311] Waiting for caches to sync for tokens
I0322 03:53:49.531441    2297 controller.go:624] quota admission added evaluator for: serviceaccounts
I0322 03:53:49.533686    2297 controllermanager.go:642] "Started controller" controller="cronjob-controller"
I0322 03:53:49.533904    2297 cronjob_controllerv2.go:139] "Starting cronjob controller v2"
I0322 03:53:49.534027    2297 shared_informer.go:311] Waiting for caches to sync for cronjob
I0322 03:53:49.534231    2297 controllermanager.go:607] "Warning: controller is disabled" controller="node-route-controller"
I0322 03:53:49.541093    2297 controllermanager.go:642] "Started controller" controller="endpointslice-mirroring-controller"
I0322 03:53:49.541302    2297 endpointslicemirroring_controller.go:223] "Starting EndpointSliceMirroring controller"
I0322 03:53:49.541327    2297 shared_informer.go:311] Waiting for caches to sync for endpoint_slice_mirroring
time="2024-03-22T03:53:49Z" level=info msg="Starting helm.cattle.io/v1, Kind=HelmChartConfig controller"
time="2024-03-22T03:53:49Z" level=info msg="Starting helm.cattle.io/v1, Kind=HelmChart controller"
time="2024-03-22T03:53:49Z" level=info msg="Starting rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding controller"
I0322 03:53:49.547663    2297 controllermanager.go:642] "Started controller" controller="pod-garbage-collector-controller"
I0322 03:53:49.547872    2297 gc_controller.go:101] "Starting GC controller"
I0322 03:53:49.547882    2297 shared_informer.go:311] Waiting for caches to sync for GC
time="2024-03-22T03:53:49Z" level=info msg="Starting batch/v1, Kind=Job controller"
I0322 03:53:49.553368    2297 controllermanager.go:642] "Started controller" controller="job-controller"
I0322 03:53:49.553392    2297 controllermanager.go:607] "Warning: controller is disabled" controller="cloud-node-lifecycle-controller"
I0322 03:53:49.553539    2297 job_controller.go:226] "Starting job controller"
I0322 03:53:49.553547    2297 shared_informer.go:311] Waiting for caches to sync for job
I0322 03:53:49.558818    2297 controllermanager.go:642] "Started controller" controller="daemonset-controller"
I0322 03:53:49.558927    2297 controllermanager.go:607] "Warning: controller is disabled" controller="bootstrap-signer-controller"
I0322 03:53:49.559043    2297 daemon_controller.go:291] "Starting daemon sets controller"
I0322 03:53:49.559073    2297 shared_informer.go:311] Waiting for caches to sync for daemon sets
I0322 03:53:49.564301    2297 controllermanager.go:642] "Started controller" controller="ttl-after-finished-controller"
I0322 03:53:49.564545    2297 ttlafterfinished_controller.go:109] "Starting TTL after finished controller"
I0322 03:53:49.564558    2297 shared_informer.go:311] Waiting for caches to sync for TTL after finished
I0322 03:53:49.570009    2297 controllermanager.go:642] "Started controller" controller="ephemeral-volume-controller"
I0322 03:53:49.570130    2297 controller.go:169] "Starting ephemeral volume controller"
I0322 03:53:49.570295    2297 shared_informer.go:311] Waiting for caches to sync for ephemeral
I0322 03:53:49.618799    2297 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0322 03:53:49.618935    2297 shared_informer.go:318] Caches are synced for RequestHeaderAuthRequestController
I0322 03:53:49.618946    2297 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0322 03:53:49.627847    2297 shared_informer.go:318] Caches are synced for tokens
time="2024-03-22T03:53:49Z" level=warning msg="Unable to fetch coredns config map: configmaps \"coredns\" not found"
time="2024-03-22T03:53:50Z" level=info msg="Updating TLS secret for kube-system/k3s-serving (count: 10): map[listener.cattle.io/cn-10.43.0.1:10.43.0.1 listener.cattle.io/cn-127.0.0.1:127.0.0.1 listener.cattle.io/cn-192.168.56.110:192.168.56.110 listener.cattle.io/cn-__1-f16284:::1 listener.cattle.io/cn-kubernetes:kubernetes listener.cattle.io/cn-kubernetes.default:kubernetes.default listener.cattle.io/cn-kubernetes.default.svc:kubernetes.default.svc listener.cattle.io/cn-kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local listener.cattle.io/cn-localhost:localhost listener.cattle.io/cn-server:server listener.cattle.io/fingerprint:SHA1=57817D08CEF13EAC5FA6EA63398833E2B858E1D0]"
time="2024-03-22T03:53:50Z" level=info msg="Updating TLS secret for kube-system/k3s-serving (count: 10): map[listener.cattle.io/cn-10.43.0.1:10.43.0.1 listener.cattle.io/cn-127.0.0.1:127.0.0.1 listener.cattle.io/cn-192.168.56.110:192.168.56.110 listener.cattle.io/cn-__1-f16284:::1 listener.cattle.io/cn-kubernetes:kubernetes listener.cattle.io/cn-kubernetes.default:kubernetes.default listener.cattle.io/cn-kubernetes.default.svc:kubernetes.default.svc listener.cattle.io/cn-kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local listener.cattle.io/cn-localhost:localhost listener.cattle.io/cn-server:server listener.cattle.io/fingerprint:SHA1=57817D08CEF13EAC5FA6EA63398833E2B858E1D0]"
time="2024-03-22T03:53:50Z" level=info msg="Active TLS secret kube-system/k3s-serving (ver=233) (count 10): map[listener.cattle.io/cn-10.43.0.1:10.43.0.1 listener.cattle.io/cn-127.0.0.1:127.0.0.1 listener.cattle.io/cn-192.168.56.110:192.168.56.110 listener.cattle.io/cn-__1-f16284:::1 listener.cattle.io/cn-kubernetes:kubernetes listener.cattle.io/cn-kubernetes.default:kubernetes.default listener.cattle.io/cn-kubernetes.default.svc:kubernetes.default.svc listener.cattle.io/cn-kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local listener.cattle.io/cn-localhost:localhost listener.cattle.io/cn-server:server listener.cattle.io/fingerprint:SHA1=57817D08CEF13EAC5FA6EA63398833E2B858E1D0]"
I0322 03:53:51.010332    2297 event.go:307] "Event occurred" object="server" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodePasswordValidationComplete" message="Deferred node password secret validation complete"
time="2024-03-22T03:53:51Z" level=warning msg="Unable to fetch coredns config map: configmaps \"coredns\" not found"
I0322 03:53:51.649031    2297 controller.go:624] quota admission added evaluator for: addons.k3s.cattle.io
I0322 03:53:51.653738    2297 event.go:307] "Event occurred" object="kube-system/ccm" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/ccm.yaml\""
time="2024-03-22T03:53:51Z" level=warning msg="Unable to fetch coredns config map: configmaps \"coredns\" not found"
I0322 03:53:51.721462    2297 event.go:307] "Event occurred" object="kube-system/ccm" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/ccm.yaml\""
I0322 03:53:51.731396    2297 event.go:307] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/coredns.yaml\""
I0322 03:53:51.767734    2297 controller.go:624] quota admission added evaluator for: deployments.apps
I0322 03:53:51.778614    2297 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.43.0.10"}
I0322 03:53:51.779483    2297 event.go:307] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/coredns.yaml\""
I0322 03:53:51.789403    2297 event.go:307] "Event occurred" object="kube-system/local-storage" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/local-storage.yaml\""
I0322 03:53:51.824248    2297 event.go:307] "Event occurred" object="kube-system/local-storage" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/local-storage.yaml\""
I0322 03:53:51.832759    2297 event.go:307] "Event occurred" object="kube-system/aggregated-metrics-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/aggregated-metrics-reader.yaml\""
I0322 03:53:51.839051    2297 event.go:307] "Event occurred" object="kube-system/aggregated-metrics-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/aggregated-metrics-reader.yaml\""
I0322 03:53:51.845742    2297 event.go:307] "Event occurred" object="kube-system/auth-delegator" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/auth-delegator.yaml\""
I0322 03:53:51.852023    2297 event.go:307] "Event occurred" object="kube-system/auth-delegator" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/auth-delegator.yaml\""
I0322 03:53:51.859110    2297 event.go:307] "Event occurred" object="kube-system/auth-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/auth-reader.yaml\""
I0322 03:53:51.864111    2297 event.go:307] "Event occurred" object="kube-system/auth-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/auth-reader.yaml\""
time="2024-03-22T03:53:52Z" level=info msg="Updated coredns node hosts entry [192.168.56.110 server]"
I0322 03:53:52.140649    2297 event.go:307] "Event occurred" object="kube-system/metrics-apiservice" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-apiservice.yaml\""
I0322 03:53:52.151052    2297 event.go:307] "Event occurred" object="kube-system/metrics-apiservice" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-apiservice.yaml\""
I0322 03:53:52.152048    2297 handler.go:275] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
W0322 03:53:52.155534    2297 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:53:52.155741    2297 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
E0322 03:53:52.155847    2297 handler_proxy.go:137] error resolving kube-system/metrics-server: service "metrics-server" not found
time="2024-03-22T03:53:52Z" level=info msg="Running kube-proxy --cluster-cidr=10.42.0.0/16 --conntrack-max-per-core=0 --conntrack-tcp-timeout-close-wait=0s --conntrack-tcp-timeout-established=0s --healthz-bind-address=127.0.0.1 --hostname-override=server --kubeconfig=/var/lib/rancher/k3s/agent/kubeproxy.kubeconfig --proxy-mode=iptables"
I0322 03:53:52.339710    2297 node.go:141] Successfully retrieved node IP: 192.168.56.110
I0322 03:53:52.345350    2297 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0322 03:53:52.347565    2297 server_others.go:152] "Using iptables Proxier"
I0322 03:53:52.347607    2297 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0322 03:53:52.347615    2297 server_others.go:438] "Defaulting to no-op detect-local"
I0322 03:53:52.347686    2297 proxier.go:250] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0322 03:53:52.347927    2297 server.go:846] "Version info" version="v1.28.7+k3s1"
I0322 03:53:52.347936    2297 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0322 03:53:52.349849    2297 config.go:188] "Starting service config controller"
I0322 03:53:52.349876    2297 shared_informer.go:311] Waiting for caches to sync for service config
I0322 03:53:52.349899    2297 config.go:97] "Starting endpoint slice config controller"
I0322 03:53:52.349902    2297 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0322 03:53:52.350388    2297 config.go:315] "Starting node config controller"
I0322 03:53:52.350402    2297 shared_informer.go:311] Waiting for caches to sync for node config
I0322 03:53:52.452003    2297 shared_informer.go:318] Caches are synced for service config
I0322 03:53:52.452130    2297 shared_informer.go:318] Caches are synced for endpoint slice config
I0322 03:53:52.452781    2297 shared_informer.go:318] Caches are synced for node config
I0322 03:53:52.528694    2297 event.go:307] "Event occurred" object="kube-system/metrics-server-deployment" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-deployment.yaml\""
I0322 03:53:52.544911    2297 event.go:307] "Event occurred" object="kube-system/metrics-server-deployment" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-deployment.yaml\""
I0322 03:53:52.840770    2297 serving.go:355] Generated self-signed cert in-memory
I0322 03:53:52.925822    2297 event.go:307] "Event occurred" object="kube-system/metrics-server-service" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-service.yaml\""
I0322 03:53:52.932592    2297 alloc.go:330] "allocated clusterIPs" service="kube-system/metrics-server" clusterIPs={"IPv4":"10.43.159.169"}
I0322 03:53:52.935670    2297 event.go:307] "Event occurred" object="kube-system/metrics-server-service" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-service.yaml\""
W0322 03:53:52.954362    2297 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:53:52.954399    2297 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
E0322 03:53:52.954443    2297 handler_proxy.go:137] error resolving kube-system/metrics-server: endpoints "metrics-server" not found
W0322 03:53:53.160360    2297 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:53:53.160409    2297 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I0322 03:53:53.160420    2297 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0322 03:53:53.160780    2297 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:53:53.160934    2297 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 03:53:53.162510    2297 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0322 03:53:53.326339    2297 event.go:307] "Event occurred" object="kube-system/resource-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/resource-reader.yaml\""
I0322 03:53:53.342121    2297 event.go:307] "Event occurred" object="kube-system/resource-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/resource-reader.yaml\""
I0322 03:53:53.379216    2297 serving.go:355] Generated self-signed cert in-memory
I0322 03:53:53.728433    2297 event.go:307] "Event occurred" object="kube-system/rolebindings" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/rolebindings.yaml\""
I0322 03:53:53.759554    2297 event.go:307] "Event occurred" object="kube-system/rolebindings" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/rolebindings.yaml\""
time="2024-03-22T03:53:54Z" level=info msg="Tunnel authorizer set Kubelet Port 10250"
I0322 03:53:54.047578    2297 controllermanager.go:168] Version: v1.28.7+k3s1
I0322 03:53:54.051792    2297 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I0322 03:53:54.051883    2297 shared_informer.go:311] Waiting for caches to sync for RequestHeaderAuthRequestController
I0322 03:53:54.052015    2297 secure_serving.go:213] Serving securely on 127.0.0.1:10258
I0322 03:53:54.052114    2297 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0322 03:53:54.052227    2297 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0322 03:53:54.052325    2297 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0322 03:53:54.052396    2297 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0322 03:53:54.052654    2297 tlsconfig.go:240] "Starting DynamicServingCertificateController"
E0322 03:53:54.067461    2297 controllermanager.go:524] unable to get all supported resources from server: unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
time="2024-03-22T03:53:54Z" level=info msg="Creating  event broadcaster"
I0322 03:53:54.133852    2297 event.go:307] "Event occurred" object="kube-system/runtimes" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/runtimes.yaml\""
I0322 03:53:54.158075    2297 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0322 03:53:54.158129    2297 shared_informer.go:318] Caches are synced for RequestHeaderAuthRequestController
I0322 03:53:54.158246    2297 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
time="2024-03-22T03:53:54Z" level=info msg="Starting /v1, Kind=Node controller"
time="2024-03-22T03:53:54Z" level=info msg="Starting /v1, Kind=Pod controller"
time="2024-03-22T03:53:54Z" level=info msg="Starting apps/v1, Kind=DaemonSet controller"
I0322 03:53:54.199775    2297 controllermanager.go:337] Started "cloud-node-controller"
time="2024-03-22T03:53:54Z" level=info msg="Starting discovery.k8s.io/v1, Kind=EndpointSlice controller"
I0322 03:53:54.200106    2297 controllermanager.go:337] Started "cloud-node-lifecycle-controller"
I0322 03:53:54.200453    2297 controllermanager.go:337] Started "service-lb-controller"
W0322 03:53:54.200507    2297 controllermanager.go:314] "node-route-controller" is disabled
I0322 03:53:54.201004    2297 node_controller.go:165] Sending events to api server.
I0322 03:53:54.201105    2297 node_controller.go:174] Waiting for informer caches to sync
I0322 03:53:54.201165    2297 node_lifecycle_controller.go:113] Sending events to api server
I0322 03:53:54.201317    2297 controller.go:231] Starting service controller
I0322 03:53:54.201417    2297 shared_informer.go:311] Waiting for caches to sync for service
I0322 03:53:54.302062    2297 shared_informer.go:318] Caches are synced for service
I0322 03:53:54.302209    2297 node_controller.go:431] Initializing node server with cloud provider
I0322 03:53:54.306912    2297 node_controller.go:502] Successfully initialized node server with cloud provider
I0322 03:53:54.307216    2297 event.go:307] "Event occurred" object="server" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="Synced" message="Node synced successfully"
I0322 03:53:54.394861    2297 event.go:307] "Event occurred" object="kube-system/runtimes" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/runtimes.yaml\""
I0322 03:53:54.527943    2297 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/traefik.yaml\""
I0322 03:53:54.532350    2297 controller.go:624] quota admission added evaluator for: helmcharts.helm.cattle.io
I0322 03:53:54.542474    2297 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/traefik.yaml\""
I0322 03:53:54.563486    2297 event.go:307] "Event occurred" object="kube-system/traefik-crd" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik-crd"
I0322 03:53:54.563530    2297 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
I0322 03:53:54.599134    2297 controller.go:624] quota admission added evaluator for: jobs.batch
time="2024-03-22T03:53:54Z" level=error msg="error syncing 'kube-system/traefik': handler helm-controller-chart-registration: helmcharts.helm.cattle.io \"traefik\" not found, requeuing"
time="2024-03-22T03:53:54Z" level=error msg="error syncing 'kube-system/traefik-crd': handler helm-controller-chart-registration: helmcharts.helm.cattle.io \"traefik-crd\" not found, requeuing"
I0322 03:53:54.639746    2297 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
I0322 03:53:54.639901    2297 event.go:307] "Event occurred" object="kube-system/traefik-crd" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik-crd"
I0322 03:53:54.748167    2297 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
I0322 03:53:54.805054    2297 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
I0322 03:53:54.864851    2297 serving.go:355] Generated self-signed cert in-memory
I0322 03:53:54.946976    2297 event.go:307] "Event occurred" object="kube-system/traefik-crd" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik-crd"
I0322 03:53:54.968418    2297 event.go:307] "Event occurred" object="kube-system/traefik-crd" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik-crd"
time="2024-03-22T03:53:55Z" level=info msg="Stopped tunnel to 127.0.0.1:6443"
time="2024-03-22T03:53:55Z" level=info msg="Connecting to proxy" url="wss://192.168.56.110:6443/v1-k3s/connect"
time="2024-03-22T03:53:55Z" level=info msg="Proxy done" err="context canceled" url="wss://127.0.0.1:6443/v1-k3s/connect"
time="2024-03-22T03:53:55Z" level=info msg="error in remotedialer server [400]: websocket: close 1006 (abnormal closure): unexpected EOF"
time="2024-03-22T03:53:55Z" level=info msg="Handling backend connection request [server]"
I0322 03:53:55.249016    2297 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.7+k3s1"
I0322 03:53:55.249126    2297 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0322 03:53:55.252502    2297 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0322 03:53:55.252605    2297 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I0322 03:53:55.252644    2297 shared_informer.go:311] Waiting for caches to sync for RequestHeaderAuthRequestController
I0322 03:53:55.253037    2297 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0322 03:53:55.253426    2297 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0322 03:53:55.253440    2297 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0322 03:53:55.253458    2297 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0322 03:53:55.253462    2297 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0322 03:53:55.353889    2297 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0322 03:53:55.354243    2297 shared_informer.go:318] Caches are synced for RequestHeaderAuthRequestController
I0322 03:53:55.354384    2297 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0322 03:53:59.596766    2297 range_allocator.go:111] "No Secondary Service CIDR provided. Skipping filtering out secondary service addresses"
I0322 03:53:59.596824    2297 controllermanager.go:642] "Started controller" controller="node-ipam-controller"
I0322 03:53:59.597009    2297 node_ipam_controller.go:162] "Starting ipam controller"
I0322 03:53:59.597021    2297 shared_informer.go:311] Waiting for caches to sync for node
I0322 03:53:59.605041    2297 controllermanager.go:642] "Started controller" controller="persistentvolume-attach-detach-controller"
I0322 03:53:59.605295    2297 attach_detach_controller.go:337] "Starting attach detach controller"
I0322 03:53:59.605308    2297 shared_informer.go:311] Waiting for caches to sync for attach detach
I0322 03:53:59.613924    2297 controllermanager.go:642] "Started controller" controller="root-ca-certificate-publisher-controller"
I0322 03:53:59.614111    2297 publisher.go:102] "Starting root CA cert publisher controller"
I0322 03:53:59.614124    2297 shared_informer.go:311] Waiting for caches to sync for crt configmap
I0322 03:53:59.622617    2297 controllermanager.go:642] "Started controller" controller="replicationcontroller-controller"
I0322 03:53:59.623022    2297 replica_set.go:214] "Starting controller" name="replicationcontroller"
I0322 03:53:59.623048    2297 shared_informer.go:311] Waiting for caches to sync for ReplicationController
E0322 03:53:59.639274    2297 resource_quota_controller.go:169] initial discovery check failure, continuing and counting on future sync update: unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I0322 03:53:59.640166    2297 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="horizontalpodautoscalers.autoscaling"
I0322 03:53:59.640278    2297 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingresses.networking.k8s.io"
I0322 03:53:59.640373    2297 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="helmcharts.helm.cattle.io"
I0322 03:53:59.640477    2297 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="deployments.apps"
I0322 03:53:59.640562    2297 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="poddisruptionbudgets.policy"
I0322 03:53:59.640645    2297 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="roles.rbac.authorization.k8s.io"
I0322 03:53:59.640742    2297 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="leases.coordination.k8s.io"
I0322 03:53:59.640823    2297 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="helmchartconfigs.helm.cattle.io"
I0322 03:53:59.640934    2297 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="endpoints"
I0322 03:53:59.641021    2297 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="cronjobs.batch"
I0322 03:53:59.641119    2297 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="endpointslices.discovery.k8s.io"
W0322 03:53:59.641216    2297 shared_informer.go:593] resyncPeriod 18h2m8.094596121s is smaller than resyncCheckPeriod 18h28m59.920638975s and the informer has already started. Changing it to 18h28m59.920638975s
I0322 03:53:59.641311    2297 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="serviceaccounts"
I0322 03:53:59.641397    2297 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="statefulsets.apps"
I0322 03:53:59.641491    2297 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="jobs.batch"
I0322 03:53:59.641568    2297 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="replicasets.apps"
I0322 03:53:59.641661    2297 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="daemonsets.apps"
I0322 03:53:59.641739    2297 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="networkpolicies.networking.k8s.io"
I0322 03:53:59.641836    2297 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="addons.k3s.cattle.io"
I0322 03:53:59.641928    2297 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="podtemplates"
I0322 03:53:59.642059    2297 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="rolebindings.rbac.authorization.k8s.io"
I0322 03:53:59.642141    2297 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="csistoragecapacities.storage.k8s.io"
I0322 03:53:59.642236    2297 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="limitranges"
I0322 03:53:59.642314    2297 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="controllerrevisions.apps"
I0322 03:53:59.642384    2297 controllermanager.go:642] "Started controller" controller="resourcequota-controller"
I0322 03:53:59.642418    2297 resource_quota_controller.go:294] "Starting resource quota controller"
I0322 03:53:59.642619    2297 shared_informer.go:311] Waiting for caches to sync for resource quota
I0322 03:53:59.642688    2297 resource_quota_monitor.go:305] "QuotaMonitor running"
E0322 03:53:59.644636    2297 resource_quota_controller.go:440] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
E0322 03:53:59.658827    2297 namespaced_resources_deleter.go:162] unable to get all supported resources from server: unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I0322 03:53:59.659026    2297 controllermanager.go:642] "Started controller" controller="namespace-controller"
I0322 03:53:59.659057    2297 namespace_controller.go:197] "Starting namespace controller"
I0322 03:53:59.659263    2297 shared_informer.go:311] Waiting for caches to sync for namespace
I0322 03:53:59.666316    2297 controllermanager.go:642] "Started controller" controller="serviceaccount-controller"
I0322 03:53:59.666610    2297 serviceaccounts_controller.go:111] "Starting service account controller"
I0322 03:53:59.666631    2297 shared_informer.go:311] Waiting for caches to sync for service account
I0322 03:53:59.673769    2297 controllermanager.go:642] "Started controller" controller="persistentvolume-expander-controller"
I0322 03:53:59.674131    2297 expand_controller.go:328] "Starting expand controller"
I0322 03:53:59.674414    2297 shared_informer.go:311] Waiting for caches to sync for expand
I0322 03:53:59.680954    2297 controllermanager.go:642] "Started controller" controller="endpointslice-controller"
I0322 03:53:59.681178    2297 endpointslice_controller.go:264] "Starting endpoint slice controller"
I0322 03:53:59.681267    2297 shared_informer.go:311] Waiting for caches to sync for endpoint_slice
I0322 03:53:59.807086    2297 controllermanager.go:642] "Started controller" controller="certificatesigningrequest-signing-controller"
I0322 03:53:59.807291    2297 certificate_controller.go:115] "Starting certificate controller" name="csrsigning-legacy-unknown"
I0322 03:53:59.807374    2297 shared_informer.go:311] Waiting for caches to sync for certificate-csrsigning-legacy-unknown
I0322 03:53:59.807463    2297 certificate_controller.go:115] "Starting certificate controller" name="csrsigning-kubelet-serving"
I0322 03:53:59.807532    2297 shared_informer.go:311] Waiting for caches to sync for certificate-csrsigning-kubelet-serving
I0322 03:53:59.807683    2297 certificate_controller.go:115] "Starting certificate controller" name="csrsigning-kubelet-client"
I0322 03:53:59.807762    2297 shared_informer.go:311] Waiting for caches to sync for certificate-csrsigning-kubelet-client
I0322 03:53:59.807868    2297 certificate_controller.go:115] "Starting certificate controller" name="csrsigning-kube-apiserver-client"
I0322 03:53:59.807944    2297 shared_informer.go:311] Waiting for caches to sync for certificate-csrsigning-kube-apiserver-client
I0322 03:53:59.808044    2297 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/var/lib/rancher/k3s/server/tls/server-ca.nochain.crt::/var/lib/rancher/k3s/server/tls/server-ca.key"
I0322 03:53:59.808180    2297 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/var/lib/rancher/k3s/server/tls/server-ca.nochain.crt::/var/lib/rancher/k3s/server/tls/server-ca.key"
I0322 03:53:59.808300    2297 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/var/lib/rancher/k3s/server/tls/client-ca.nochain.crt::/var/lib/rancher/k3s/server/tls/client-ca.key"
I0322 03:53:59.808412    2297 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/var/lib/rancher/k3s/server/tls/client-ca.nochain.crt::/var/lib/rancher/k3s/server/tls/client-ca.key"
I0322 03:53:59.830638    2297 node_lifecycle_controller.go:431] "Controller will reconcile labels"
I0322 03:53:59.830683    2297 controllermanager.go:642] "Started controller" controller="node-lifecycle-controller"
I0322 03:53:59.830774    2297 node_lifecycle_controller.go:465] "Sending events to api server"
I0322 03:53:59.830794    2297 node_lifecycle_controller.go:476] "Starting node controller"
I0322 03:53:59.830801    2297 shared_informer.go:311] Waiting for caches to sync for taint
I0322 03:53:59.981892    2297 controllermanager.go:642] "Started controller" controller="persistentvolume-binder-controller"
I0322 03:53:59.982129    2297 pv_controller_base.go:319] "Starting persistent volume controller"
I0322 03:53:59.982253    2297 shared_informer.go:311] Waiting for caches to sync for persistent volume
I0322 03:54:00.133118    2297 controllermanager.go:642] "Started controller" controller="statefulset-controller"
I0322 03:54:00.133661    2297 stateful_set.go:161] "Starting stateful set controller"
I0322 03:54:00.133683    2297 shared_informer.go:311] Waiting for caches to sync for stateful set
I0322 03:54:00.180042    2297 controllermanager.go:642] "Started controller" controller="certificatesigningrequest-cleaner-controller"
I0322 03:54:00.180278    2297 cleaner.go:83] "Starting CSR cleaner controller"
I0322 03:54:00.330502    2297 controllermanager.go:642] "Started controller" controller="ttl-controller"
I0322 03:54:00.330523    2297 controllermanager.go:607] "Warning: controller is disabled" controller="service-lb-controller"
I0322 03:54:00.330603    2297 ttl_controller.go:124] "Starting TTL controller"
I0322 03:54:00.330612    2297 shared_informer.go:311] Waiting for caches to sync for TTL
I0322 03:54:00.587259    2297 controllermanager.go:642] "Started controller" controller="garbage-collector-controller"
I0322 03:54:00.588327    2297 garbagecollector.go:155] "Starting controller" controller="garbagecollector"
I0322 03:54:00.588571    2297 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0322 03:54:00.588954    2297 graph_builder.go:294] "Running" component="GraphBuilder"
I0322 03:54:00.838608    2297 garbagecollector.go:816] "failed to discover some groups" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"
I0322 03:54:00.838968    2297 controllermanager.go:642] "Started controller" controller="replicaset-controller"
I0322 03:54:00.839052    2297 replica_set.go:214] "Starting controller" name="replicaset"
I0322 03:54:00.839064    2297 shared_informer.go:311] Waiting for caches to sync for ReplicaSet
I0322 03:54:01.662512    2297 controllermanager.go:642] "Started controller" controller="horizontal-pod-autoscaler-controller"
I0322 03:54:01.662778    2297 horizontal.go:200] "Starting HPA controller"
I0322 03:54:01.663165    2297 shared_informer.go:311] Waiting for caches to sync for HPA
I0322 03:54:01.672918    2297 controllermanager.go:642] "Started controller" controller="disruption-controller"
I0322 03:54:01.673042    2297 disruption.go:433] "Sending events to api server."
I0322 03:54:01.673067    2297 disruption.go:444] "Starting disruption controller"
I0322 03:54:01.673074    2297 shared_informer.go:311] Waiting for caches to sync for disruption
I0322 03:54:01.677993    2297 controllermanager.go:642] "Started controller" controller="clusterrole-aggregation-controller"
I0322 03:54:01.678124    2297 clusterroleaggregation_controller.go:189] "Starting ClusterRoleAggregator controller"
I0322 03:54:01.678132    2297 shared_informer.go:311] Waiting for caches to sync for ClusterRoleAggregator
I0322 03:54:01.685380    2297 controllermanager.go:642] "Started controller" controller="persistentvolume-protection-controller"
I0322 03:54:01.685623    2297 pv_protection_controller.go:78] "Starting PV protection controller"
I0322 03:54:01.685636    2297 shared_informer.go:311] Waiting for caches to sync for PV protection
I0322 03:54:01.784460    2297 controllermanager.go:642] "Started controller" controller="persistentvolumeclaim-protection-controller"
I0322 03:54:01.784594    2297 pvc_protection_controller.go:102] "Starting PVC protection controller"
I0322 03:54:01.784604    2297 shared_informer.go:311] Waiting for caches to sync for PVC protection
I0322 03:54:01.932026    2297 controllermanager.go:642] "Started controller" controller="endpoints-controller"
I0322 03:54:01.932158    2297 endpoints_controller.go:174] "Starting endpoint controller"
I0322 03:54:01.932172    2297 shared_informer.go:311] Waiting for caches to sync for endpoint
I0322 03:54:02.088830    2297 controllermanager.go:642] "Started controller" controller="deployment-controller"
I0322 03:54:02.089039    2297 deployment_controller.go:168] "Starting controller" controller="deployment"
I0322 03:54:02.089063    2297 shared_informer.go:311] Waiting for caches to sync for deployment
I0322 03:54:02.137555    2297 controllermanager.go:642] "Started controller" controller="certificatesigningrequest-approving-controller"
I0322 03:54:02.138056    2297 certificate_controller.go:115] "Starting certificate controller" name="csrapproving"
I0322 03:54:02.138571    2297 shared_informer.go:311] Waiting for caches to sync for certificate-csrapproving
I0322 03:54:02.281687    2297 controllermanager.go:642] "Started controller" controller="token-cleaner-controller"
I0322 03:54:02.282048    2297 tokencleaner.go:112] "Starting token cleaner controller"
I0322 03:54:02.282158    2297 shared_informer.go:311] Waiting for caches to sync for token_cleaner
I0322 03:54:02.282168    2297 shared_informer.go:318] Caches are synced for token_cleaner
I0322 03:54:02.287825    2297 shared_informer.go:311] Waiting for caches to sync for resource quota
I0322 03:54:02.300138    2297 actual_state_of_world.go:547] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"server\" does not exist"
I0322 03:54:02.303153    2297 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:54:02.303467    2297 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:54:02.314220    2297 shared_informer.go:318] Caches are synced for crt configmap
I0322 03:54:02.321588    2297 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0322 03:54:02.330639    2297 shared_informer.go:318] Caches are synced for TTL
I0322 03:54:02.334442    2297 shared_informer.go:318] Caches are synced for cronjob
I0322 03:54:02.334487    2297 shared_informer.go:318] Caches are synced for stateful set
I0322 03:54:02.339372    2297 shared_informer.go:318] Caches are synced for certificate-csrapproving
I0322 03:54:02.339415    2297 shared_informer.go:318] Caches are synced for ReplicaSet
I0322 03:54:02.351749    2297 shared_informer.go:318] Caches are synced for GC
I0322 03:54:02.354078    2297 shared_informer.go:318] Caches are synced for job
I0322 03:54:02.360205    2297 shared_informer.go:318] Caches are synced for namespace
I0322 03:54:02.360411    2297 shared_informer.go:318] Caches are synced for daemon sets
I0322 03:54:02.363641    2297 shared_informer.go:318] Caches are synced for HPA
I0322 03:54:02.364900    2297 shared_informer.go:318] Caches are synced for TTL after finished
I0322 03:54:02.374976    2297 shared_informer.go:318] Caches are synced for ephemeral
I0322 03:54:02.375191    2297 shared_informer.go:318] Caches are synced for expand
I0322 03:54:02.375400    2297 shared_informer.go:318] Caches are synced for service account
I0322 03:54:02.384794    2297 shared_informer.go:318] Caches are synced for PVC protection
I0322 03:54:02.385041    2297 shared_informer.go:318] Caches are synced for persistent volume
I0322 03:54:02.385053    2297 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I0322 03:54:02.385654    2297 shared_informer.go:318] Caches are synced for PV protection
I0322 03:54:02.397102    2297 shared_informer.go:318] Caches are synced for node
I0322 03:54:02.397210    2297 range_allocator.go:174] "Sending events to api server"
I0322 03:54:02.397298    2297 range_allocator.go:178] "Starting range CIDR allocator"
I0322 03:54:02.397368    2297 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I0322 03:54:02.397400    2297 shared_informer.go:318] Caches are synced for cidrallocator
I0322 03:54:02.395108    2297 shared_informer.go:318] Caches are synced for deployment
I0322 03:54:02.406225    2297 range_allocator.go:380] "Set node PodCIDR" node="server" podCIDRs=["10.42.0.0/24"]
time="2024-03-22T03:54:02Z" level=info msg="Flannel found PodCIDR assigned for node server"
I0322 03:54:02.406793    2297 shared_informer.go:318] Caches are synced for attach detach
I0322 03:54:02.408065    2297 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0322 03:54:02.408728    2297 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I0322 03:54:02.408844    2297 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I0322 03:54:02.408930    2297 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
time="2024-03-22T03:54:02Z" level=info msg="The interface enp0s3 with ipv4 address 10.0.2.15 will be used by flannel"
I0322 03:54:02.413967    2297 kube.go:139] Waiting 10m0s for node controller to sync
I0322 03:54:02.414061    2297 kube.go:461] Starting kube subnet manager
I0322 03:54:02.423782    2297 shared_informer.go:318] Caches are synced for ReplicationController
I0322 03:54:02.473704    2297 shared_informer.go:318] Caches are synced for disruption
time="2024-03-22T03:54:02Z" level=info msg="Starting network policy controller version v2.0.1, built on 2024-02-29T20:36:21Z, go1.21.7"
I0322 03:54:02.530332    2297 network_policy_controller.go:164] Starting network policy controller
I0322 03:54:02.530840    2297 shared_informer.go:318] Caches are synced for taint
I0322 03:54:02.530917    2297 node_lifecycle_controller.go:1225] "Initializing eviction metric for zone" zone=""
I0322 03:54:02.531003    2297 node_lifecycle_controller.go:877] "Missing timestamp for Node. Assuming now as a timestamp" node="server"
I0322 03:54:02.531049    2297 node_lifecycle_controller.go:1071] "Controller detected that zone is now in new state" zone="" newState="Normal"
I0322 03:54:02.531067    2297 taint_manager.go:205] "Starting NoExecuteTaintManager"
I0322 03:54:02.531094    2297 taint_manager.go:210] "Sending events to api server"
I0322 03:54:02.531811    2297 event.go:307] "Event occurred" object="server" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node server event: Registered Node server in Controller"
I0322 03:54:02.533076    2297 shared_informer.go:318] Caches are synced for endpoint
I0322 03:54:02.541374    2297 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I0322 03:54:02.543518    2297 shared_informer.go:318] Caches are synced for resource quota
I0322 03:54:02.582369    2297 shared_informer.go:318] Caches are synced for endpoint_slice
I0322 03:54:02.588044    2297 shared_informer.go:318] Caches are synced for resource quota
I0322 03:54:02.593708    2297 network_policy_controller.go:176] Starting network policy controller full sync goroutine
I0322 03:54:02.744637    2297 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:54:02.744804    2297 event.go:307] "Event occurred" object="kube-system/helm-install-traefik-crd" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: helm-install-traefik-crd-489zp"
I0322 03:54:02.744883    2297 event.go:307] "Event occurred" object="kube-system/helm-install-traefik" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: helm-install-traefik-jr7x9"
I0322 03:54:02.752178    2297 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:54:02.755900    2297 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:54:02.756118    2297 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:54:02.759043    2297 topology_manager.go:215] "Topology Admit Handler" podUID="672ca8db-0327-42dc-912b-c1a93de456bf" podNamespace="kube-system" podName="helm-install-traefik-jr7x9"
I0322 03:54:02.759630    2297 topology_manager.go:215] "Topology Admit Handler" podUID="1e9ad693-befe-4051-a6f1-ff59a7995d27" podNamespace="kube-system" podName="helm-install-traefik-crd-489zp"
I0322 03:54:02.761853    2297 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:54:02.762263    2297 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:54:02.778943    2297 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/672ca8db-0327-42dc-912b-c1a93de456bf-klipper-config\") pod \"helm-install-traefik-jr7x9\" (UID: \"672ca8db-0327-42dc-912b-c1a93de456bf\") " pod="kube-system/helm-install-traefik-jr7x9"
I0322 03:54:02.778993    2297 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/672ca8db-0327-42dc-912b-c1a93de456bf-tmp\") pod \"helm-install-traefik-jr7x9\" (UID: \"672ca8db-0327-42dc-912b-c1a93de456bf\") " pod="kube-system/helm-install-traefik-jr7x9"
I0322 03:54:02.779018    2297 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"values\" (UniqueName: \"kubernetes.io/secret/672ca8db-0327-42dc-912b-c1a93de456bf-values\") pod \"helm-install-traefik-jr7x9\" (UID: \"672ca8db-0327-42dc-912b-c1a93de456bf\") " pod="kube-system/helm-install-traefik-jr7x9"
I0322 03:54:02.779044    2297 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"content\" (UniqueName: \"kubernetes.io/configmap/1e9ad693-befe-4051-a6f1-ff59a7995d27-content\") pod \"helm-install-traefik-crd-489zp\" (UID: \"1e9ad693-befe-4051-a6f1-ff59a7995d27\") " pod="kube-system/helm-install-traefik-crd-489zp"
I0322 03:54:02.779074    2297 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/1e9ad693-befe-4051-a6f1-ff59a7995d27-tmp\") pod \"helm-install-traefik-crd-489zp\" (UID: \"1e9ad693-befe-4051-a6f1-ff59a7995d27\") " pod="kube-system/helm-install-traefik-crd-489zp"
I0322 03:54:02.779099    2297 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"values\" (UniqueName: \"kubernetes.io/secret/1e9ad693-befe-4051-a6f1-ff59a7995d27-values\") pod \"helm-install-traefik-crd-489zp\" (UID: \"1e9ad693-befe-4051-a6f1-ff59a7995d27\") " pod="kube-system/helm-install-traefik-crd-489zp"
I0322 03:54:02.779126    2297 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/672ca8db-0327-42dc-912b-c1a93de456bf-klipper-helm\") pod \"helm-install-traefik-jr7x9\" (UID: \"672ca8db-0327-42dc-912b-c1a93de456bf\") " pod="kube-system/helm-install-traefik-jr7x9"
I0322 03:54:02.779176    2297 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/1e9ad693-befe-4051-a6f1-ff59a7995d27-klipper-config\") pod \"helm-install-traefik-crd-489zp\" (UID: \"1e9ad693-befe-4051-a6f1-ff59a7995d27\") " pod="kube-system/helm-install-traefik-crd-489zp"
I0322 03:54:02.779206    2297 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/672ca8db-0327-42dc-912b-c1a93de456bf-klipper-cache\") pod \"helm-install-traefik-jr7x9\" (UID: \"672ca8db-0327-42dc-912b-c1a93de456bf\") " pod="kube-system/helm-install-traefik-jr7x9"
I0322 03:54:02.779249    2297 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"content\" (UniqueName: \"kubernetes.io/configmap/672ca8db-0327-42dc-912b-c1a93de456bf-content\") pod \"helm-install-traefik-jr7x9\" (UID: \"672ca8db-0327-42dc-912b-c1a93de456bf\") " pod="kube-system/helm-install-traefik-jr7x9"
I0322 03:54:02.779282    2297 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-phtm9\" (UniqueName: \"kubernetes.io/projected/672ca8db-0327-42dc-912b-c1a93de456bf-kube-api-access-phtm9\") pod \"helm-install-traefik-jr7x9\" (UID: \"672ca8db-0327-42dc-912b-c1a93de456bf\") " pod="kube-system/helm-install-traefik-jr7x9"
I0322 03:54:02.779308    2297 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/1e9ad693-befe-4051-a6f1-ff59a7995d27-klipper-helm\") pod \"helm-install-traefik-crd-489zp\" (UID: \"1e9ad693-befe-4051-a6f1-ff59a7995d27\") " pod="kube-system/helm-install-traefik-crd-489zp"
I0322 03:54:02.779334    2297 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/1e9ad693-befe-4051-a6f1-ff59a7995d27-klipper-cache\") pod \"helm-install-traefik-crd-489zp\" (UID: \"1e9ad693-befe-4051-a6f1-ff59a7995d27\") " pod="kube-system/helm-install-traefik-crd-489zp"
I0322 03:54:02.779379    2297 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-sk2wr\" (UniqueName: \"kubernetes.io/projected/1e9ad693-befe-4051-a6f1-ff59a7995d27-kube-api-access-sk2wr\") pod \"helm-install-traefik-crd-489zp\" (UID: \"1e9ad693-befe-4051-a6f1-ff59a7995d27\") " pod="kube-system/helm-install-traefik-crd-489zp"
I0322 03:54:02.792616    2297 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:54:02.825219    2297 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
W0322 03:54:02.826116    2297 watcher.go:93] Error while processing event ("/sys/fs/cgroup/devices/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod1e9ad693_befe_4051_a6f1_ff59a7995d27.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/devices/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod1e9ad693_befe_4051_a6f1_ff59a7995d27.slice: no such file or directory
I0322 03:54:02.991723    2297 shared_informer.go:318] Caches are synced for garbage collector
I0322 03:54:02.991756    2297 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I0322 03:54:02.999599    2297 controller.go:624] quota admission added evaluator for: replicasets.apps
I0322 03:54:03.007167    2297 event.go:307] "Event occurred" object="kube-system/local-path-provisioner" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set local-path-provisioner-6c86858495 to 1"
I0322 03:54:03.017861    2297 event.go:307] "Event occurred" object="kube-system/metrics-server" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set metrics-server-67c658944b to 1"
I0322 03:54:03.018065    2297 event.go:307] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set coredns-6799fbcd5 to 1"
I0322 03:54:03.021844    2297 shared_informer.go:318] Caches are synced for garbage collector
W0322 03:54:03.054167    2297 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:54:03.054235    2297 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
E0322 03:54:03.054314    2297 handler_proxy.go:137] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
I0322 03:54:03.189987    2297 event.go:307] "Event occurred" object="kube-system/local-path-provisioner-6c86858495" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: local-path-provisioner-6c86858495-hxd2h"
I0322 03:54:03.190014    2297 event.go:307] "Event occurred" object="kube-system/metrics-server-67c658944b" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: metrics-server-67c658944b-6v69k"
I0322 03:54:03.201816    2297 topology_manager.go:215] "Topology Admit Handler" podUID="bbec1420-2617-4157-9d96-117d9b5aa367" podNamespace="kube-system" podName="metrics-server-67c658944b-6v69k"
I0322 03:54:03.211342    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="193.834507ms"
I0322 03:54:03.211997    2297 event.go:307] "Event occurred" object="kube-system/coredns-6799fbcd5" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: coredns-6799fbcd5-4tsrq"
I0322 03:54:03.221061    2297 topology_manager.go:215] "Topology Admit Handler" podUID="b310a3fd-835f-4f27-94b6-9764abc03806" podNamespace="kube-system" podName="local-path-provisioner-6c86858495-hxd2h"
I0322 03:54:03.230502    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="212.29917ms"
I0322 03:54:03.230793    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="222.396818ms"
I0322 03:54:03.249247    2297 topology_manager.go:215] "Topology Admit Handler" podUID="5ddeb8bc-cc64-44fe-8dfd-b2e974d11f11" podNamespace="kube-system" podName="coredns-6799fbcd5-4tsrq"
I0322 03:54:03.269981    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="58.589872ms"
I0322 03:54:03.282012    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="55.365s"
I0322 03:54:03.270722    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="39.900072ms"
I0322 03:54:03.282089    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="23.696s"
I0322 03:54:03.284534    2297 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp-dir\" (UniqueName: \"kubernetes.io/empty-dir/bbec1420-2617-4157-9d96-117d9b5aa367-tmp-dir\") pod \"metrics-server-67c658944b-6v69k\" (UID: \"bbec1420-2617-4157-9d96-117d9b5aa367\") " pod="kube-system/metrics-server-67c658944b-6v69k"
I0322 03:54:03.284573    2297 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"custom-config-volume\" (UniqueName: \"kubernetes.io/configmap/5ddeb8bc-cc64-44fe-8dfd-b2e974d11f11-custom-config-volume\") pod \"coredns-6799fbcd5-4tsrq\" (UID: \"5ddeb8bc-cc64-44fe-8dfd-b2e974d11f11\") " pod="kube-system/coredns-6799fbcd5-4tsrq"
I0322 03:54:03.284591    2297 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-vjtws\" (UniqueName: \"kubernetes.io/projected/5ddeb8bc-cc64-44fe-8dfd-b2e974d11f11-kube-api-access-vjtws\") pod \"coredns-6799fbcd5-4tsrq\" (UID: \"5ddeb8bc-cc64-44fe-8dfd-b2e974d11f11\") " pod="kube-system/coredns-6799fbcd5-4tsrq"
I0322 03:54:03.284611    2297 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/5ddeb8bc-cc64-44fe-8dfd-b2e974d11f11-config-volume\") pod \"coredns-6799fbcd5-4tsrq\" (UID: \"5ddeb8bc-cc64-44fe-8dfd-b2e974d11f11\") " pod="kube-system/coredns-6799fbcd5-4tsrq"
I0322 03:54:03.284628    2297 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-69spq\" (UniqueName: \"kubernetes.io/projected/bbec1420-2617-4157-9d96-117d9b5aa367-kube-api-access-69spq\") pod \"metrics-server-67c658944b-6v69k\" (UID: \"bbec1420-2617-4157-9d96-117d9b5aa367\") " pod="kube-system/metrics-server-67c658944b-6v69k"
I0322 03:54:03.284646    2297 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/b310a3fd-835f-4f27-94b6-9764abc03806-config-volume\") pod \"local-path-provisioner-6c86858495-hxd2h\" (UID: \"b310a3fd-835f-4f27-94b6-9764abc03806\") " pod="kube-system/local-path-provisioner-6c86858495-hxd2h"
I0322 03:54:03.284664    2297 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-hp95s\" (UniqueName: \"kubernetes.io/projected/b310a3fd-835f-4f27-94b6-9764abc03806-kube-api-access-hp95s\") pod \"local-path-provisioner-6c86858495-hxd2h\" (UID: \"b310a3fd-835f-4f27-94b6-9764abc03806\") " pod="kube-system/local-path-provisioner-6c86858495-hxd2h"
I0322 03:54:03.294945    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="64.402255ms"
I0322 03:54:03.296123    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="61.44s"
I0322 03:54:03.321837    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="26.809972ms"
I0322 03:54:03.322157    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="171.589s"
I0322 03:54:03.414782    2297 kube.go:146] Node controller sync successful
I0322 03:54:03.414861    2297 vxlan.go:141] VXLAN config: VNI=1 Port=0 GBP=false Learning=false DirectRouting=false
I0322 03:54:03.417882    2297 kube.go:621] List of node(server) annotations: map[string]string{"alpha.kubernetes.io/provided-node-ip":"192.168.56.110", "k3s.io/hostname":"server", "k3s.io/internal-ip":"192.168.56.110", "k3s.io/node-args":"[\"server\",\"--node-ip\",\"192.168.56.110\",\"--write-kubeconfig-mode\",\"644\",\"--log\",\"/vagrant/logs/server.logs\"]", "k3s.io/node-config-hash":"OEDLXPLCFN65I5K4IWF2M6JMEKD34L47WZJH7YZKLHVRXJTJG6TQ====", "k3s.io/node-env":"{\"K3S_DATA_DIR\":\"/var/lib/rancher/k3s/data/a3b46c0299091b71bfcc617b1e1fec1845c13bdd848584ceb39d2e700e702a4b\"}", "node.alpha.kubernetes.io/ttl":"0", "volumes.kubernetes.io/controller-managed-attach-detach":"true"}
I0322 03:54:03.468047    2297 kube.go:482] Creating the node lease for IPv4. This is the n.Spec.PodCIDRs: [10.42.0.0/24]
time="2024-03-22T03:54:03Z" level=info msg="Wrote flannel subnet file to /run/flannel/subnet.env"
time="2024-03-22T03:54:03Z" level=info msg="Running flannel backend."
I0322 03:54:03.474720    2297 vxlan_network.go:65] watching for new subnet leases
I0322 03:54:03.474749    2297 iptables.go:290] generated 3 rules
I0322 03:54:03.475754    2297 iptables.go:290] generated 7 rules
I0322 03:54:03.489897    2297 iptables.go:283] bootstrap done
I0322 03:54:03.496464    2297 iptables.go:283] bootstrap done
W0322 03:54:04.054734    2297 handler_proxy.go:93] no RequestInfo found in the context
W0322 03:54:04.054775    2297 handler_proxy.go:93] no RequestInfo found in the context
E0322 03:54:04.054845    2297 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
E0322 03:54:04.054933    2297 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 03:54:04.054961    2297 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0322 03:54:04.061137    2297 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0322 03:54:08.372072    2297 kuberuntime_manager.go:1528] "Updating runtime config through cri with podcidr" CIDR="10.42.0.0/24"
I0322 03:54:08.373057    2297 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.42.0.0/24"
I0322 03:54:13.554531    2297 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/local-path-provisioner-6c86858495-hxd2h" containerName="local-path-provisioner"
I0322 03:54:14.747770    2297 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/local-path-provisioner-6c86858495-hxd2h" podStartSLOduration=4.852590172 podCreationTimestamp="2024-03-22 03:54:03 +0000 UTC" firstStartedPulling="2024-03-22 03:54:06.655756415 +0000 UTC m=+22.392722588" lastFinishedPulling="2024-03-22 03:54:13.550300357 +0000 UTC m=+29.287266529" observedRunningTime="2024-03-22 03:54:14.746532605 +0000 UTC m=+30.483498797" watchObservedRunningTime="2024-03-22 03:54:14.747134113 +0000 UTC m=+30.484100318"
I0322 03:54:14.766490    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="16.503967ms"
I0322 03:54:14.766745    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="129.079s"
I0322 03:54:17.584338    2297 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/coredns-6799fbcd5-4tsrq" containerName="coredns"
I0322 03:54:18.793113    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="183.786s"
I0322 03:54:19.619236    2297 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/coredns-6799fbcd5-4tsrq" podStartSLOduration=5.693000587 podCreationTimestamp="2024-03-22 03:54:03 +0000 UTC" firstStartedPulling="2024-03-22 03:54:06.654548669 +0000 UTC m=+22.391514841" lastFinishedPulling="2024-03-22 03:54:17.579557346 +0000 UTC m=+33.316523530" observedRunningTime="2024-03-22 03:54:18.800533924 +0000 UTC m=+34.537500101" watchObservedRunningTime="2024-03-22 03:54:19.618009276 +0000 UTC m=+35.354975516"
I0322 03:54:19.638357    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="15.996427ms"
I0322 03:54:19.642457    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="56.758s"
I0322 03:54:29.331775    2297 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/helm-install-traefik-jr7x9" containerName="helm"
I0322 03:54:29.334596    2297 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/helm-install-traefik-crd-489zp" containerName="helm"
I0322 03:54:29.888061    2297 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:54:29.916784    2297 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:54:29.927640    2297 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/helm-install-traefik-crd-489zp" podStartSLOduration=5.282994842 podCreationTimestamp="2024-03-22 03:54:02 +0000 UTC" firstStartedPulling="2024-03-22 03:54:06.680234206 +0000 UTC m=+22.417200380" lastFinishedPulling="2024-03-22 03:54:29.319324184 +0000 UTC m=+45.056290358" observedRunningTime="2024-03-22 03:54:29.875056268 +0000 UTC m=+45.612022456" watchObservedRunningTime="2024-03-22 03:54:29.92208482 +0000 UTC m=+45.659050997"
I0322 03:54:30.982237    2297 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:54:30.992694    2297 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
W0322 03:54:31.084083    2297 aggregator.go:166] failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 03:54:31.299722    2297 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/metrics-server-67c658944b-6v69k" containerName="metrics-server"
I0322 03:54:31.855389    2297 scope.go:117] "RemoveContainer" containerID="71c2c5ae3362fee65f40a10b23e36edac8aec70fb8c61d789443b1d56d0821ac"
I0322 03:54:31.874672    2297 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/helm-install-traefik-jr7x9" containerName="helm"
I0322 03:54:32.009528    2297 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/helm-install-traefik-jr7x9" podStartSLOduration=7.355221734 podCreationTimestamp="2024-03-22 03:54:02 +0000 UTC" firstStartedPulling="2024-03-22 03:54:06.668013769 +0000 UTC m=+22.404979941" lastFinishedPulling="2024-03-22 03:54:29.320692405 +0000 UTC m=+45.057658580" observedRunningTime="2024-03-22 03:54:29.928048479 +0000 UTC m=+45.665014657" watchObservedRunningTime="2024-03-22 03:54:32.007900373 +0000 UTC m=+47.744866556"
I0322 03:54:32.068798    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="10.089293ms"
I0322 03:54:32.190926    2297 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:54:32.208967    2297 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/metrics-server-67c658944b-6v69k" podStartSLOduration=4.561350226 podCreationTimestamp="2024-03-22 03:54:03 +0000 UTC" firstStartedPulling="2024-03-22 03:54:06.640532825 +0000 UTC m=+22.377498998" lastFinishedPulling="2024-03-22 03:54:31.288097012 +0000 UTC m=+47.025063187" observedRunningTime="2024-03-22 03:54:32.010237649 +0000 UTC m=+47.747203832" watchObservedRunningTime="2024-03-22 03:54:32.208914415 +0000 UTC m=+47.945880595"
E0322 03:54:32.593648    2297 resource_quota_controller.go:440] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I0322 03:54:32.625449    2297 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0322 03:54:32.681597    2297 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0322 03:54:32.721129    2297 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0322 03:54:32.747135    2297 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0322 03:54:32.773728    2297 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0322 03:54:32.811960    2297 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0322 03:54:32.843275    2297 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0322 03:54:32.871556    2297 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0322 03:54:32.976075    2297 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:54:33.000265    2297 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0322 03:54:33.005026    2297 trace.go:236] Trace[2072563874]: "Create" accept:application/json,audit-id:5e7e10ab-b274-4e0e-8b4e-8efad2b30655,client:10.42.0.3,protocol:HTTP/2.0,resource:customresourcedefinitions,scope:resource,url:/apis/apiextensions.k8s.io/v1/customresourcedefinitions,user-agent:Helm/3.12.3,verb:POST (22-Mar-2024 03:54:32.497) (total time: 506ms):
Trace[2072563874]: [506.116881ms] [506.116881ms] END
I0322 03:54:33.005802    2297 trace.go:236] Trace[274196202]: "Create" accept:application/json,audit-id:56960538-d7b3-4195-95b4-b93cbe657ce2,client:10.42.0.3,protocol:HTTP/2.0,resource:customresourcedefinitions,scope:resource,url:/apis/apiextensions.k8s.io/v1/customresourcedefinitions,user-agent:Helm/3.12.3,verb:POST (22-Mar-2024 03:54:32.478) (total time: 527ms):
Trace[274196202]: ["Create etcd3" audit-id:56960538-d7b3-4195-95b4-b93cbe657ce2,key:/apiextensions.k8s.io/customresourcedefinitions/middlewaretcps.traefik.containo.us,type:*apiextensions.CustomResourceDefinition,resource:customresourcedefinitions.apiextensions.k8s.io 516ms (03:54:32.489)
Trace[274196202]:  ---"Txn call succeeded" 510ms (03:54:33.005)]
Trace[274196202]: [527.233544ms] [527.233544ms] END
I0322 03:54:33.007397    2297 trace.go:236] Trace[1154512253]: "Create" accept:application/json,audit-id:631f0495-18be-4d50-84e4-509723303541,client:10.42.0.3,protocol:HTTP/2.0,resource:customresourcedefinitions,scope:resource,url:/apis/apiextensions.k8s.io/v1/customresourcedefinitions,user-agent:Helm/3.12.3,verb:POST (22-Mar-2024 03:54:32.498) (total time: 509ms):
Trace[1154512253]: ["Create etcd3" audit-id:631f0495-18be-4d50-84e4-509723303541,key:/apiextensions.k8s.io/customresourcedefinitions/middlewaretcps.traefik.io,type:*apiextensions.CustomResourceDefinition,resource:customresourcedefinitions.apiextensions.k8s.io 504ms (03:54:32.503)
Trace[1154512253]:  ---"Txn call succeeded" 501ms (03:54:33.007)]
Trace[1154512253]: [509.029812ms] [509.029812ms] END
I0322 03:54:33.048327    2297 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0322 03:54:33.057769    2297 garbagecollector.go:816] "failed to discover some groups" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"
I0322 03:54:33.073369    2297 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0322 03:54:33.086013    2297 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0322 03:54:33.132130    2297 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0322 03:54:33.218242    2297 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0322 03:54:33.560792    2297 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0322 03:54:33.579063    2297 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0322 03:54:33.600426    2297 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0322 03:54:33.612910    2297 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0322 03:54:33.615193    2297 shared_informer.go:318] Caches are synced for garbage collector
I0322 03:54:33.630180    2297 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0322 03:54:33.677185    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="55.820558ms"
I0322 03:54:33.677426    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="180.645s"
I0322 03:54:33.719627    2297 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0322 03:54:33.901423    2297 handler.go:275] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0322 03:54:33.944923    2297 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:54:34.990093    2297 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:54:35.634269    2297 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/1e9ad693-befe-4051-a6f1-ff59a7995d27-klipper-cache\") pod \"1e9ad693-befe-4051-a6f1-ff59a7995d27\" (UID: \"1e9ad693-befe-4051-a6f1-ff59a7995d27\") "
I0322 03:54:35.635032    2297 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-sk2wr\" (UniqueName: \"kubernetes.io/projected/1e9ad693-befe-4051-a6f1-ff59a7995d27-kube-api-access-sk2wr\") pod \"1e9ad693-befe-4051-a6f1-ff59a7995d27\" (UID: \"1e9ad693-befe-4051-a6f1-ff59a7995d27\") "
I0322 03:54:35.635063    2297 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/1e9ad693-befe-4051-a6f1-ff59a7995d27-klipper-config\") pod \"1e9ad693-befe-4051-a6f1-ff59a7995d27\" (UID: \"1e9ad693-befe-4051-a6f1-ff59a7995d27\") "
I0322 03:54:35.635083    2297 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/1e9ad693-befe-4051-a6f1-ff59a7995d27-klipper-helm\") pod \"1e9ad693-befe-4051-a6f1-ff59a7995d27\" (UID: \"1e9ad693-befe-4051-a6f1-ff59a7995d27\") "
I0322 03:54:35.635107    2297 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"content\" (UniqueName: \"kubernetes.io/configmap/1e9ad693-befe-4051-a6f1-ff59a7995d27-content\") pod \"1e9ad693-befe-4051-a6f1-ff59a7995d27\" (UID: \"1e9ad693-befe-4051-a6f1-ff59a7995d27\") "
I0322 03:54:35.635127    2297 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/1e9ad693-befe-4051-a6f1-ff59a7995d27-tmp\") pod \"1e9ad693-befe-4051-a6f1-ff59a7995d27\" (UID: \"1e9ad693-befe-4051-a6f1-ff59a7995d27\") "
I0322 03:54:35.635162    2297 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"values\" (UniqueName: \"kubernetes.io/secret/1e9ad693-befe-4051-a6f1-ff59a7995d27-values\") pod \"1e9ad693-befe-4051-a6f1-ff59a7995d27\" (UID: \"1e9ad693-befe-4051-a6f1-ff59a7995d27\") "
I0322 03:54:35.671674    2297 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/1e9ad693-befe-4051-a6f1-ff59a7995d27-kube-api-access-sk2wr" (OuterVolumeSpecName: "kube-api-access-sk2wr") pod "1e9ad693-befe-4051-a6f1-ff59a7995d27" (UID: "1e9ad693-befe-4051-a6f1-ff59a7995d27"). InnerVolumeSpecName "kube-api-access-sk2wr". PluginName "kubernetes.io/projected", VolumeGidValue ""
I0322 03:54:35.676267    2297 alloc.go:330] "allocated clusterIPs" service="kube-system/traefik" clusterIPs={"IPv4":"10.43.55.214"}
I0322 03:54:35.677525    2297 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/1e9ad693-befe-4051-a6f1-ff59a7995d27-klipper-cache" (OuterVolumeSpecName: "klipper-cache") pod "1e9ad693-befe-4051-a6f1-ff59a7995d27" (UID: "1e9ad693-befe-4051-a6f1-ff59a7995d27"). InnerVolumeSpecName "klipper-cache". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0322 03:54:35.677947    2297 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/1e9ad693-befe-4051-a6f1-ff59a7995d27-klipper-helm" (OuterVolumeSpecName: "klipper-helm") pod "1e9ad693-befe-4051-a6f1-ff59a7995d27" (UID: "1e9ad693-befe-4051-a6f1-ff59a7995d27"). InnerVolumeSpecName "klipper-helm". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0322 03:54:35.678548    2297 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/1e9ad693-befe-4051-a6f1-ff59a7995d27-values" (OuterVolumeSpecName: "values") pod "1e9ad693-befe-4051-a6f1-ff59a7995d27" (UID: "1e9ad693-befe-4051-a6f1-ff59a7995d27"). InnerVolumeSpecName "values". PluginName "kubernetes.io/secret", VolumeGidValue ""
I0322 03:54:35.699845    2297 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/1e9ad693-befe-4051-a6f1-ff59a7995d27-tmp" (OuterVolumeSpecName: "tmp") pod "1e9ad693-befe-4051-a6f1-ff59a7995d27" (UID: "1e9ad693-befe-4051-a6f1-ff59a7995d27"). InnerVolumeSpecName "tmp". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0322 03:54:35.725763    2297 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/configmap/1e9ad693-befe-4051-a6f1-ff59a7995d27-content" (OuterVolumeSpecName: "content") pod "1e9ad693-befe-4051-a6f1-ff59a7995d27" (UID: "1e9ad693-befe-4051-a6f1-ff59a7995d27"). InnerVolumeSpecName "content". PluginName "kubernetes.io/configmap", VolumeGidValue ""
I0322 03:54:35.737828    2297 reconciler_common.go:300] "Volume detached for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/1e9ad693-befe-4051-a6f1-ff59a7995d27-klipper-helm\") on node \"server\" DevicePath \"\""
I0322 03:54:35.737990    2297 reconciler_common.go:300] "Volume detached for volume \"content\" (UniqueName: \"kubernetes.io/configmap/1e9ad693-befe-4051-a6f1-ff59a7995d27-content\") on node \"server\" DevicePath \"\""
I0322 03:54:35.738150    2297 reconciler_common.go:300] "Volume detached for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/1e9ad693-befe-4051-a6f1-ff59a7995d27-tmp\") on node \"server\" DevicePath \"\""
I0322 03:54:35.738242    2297 reconciler_common.go:300] "Volume detached for volume \"values\" (UniqueName: \"kubernetes.io/secret/1e9ad693-befe-4051-a6f1-ff59a7995d27-values\") on node \"server\" DevicePath \"\""
I0322 03:54:35.738329    2297 reconciler_common.go:300] "Volume detached for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/1e9ad693-befe-4051-a6f1-ff59a7995d27-klipper-cache\") on node \"server\" DevicePath \"\""
I0322 03:54:35.738500    2297 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-sk2wr\" (UniqueName: \"kubernetes.io/projected/1e9ad693-befe-4051-a6f1-ff59a7995d27-kube-api-access-sk2wr\") on node \"server\" DevicePath \"\""
I0322 03:54:35.738756    2297 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Service" apiVersion="v1" type="Normal" reason="EnsuringLoadBalancer" message="Ensuring load balancer"
I0322 03:54:35.739146    2297 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/1e9ad693-befe-4051-a6f1-ff59a7995d27-klipper-config" (OuterVolumeSpecName: "klipper-config") pod "1e9ad693-befe-4051-a6f1-ff59a7995d27" (UID: "1e9ad693-befe-4051-a6f1-ff59a7995d27"). InnerVolumeSpecName "klipper-config". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0322 03:54:35.749876    2297 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:54:35.848550    2297 reconciler_common.go:300] "Volume detached for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/1e9ad693-befe-4051-a6f1-ff59a7995d27-klipper-config\") on node \"server\" DevicePath \"\""
I0322 03:54:35.874170    2297 controller.go:624] quota admission added evaluator for: ingressroutes.traefik.io
I0322 03:54:35.877051    2297 controller.go:624] quota admission added evaluator for: daemonsets.apps
I0322 03:54:35.887819    2297 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set traefik-f4564c4f4 to 1"
I0322 03:54:35.928147    2297 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Service" apiVersion="v1" type="Normal" reason="AppliedDaemonSet" message="Applied LoadBalancer DaemonSet kube-system/svclb-traefik-2c133370"
I0322 03:54:35.986391    2297 event.go:307] "Event occurred" object="kube-system/traefik-f4564c4f4" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: traefik-f4564c4f4-d2b8m"
I0322 03:54:35.991128    2297 controller.go:624] quota admission added evaluator for: controllerrevisions.apps
I0322 03:54:36.062553    2297 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="40c9ecc3e61ba35ded6f3d69568a362d557362dcdef32fd246f081168daea0b4"
I0322 03:54:36.078013    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="210.124065ms"
I0322 03:54:36.142971    2297 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:54:36.173626    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="88.645674ms"
I0322 03:54:36.173873    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="65.781s"
I0322 03:54:36.174456    2297 topology_manager.go:215] "Topology Admit Handler" podUID="071534ad-b847-4778-843c-690278292cda" podNamespace="kube-system" podName="traefik-f4564c4f4-d2b8m"
E0322 03:54:36.176578    2297 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="1e9ad693-befe-4051-a6f1-ff59a7995d27" containerName="helm"
I0322 03:54:36.177286    2297 memory_manager.go:346] "RemoveStaleState removing state" podUID="1e9ad693-befe-4051-a6f1-ff59a7995d27" containerName="helm"
I0322 03:54:36.178615    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="434.974s"
I0322 03:54:36.204726    2297 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:54:36.212701    2297 event.go:307] "Event occurred" object="kube-system/svclb-traefik-2c133370" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: svclb-traefik-2c133370-x894w"
I0322 03:54:36.225909    2297 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:54:36.245896    2297 topology_manager.go:215] "Topology Admit Handler" podUID="785c2151-5c9a-45ba-be8e-a0e936605914" podNamespace="kube-system" podName="svclb-traefik-2c133370-x894w"
I0322 03:54:36.250943    2297 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 03:54:36.251310    2297 event.go:307] "Event occurred" object="kube-system/helm-install-traefik-crd" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="Completed" message="Job completed"
I0322 03:54:36.254005    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="43.24s"
I0322 03:54:36.269558    2297 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-4b65q\" (UniqueName: \"kubernetes.io/projected/071534ad-b847-4778-843c-690278292cda-kube-api-access-4b65q\") pod \"traefik-f4564c4f4-d2b8m\" (UID: \"071534ad-b847-4778-843c-690278292cda\") " pod="kube-system/traefik-f4564c4f4-d2b8m"
I0322 03:54:36.269603    2297 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"data\" (UniqueName: \"kubernetes.io/empty-dir/071534ad-b847-4778-843c-690278292cda-data\") pod \"traefik-f4564c4f4-d2b8m\" (UID: \"071534ad-b847-4778-843c-690278292cda\") " pod="kube-system/traefik-f4564c4f4-d2b8m"
I0322 03:54:36.269663    2297 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/071534ad-b847-4778-843c-690278292cda-tmp\") pod \"traefik-f4564c4f4-d2b8m\" (UID: \"071534ad-b847-4778-843c-690278292cda\") " pod="kube-system/traefik-f4564c4f4-d2b8m"
I0322 03:54:37.056192    2297 scope.go:117] "RemoveContainer" containerID="71c2c5ae3362fee65f40a10b23e36edac8aec70fb8c61d789443b1d56d0821ac"
I0322 03:54:37.091160    2297 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:54:38.110925    2297 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:54:38.113524    2297 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:54:38.349249    2297 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:54:38.406593    2297 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/672ca8db-0327-42dc-912b-c1a93de456bf-tmp\") pod \"672ca8db-0327-42dc-912b-c1a93de456bf\" (UID: \"672ca8db-0327-42dc-912b-c1a93de456bf\") "
I0322 03:54:38.407583    2297 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/672ca8db-0327-42dc-912b-c1a93de456bf-klipper-helm\") pod \"672ca8db-0327-42dc-912b-c1a93de456bf\" (UID: \"672ca8db-0327-42dc-912b-c1a93de456bf\") "
I0322 03:54:38.407666    2297 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/672ca8db-0327-42dc-912b-c1a93de456bf-klipper-cache\") pod \"672ca8db-0327-42dc-912b-c1a93de456bf\" (UID: \"672ca8db-0327-42dc-912b-c1a93de456bf\") "
I0322 03:54:38.407918    2297 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-phtm9\" (UniqueName: \"kubernetes.io/projected/672ca8db-0327-42dc-912b-c1a93de456bf-kube-api-access-phtm9\") pod \"672ca8db-0327-42dc-912b-c1a93de456bf\" (UID: \"672ca8db-0327-42dc-912b-c1a93de456bf\") "
I0322 03:54:38.407962    2297 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/672ca8db-0327-42dc-912b-c1a93de456bf-klipper-config\") pod \"672ca8db-0327-42dc-912b-c1a93de456bf\" (UID: \"672ca8db-0327-42dc-912b-c1a93de456bf\") "
I0322 03:54:38.408009    2297 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"values\" (UniqueName: \"kubernetes.io/secret/672ca8db-0327-42dc-912b-c1a93de456bf-values\") pod \"672ca8db-0327-42dc-912b-c1a93de456bf\" (UID: \"672ca8db-0327-42dc-912b-c1a93de456bf\") "
I0322 03:54:38.408361    2297 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"content\" (UniqueName: \"kubernetes.io/configmap/672ca8db-0327-42dc-912b-c1a93de456bf-content\") pod \"672ca8db-0327-42dc-912b-c1a93de456bf\" (UID: \"672ca8db-0327-42dc-912b-c1a93de456bf\") "
I0322 03:54:38.411311    2297 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/configmap/672ca8db-0327-42dc-912b-c1a93de456bf-content" (OuterVolumeSpecName: "content") pod "672ca8db-0327-42dc-912b-c1a93de456bf" (UID: "672ca8db-0327-42dc-912b-c1a93de456bf"). InnerVolumeSpecName "content". PluginName "kubernetes.io/configmap", VolumeGidValue ""
I0322 03:54:38.413380    2297 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/672ca8db-0327-42dc-912b-c1a93de456bf-klipper-helm" (OuterVolumeSpecName: "klipper-helm") pod "672ca8db-0327-42dc-912b-c1a93de456bf" (UID: "672ca8db-0327-42dc-912b-c1a93de456bf"). InnerVolumeSpecName "klipper-helm". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0322 03:54:38.416086    2297 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/672ca8db-0327-42dc-912b-c1a93de456bf-klipper-cache" (OuterVolumeSpecName: "klipper-cache") pod "672ca8db-0327-42dc-912b-c1a93de456bf" (UID: "672ca8db-0327-42dc-912b-c1a93de456bf"). InnerVolumeSpecName "klipper-cache". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0322 03:54:38.419005    2297 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/672ca8db-0327-42dc-912b-c1a93de456bf-klipper-config" (OuterVolumeSpecName: "klipper-config") pod "672ca8db-0327-42dc-912b-c1a93de456bf" (UID: "672ca8db-0327-42dc-912b-c1a93de456bf"). InnerVolumeSpecName "klipper-config". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0322 03:54:38.423653    2297 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/672ca8db-0327-42dc-912b-c1a93de456bf-kube-api-access-phtm9" (OuterVolumeSpecName: "kube-api-access-phtm9") pod "672ca8db-0327-42dc-912b-c1a93de456bf" (UID: "672ca8db-0327-42dc-912b-c1a93de456bf"). InnerVolumeSpecName "kube-api-access-phtm9". PluginName "kubernetes.io/projected", VolumeGidValue ""
I0322 03:54:38.425204    2297 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/672ca8db-0327-42dc-912b-c1a93de456bf-tmp" (OuterVolumeSpecName: "tmp") pod "672ca8db-0327-42dc-912b-c1a93de456bf" (UID: "672ca8db-0327-42dc-912b-c1a93de456bf"). InnerVolumeSpecName "tmp". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0322 03:54:38.430788    2297 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/672ca8db-0327-42dc-912b-c1a93de456bf-values" (OuterVolumeSpecName: "values") pod "672ca8db-0327-42dc-912b-c1a93de456bf" (UID: "672ca8db-0327-42dc-912b-c1a93de456bf"). InnerVolumeSpecName "values". PluginName "kubernetes.io/secret", VolumeGidValue ""
I0322 03:54:38.516818    2297 reconciler_common.go:300] "Volume detached for volume \"content\" (UniqueName: \"kubernetes.io/configmap/672ca8db-0327-42dc-912b-c1a93de456bf-content\") on node \"server\" DevicePath \"\""
I0322 03:54:38.516856    2297 reconciler_common.go:300] "Volume detached for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/672ca8db-0327-42dc-912b-c1a93de456bf-klipper-helm\") on node \"server\" DevicePath \"\""
I0322 03:54:38.516870    2297 reconciler_common.go:300] "Volume detached for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/672ca8db-0327-42dc-912b-c1a93de456bf-klipper-cache\") on node \"server\" DevicePath \"\""
I0322 03:54:38.516881    2297 reconciler_common.go:300] "Volume detached for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/672ca8db-0327-42dc-912b-c1a93de456bf-tmp\") on node \"server\" DevicePath \"\""
I0322 03:54:38.516895    2297 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-phtm9\" (UniqueName: \"kubernetes.io/projected/672ca8db-0327-42dc-912b-c1a93de456bf-kube-api-access-phtm9\") on node \"server\" DevicePath \"\""
I0322 03:54:38.516919    2297 reconciler_common.go:300] "Volume detached for volume \"values\" (UniqueName: \"kubernetes.io/secret/672ca8db-0327-42dc-912b-c1a93de456bf-values\") on node \"server\" DevicePath \"\""
I0322 03:54:38.516944    2297 reconciler_common.go:300] "Volume detached for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/672ca8db-0327-42dc-912b-c1a93de456bf-klipper-config\") on node \"server\" DevicePath \"\""
I0322 03:54:39.090318    2297 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="0625efc1b94159e645b5f5c6ad71640b0432fbd446467f4e3ad471d87a7f19f8"
I0322 03:54:39.111167    2297 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:54:39.129064    2297 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:54:39.138978    2297 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:54:39.146251    2297 event.go:307] "Event occurred" object="kube-system/helm-install-traefik" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="Completed" message="Job completed"
I0322 03:54:39.146564    2297 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:54:44.474286    2297 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/svclb-traefik-2c133370-x894w" containerName="lb-tcp-80"
I0322 03:54:44.688194    2297 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/svclb-traefik-2c133370-x894w" containerName="lb-tcp-443"
I0322 03:54:45.163356    2297 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/svclb-traefik-2c133370-x894w" podStartSLOduration=1.505315023 podCreationTimestamp="2024-03-22 03:54:36 +0000 UTC" firstStartedPulling="2024-03-22 03:54:36.811844164 +0000 UTC m=+52.548810337" lastFinishedPulling="2024-03-22 03:54:44.466127639 +0000 UTC m=+60.203093814" observedRunningTime="2024-03-22 03:54:45.157226008 +0000 UTC m=+60.894192186" watchObservedRunningTime="2024-03-22 03:54:45.1595985 +0000 UTC m=+60.896564668"
I0322 03:54:45.218092    2297 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Service" apiVersion="v1" type="Normal" reason="UpdatedLoadBalancer" message="Updated LoadBalancer with new IPs: [] -> [192.168.56.110]"
I0322 03:54:47.444141    2297 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/traefik-f4564c4f4-d2b8m" containerName="traefik"
I0322 03:54:48.471334    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="4.006989ms"
I0322 03:54:49.230214    2297 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/traefik-f4564c4f4-d2b8m" podStartSLOduration=3.625044297 podCreationTimestamp="2024-03-22 03:54:35 +0000 UTC" firstStartedPulling="2024-03-22 03:54:36.838454427 +0000 UTC m=+52.575420595" lastFinishedPulling="2024-03-22 03:54:47.438537979 +0000 UTC m=+63.175504155" observedRunningTime="2024-03-22 03:54:48.449431243 +0000 UTC m=+64.186397422" watchObservedRunningTime="2024-03-22 03:54:49.225127857 +0000 UTC m=+64.962094035"
I0322 03:54:49.246006    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="20.111602ms"
I0322 03:54:49.246109    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="40.582s"
E0322 03:57:51.813365    2297 health_controller.go:162] Metrics Controller heartbeat missed
I0322 03:57:45.739842    2297 request.go:697] Waited for 54.458372501s due to client-side throttling, not priority and fairness, request: GET:https://127.0.0.1:6444/api
E0322 03:59:29.997960    2297 health_controller.go:162] Metrics Controller heartbeat missed
W0322 03:59:35.301003    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:35.357482    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.DaemonSet ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:36.007064    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:39.713356    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:41.097739    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:42.525127    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I0322 03:59:42.750775    2297 request.go:697] Waited for 55.276146907s due to client-side throttling, not priority and fairness, request: GET:https://127.0.0.1:6444/api
W0322 03:59:42.808067    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.StorageClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:43.191119    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.RoleBinding ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:43.207559    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Lease ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:43.215045    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Namespace ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:43.636437    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.StorageClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:48.538549    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Namespace ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:42.033115    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PodDisruptionBudget ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:59:48.539791    2297 remote_runtime.go:633] "Status from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded"
W0322 03:59:48.545409    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1beta3.PriorityLevelConfiguration ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:59:48.546550    2297 kubelet.go:2840] "Container runtime sanity check failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded"
I0322 03:59:48.623169    2297 trace.go:236] Trace[1167346859]: "Calculate volume metrics of tmp for pod kube-system/traefik-f4564c4f4-d2b8m" (22-Mar-2024 03:59:42.262) (total time: 6358ms):
Trace[1167346859]: [6.358346799s] [6.358346799s] END
W0322 03:59:42.291169    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.MutatingWebhookConfiguration ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:48.630984    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:48.631864    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:48.632907    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.LimitRange ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:48.635150    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.StatefulSet ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:48.635254    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.RoleBinding ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:48.637847    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ReplicationController ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:48.638584    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ResourceQuota ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:48.640195    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PodDisruptionBudget ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:48.682688    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:48.684293    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.RuntimeClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:48.684321    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ServiceAccount ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:48.684345    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:48.684356    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Secret ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:48.684374    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CustomResourceDefinition ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:48.685917    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ClusterRoleBinding ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:48.692354    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:48.692405    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:48.693703    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Endpoints ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:48.693729    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.RuntimeClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:48.697093    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:48.698639    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ClusterRole ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I0322 03:59:48.699558    2297 trace.go:236] Trace[1862793801]: "iptables ChainExists" (22-Mar-2024 03:55:57.082) (total time: 231616ms):
Trace[1862793801]: [3m51.616584458s] [3m51.616584458s] END
W0322 03:59:48.699810    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PriorityClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:48.724758    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:48.724996    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Lease ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:48.735048    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.IngressClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:48.747202    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ValidatingWebhookConfiguration ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:48.751907    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PersistentVolume ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:48.782991    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.DaemonSet ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:48.792879    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.381070    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.StorageClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:48.797853    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ResourceQuota ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:48.798787    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ServiceAccount ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:49.040059    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.RuntimeClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:49.102150    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ReplicaSet ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:59:49.133647    2297 remote_runtime.go:294] "ListPodSandbox with filter from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" filter="nil"
E0322 03:59:49.161429    2297 remote_runtime.go:407] "ListContainers with filter from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" filter="&ContainerFilter{Id:,State:nil,PodSandboxId:,LabelSelector:map[string]string{},}"
E0322 03:59:49.164434    2297 container_log_manager.go:185] "Failed to rotate container logs" err="failed to list containers: rpc error: code = DeadlineExceeded desc = context deadline exceeded"
time="2024-03-22T03:59:25Z" level=info msg="error in remotedialer server [400]: read tcp 192.168.56.110:6443->192.168.56.110:35450: i/o timeout"
time="2024-03-22T03:59:48Z" level=error msg="Compact failed: failed to begin transaction: context deadline exceeded"
W0322 03:59:48.682059    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Lease ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.486918    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I0322 03:59:49.396340    2297 trace.go:236] Trace[589848354]: "DeltaFIFO Pop Process" ID:traefikservices.traefik.containo.us,Depth:21,Reason:slow event handlers blocking the queue (22-Mar-2024 03:59:48.545) (total time: 845ms):
Trace[589848354]: [845.335683ms] [845.335683ms] END
W0322 03:59:45.488622    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
time="2024-03-22T03:59:36Z" level=error msg="Error writing ping" error="write tcp 192.168.56.110:35450->192.168.56.110:6443: i/o timeout"
time="2024-03-22T03:59:49Z" level=error msg="Error writing ping" error="write tcp 192.168.56.110:35450->192.168.56.110:6443: i/o timeout"
time="2024-03-22T03:59:41Z" level=info msg="Slow SQL (started: 2024-03-22 03:55:00.366983428 +0000 UTC m=+76.103949614) (total time: 2m29.283578438s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC LIMIT 500 : [[% 668]]"
W0322 03:59:45.489983    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Ingress ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
time="2024-03-22T03:59:47Z" level=info msg="Slow SQL (started: 2024-03-22 03:56:19.918934345 +0000 UTC m=+155.655900550) (total time: 3m28.036865184s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC LIMIT 1 : [[/registry/masterleases/192.168.56.110 true]]"
W0322 03:59:45.489607    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:49.513250    2297 reflector.go:458] object-"kube-system"/"kube-root-ca.crt": watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
time="2024-03-22T03:59:49Z" level=error msg="Remotedialer proxy error" error="read tcp 192.168.56.110:35450->192.168.56.110:6443: i/o timeout"
W0322 03:59:45.490469    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.490590    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.IngressClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.491340    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.491126    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PersistentVolumeClaim ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.491489    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.491807    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CSIStorageCapacity ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.492540    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Role ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.491603    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.492656    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.492962    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.493199    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CronJob ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.493336    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ReplicationController ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.493531    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.MutatingWebhookConfiguration ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.494088    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ValidatingWebhookConfiguration ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.494690    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.526544    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CertificateSigningRequest ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.528135    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CSINode ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.528696    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.528879    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.529039    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.StatefulSet ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.529599    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ControllerRevision ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.529830    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Endpoints ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.529944    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.530167    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.530284    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ClusterRole ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.530578    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1beta3.PriorityLevelConfiguration ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.530748    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Deployment ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.530982    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PriorityClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.531490    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Job ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.531814    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.531950    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.532051    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.VolumeAttachment ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.532360    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1beta3.FlowSchema ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.533102    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PersistentVolume ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.533226    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CSIDriver ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.533498    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PodTemplate ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.533624    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.NetworkPolicy ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.533746    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ClusterRoleBinding ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.533836    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.534031    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ReplicaSet ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.534323    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.EndpointSlice ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.534360    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Secret ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.610418    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Endpoints ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.676560    2297 reflector.go:458] object-"kube-system"/"local-path-config": watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.685575    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.688235    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.688624    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CSIDriver ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.689452    2297 reflector.go:458] object-"kube-system"/"coredns-custom": watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.760212    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CSINode ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:59:46.071836    2297 health_controller.go:162] Metrics Controller heartbeat missed
E0322 03:59:49.800806    2297 health_controller.go:162] Metrics Controller heartbeat missed
E0322 03:59:46.243853    2297 controller.go:113] loading OpenAPI spec for "k8s_internal_local_delegation_chain_0000000001" failed with: Error, could not get list of group versions for APIService
I0322 03:59:49.811201    2297 controller.go:126] OpenAPI AggregationController: action for item k8s_internal_local_delegation_chain_0000000001: Rate Limited Requeue.
W0322 03:59:46.343626    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.NetworkPolicy ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I0322 03:59:46.348727    2297 trace.go:236] Trace[713975380]: "DeltaFIFO Pop Process" ID:v2.autoscaling,Depth:24,Reason:slow event handlers blocking the queue (22-Mar-2024 03:59:46.101) (total time: 245ms):
Trace[713975380]: [245.505741ms] [245.505741ms] END
W0322 03:59:46.349697    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Namespace ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:46.350520    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:46.358542    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:46.358557    2297 reflector.go:458] object-"kube-system"/"coredns": watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:46.361454    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CSIDriver ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:46.540464    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PersistentVolume ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:47.073966    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:59:47.142041    2297 remote_image.go:128] "ListImages with filter from image service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" filter="nil"
E0322 03:59:49.963399    2297 kuberuntime_image.go:103] "Failed to list images" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded"
I0322 03:59:49.966938    2297 image_gc_manager.go:210] "Failed to update image list" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded"
W0322 03:59:47.627549    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.HelmChart ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:36.664496    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ClusterRoleBinding ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I0322 03:59:47.673842    2297 handler.go:275] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
W0322 03:59:47.677844    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ServiceAccount ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:47.688615    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.LimitRange ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:47.751047    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Addon ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:47.754950    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:47.946301    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:47.946258    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:48.017316    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:48.019597    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:48.080458    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:48.145032    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:48.145103    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.EndpointSlice ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:48.149687    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Job ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:48.151209    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Secret ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:48.151263    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.HelmChartConfig ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:48.459724    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.EndpointSlice ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:48.474149    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:48.474741    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1beta3.FlowSchema ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:48.479115    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:48.479755    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PersistentVolumeClaim ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:48.516760    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v2.HorizontalPodAutoscaler ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:48.520174    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.VolumeAttachment ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:48.520600    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.APIService ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:48.521181    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:45.492841    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Namespace ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:59:50.101031    2297 watch.go:287] unable to encode watch object *v1.WatchEvent: client disconnected (&streaming.encoderWithAllocator{writer:responsewriter.outerWithCloseNotifyAndFlush{UserProvidedDecorator:(*metrics.ResponseWriterDelegator)(0xc00ebe23c0), InnerCloseNotifierFlusher:struct { httpsnoop.Unwrapper; http.ResponseWriter; http.Flusher; http.CloseNotifier; http.Pusher }{Unwrapper:(*httpsnoop.rw)(0xc00d5f0640), ResponseWriter:(*httpsnoop.rw)(0xc00d5f0640), Flusher:(*httpsnoop.rw)(0xc00d5f0640), CloseNotifier:(*httpsnoop.rw)(0xc00d5f0640), Pusher:(*httpsnoop.rw)(0xc00d5f0640)}}, encoder:(*versioning.codec)(0xc00f59cc80), memAllocator:(*runtime.Allocator)(0xc00ce653c8)})
W0322 03:59:50.152896    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:50.178228    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:47.440573    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CSIStorageCapacity ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I0322 03:59:50.392922    2297 trace.go:236] Trace[1329970982]: "iptables ChainExists" (22-Mar-2024 03:55:29.018) (total time: 261371ms):
Trace[1329970982]: [4m21.371089334s] [4m21.371089334s] END
E0322 03:59:50.467738    2297 kuberuntime_sandbox.go:297] "Failed to list pod sandboxes" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded"
E0322 03:59:50.469157    2297 generic.go:238] "GenericPLEG: Unable to retrieve pods" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded"
I0322 03:59:50.556155    2297 request.go:697] Waited for 2.392435335s due to client-side throttling, not priority and fairness, request: POST:https://127.0.0.1:6443/api/v1/namespaces/kube-system/events
I0322 03:59:50.654514    2297 trace.go:236] Trace[776826545]: "DeltaFIFO Pop Process" ID:ingressrouteudps.traefik.io,Depth:19,Reason:slow event handlers blocking the queue (22-Mar-2024 03:59:49.478) (total time: 242ms):
Trace[776826545]: [242.964032ms] [242.964032ms] END
W0322 03:59:50.818428    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Role ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I0322 03:59:50.929500    2297 log.go:245] http: TLS handshake error from 10.42.0.5:46354: EOF
W0322 03:59:50.959692    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 03:59:51.036014    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 03:59:51.773586    2297 server.go:310] "Unable to authenticate the request due to an error" err="context canceled"
I0322 03:59:51.986722    2297 log.go:245] http: TLS handshake error from 10.42.0.5:37820: write tcp 192.168.56.110:10250->10.42.0.5:37820: write: broken pipe
I0322 03:59:51.991729    2297 log.go:245] http: TLS handshake error from 10.42.0.5:41402: write tcp 192.168.56.110:10250->10.42.0.5:41402: write: broken pipe
I0322 03:59:51.991775    2297 log.go:245] http: TLS handshake error from 10.42.0.5:58954: write tcp 192.168.56.110:10250->10.42.0.5:58954: write: broken pipe
I0322 03:59:51.991795    2297 log.go:245] http: TLS handshake error from 10.42.0.5:35204: write tcp 192.168.56.110:10250->10.42.0.5:35204: write: broken pipe
I0322 03:59:51.991809    2297 log.go:245] http: TLS handshake error from 10.42.0.5:58368: write tcp 192.168.56.110:10250->10.42.0.5:58368: write: broken pipe
I0322 03:59:51.991823    2297 log.go:245] http: TLS handshake error from 10.42.0.5:35874: write tcp 192.168.56.110:10250->10.42.0.5:35874: write: broken pipe
I0322 03:59:51.991839    2297 log.go:245] http: TLS handshake error from 10.42.0.5:50276: write tcp 192.168.56.110:10250->10.42.0.5:50276: write: broken pipe
I0322 03:59:51.991871    2297 log.go:245] http: TLS handshake error from 10.42.0.5:47210: write tcp 192.168.56.110:10250->10.42.0.5:47210: write: broken pipe
I0322 03:59:51.991886    2297 log.go:245] http: TLS handshake error from 10.42.0.5:38046: write tcp 192.168.56.110:10250->10.42.0.5:38046: write: broken pipe
I0322 03:59:51.991900    2297 log.go:245] http: TLS handshake error from 10.42.0.5:44160: write tcp 192.168.56.110:10250->10.42.0.5:44160: write: broken pipe
I0322 03:59:51.991919    2297 log.go:245] http: TLS handshake error from 10.42.0.5:39002: write tcp 192.168.56.110:10250->10.42.0.5:39002: write: broken pipe
I0322 03:59:51.991932    2297 log.go:245] http: TLS handshake error from 10.42.0.5:49228: write tcp 192.168.56.110:10250->10.42.0.5:49228: write: broken pipe
I0322 03:59:51.991945    2297 log.go:245] http: TLS handshake error from 10.42.0.5:57764: EOF
I0322 03:59:51.991958    2297 log.go:245] http: TLS handshake error from 10.42.0.5:56982: EOF
I0322 03:59:51.991972    2297 log.go:245] http: TLS handshake error from 10.42.0.5:46046: EOF
I0322 03:59:51.991988    2297 log.go:245] http: TLS handshake error from 10.42.0.5:34820: write tcp 192.168.56.110:10250->10.42.0.5:34820: write: broken pipe
E0322 03:59:52.073747    2297 kubelet_node_status.go:540] "Error updating node status, will retry" err="error getting node \"server\": Get \"https://127.0.0.1:6443/api/v1/nodes/server?resourceVersion=0&timeout=10s\": context deadline exceeded"
time="2024-03-22T03:59:52Z" level=info msg="Slow SQL (started: 2024-03-22 03:59:50.368776766 +0000 UTC m=+366.105742945) (total time: 1.049916993s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC LIMIT 500 : [[% 668]]"
I0322 03:59:52.600785    2297 trace.go:236] Trace[329560042]: "iptables ChainExists" (22-Mar-2024 03:55:35.561) (total time: 255358ms):
Trace[329560042]: [4m15.3586912s] [4m15.3586912s] END
I0322 03:59:52.619137    2297 trace.go:236] Trace[1933446719]: "DeltaFIFO Pop Process" ID:middlewares.traefik.io,Depth:17,Reason:slow event handlers blocking the queue (22-Mar-2024 03:59:50.657) (total time: 783ms):
Trace[1933446719]: [783.11403ms] [783.11403ms] END
E0322 03:59:53.131088    2297 watch.go:287] unable to encode watch object *v1.WatchEvent: client disconnected (&streaming.encoderWithAllocator{writer:responsewriter.outerWithCloseNotifyAndFlush{UserProvidedDecorator:(*metrics.ResponseWriterDelegator)(0xc00f064150), InnerCloseNotifierFlusher:struct { httpsnoop.Unwrapper; http.ResponseWriter; http.Flusher; http.CloseNotifier; http.Pusher }{Unwrapper:(*httpsnoop.rw)(0xc00db21950), ResponseWriter:(*httpsnoop.rw)(0xc00db21950), Flusher:(*httpsnoop.rw)(0xc00db21950), CloseNotifier:(*httpsnoop.rw)(0xc00db21950), Pusher:(*httpsnoop.rw)(0xc00db21950)}}, encoder:(*versioning.codec)(0xc00a06dcc0), memAllocator:(*runtime.Allocator)(0xc00d108438)})
time="2024-03-22T03:59:53Z" level=info msg="Slow SQL (started: 2024-03-22 03:59:52.369027099 +0000 UTC m=+368.105993283) (total time: 1.156901978s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/events/% false]]"
I0322 03:59:53.986465    2297 trace.go:236] Trace[337404174]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:419b5334-c8db-41e0-8710-9cc6b4a6438e,client:127.0.0.1,protocol:HTTP/2.0,resource:serviceaccounts,scope:cluster,url:/api/v1/serviceaccounts,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:59:53.440) (total time: 540ms):
Trace[337404174]: ---"Writing http response done" count:41 538ms (03:59:53.980)
Trace[337404174]: [540.342339ms] [540.342339ms] END
I0322 03:59:53.998667    2297 trace.go:236] Trace[1809945613]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:d664ca03-35c0-4080-8a7e-166477eb1f46,client:127.0.0.1,protocol:HTTP/2.0,resource:rolebindings,scope:cluster,url:/apis/rbac.authorization.k8s.io/v1/rolebindings,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:59:53.453) (total time: 544ms):
Trace[1809945613]: ---"Writing http response done" count:9 541ms (03:59:53.998)
Trace[1809945613]: [544.92487ms] [544.92487ms] END
I0322 03:59:53.998927    2297 trace.go:236] Trace[542309550]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:f85933d4-ee7a-4a5f-9f91-0fb9a57715bd,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:namespace,url:/api/v1/namespaces/kube-system/configmaps,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:59:53.460) (total time: 538ms):
Trace[542309550]: ---"Writing http response done" count:8 538ms (03:59:53.998)
Trace[542309550]: [538.550593ms] [538.550593ms] END
I0322 03:59:53.999153    2297 trace.go:236] Trace[866579405]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:7f0b8a26-8fa0-4da1-b126-a8ff4c34535e,client:127.0.0.1,protocol:HTTP/2.0,resource:clusterrolebindings,scope:cluster,url:/apis/rbac.authorization.k8s.io/v1/clusterrolebindings,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:59:53.480) (total time: 518ms):
Trace[866579405]: ---"Writing http response done" count:55 517ms (03:59:53.999)
Trace[866579405]: [518.49782ms] [518.49782ms] END
I0322 03:59:54.086482    2297 trace.go:236] Trace[476212458]: "iptables ChainExists" (22-Mar-2024 03:55:57.019) (total time: 233644ms):
Trace[476212458]: [3m53.6443068s] [3m53.6443068s] END
time="2024-03-22T03:59:54Z" level=info msg="Slow SQL (started: 2024-03-22 03:59:51.637846169 +0000 UTC m=+367.374812345) (total time: 1.06834734s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC  : [[/registry/ranges/serviceips false]]"
time="2024-03-22T03:59:54Z" level=info msg="Slow SQL (started: 2024-03-22 03:59:53.080622887 +0000 UTC m=+368.817589074) (total time: 1.20272535s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/volumeattachments/% 667]]"
time="2024-03-22T03:59:54Z" level=info msg="Slow SQL (started: 2024-03-22 03:59:53.091362422 +0000 UTC m=+368.828328598) (total time: 1.197315222s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/traefik.containo.us/ingressroutetcps/% 668]]"
time="2024-03-22T03:59:54Z" level=info msg="Slow SQL (started: 2024-03-22 03:59:53.095979246 +0000 UTC m=+368.832945421) (total time: 1.19401928s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/traefik.io/serverstransports/% false]]"
time="2024-03-22T03:59:54Z" level=info msg="Slow SQL (started: 2024-03-22 03:59:53.0893023 +0000 UTC m=+368.826268471) (total time: 1.220669733s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/services/specs/% 665]]"
time="2024-03-22T03:59:54Z" level=info msg="Slow SQL (started: 2024-03-22 03:59:53.097453561 +0000 UTC m=+368.834419745) (total time: 1.21347239s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/mutatingwebhookconfigurations/% false]]"
time="2024-03-22T03:59:54Z" level=info msg="Slow SQL (started: 2024-03-22 03:59:52.363616751 +0000 UTC m=+368.100582956) (total time: 1.989629169s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/controllers/% false]]"
time="2024-03-22T03:59:54Z" level=info msg="Slow SQL (started: 2024-03-22 03:59:51.736964881 +0000 UTC m=+367.473931058) (total time: 2.620779833s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC  : [[/registry/masterleases/192.168.56.110 false]]"
time="2024-03-22T03:59:54Z" level=info msg="Slow SQL (started: 2024-03-22 03:59:52.939032374 +0000 UTC m=+368.675998557) (total time: 1.470948962s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/csistoragecapacities/% false]]"
time="2024-03-22T03:59:54Z" level=info msg="Slow SQL (started: 2024-03-22 03:59:53.338726194 +0000 UTC m=+369.075692372) (total time: 1.176081978s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/controllers/% 667]]"
time="2024-03-22T03:59:54Z" level=info msg="Slow SQL (started: 2024-03-22 03:59:52.147742763 +0000 UTC m=+367.884708939) (total time: 2.370046199s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/ingressclasses/% false]]"
time="2024-03-22T03:59:54Z" level=info msg="Slow SQL (started: 2024-03-22 03:59:51.364329766 +0000 UTC m=+367.101295951) (total time: 2.046528616s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/clusterroles/% false]]"
time="2024-03-22T03:59:54Z" level=info msg="Connecting to proxy" url="wss://192.168.56.110:6443/v1-k3s/connect"
I0322 03:59:54.655892    2297 trace.go:236] Trace[113974163]: "List" accept:application/json, */*,audit-id:b0311e1e-daf3-4a30-bbfc-666ccf8bfd7d,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:59:53.507) (total time: 1143ms):
Trace[113974163]: ---"Writing http response done" count:1 1143ms (03:59:54.650)
Trace[113974163]: [1.143274411s] [1.143274411s] END
time="2024-03-22T03:59:54Z" level=info msg="Slow SQL (started: 2024-03-22 03:59:52.782749755 +0000 UTC m=+368.519715937) (total time: 1.500403553s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/events/% false]]"
I0322 03:59:54.720495    2297 trace.go:236] Trace[1071439473]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:b33236c5-3bd7-4021-b684-b85f63b0e74d,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:namespace,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:59:53.842) (total time: 875ms):
Trace[1071439473]: ---"About to List from storage" 860ms (03:59:54.703)
Trace[1071439473]: [875.511422ms] [875.511422ms] END
I0322 03:59:54.908376    2297 trace.go:236] Trace[1370582058]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:59:44.746) (total time: 10156ms):
Trace[1370582058]: ---"Objects listed" error:<nil> 10156ms (03:59:54.903)
Trace[1370582058]: [10.156518968s] [10.156518968s] END
I0322 03:59:55.061812    2297 trace.go:236] Trace[2020681581]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:ea362b51-0509-433b-aaf8-fe6375e39170,client:127.0.0.1,protocol:HTTP/2.0,resource:replicasets,scope:cluster,url:/apis/apps/v1/replicasets,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/scheduler,verb:LIST (22-Mar-2024 03:59:53.337) (total time: 1710ms):
Trace[2020681581]: ---"Writing http response done" count:4 1710ms (03:59:55.048)
Trace[2020681581]: [1.71059992s] [1.71059992s] END
I0322 03:59:55.159409    2297 trace.go:236] Trace[1026599810]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:7d92525e-5871-4a04-be4e-90563c12bc48,client:127.0.0.1,protocol:HTTP/2.0,resource:apiservices,scope:cluster,url:/apis/apiregistration.k8s.io/v1/apiservices,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:59:53.385) (total time: 1767ms):
Trace[1026599810]: ---"Writing http response done" count:26 1763ms (03:59:55.152)
Trace[1026599810]: [1.767640884s] [1.767640884s] END
I0322 03:59:55.165959    2297 trace.go:236] Trace[1830975568]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:d81da215-3d25-4d81-b9e4-42806ff8c7a7,client:127.0.0.1,protocol:HTTP/2.0,resource:pods,scope:cluster,url:/api/v1/pods,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/scheduler,verb:LIST (22-Mar-2024 03:59:53.296) (total time: 1868ms):
Trace[1830975568]: ---"Writing http response done" count:5 1866ms (03:59:55.165)
Trace[1830975568]: [1.868977708s] [1.868977708s] END
I0322 03:59:55.188174    2297 trace.go:236] Trace[1861861120]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:b63da132-3fdf-4d2e-987f-349db27bae8f,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:cluster,url:/api/v1/nodes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:59:53.845) (total time: 1342ms):
Trace[1861861120]: ---"Writing http response done" count:1 1333ms (03:59:55.188)
Trace[1861861120]: [1.342686958s] [1.342686958s] END
I0322 03:59:55.188879    2297 trace.go:236] Trace[1315010234]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:1710df92-2964-4552-b939-6dd3ff267c4c,client:127.0.0.1,protocol:HTTP/2.0,resource:flowschemas,scope:cluster,url:/apis/flowcontrol.apiserver.k8s.io/v1beta3/flowschemas,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:59:53.884) (total time: 1304ms):
Trace[1315010234]: ---"Writing http response done" count:13 1301ms (03:59:55.188)
Trace[1315010234]: [1.304674805s] [1.304674805s] END
I0322 03:59:55.189245    2297 trace.go:236] Trace[1575652803]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:c654c5bf-ce77-4840-883b-000072a4c7e4,client:127.0.0.1,protocol:HTTP/2.0,resource:runtimeclasses,scope:cluster,url:/apis/node.k8s.io/v1/runtimeclasses,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:59:53.932) (total time: 1256ms):
Trace[1575652803]: ---"Writing http response done" count:10 1251ms (03:59:55.189)
Trace[1575652803]: [1.256474864s] [1.256474864s] END
I0322 03:59:55.189580    2297 trace.go:236] Trace[726178033]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:34f484ca-075a-4bcf-ba41-656ab9fe9762,client:127.0.0.1,protocol:HTTP/2.0,resource:secrets,scope:cluster,url:/api/v1/secrets,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:59:53.411) (total time: 1778ms):
Trace[726178033]: ---"Writing http response done" count:6 1774ms (03:59:55.189)
Trace[726178033]: [1.778095981s] [1.778095981s] END
I0322 03:59:55.294008    2297 trace.go:236] Trace[1407410603]: "Proxy via http_connect protocol over tcp" address:10.42.0.5:10250 (22-Mar-2024 03:59:53.594) (total time: 1694ms):
Trace[1407410603]: [1.694593314s] [1.694593314s] END
I0322 03:59:55.462280    2297 trace.go:236] Trace[251256073]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:180c79a0-b346-4e99-8ced-831394328665,client:127.0.0.1,protocol:HTTP/2.0,resource:ingressclasses,scope:cluster,url:/apis/networking.k8s.io/v1/ingressclasses,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:59:54.670) (total time: 785ms):
Trace[251256073]: ---"Writing http response done" count:1 770ms (03:59:55.456)
Trace[251256073]: [785.891553ms] [785.891553ms] END
time="2024-03-22T03:59:55Z" level=info msg="Slow SQL (started: 2024-03-22 03:59:54.468455167 +0000 UTC m=+370.205421342) (total time: 1.025208949s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/deployments/% 667]]"
I0322 03:59:56.084790    2297 trace.go:236] Trace[878343056]: "DeltaFIFO Pop Process" ID:v1.apiextensions.k8s.io,Depth:23,Reason:slow event handlers blocking the queue (22-Mar-2024 03:59:55.961) (total time: 117ms):
Trace[878343056]: [117.21417ms] [117.21417ms] END
I0322 03:59:56.111470    2297 trace.go:236] Trace[420479879]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:c4eb5e24-235a-47dd-b34b-d30421cb8fa5,client:127.0.0.1,protocol:HTTP/2.0,resource:statefulsets,scope:cluster,url:/apis/apps/v1/statefulsets,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/scheduler,verb:LIST (22-Mar-2024 03:59:53.269) (total time: 1232ms):
Trace[420479879]: ---"Writing http response done" count:0 1216ms (03:59:54.501)
Trace[420479879]: [1.232265015s] [1.232265015s] END
I0322 03:59:56.299162    2297 trace.go:236] Trace[251336291]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:120f4313-a3f9-4f49-b209-d700d31e8d21,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:cluster,url:/api/v1/nodes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/scheduler,verb:LIST (22-Mar-2024 03:59:54.467) (total time: 1828ms):
Trace[251336291]: ["cacher list" audit-id:120f4313-a3f9-4f49-b209-d700d31e8d21,type:nodes 1817ms (03:59:54.478)
Trace[251336291]:  ---"Listed items from cache" count:1 1021ms (03:59:55.500)]
Trace[251336291]: ---"Writing http response done" count:1 781ms (03:59:56.295)
Trace[251336291]: [1.828085346s] [1.828085346s] END
I0322 03:59:56.708461    2297 trace.go:236] Trace[67613273]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:59:40.612) (total time: 16092ms):
Trace[67613273]: ---"Objects listed" error:<nil> 16092ms (03:59:56.705)
Trace[67613273]: [16.092551436s] [16.092551436s] END
I0322 03:59:56.843431    2297 trace.go:236] Trace[1141081005]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:c3852e99-7714-4844-8a2c-01eee5c6a407,client:127.0.0.1,protocol:HTTP/2.0,resource:pods,scope:namespace,url:/api/v1/namespaces/kube-system/pods,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/,verb:LIST (22-Mar-2024 03:59:55.739) (total time: 1100ms):
Trace[1141081005]: ---"Writing http response done" count:7 1099ms (03:59:56.840)
Trace[1141081005]: [1.100725136s] [1.100725136s] END
I0322 03:59:56.905756    2297 trace.go:236] Trace[850991070]: "Proxy via http_connect protocol over tcp" address:10.42.0.5:10250 (22-Mar-2024 03:59:49.785) (total time: 7116ms):
Trace[850991070]: [7.116704183s] [7.116704183s] END
I0322 03:59:56.912079    2297 trace.go:236] Trace[278666439]: "Proxy via http_connect protocol over tcp" address:10.42.0.5:10250 (22-Mar-2024 03:59:49.785) (total time: 7126ms):
Trace[278666439]: [7.126480829s] [7.126480829s] END
I0322 03:59:56.919083    2297 trace.go:236] Trace[529101358]: "Proxy via http_connect protocol over tcp" address:10.42.0.5:10250 (22-Mar-2024 03:59:49.785) (total time: 7133ms):
Trace[529101358]: [7.133262934s] [7.133262934s] END
I0322 03:59:56.931163    2297 trace.go:236] Trace[954890594]: "Proxy via http_connect protocol over tcp" address:10.42.0.5:10250 (22-Mar-2024 03:59:51.188) (total time: 5742ms):
Trace[954890594]: [5.742919452s] [5.742919452s] END
I0322 03:59:56.941023    2297 trace.go:236] Trace[1581438044]: "DeltaFIFO Pop Process" ID:clustercidrs-node,Depth:53,Reason:slow event handlers blocking the queue (22-Mar-2024 03:59:56.814) (total time: 121ms):
Trace[1581438044]: [121.375996ms] [121.375996ms] END
I0322 03:59:56.943435    2297 trace.go:236] Trace[921442266]: "Proxy via http_connect protocol over tcp" address:10.42.0.5:10250 (22-Mar-2024 03:59:49.786) (total time: 7157ms):
Trace[921442266]: [7.157183478s] [7.157183478s] END
I0322 03:59:56.946295    2297 trace.go:236] Trace[302667875]: "Proxy via http_connect protocol over tcp" address:10.42.0.5:10250 (22-Mar-2024 03:59:49.761) (total time: 7184ms):
Trace[302667875]: [7.184391993s] [7.184391993s] END
I0322 03:59:56.951833    2297 trace.go:236] Trace[908095132]: "DeltaFIFO Pop Process" ID:kube-public/default,Depth:38,Reason:slow event handlers blocking the queue (22-Mar-2024 03:59:56.814) (total time: 137ms):
Trace[908095132]: [137.163107ms] [137.163107ms] END
time="2024-03-22T03:59:57Z" level=info msg="Handling backend connection request [server]"
E0322 03:59:57.464007    2297 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: request timed out, Header: map[]
I0322 03:59:57.464880    2297 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0322 03:59:57.471999    2297 trace.go:236] Trace[277092128]: "List" accept:application/json, */*,audit-id:902f9fc7-fafb-4f40-8040-87294ceadfab,client:127.0.0.1,protocol:HTTP/2.0,resource:pods,scope:cluster,url:/api/v1/pods,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:59:56.103) (total time: 1365ms):
Trace[277092128]: ---"Writing http response done" count:7 1364ms (03:59:57.468)
Trace[277092128]: [1.365168496s] [1.365168496s] END
I0322 03:59:57.642786    2297 trace.go:236] Trace[1085580209]: "DeltaFIFO Pop Process" ID:clustercidrs-node,Depth:53,Reason:slow event handlers blocking the queue (22-Mar-2024 03:59:57.528) (total time: 103ms):
Trace[1085580209]: [103.851305ms] [103.851305ms] END
I0322 03:59:57.654446    2297 trace.go:236] Trace[116557589]: "DeltaFIFO Pop Process" ID:etcdsnapshotfiles.k3s.cattle.io,Depth:21,Reason:slow event handlers blocking the queue (22-Mar-2024 03:59:57.528) (total time: 126ms):
Trace[116557589]: [126.156513ms] [126.156513ms] END
I0322 03:59:57.661963    2297 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 03:59:57.685913    2297 trace.go:236] Trace[1309335298]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:7e963f15-a9f1-4179-a7c0-aa380b03805b,client:127.0.0.1,protocol:HTTP/2.0,resource:prioritylevelconfigurations,scope:cluster,url:/apis/flowcontrol.apiserver.k8s.io/v1beta3/prioritylevelconfigurations,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:59:53.424) (total time: 4261ms):
Trace[1309335298]: ["cacher list" audit-id:7e963f15-a9f1-4179-a7c0-aa380b03805b,type:prioritylevelconfigurations.flowcontrol.apiserver.k8s.io 4258ms (03:59:53.426)
Trace[1309335298]:  ---"watchCache locked acquired" 1168ms (03:59:54.595)]
Trace[1309335298]: ---"Writing http response done" count:8 2336ms (03:59:57.685)
Trace[1309335298]: [4.261249291s] [4.261249291s] END
I0322 03:59:57.708951    2297 trace.go:236] Trace[739922818]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:d639266a-7a6f-4d40-b844-88b1906437f3,client:127.0.0.1,protocol:HTTP/2.0,resource:pods,scope:cluster,url:/api/v1/pods,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:59:56.149) (total time: 1559ms):
Trace[739922818]: ---"Writing http response done" count:7 1559ms (03:59:57.708)
Trace[739922818]: [1.559275931s] [1.559275931s] END
I0322 03:59:57.721194    2297 trace.go:236] Trace[1187156896]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:b5522f8e-5de4-498c-acbd-5b3bb4065447,client:127.0.0.1,protocol:HTTP/2.0,resource:daemonsets,scope:cluster,url:/apis/apps/v1/daemonsets,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/shared-informers,verb:LIST (22-Mar-2024 03:59:56.371) (total time: 1349ms):
Trace[1187156896]: ---"Writing http response done" count:1 1347ms (03:59:57.721)
Trace[1187156896]: [1.349460108s] [1.349460108s] END
I0322 03:59:57.744872    2297 trace.go:236] Trace[1265055800]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:3170e882-94eb-45fd-a53a-361f57881cd7,client:127.0.0.1,protocol:HTTP/2.0,resource:clusterroles,scope:cluster,url:/apis/rbac.authorization.k8s.io/v1/clusterroles,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:59:56.062) (total time: 1682ms):
Trace[1265055800]: ---"Writing http response done" count:70 1294ms (03:59:57.744)
Trace[1265055800]: [1.682146914s] [1.682146914s] END
I0322 03:59:57.745694    2297 trace.go:236] Trace[64108246]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:e10670f7-3a03-4cd6-85a3-190064b1f39e,client:127.0.0.1,protocol:HTTP/2.0,resource:customresourcedefinitions,scope:cluster,url:/apis/apiextensions.k8s.io/v1/customresourcedefinitions,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:59:53.911) (total time: 3833ms):
Trace[64108246]: ["cacher list" audit-id:e10670f7-3a03-4cd6-85a3-190064b1f39e,type:customresourcedefinitions.apiextensions.k8s.io 2942ms (03:59:54.802)
Trace[64108246]:  ---"Listed items from cache" count:23 1058ms (03:59:55.877)]
Trace[64108246]: ---"Writing http response done" count:23 1866ms (03:59:57.745)
Trace[64108246]: [3.833821746s] [3.833821746s] END
I0322 03:59:57.764985    2297 trace.go:236] Trace[1843847223]: "DeltaFIFO Pop Process" ID:kube-node-lease/default,Depth:39,Reason:slow event handlers blocking the queue (22-Mar-2024 03:59:57.530) (total time: 234ms):
Trace[1843847223]: [234.00542ms] [234.00542ms] END
I0322 03:59:57.777683    2297 trace.go:236] Trace[1053084301]: "DeltaFIFO Pop Process" ID:kube-system/auth-delegator,Depth:11,Reason:slow event handlers blocking the queue (22-Mar-2024 03:59:57.531) (total time: 245ms):
Trace[1053084301]: [245.893475ms] [245.893475ms] END
I0322 03:59:57.919204    2297 trace.go:236] Trace[205800111]: "DeltaFIFO Pop Process" ID:kube-public/default,Depth:38,Reason:slow event handlers blocking the queue (22-Mar-2024 03:59:57.765) (total time: 147ms):
Trace[205800111]: [147.169292ms] [147.169292ms] END
I0322 03:59:57.920065    2297 trace.go:236] Trace[1317853315]: "DeltaFIFO Pop Process" ID:cluster-admin,Depth:68,Reason:slow event handlers blocking the queue (22-Mar-2024 03:59:57.662) (total time: 257ms):
Trace[1317853315]: [257.057843ms] [257.057843ms] END
I0322 03:59:58.123765    2297 trace.go:236] Trace[1783340044]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:5eab25e7-690b-4e24-bef2-903694b9448a,client:127.0.0.1,protocol:HTTP/2.0,resource:daemonsets,scope:namespace,url:/apis/apps/v1/namespaces/kube-system/daemonsets,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/,verb:LIST (22-Mar-2024 03:59:55.771) (total time: 1044ms):
Trace[1783340044]: ---"Writing http response done" count:1 1041ms (03:59:56.816)
Trace[1783340044]: [1.044995278s] [1.044995278s] END
I0322 03:59:58.260833    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="2.844752ms"
I0322 03:59:58.265800    2297 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
time="2024-03-22T03:59:58Z" level=info msg="Slow SQL (started: 2024-03-22 03:59:56.967512519 +0000 UTC m=+372.704478716) (total time: 1.301586144s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/traefik.containo.us/traefikservices/% false]]"
I0322 03:59:58.349639    2297 trace.go:236] Trace[1158680714]: "DeltaFIFO Pop Process" ID:kube-system/attachdetach-controller,Depth:37,Reason:slow event handlers blocking the queue (22-Mar-2024 03:59:57.919) (total time: 421ms):
Trace[1158680714]: [421.976369ms] [421.976369ms] END
I0322 03:59:58.378617    2297 trace.go:236] Trace[222326279]: "DeltaFIFO Pop Process" ID:edit,Depth:66,Reason:slow event handlers blocking the queue (22-Mar-2024 03:59:57.920) (total time: 458ms):
Trace[222326279]: [458.469483ms] [458.469483ms] END
I0322 03:59:58.550891    2297 trace.go:236] Trace[323474362]: "List" accept:application/json, */*,audit-id:9b9527db-d274-495f-b53e-e77f8206ceec,client:127.0.0.1,protocol:HTTP/2.0,resource:secrets,scope:cluster,url:/api/v1/secrets,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:LIST (22-Mar-2024 03:59:55.649) (total time: 2891ms):
Trace[323474362]: ---"Writing http response done" count:6 2177ms (03:59:58.541)
Trace[323474362]: [2.891363818s] [2.891363818s] END
I0322 03:59:58.610871    2297 trace.go:236] Trace[1570089364]: "List" accept:application/json, */*,audit-id:b4d56c9b-f448-4b31-a1a7-f4c92c26e6ce,client:10.42.0.8,protocol:HTTP/2.0,resource:endpoints,scope:cluster,url:/api/v1/endpoints,user-agent:traefik/2.10.5 (linux/amd64) kubernetes/crd,verb:LIST (22-Mar-2024 03:59:57.083) (total time: 1527ms):
Trace[1570089364]: ---"Writing http response done" count:4 1521ms (03:59:58.610)
Trace[1570089364]: [1.527607462s] [1.527607462s] END
I0322 03:59:58.618314    2297 trace.go:236] Trace[585413143]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:a1a04c59-90c5-43ef-99d5-924c0661b86b,client:127.0.0.1,protocol:HTTP/2.0,resource:poddisruptionbudgets,scope:cluster,url:/apis/policy/v1/poddisruptionbudgets,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/scheduler,verb:LIST (22-Mar-2024 03:59:56.331) (total time: 2286ms):
Trace[585413143]: ---"About to List from storage" 757ms (03:59:57.088)
Trace[585413143]: ---"Writing http response done" count:0 1520ms (03:59:58.618)
Trace[585413143]: [2.286768904s] [2.286768904s] END
I0322 03:59:58.640277    2297 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
I0322 03:59:58.640405    2297 event.go:307] "Event occurred" object="kube-system/traefik-crd" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik-crd"
E0322 03:59:58.923835    2297 controller.go:193] "Failed to update lease" err="Put \"https://127.0.0.1:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/server?timeout=10s\": context deadline exceeded"
I0322 03:59:59.026422    2297 trace.go:236] Trace[1984368026]: "List" accept:application/json, */*,audit-id:d84e920b-7b54-463a-a27a-41f245bb086a,client:10.42.0.8,protocol:HTTP/2.0,resource:ingressroutes,scope:cluster,url:/apis/traefik.containo.us/v1alpha1/ingressroutes,user-agent:traefik/2.10.5 (linux/amd64) kubernetes/crd,verb:LIST (22-Mar-2024 03:59:57.601) (total time: 1421ms):
Trace[1984368026]: ["cacher list" audit-id:d84e920b-7b54-463a-a27a-41f245bb086a,type:ingressroutes.traefik.containo.us 1421ms (03:59:57.602)]
Trace[1984368026]: [1.421777413s] [1.421777413s] END
I0322 03:59:59.103776    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="14.719265ms"
I0322 03:59:59.161729    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="1.403667141s"
E0322 03:59:59.460798    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 03:59:59.463272    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 03:59:59.463778    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 03:59:59.552070    2297 trace.go:236] Trace[1768272001]: "List" accept:application/json, */*,audit-id:60ca7892-4c6d-4e6e-8f9e-69ad4167413e,client:10.42.0.8,protocol:HTTP/2.0,resource:ingressroutetcps,scope:cluster,url:/apis/traefik.io/v1alpha1/ingressroutetcps,user-agent:traefik/2.10.5 (linux/amd64) kubernetes/crd,verb:LIST (22-Mar-2024 03:59:58.583) (total time: 965ms):
Trace[1768272001]: [965.839469ms] [965.839469ms] END
I0322 03:59:59.552415    2297 trace.go:236] Trace[1047568830]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:59:47.554) (total time: 11996ms):
Trace[1047568830]: ---"Objects listed" error:<nil> 11995ms (03:59:59.549)
Trace[1047568830]: [11.996482805s] [11.996482805s] END
I0322 03:59:59.696896    2297 trace.go:236] Trace[1529644379]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:d2f04212-07a4-4c0b-8d0a-ada1d7cd2400,client:10.42.0.6,protocol:HTTP/2.0,resource:endpointslices,scope:cluster,url:/apis/discovery.k8s.io/v1/endpointslices,user-agent:coredns/v0.0.0 (linux/amd64) kubernetes/$Format,verb:LIST (22-Mar-2024 03:59:58.730) (total time: 961ms):
Trace[1529644379]: ---"About to List from storage" 954ms (03:59:59.684)
Trace[1529644379]: [961.553641ms] [961.553641ms] END
I0322 03:59:59.696958    2297 trace.go:236] Trace[1166703183]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:f2d2d934-6c9a-45ed-93d0-4a19d4e97e0c,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/server,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PUT (22-Mar-2024 03:59:57.497) (total time: 2196ms):
Trace[1166703183]: ---"Write to database call failed" len:464,err:Timeout: request did not complete within requested timeout - context canceled 1845ms (03:59:59.453)
Trace[1166703183]: [2.196261055s] [2.196261055s] END
I0322 03:59:59.702551    2297 trace.go:236] Trace[1707798359]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:59:49.694) (total time: 10008ms):
Trace[1707798359]: ---"Objects listed" error:<nil> 10007ms (03:59:59.701)
Trace[1707798359]: [10.008493938s] [10.008493938s] END
I0322 03:59:59.715939    2297 trace.go:236] Trace[1286517841]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:59:49.603) (total time: 10112ms):
Trace[1286517841]: ---"Objects listed" error:<nil> 10110ms (03:59:59.713)
Trace[1286517841]: [10.112522984s] [10.112522984s] END
time="2024-03-22T03:59:59Z" level=info msg="Slow SQL (started: 2024-03-22 03:59:57.374039252 +0000 UTC m=+373.111005446) (total time: 2.346504771s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC LIMIT 500 : [[% 669]]"
I0322 03:59:59.741375    2297 trace.go:236] Trace[580819375]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:e0d38fe7-54ee-4d1e-8205-f7118828ae06,client:127.0.0.1,protocol:HTTP/2.0,resource:endpointslices,scope:cluster,url:/apis/discovery.k8s.io/v1/endpointslices,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:59:56.687) (total time: 3052ms):
Trace[580819375]: ["cacher list" audit-id:e0d38fe7-54ee-4d1e-8205-f7118828ae06,type:endpointslices.discovery.k8s.io 3050ms (03:59:56.690)]
Trace[580819375]: ---"Writing http response done" count:4 1717ms (03:59:59.740)
Trace[580819375]: [3.052926982s] [3.052926982s] END
I0322 03:59:59.761593    2297 trace.go:236] Trace[1320024486]: "List" accept:application/json, */*,audit-id:f9d920ef-08c4-40d6-a01c-9d3fd6f51410,client:127.0.0.1,protocol:HTTP/2.0,resource:addons,scope:cluster,url:/apis/k3s.cattle.io/v1/addons,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:LIST (22-Mar-2024 03:59:56.355) (total time: 3405ms):
Trace[1320024486]: ---"About to List from storage" 776ms (03:59:57.132)
Trace[1320024486]: ---"Writing http response done" count:13 2627ms (03:59:59.761)
Trace[1320024486]: [3.405732651s] [3.405732651s] END
E0322 03:59:59.812246    2297 timeout.go:142] post-timeout activity - time-elapsed: 113.824892ms, PUT "/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/server" result: <nil>
I0322 03:59:59.716151    2297 trace.go:236] Trace[1277849369]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:8970341d-2cf3-440c-ae36-5aff977cfb26,client:127.0.0.1,protocol:HTTP/2.0,resource:roles,scope:cluster,url:/apis/rbac.authorization.k8s.io/v1/roles,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:59:57.335) (total time: 1435ms):
Trace[1277849369]: ["cacher list" audit-id:8970341d-2cf3-440c-ae36-5aff977cfb26,type:roles.rbac.authorization.k8s.io 2375ms (03:59:57.339)
Trace[1277849369]:  ---"watchCache locked acquired" 1410ms (03:59:58.750)]
Trace[1277849369]: [1.435373502s] [1.435373502s] END
I0322 04:00:00.371950    2297 trace.go:236] Trace[1816806779]: "List" accept:application/json, */*,audit-id:0bc3acbd-085d-43a6-b0a3-ba9e279e4c3a,client:127.0.0.1,protocol:HTTP/2.0,resource:jobs,scope:cluster,url:/apis/batch/v1/jobs,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:LIST (22-Mar-2024 03:59:55.655) (total time: 4715ms):
Trace[1816806779]: ---"Writing http response done" count:2 4711ms (04:00:00.370)
Trace[1816806779]: [4.715200081s] [4.715200081s] END
I0322 04:00:00.376625    2297 trace.go:236] Trace[1711475447]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:eb113e00-7355-450f-8bfa-ac40cddf61e1,client:10.42.0.6,protocol:HTTP/2.0,resource:services,scope:cluster,url:/api/v1/services,user-agent:coredns/v0.0.0 (linux/amd64) kubernetes/$Format,verb:LIST (22-Mar-2024 03:59:57.387) (total time: 2988ms):
Trace[1711475447]: ---"Writing http response done" count:4 2984ms (04:00:00.376)
Trace[1711475447]: [2.988779873s] [2.988779873s] END
I0322 04:00:00.656717    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="887.108597ms"
I0322 04:00:00.975837    2297 trace.go:236] Trace[1044113627]: "List" accept:application/json, */*,audit-id:1c68c84e-a56d-4995-8e5b-8d2994c5c665,client:10.42.0.8,protocol:HTTP/2.0,resource:secrets,scope:cluster,url:/api/v1/secrets,user-agent:traefik/2.10.5 (linux/amd64) kubernetes/ingress,verb:LIST (22-Mar-2024 03:59:58.429) (total time: 1627ms):
Trace[1044113627]: ["cacher list" audit-id:1c68c84e-a56d-4995-8e5b-8d2994c5c665,type:secrets 2542ms (03:59:58.431)
Trace[1044113627]:  ---"Resized result" 1048ms (03:59:59.481)]
Trace[1044113627]: ---"Writing http response done" count:4 573ms (04:00:00.056)
Trace[1044113627]: [1.627102954s] [1.627102954s] END
I0322 04:00:00.975920    2297 trace.go:236] Trace[1345741428]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:59:50.288) (total time: 10681ms):
Trace[1345741428]: ---"Objects listed" error:<nil> 10676ms (04:00:00.964)
Trace[1345741428]: [10.681781238s] [10.681781238s] END
I0322 04:00:00.976866    2297 trace.go:236] Trace[1437268278]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:59:48.705) (total time: 11360ms):
Trace[1437268278]: ---"Objects listed" error:<nil> 11354ms (04:00:00.060)
Trace[1437268278]: [11.360671444s] [11.360671444s] END
I0322 04:00:01.025848    2297 trace.go:236] Trace[1017923230]: "Proxy via http_connect protocol over tcp" address:10.42.0.5:10250 (22-Mar-2024 03:59:53.598) (total time: 6701ms):
Trace[1017923230]: [6.701554245s] [6.701554245s] END
I0322 04:00:01.127012    2297 trace.go:236] Trace[448587647]: "List" accept:application/json, */*,audit-id:6a5cf274-58cb-4748-978e-c76af2b03c1f,client:10.42.0.2,protocol:HTTP/2.0,resource:persistentvolumes,scope:cluster,url:/api/v1/persistentvolumes,user-agent:local-path-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:LIST (22-Mar-2024 04:00:00.521) (total time: 600ms):
Trace[448587647]: ["cacher list" audit-id:6a5cf274-58cb-4748-978e-c76af2b03c1f,type:persistentvolumes 594ms (04:00:00.527)]
Trace[448587647]: [600.343368ms] [600.343368ms] END
I0322 04:00:01.130185    2297 trace.go:236] Trace[972512259]: "List" accept:application/json, */*,audit-id:e98f74a5-9138-4b32-a28c-6216e8be791f,client:10.42.0.2,protocol:HTTP/2.0,resource:persistentvolumeclaims,scope:cluster,url:/api/v1/persistentvolumeclaims,user-agent:local-path-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:LIST (22-Mar-2024 03:59:59.655) (total time: 900ms):
Trace[972512259]: ---"Writing http response done" count:0 886ms (04:00:00.556)
Trace[972512259]: [900.65487ms] [900.65487ms] END
I0322 04:00:00.807096    2297 trace.go:236] Trace[2127410812]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:59:49.698) (total time: 11100ms):
Trace[2127410812]: ---"Objects listed" error:<nil> 11099ms (04:00:00.798)
Trace[2127410812]: [11.100545327s] [11.100545327s] END
I0322 04:00:02.063976    2297 trace.go:236] Trace[1141552248]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:59:51.237) (total time: 10258ms):
Trace[1141552248]: ---"Objects listed" error:<nil> 10249ms (04:00:01.486)
Trace[1141552248]: [10.258109088s] [10.258109088s] END
I0322 04:00:02.190962    2297 trace.go:236] Trace[666978754]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:59:51.437) (total time: 10146ms):
Trace[666978754]: ---"Objects listed" error:<nil> 10140ms (04:00:01.577)
Trace[666978754]: [10.146693231s] [10.146693231s] END
E0322 04:00:02.493680    2297 server.go:310] "Unable to authenticate the request due to an error" err="context canceled"
E0322 04:00:01.843536    2297 kubelet.go:2477] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="4m54.558s"
E0322 04:00:02.866619    2297 kubelet_node_status.go:540] "Error updating node status, will retry" err="error getting node \"server\": Get \"https://127.0.0.1:6443/api/v1/nodes/server?timeout=10s\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)"
W0322 04:00:02.947492    2297 transport.go:301] Unable to cancel request for *otelhttp.Transport
I0322 04:00:03.305488    2297 trace.go:236] Trace[1503668454]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:59:51.621) (total time: 11157ms):
Trace[1503668454]: ---"Objects listed" error:<nil> 11155ms (04:00:02.777)
Trace[1503668454]: [11.157081658s] [11.157081658s] END
time="2024-03-22T04:00:03Z" level=info msg="Slow SQL (started: 2024-03-22 04:00:02.375518396 +0000 UTC m=+378.112484578) (total time: 1.047702736s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC LIMIT 500 : [[% 669]]"
time="2024-03-22T04:00:03Z" level=info msg="Slow SQL (started: 2024-03-22 04:00:02.220824217 +0000 UTC m=+377.957790403) (total time: 1.332676271s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC  : [[/registry/services/specs/% false]]"
E0322 04:00:03.589472    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:00:03.593013    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:00:04.079927    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:00:04.082727    2297 finisher.go:175] FinishRequest: post-timeout activity - time-elapsed: 4.617186135s, panicked: false, err: context canceled, panic-reason: <nil>
I0322 04:00:04.084309    2297 trace.go:236] Trace[692488535]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:b30e0ced-9089-4090-8e04-1ec32fc1d184,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/server,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 03:59:57.478) (total time: 6602ms):
Trace[692488535]: ---"About to write a response" 5237ms (04:00:02.716)
Trace[692488535]: ---"Writing http response done" 1364ms (04:00:04.080)
Trace[692488535]: [6.602064907s] [6.602064907s] END
I0322 04:00:04.340468    2297 trace.go:236] Trace[762688617]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 03:59:51.432) (total time: 12902ms):
Trace[762688617]: ---"Objects listed" error:<nil> 12897ms (04:00:04.329)
Trace[762688617]: [12.902123481s] [12.902123481s] END
time="2024-03-22T04:00:04Z" level=info msg="Slow SQL (started: 2024-03-22 04:00:03.117393841 +0000 UTC m=+378.854360021) (total time: 1.03203617s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC  : [[/registry/services/specs/default/kubernetes false]]"
W0322 04:00:04.752737    2297 controller.go:134] slow openapi aggregation of "addons.k3s.cattle.io": 14.973297884s
I0322 04:00:04.905821    2297 trace.go:236] Trace[1145415013]: "Proxy via http_connect protocol over tcp" address:10.42.0.5:10250 (22-Mar-2024 03:59:59.464) (total time: 5432ms):
Trace[1145415013]: [5.432334667s] [5.432334667s] END
I0322 04:00:05.040650    2297 trace.go:236] Trace[1544633119]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.56.110,type:*v1.Endpoints,resource:apiServerIPInfo (22-Mar-2024 03:59:30.190) (total time: 34472ms):
Trace[1544633119]: ---"initial value restored" 26401ms (03:59:56.592)
Trace[1544633119]: ---"Transaction prepared" 4146ms (04:00:00.740)
Trace[1544633119]: ---"Txn call completed" 3912ms (04:00:04.653)
Trace[1544633119]: [34.47206967s] [34.47206967s] END
E0322 04:00:04.724407    2297 timeout.go:142] post-timeout activity - time-elapsed: 1.413293119s, GET "/api/v1/nodes/server" result: <nil>
I0322 04:00:05.184322    2297 trace.go:236] Trace[349654867]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:50c9edd3-9607-4284-9ba5-efc3e680b8f9,client:127.0.0.1,protocol:HTTP/2.0,resource:tokenreviews,scope:resource,url:/apis/authentication.k8s.io/v1/tokenreviews,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:POST (22-Mar-2024 03:59:57.547) (total time: 7632ms):
Trace[349654867]: ---"Write to database call succeeded" len:1152 7153ms (04:00:04.736)
Trace[349654867]: ---"Writing http response done" 444ms (04:00:05.180)
Trace[349654867]: [7.632995815s] [7.632995815s] END
I0322 04:00:06.420950    2297 trace.go:236] Trace[1453113591]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:ce9a97bc-280f-496e-8a19-0144e9581542,client:127.0.0.1,protocol:HTTP/2.0,resource:services,scope:resource,url:/api/v1/namespaces/default/services/kubernetes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/,verb:GET (22-Mar-2024 04:00:00.920) (total time: 5496ms):
Trace[1453113591]: ---"About to write a response" 5492ms (04:00:06.413)
Trace[1453113591]: [5.496167758s] [5.496167758s] END
E0322 04:00:06.428907    2297 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, context canceled]"
E0322 04:00:06.444859    2297 wrap.go:54] timeout or abort while handling: method=GET URI="/apis/networking.k8s.io/v1/ingresses?resourceVersion=640" audit-ID="e5f231e4-d7d4-44b2-b351-9a4426225266"
E0322 04:00:06.448255    2297 timeout.go:142] post-timeout activity - time-elapsed: 549.347s, GET "/apis/networking.k8s.io/v1/ingresses" result: <nil>
I0322 04:00:06.476125    2297 trace.go:236] Trace[2101795903]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:e57719e2-01f4-4410-96a8-afe4f6e67f3b,client:127.0.0.1,protocol:HTTP/2.0,resource:services,scope:cluster,url:/api/v1/services,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 03:59:57.741) (total time: 8734ms):
Trace[2101795903]: ["List(recursive=true) etcd3" audit-id:e57719e2-01f4-4410-96a8-afe4f6e67f3b,key:/services/specs,resourceVersion:,resourceVersionMatch:,limit:0,continue: 7359ms (03:59:59.116)]
Trace[2101795903]: ---"Writing http response done" count:4 1485ms (04:00:06.476)
Trace[2101795903]: [8.734906346s] [8.734906346s] END
I0322 04:00:06.477449    2297 trace.go:236] Trace[1870858681]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:89aebd0a-6d8a-4886-99de-413843f65be8,client:127.0.0.1,protocol:HTTP/2.0,resource:services,scope:cluster,url:/api/v1/services,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 04:00:02.667) (total time: 3809ms):
Trace[1870858681]: ["List(recursive=true) etcd3" audit-id:89aebd0a-6d8a-4886-99de-413843f65be8,key:/services/specs,resourceVersion:,resourceVersionMatch:,limit:0,continue: 3268ms (04:00:03.209)]
Trace[1870858681]: ---"Writing http response done" count:4 508ms (04:00:06.477)
Trace[1870858681]: [3.809397671s] [3.809397671s] END
I0322 04:00:06.500841    2297 trace.go:236] Trace[712026174]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:44ae09e2-00c1-4667-8f6a-b60d99661662,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-7zoch227uv4owxg75pgtnmv3jq,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PUT (22-Mar-2024 03:59:54.669) (total time: 11527ms):
Trace[712026174]: ---"limitedReadBody succeeded" len:590 248ms (03:59:54.918)
Trace[712026174]: ["GuaranteedUpdate etcd3" audit-id:44ae09e2-00c1-4667-8f6a-b60d99661662,key:/leases/kube-system/apiserver-7zoch227uv4owxg75pgtnmv3jq,type:*coordination.Lease,resource:leases.coordination.k8s.io 9712ms (03:59:56.486)
Trace[712026174]:  ---"initial value restored" 1305ms (03:59:57.792)
Trace[712026174]:  ---"About to Encode" 3405ms (04:00:01.197)
Trace[712026174]:  ---"Txn call completed" 4622ms (04:00:05.824)
Trace[712026174]:  ---"decode succeeded" len:587 365ms (04:00:06.190)]
Trace[712026174]: [11.527644098s] [11.527644098s] END
E0322 04:00:06.149272    2297 resource_quota_controller.go:440] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I0322 04:00:06.535081    2297 trace.go:236] Trace[481602454]: "Create" accept:application/json, */*,audit-id:e3ebf1da-85d5-4e2a-850e-148c1b1c6fa3,client:10.42.0.5,protocol:HTTP/2.0,resource:subjectaccessreviews,scope:resource,url:/apis/authorization.k8s.io/v1/subjectaccessreviews,user-agent:metrics-server/v0.6.3 (linux/amd64) kubernetes/a938798,verb:POST (22-Mar-2024 04:00:05.189) (total time: 1345ms):
Trace[481602454]: ---"Write to database call succeeded" len:265 1337ms (04:00:06.534)
Trace[481602454]: [1.345723627s] [1.345723627s] END
W0322 04:00:06.549083    2297 shared_informer.go:593] resyncPeriod 12h6m6.359447746s is smaller than resyncCheckPeriod 23h56m20.738889615s and the informer has already started. Changing it to 23h56m20.738889615s
I0322 04:00:06.553487    2297 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressrouteudps.traefik.containo.us"
I0322 04:00:06.559039    2297 trace.go:236] Trace[1591867092]: "Get" accept:application/json, */*,audit-id:69e5bf30-edaf-4e29-8782-ca5249914694,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps/coredns,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:GET (22-Mar-2024 04:00:03.437) (total time: 3121ms):
Trace[1591867092]: ---"About to write a response" 2878ms (04:00:06.315)
Trace[1591867092]: ---"Writing http response done" 243ms (04:00:06.559)
Trace[1591867092]: [3.121515899s] [3.121515899s] END
I0322 04:00:06.565977    2297 trace.go:236] Trace[1611230808]: "Create" accept:application/json, */*,audit-id:69e66eb3-13cc-4e08-91a7-e9b2aec5c446,client:10.42.0.5,protocol:HTTP/2.0,resource:subjectaccessreviews,scope:resource,url:/apis/authorization.k8s.io/v1/subjectaccessreviews,user-agent:metrics-server/v0.6.3 (linux/amd64) kubernetes/a938798,verb:POST (22-Mar-2024 04:00:04.787) (total time: 1778ms):
Trace[1611230808]: ---"Conversion done" 422ms (04:00:05.216)
Trace[1611230808]: ---"Write to database call succeeded" len:265 1348ms (04:00:06.565)
Trace[1611230808]: [1.778098137s] [1.778098137s] END
I0322 04:00:06.684925    2297 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="serverstransporttcps.traefik.io"
W0322 04:00:06.685825    2297 shared_informer.go:593] resyncPeriod 17h1m42.045099498s is smaller than resyncCheckPeriod 23h56m20.738889615s and the informer has already started. Changing it to 23h56m20.738889615s
I0322 04:00:06.685989    2297 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="tlsstores.traefik.io"
I0322 04:00:06.687193    2297 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressroutetcps.traefik.io"
I0322 04:00:06.687475    2297 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="serverstransports.traefik.io"
I0322 04:00:06.687647    2297 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressrouteudps.traefik.io"
W0322 04:00:06.687938    2297 shared_informer.go:593] resyncPeriod 12h37m15.12019861s is smaller than resyncCheckPeriod 23h56m20.738889615s and the informer has already started. Changing it to 23h56m20.738889615s
I0322 04:00:06.688173    2297 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="tlsoptions.traefik.containo.us"
I0322 04:00:06.688384    2297 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="middlewaretcps.traefik.containo.us"
W0322 04:00:06.688618    2297 shared_informer.go:593] resyncPeriod 14h17m21.801371939s is smaller than resyncCheckPeriod 23h56m20.738889615s and the informer has already started. Changing it to 23h56m20.738889615s
I0322 04:00:06.688753    2297 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="middlewares.traefik.io"
I0322 04:00:06.689039    2297 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="middlewares.traefik.containo.us"
I0322 04:00:06.689203    2297 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="tlsstores.traefik.containo.us"
I0322 04:00:06.689428    2297 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="serverstransports.traefik.containo.us"
W0322 04:00:06.689541    2297 shared_informer.go:593] resyncPeriod 12h11m26.927854071s is smaller than resyncCheckPeriod 23h56m20.738889615s and the informer has already started. Changing it to 23h56m20.738889615s
I0322 04:00:06.689785    2297 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="traefikservices.traefik.io"
W0322 04:00:06.689950    2297 shared_informer.go:593] resyncPeriod 18h34m28.906479349s is smaller than resyncCheckPeriod 23h56m20.738889615s and the informer has already started. Changing it to 23h56m20.738889615s
I0322 04:00:06.715499    2297 trace.go:236] Trace[1299556023]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:51e365ca-a0af-4d1e-ac29-34fd87996093,client:127.0.0.1,protocol:HTTP/2.0,resource:resourcequotas,scope:namespace,url:/api/v1/namespaces/kube-system/resourcequotas,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 04:00:04.001) (total time: 2705ms):
Trace[1299556023]: ---"About to List from storage" 644ms (04:00:04.645)
Trace[1299556023]: ["List(recursive=true) etcd3" audit-id:51e365ca-a0af-4d1e-ac29-34fd87996093,key:/resourcequotas/kube-system,resourceVersion:,resourceVersionMatch:,limit:0,continue: 1677ms (04:00:05.030)]
Trace[1299556023]: [2.705832824s] [2.705832824s] END
I0322 04:00:06.760483    2297 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressroutes.traefik.io"
W0322 04:00:06.761331    2297 shared_informer.go:593] resyncPeriod 12h58m1.821207779s is smaller than resyncCheckPeriod 23h56m20.738889615s and the informer has already started. Changing it to 23h56m20.738889615s
I0322 04:00:06.761508    2297 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressroutetcps.traefik.containo.us"
W0322 04:00:06.761844    2297 shared_informer.go:593] resyncPeriod 17h41m17.691236054s is smaller than resyncCheckPeriod 23h56m20.738889615s and the informer has already started. Changing it to 23h56m20.738889615s
I0322 04:00:06.762125    2297 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="traefikservices.traefik.containo.us"
W0322 04:00:06.762306    2297 shared_informer.go:593] resyncPeriod 15h39m4.515716942s is smaller than resyncCheckPeriod 23h56m20.738889615s and the informer has already started. Changing it to 23h56m20.738889615s
I0322 04:00:06.762575    2297 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="tlsoptions.traefik.io"
W0322 04:00:06.762647    2297 shared_informer.go:593] resyncPeriod 15h35m14.062269446s is smaller than resyncCheckPeriod 23h56m20.738889615s and the informer has already started. Changing it to 23h56m20.738889615s
I0322 04:00:06.762792    2297 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressroutes.traefik.containo.us"
I0322 04:00:06.763215    2297 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="middlewaretcps.traefik.io"
I0322 04:00:06.766245    2297 shared_informer.go:311] Waiting for caches to sync for resource quota
I0322 04:00:06.780361    2297 trace.go:236] Trace[885307986]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:8036ab8f-eb8b-4c2f-8c54-37802784c1bb,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/server,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 04:00:05.421) (total time: 1359ms):
Trace[885307986]: ---"About to write a response" 1268ms (04:00:06.692)
Trace[885307986]: [1.359333016s] [1.359333016s] END
I0322 04:00:07.022604    2297 trace.go:236] Trace[1266347783]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:a9f5f956-422e-4f72-b784-71eaa60fb75a,client:127.0.0.1,protocol:HTTP/2.0,resource:apiservices,scope:resource,url:/apis/apiregistration.k8s.io/v1/apiservices/v1beta1.metrics.k8s.io/status,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PUT (22-Mar-2024 04:00:01.473) (total time: 5541ms):
Trace[1266347783]: ---"limitedReadBody succeeded" len:2079 249ms (04:00:01.722)
Trace[1266347783]: ["GuaranteedUpdate etcd3" audit-id:a9f5f956-422e-4f72-b784-71eaa60fb75a,key:/apiregistration.k8s.io/apiservices/v1beta1.metrics.k8s.io,type:*apiregistration.APIService,resource:apiservices.apiregistration.k8s.io 4730ms (04:00:02.285)
Trace[1266347783]:  ---"About to Encode" 3918ms (04:00:06.212)
Trace[1266347783]:  ---"Txn call completed" 722ms (04:00:06.939)]
Trace[1266347783]: ---"Write to database call succeeded" len:2079 50ms (04:00:07.006)
Trace[1266347783]: [5.54158924s] [5.54158924s] END
I0322 04:00:07.107291    2297 trace.go:236] Trace[513805516]: "GuaranteedUpdate etcd3" audit-id:,key:/ranges/serviceips,type:*core.RangeAllocation,resource:serviceipallocations (22-Mar-2024 04:00:06.548) (total time: 558ms):
Trace[513805516]: ---"initial value restored" 557ms (04:00:07.106)
Trace[513805516]: [558.314932ms] [558.314932ms] END
W0322 04:00:07.128682    2297 controller.go:134] slow openapi aggregation of "traefikservices.traefik.containo.us": 2.371739756s
I0322 04:00:07.176992    2297 trace.go:236] Trace[307141137]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:f1306472-7b7a-4c51-8619-4a18e0abda3e,client:127.0.0.1,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:POST (22-Mar-2024 03:59:57.546) (total time: 9628ms):
Trace[307141137]: ---"limitedReadBody succeeded" len:429 78ms (03:59:57.624)
Trace[307141137]: ["Create etcd3" audit-id:f1306472-7b7a-4c51-8619-4a18e0abda3e,key:/events/kube-system/coredns-6799fbcd5-4tsrq.17befa2ef35bfce8,type:*core.Event,resource:events 5312ms (04:00:01.864)
Trace[307141137]:  ---"Encode succeeded" len:736 756ms (04:00:02.621)
Trace[307141137]:  ---"TransformToStorage succeeded" 2552ms (04:00:05.174)
Trace[307141137]:  ---"Txn call succeeded" 1759ms (04:00:06.933)]
Trace[307141137]: ---"Write to database call succeeded" len:429 35ms (04:00:06.969)
Trace[307141137]: ---"Writing http response done" 206ms (04:00:07.175)
Trace[307141137]: [9.628907775s] [9.628907775s] END
E0322 04:00:07.184068    2297 available_controller.go:460] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.42.0.5:10250/apis/metrics.k8s.io/v1beta1: Get "https://10.42.0.5:10250/apis/metrics.k8s.io/v1beta1": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
E0322 04:00:07.204969    2297 kubelet.go:2477] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="4.619s"
I0322 04:00:07.255419    2297 trace.go:236] Trace[759407403]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:818de004-79db-4cfe-bf67-39d70a711690,client:127.0.0.1,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/default/endpoints/kubernetes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 04:00:06.523) (total time: 732ms):
Trace[759407403]: ---"About to write a response" 731ms (04:00:07.254)
Trace[759407403]: [732.036418ms] [732.036418ms] END
I0322 04:00:07.256674    2297 trace.go:236] Trace[1959990485]: "GuaranteedUpdate etcd3" audit-id:,key:/ranges/servicenodeports,type:*core.RangeAllocation,resource:servicenodeportallocations (22-Mar-2024 04:00:06.548) (total time: 708ms):
Trace[1959990485]: ---"initial value restored" 707ms (04:00:07.256)
Trace[1959990485]: [708.520115ms] [708.520115ms] END
I0322 04:00:07.262736    2297 trace.go:236] Trace[1247508649]: "Get" accept:application/json, */*,audit-id:9825891f-dfba-4782-a90c-bf57a01d9844,client:127.0.0.1,protocol:HTTP/2.0,resource:secrets,scope:resource,url:/api/v1/namespaces/kube-system/secrets/k3s-serving,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:GET (22-Mar-2024 04:00:06.044) (total time: 1217ms):
Trace[1247508649]: ---"About to write a response" 1055ms (04:00:07.103)
Trace[1247508649]: ---"Writing http response done" 159ms (04:00:07.262)
Trace[1247508649]: [1.217705084s] [1.217705084s] END
I0322 04:00:07.304098    2297 trace.go:236] Trace[501001720]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:8d168c29-8928-4f04-bfed-5e5c2ba276f3,client:127.0.0.1,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/kube-system/pods/metrics-server-67c658944b-6v69k,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 04:00:06.170) (total time: 1133ms):
Trace[501001720]: ---"About to write a response" 1094ms (04:00:07.265)
Trace[501001720]: [1.133100807s] [1.133100807s] END
time="2024-03-22T04:00:07Z" level=info msg="Updating TLS secret for kube-system/k3s-serving (count: 10): map[listener.cattle.io/cn-10.43.0.1:10.43.0.1 listener.cattle.io/cn-127.0.0.1:127.0.0.1 listener.cattle.io/cn-192.168.56.110:192.168.56.110 listener.cattle.io/cn-__1-f16284:::1 listener.cattle.io/cn-kubernetes:kubernetes listener.cattle.io/cn-kubernetes.default:kubernetes.default listener.cattle.io/cn-kubernetes.default.svc:kubernetes.default.svc listener.cattle.io/cn-kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local listener.cattle.io/cn-localhost:localhost listener.cattle.io/cn-server:server listener.cattle.io/fingerprint:SHA1=57817D08CEF13EAC5FA6EA63398833E2B858E1D0]"
I0322 04:00:07.421389    2297 trace.go:236] Trace[1135938175]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:b89a28ce-dd7f-4b1d-aafa-d939c5beaf34,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/server,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PUT (22-Mar-2024 04:00:04.110) (total time: 3310ms):
Trace[1135938175]: ---"limitedReadBody succeeded" len:465 236ms (04:00:04.347)
Trace[1135938175]: ---"Conversion done" 576ms (04:00:04.929)
Trace[1135938175]: ---"About to store object in database" 551ms (04:00:05.481)
Trace[1135938175]: ["GuaranteedUpdate etcd3" audit-id:b89a28ce-dd7f-4b1d-aafa-d939c5beaf34,key:/leases/kube-node-lease/server,type:*coordination.Lease,resource:leases.coordination.k8s.io 1366ms (04:00:06.054)
Trace[1135938175]:  ---"initial value restored" 413ms (04:00:06.468)
Trace[1135938175]:  ---"About to Encode" 232ms (04:00:06.700)
Trace[1135938175]:  ---"Txn call completed" 719ms (04:00:07.421)]
Trace[1135938175]: [3.310606781s] [3.310606781s] END
I0322 04:00:07.507370    2297 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/coredns-6799fbcd5-4tsrq" containerName="coredns"
I0322 04:00:07.569187    2297 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/traefik-f4564c4f4-d2b8m" containerName="traefik"
I0322 04:00:07.747501    2297 trace.go:236] Trace[949647275]: "Create" accept:application/json, */*,audit-id:e52fa252-783f-4716-b028-a0c968b68969,client:127.0.0.1,protocol:HTTP/2.0,resource:secrets,scope:resource,url:/api/v1/namespaces/kube-system/secrets,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:POST (22-Mar-2024 03:59:56.587) (total time: 11154ms):
Trace[949647275]: ["Create etcd3" audit-id:e52fa252-783f-4716-b028-a0c968b68969,key:/secrets/kube-system/serverworker.node-password.k3s,type:*core.Secret,resource:secrets 550ms (04:00:07.191)
Trace[949647275]:  ---"Txn call succeeded" 539ms (04:00:07.732)]
Trace[949647275]: [11.154465251s] [11.154465251s] END
I0322 04:00:07.781353    2297 trace.go:236] Trace[1614258837]: "Patch" accept:application/json, */*,audit-id:8a047d86-9ea0-4448-82bf-42d5bfffdae7,client:127.0.0.1,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events/traefik.17bef9dca87f65ac,user-agent:helm-controller@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:PATCH (22-Mar-2024 04:00:02.177) (total time: 5603ms):
Trace[1614258837]: ---"limitedReadBody succeeded" len:124 64ms (04:00:02.241)
Trace[1614258837]: ["GuaranteedUpdate etcd3" audit-id:8a047d86-9ea0-4448-82bf-42d5bfffdae7,key:/events/kube-system/traefik.17bef9dca87f65ac,type:*core.Event,resource:events 5207ms (04:00:02.573)
Trace[1614258837]:  ---"initial value restored" 4097ms (04:00:06.670)
Trace[1614258837]:  ---"About to Encode" 578ms (04:00:07.248)
Trace[1614258837]:  ---"Txn call completed" 416ms (04:00:07.667)]
Trace[1614258837]: ---"About to check admission control" 469ms (04:00:07.141)
Trace[1614258837]: ---"Object stored in database" 638ms (04:00:07.779)
Trace[1614258837]: [5.603676708s] [5.603676708s] END
I0322 04:00:07.783348    2297 trace.go:236] Trace[1093825712]: "Proxy via http_connect protocol over tcp" address:10.42.0.5:10250 (22-Mar-2024 04:00:07.198) (total time: 584ms):
Trace[1093825712]: [584.741949ms] [584.741949ms] END
I0322 04:00:07.783719    2297 trace.go:236] Trace[1981846185]: "Proxy via http_connect protocol over tcp" address:10.42.0.5:10250 (22-Mar-2024 04:00:07.194) (total time: 589ms):
Trace[1981846185]: [589.680757ms] [589.680757ms] END
I0322 04:00:07.783953    2297 trace.go:236] Trace[1750109106]: "Proxy via http_connect protocol over tcp" address:10.42.0.5:10250 (22-Mar-2024 04:00:07.198) (total time: 585ms):
Trace[1750109106]: [585.642862ms] [585.642862ms] END
I0322 04:00:07.784222    2297 trace.go:236] Trace[909540572]: "Proxy via http_connect protocol over tcp" address:10.42.0.5:10250 (22-Mar-2024 04:00:07.198) (total time: 585ms):
Trace[909540572]: [585.689217ms] [585.689217ms] END
I0322 04:00:07.824245    2297 trace.go:236] Trace[522801362]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:92ca7db2-edc1-4ab9-a55b-aa57d5308808,client:127.0.0.1,protocol:HTTP/2.0,resource:services,scope:resource,url:/api/v1/namespaces/kube-system/services/kube-dns,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/,verb:GET (22-Mar-2024 04:00:07.141) (total time: 683ms):
Trace[522801362]: ---"About to write a response" 438ms (04:00:07.579)
Trace[522801362]: ---"Writing http response done" 244ms (04:00:07.824)
Trace[522801362]: [683.15502ms] [683.15502ms] END
I0322 04:00:07.878984    2297 trace.go:236] Trace[525309377]: "List(recursive=true) etcd3" audit-id:,key:/masterleases/,resourceVersion:0,resourceVersionMatch:NotOlderThan,limit:0,continue: (22-Mar-2024 04:00:07.301) (total time: 577ms):
Trace[525309377]: [577.696621ms] [577.696621ms] END
I0322 04:00:08.086910    2297 garbagecollector.go:816] "failed to discover some groups" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"
time="2024-03-22T04:00:08Z" level=info msg="certificate CN=serverworker signed by CN=k3s-server-ca@1711079624: notBefore=2024-03-22 03:53:44 +0000 UTC notAfter=2025-03-22 04:00:08 +0000 UTC"
I0322 04:00:08.167884    2297 trace.go:236] Trace[1565077264]: "Proxy via http_connect protocol over tcp" address:10.42.0.5:10250 (22-Mar-2024 04:00:07.388) (total time: 778ms):
Trace[1565077264]: [778.222532ms] [778.222532ms] END
E0322 04:00:08.174198    2297 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: error trying to reach service: proxy error from 127.0.0.1:6443 while dialing 10.42.0.5:10250, code 502: 502 Bad Gateway
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 04:00:08.205440    2297 trace.go:236] Trace[195232291]: "Update" accept:application/json, */*,audit-id:70d7e8a3-eb47-436f-9a1d-a5acbbaf7155,client:127.0.0.1,protocol:HTTP/2.0,resource:secrets,scope:resource,url:/api/v1/namespaces/kube-system/secrets/k3s-serving,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:PUT (22-Mar-2024 04:00:07.471) (total time: 733ms):
Trace[195232291]: ---"limitedReadBody succeeded" len:3803 15ms (04:00:07.486)
Trace[195232291]: ["GuaranteedUpdate etcd3" audit-id:70d7e8a3-eb47-436f-9a1d-a5acbbaf7155,key:/secrets/kube-system/k3s-serving,type:*core.Secret,resource:secrets 702ms (04:00:07.502)]
Trace[195232291]: ---"Write to database call succeeded" len:3803 20ms (04:00:08.204)
Trace[195232291]: [733.546102ms] [733.546102ms] END
I0322 04:00:08.285003    2297 trace.go:236] Trace[709073219]: "Proxy via http_connect protocol over tcp" address:10.42.0.5:10250 (22-Mar-2024 04:00:07.385) (total time: 899ms):
Trace[709073219]: [899.658441ms] [899.658441ms] END
I0322 04:00:08.717823    2297 trace.go:236] Trace[390094580]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:e8ae6546-fd28-40e6-b0f8-fca2a283c604,client:127.0.0.1,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/kube-system/pods/metrics-server-67c658944b-6v69k/status,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PATCH (22-Mar-2024 04:00:07.797) (total time: 911ms):
Trace[390094580]: ["GuaranteedUpdate etcd3" audit-id:e8ae6546-fd28-40e6-b0f8-fca2a283c604,key:/pods/kube-system/metrics-server-67c658944b-6v69k,type:*core.Pod,resource:pods 888ms (04:00:07.822)
Trace[390094580]:  ---"initial value restored" 214ms (04:00:08.037)
Trace[390094580]:  ---"About to Encode" 73ms (04:00:08.110)
Trace[390094580]:  ---"Txn call completed" 259ms (04:00:08.370)]
Trace[390094580]: ---"About to check admission control" 18ms (04:00:08.055)
Trace[390094580]: ---"Object stored in database" 556ms (04:00:08.612)
Trace[390094580]: ---"Writing http response done" 96ms (04:00:08.709)
Trace[390094580]: [911.480737ms] [911.480737ms] END
I0322 04:00:08.719130    2297 trace.go:236] Trace[1673894255]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:599b0b44-4fc4-4c64-a0a5-5f85840985fa,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/server/status,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PATCH (22-Mar-2024 04:00:07.794) (total time: 924ms):
Trace[1673894255]: ---"Recorded the audit event" 143ms (04:00:07.938)
Trace[1673894255]: ---"About to check admission control" 157ms (04:00:08.103)
Trace[1673894255]: ---"Object stored in database" 302ms (04:00:08.405)
Trace[1673894255]: ---"Writing http response done" 313ms (04:00:08.719)
Trace[1673894255]: [924.09684ms] [924.09684ms] END
I0322 04:00:08.734051    2297 trace.go:236] Trace[1122659608]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:31db9e8f-6c59-4f9a-ae52-9df6bd94275c,client:127.0.0.1,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:POST (22-Mar-2024 04:00:07.434) (total time: 1298ms):
Trace[1122659608]: ---"Conversion done" 190ms (04:00:07.624)
Trace[1122659608]: ["Create etcd3" audit-id:31db9e8f-6c59-4f9a-ae52-9df6bd94275c,key:/events/kube-system/traefik-f4564c4f4-d2b8m.17befa2ef5b7d9f5,type:*core.Event,resource:events 1095ms (04:00:07.637)
Trace[1122659608]:  ---"Txn call succeeded" 782ms (04:00:08.421)]
Trace[1122659608]: ---"Write to database call succeeded" len:427 22ms (04:00:08.460)
Trace[1122659608]: ---"Writing http response done" 272ms (04:00:08.732)
Trace[1122659608]: [1.298685727s] [1.298685727s] END
I0322 04:00:09.454965    2297 trace.go:236] Trace[1820979744]: "List" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json,audit-id:2663c03c-0620-48e9-a88a-6dc9a520771e,client:127.0.0.1,protocol:HTTP/2.0,resource:serverstransporttcps,scope:cluster,url:/apis/traefik.io/v1alpha1/serverstransporttcps,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/metadata-informers,verb:LIST (22-Mar-2024 04:00:08.100) (total time: 1349ms):
Trace[1820979744]: ["List(recursive=true) etcd3" audit-id:2663c03c-0620-48e9-a88a-6dc9a520771e,key:/traefik.io/serverstransporttcps,resourceVersion:0,resourceVersionMatch:,limit:500,continue: 1349ms (04:00:08.101)]
Trace[1820979744]: ---"Listing from storage done" 350ms (04:00:09.242)
Trace[1820979744]: ---"Writing http response done" count:0 206ms (04:00:09.449)
Trace[1820979744]: [1.349183192s] [1.349183192s] END
I0322 04:00:09.849034    2297 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/metrics-server-67c658944b-6v69k" containerName="metrics-server"
I0322 04:00:09.870615    2297 trace.go:236] Trace[1452259885]: "Proxy via http_connect protocol over tcp" address:10.42.0.5:10250 (22-Mar-2024 04:00:08.219) (total time: 1643ms):
Trace[1452259885]: [1.64358991s] [1.64358991s] END
I0322 04:00:09.940306    2297 trace.go:236] Trace[534096790]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:401e5422-38e6-49cb-84a9-371c327cd54f,client:127.0.0.1,protocol:HTTP/2.0,resource:apiservices,scope:resource,url:/apis/apiregistration.k8s.io/v1/apiservices/v1beta1.metrics.k8s.io/status,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PUT (22-Mar-2024 04:00:08.149) (total time: 1789ms):
Trace[534096790]: ["GuaranteedUpdate etcd3" audit-id:401e5422-38e6-49cb-84a9-371c327cd54f,key:/apiregistration.k8s.io/apiservices/v1beta1.metrics.k8s.io,type:*apiregistration.APIService,resource:apiservices.apiregistration.k8s.io 1774ms (04:00:08.164)]
Trace[534096790]: ---"Write to database call failed" len:2063,err:Operation cannot be fulfilled on apiservices.apiregistration.k8s.io "v1beta1.metrics.k8s.io": the object has been modified; please apply your changes to the latest version and try again 306ms (04:00:09.685)
Trace[534096790]: [1.789402298s] [1.789402298s] END
W0322 04:00:10.754052    2297 handler_proxy.go:93] no RequestInfo found in the context
E0322 04:00:10.997995    2297 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: error trying to reach service: proxy error from 127.0.0.1:6443 while dialing 10.42.0.5:10250, code 502: 502 Bad Gateway
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 04:00:11.008571    2297 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0322 04:00:11.434326    2297 trace.go:236] Trace[1689656748]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:78fd6c36-3f47-4671-93d0-4bdce6609dda,client:127.0.0.1,protocol:HTTP/2.0,resource:services,scope:resource,url:/api/v1/namespaces/kube-system/services/metrics-server,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/,verb:GET (22-Mar-2024 04:00:08.322) (total time: 3098ms):
Trace[1689656748]: ---"About to write a response" 2409ms (04:00:10.731)
Trace[1689656748]: ---"Writing http response done" 689ms (04:00:11.420)
Trace[1689656748]: [3.098239522s] [3.098239522s] END
I0322 04:00:11.475551    2297 shared_informer.go:318] Caches are synced for resource quota
E0322 04:00:11.476326    2297 available_controller.go:460] v1beta1.metrics.k8s.io failed with: Operation cannot be fulfilled on apiservices.apiregistration.k8s.io "v1beta1.metrics.k8s.io": the object has been modified; please apply your changes to the latest version and try again
E0322 04:00:11.512100    2297 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 04:00:12.585164    2297 trace.go:236] Trace[1108144748]: "Proxy via http_connect protocol over tcp" address:10.42.0.5:10250 (22-Mar-2024 04:00:08.316) (total time: 3869ms):
Trace[1108144748]: [3.869538179s] [3.869538179s] END
W0322 04:00:12.998382    2297 handler_proxy.go:93] no RequestInfo found in the context
E0322 04:00:13.015615    2297 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 04:00:13.016353    2297 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0322 04:00:14.795075    2297 trace.go:236] Trace[1685399186]: "List" accept:application/json, */*,audit-id:183c2ce0-ee03-403a-b0df-d2cadb74e85a,client:10.42.0.8,protocol:HTTP/2.0,resource:ingressroutes,scope:cluster,url:/apis/traefik.io/v1alpha1/ingressroutes,user-agent:traefik/2.10.5 (linux/amd64) kubernetes/crd,verb:LIST (22-Mar-2024 04:00:13.984) (total time: 807ms):
Trace[1685399186]: ---"Writing http response done" count:1 807ms (04:00:14.792)
Trace[1685399186]: [807.949811ms] [807.949811ms] END
W0322 04:00:14.801606    2297 controller.go:134] slow openapi aggregation of "helmcharts.helm.cattle.io": 5.670508357s
I0322 04:00:15.147535    2297 trace.go:236] Trace[1962168578]: "List" accept:application/json, */*,audit-id:f23ec9c5-1a86-4c1b-a1cb-55d32429a52e,client:10.42.0.8,protocol:HTTP/2.0,resource:traefikservices,scope:cluster,url:/apis/traefik.containo.us/v1alpha1/traefikservices,user-agent:traefik/2.10.5 (linux/amd64) kubernetes/crd,verb:LIST (22-Mar-2024 04:00:13.956) (total time: 790ms):
Trace[1962168578]: ---"Writing http response done" count:0 788ms (04:00:14.746)
Trace[1962168578]: [790.398607ms] [790.398607ms] END
I0322 04:00:15.425421    2297 trace.go:236] Trace[1392327685]: "Proxy via http_connect protocol over tcp" address:10.42.0.5:10250 (22-Mar-2024 04:00:11.564) (total time: 3855ms):
Trace[1392327685]: [3.855205681s] [3.855205681s] END
I0322 04:00:15.605232    2297 trace.go:236] Trace[1742502477]: "Proxy via http_connect protocol over tcp" address:10.42.0.5:10250 (22-Mar-2024 04:00:11.626) (total time: 3973ms):
Trace[1742502477]: [3.973691521s] [3.973691521s] END
I0322 04:00:15.644810    2297 trace.go:236] Trace[957499169]: "List" accept:application/json, */*,audit-id:e378ed56-eb44-410c-84b1-9b9ba5d86d94,client:10.42.0.8,protocol:HTTP/2.0,resource:tlsstores,scope:cluster,url:/apis/traefik.containo.us/v1alpha1/tlsstores,user-agent:traefik/2.10.5 (linux/amd64) kubernetes/crd,verb:LIST (22-Mar-2024 04:00:13.943) (total time: 1063ms):
Trace[957499169]: ["cacher list" audit-id:e378ed56-eb44-410c-84b1-9b9ba5d86d94,type:tlsstores.traefik.containo.us 1054ms (04:00:13.953)
Trace[957499169]:  ---"watchCache locked acquired" 748ms (04:00:14.701)]
Trace[957499169]: [1.063399976s] [1.063399976s] END
I0322 04:00:15.677019    2297 trace.go:236] Trace[811093625]: "List" accept:application/json, */*,audit-id:6d346be2-6cd7-4c5c-9661-3cfd77f020d5,client:10.42.0.8,protocol:HTTP/2.0,resource:services,scope:cluster,url:/api/v1/services,user-agent:traefik/2.10.5 (linux/amd64) kubernetes/crd,verb:LIST (22-Mar-2024 04:00:14.585) (total time: 697ms):
Trace[811093625]: ---"About to List from storage" 317ms (04:00:14.902)
Trace[811093625]: ---"Writing http response done" count:4 377ms (04:00:15.282)
Trace[811093625]: [697.435451ms] [697.435451ms] END
I0322 04:00:16.117940    2297 trace.go:236] Trace[1502058803]: "Proxy via http_connect protocol over tcp" address:10.42.0.5:10250 (22-Mar-2024 04:00:11.585) (total time: 3950ms):
Trace[1502058803]: [3.950960579s] [3.950960579s] END
I0322 04:00:16.285420    2297 trace.go:236] Trace[1922778007]: "Proxy via http_connect protocol over tcp" address:10.42.0.5:10250 (22-Mar-2024 04:00:11.585) (total time: 4696ms):
Trace[1922778007]: [4.696122485s] [4.696122485s] END
I0322 04:00:16.364941    2297 trace.go:236] Trace[1001054320]: "Proxy via http_connect protocol over tcp" address:10.42.0.5:10250 (22-Mar-2024 04:00:11.626) (total time: 4736ms):
Trace[1001054320]: [4.736052932s] [4.736052932s] END
I0322 04:00:15.845440    2297 trace.go:236] Trace[1613108871]: "List" accept:application/json, */*,audit-id:57989d24-ff62-463e-bb01-31f19b6b3463,client:10.42.0.8,protocol:HTTP/2.0,resource:middlewaretcps,scope:cluster,url:/apis/traefik.containo.us/v1alpha1/middlewaretcps,user-agent:traefik/2.10.5 (linux/amd64) kubernetes/crd,verb:LIST (22-Mar-2024 04:00:14.882) (total time: 957ms):
Trace[1613108871]: ---"Writing http response done" count:0 939ms (04:00:15.840)
Trace[1613108871]: [957.985732ms] [957.985732ms] END
I0322 04:00:16.466424    2297 trace.go:236] Trace[1391863087]: "List" accept:application/json, */*,audit-id:06de30cd-7b0f-46f1-b4a8-9378961dbac1,client:10.42.0.8,protocol:HTTP/2.0,resource:traefikservices,scope:cluster,url:/apis/traefik.io/v1alpha1/traefikservices,user-agent:traefik/2.10.5 (linux/amd64) kubernetes/crd,verb:LIST (22-Mar-2024 04:00:15.740) (total time: 711ms):
Trace[1391863087]: ---"About to List from storage" 706ms (04:00:16.447)
Trace[1391863087]: [711.199383ms] [711.199383ms] END
I0322 04:00:16.776121    2297 trace.go:236] Trace[1083196731]: "List" accept:application/json, */*,audit-id:82dad88f-0ab5-4b04-b06d-45d34a7368d7,client:10.42.0.8,protocol:HTTP/2.0,resource:secrets,scope:cluster,url:/api/v1/secrets,user-agent:traefik/2.10.5 (linux/amd64) kubernetes/crd,verb:LIST (22-Mar-2024 04:00:15.455) (total time: 1310ms):
Trace[1083196731]: ---"Writing http response done" count:5 1306ms (04:00:16.766)
Trace[1083196731]: [1.310897663s] [1.310897663s] END
I0322 04:00:17.012835    2297 trace.go:236] Trace[53618909]: "Get" accept:application/json, */*,audit-id:2b160eda-30a2-4897-a421-071bcb82d7b1,client:127.0.0.1,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps/coredns,user-agent:k3s-supervisor@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:GET (22-Mar-2024 04:00:10.986) (total time: 6017ms):
Trace[53618909]: ---"About to write a response" 4838ms (04:00:15.828)
Trace[53618909]: ---"Writing http response done" 1176ms (04:00:17.004)
Trace[53618909]: [6.017862995s] [6.017862995s] END
I0322 04:00:17.451272    2297 trace.go:236] Trace[1248504196]: "List" accept:application/json, */*,audit-id:8184f608-9a7e-467a-af39-2c7914a541c9,client:10.42.0.8,protocol:HTTP/2.0,resource:tlsoptions,scope:cluster,url:/apis/traefik.containo.us/v1alpha1/tlsoptions,user-agent:traefik/2.10.5 (linux/amd64) kubernetes/crd,verb:LIST (22-Mar-2024 04:00:15.782) (total time: 1167ms):
Trace[1248504196]: ---"Writing http response done" count:0 625ms (04:00:16.950)
Trace[1248504196]: [1.167724479s] [1.167724479s] END
I0322 04:00:17.571285    2297 trace.go:236] Trace[1525101889]: "List" accept:application/json, */*,audit-id:2d95f4a4-f34f-4ddf-80fd-31e7cf139d37,client:10.42.0.8,protocol:HTTP/2.0,resource:ingressroutetcps,scope:cluster,url:/apis/traefik.containo.us/v1alpha1/ingressroutetcps,user-agent:traefik/2.10.5 (linux/amd64) kubernetes/crd,verb:LIST (22-Mar-2024 04:00:17.055) (total time: 508ms):
Trace[1525101889]: ---"About to List from storage" 506ms (04:00:17.562)
Trace[1525101889]: [508.278033ms] [508.278033ms] END
I0322 04:00:18.109185    2297 trace.go:236] Trace[1860630557]: "List" accept:application/json, */*,audit-id:bae7b09b-4042-411e-b10d-ed197dc99a5d,client:10.42.0.8,protocol:HTTP/2.0,resource:ingressroutes,scope:cluster,url:/apis/traefik.containo.us/v1alpha1/ingressroutes,user-agent:traefik/2.10.5 (linux/amd64) kubernetes/crd,verb:LIST (22-Mar-2024 04:00:17.004) (total time: 1098ms):
Trace[1860630557]: ---"About to List from storage" 533ms (04:00:17.538)
Trace[1860630557]: ---"Writing http response done" count:0 560ms (04:00:18.103)
Trace[1860630557]: [1.098675611s] [1.098675611s] END
I0322 04:00:18.401124    2297 trace.go:236] Trace[1609967834]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:42423448-5a3d-4dec-a969-80d14037de7c,client:10.42.0.6,protocol:HTTP/2.0,resource:endpointslices,scope:cluster,url:/apis/discovery.k8s.io/v1/endpointslices,user-agent:coredns/v0.0.0 (linux/amd64) kubernetes/$Format,verb:LIST (22-Mar-2024 04:00:17.083) (total time: 1310ms):
Trace[1609967834]: ---"Writing http response done" count:4 757ms (04:00:18.393)
Trace[1609967834]: [1.310125786s] [1.310125786s] END
I0322 04:00:18.425165    2297 trace.go:236] Trace[1534702163]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:17c78338-8116-4631-9484-335721901bc6,client:127.0.0.1,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/kube-system/pods/coredns-6799fbcd5-4tsrq,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 04:00:12.298) (total time: 6126ms):
Trace[1534702163]: ---"About to write a response" 5935ms (04:00:18.234)
Trace[1534702163]: ---"Writing http response done" 190ms (04:00:18.425)
Trace[1534702163]: [6.126474497s] [6.126474497s] END
E0322 04:00:18.767925    2297 kubelet.go:2477] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="8.204s"
I0322 04:00:19.104374    2297 trace.go:236] Trace[199048031]: "List" accept:application/json, */*,audit-id:077b7ffc-023f-4ce5-9521-6b1bcbc41b63,client:10.42.0.8,protocol:HTTP/2.0,resource:endpoints,scope:cluster,url:/api/v1/endpoints,user-agent:traefik/2.10.5 (linux/amd64) kubernetes/crd,verb:LIST (22-Mar-2024 04:00:17.777) (total time: 1319ms):
Trace[199048031]: ---"Writing http response done" count:4 880ms (04:00:19.097)
Trace[199048031]: [1.319978464s] [1.319978464s] END
I0322 04:00:19.210775    2297 trace.go:236] Trace[1352522840]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.56.110,type:*v1.Endpoints,resource:apiServerIPInfo (22-Mar-2024 04:00:09.660) (total time: 9544ms):
Trace[1352522840]: ---"initial value restored" 2158ms (04:00:11.819)
Trace[1352522840]: ---"Transaction prepared" 3011ms (04:00:14.832)
Trace[1352522840]: ---"Txn call completed" 4092ms (04:00:18.925)
Trace[1352522840]: ---"decode succeeded" len:67 278ms (04:00:19.204)
Trace[1352522840]: [9.544020746s] [9.544020746s] END
I0322 04:00:19.882381    2297 trace.go:236] Trace[2095358781]: "List" accept:application/json, */*,audit-id:9ae2bf5b-3f31-47fa-b7c4-6a52902309cf,client:10.42.0.8,protocol:HTTP/2.0,resource:endpoints,scope:cluster,url:/api/v1/endpoints,user-agent:traefik/2.10.5 (linux/amd64) kubernetes/ingress,verb:LIST (22-Mar-2024 04:00:19.329) (total time: 547ms):
Trace[2095358781]: ---"Writing http response done" count:4 546ms (04:00:19.876)
Trace[2095358781]: [547.371452ms] [547.371452ms] END
I0322 04:00:20.470369    2297 trace.go:236] Trace[379804079]: "Proxy via http_connect protocol over tcp" address:10.42.0.5:10250 (22-Mar-2024 04:00:15.723) (total time: 4739ms):
Trace[379804079]: [4.739758896s] [4.739758896s] END
I0322 04:00:20.557199    2297 trace.go:236] Trace[1171337499]: "List" accept:application/json, */*,audit-id:8df17160-19cd-40db-b7c7-daa510d635f4,client:10.42.0.2,protocol:HTTP/2.0,resource:storageclasses,scope:cluster,url:/apis/storage.k8s.io/v1/storageclasses,user-agent:local-path-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:LIST (22-Mar-2024 04:00:19.413) (total time: 1140ms):
Trace[1171337499]: ["cacher list" audit-id:8df17160-19cd-40db-b7c7-daa510d635f4,type:storageclasses.storage.k8s.io 1138ms (04:00:19.414)
Trace[1171337499]:  ---"watchCache locked acquired" 556ms (04:00:19.970)]
Trace[1171337499]: ---"Writing http response done" count:1 580ms (04:00:20.553)
Trace[1171337499]: [1.140192367s] [1.140192367s] END
I0322 04:00:21.102525    2297 trace.go:236] Trace[2092329418]: "Patch" accept:application/json, */*,audit-id:2e20cb18-51b8-4826-a217-23fd73a893ac,client:127.0.0.1,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events/traefik-crd.17bef9dca8728afe,user-agent:helm-controller@server/v1.28.7+k3s1 (linux/amd64) k3s/051b14b2,verb:PATCH (22-Mar-2024 04:00:08.211) (total time: 12883ms):
Trace[2092329418]: ["GuaranteedUpdate etcd3" audit-id:2e20cb18-51b8-4826-a217-23fd73a893ac,key:/events/kube-system/traefik-crd.17bef9dca8728afe,type:*core.Event,resource:events 12883ms (04:00:08.212)
Trace[2092329418]:  ---"initial value restored" 1342ms (04:00:09.554)
Trace[2092329418]:  ---"About to Encode" 6726ms (04:00:16.281)
Trace[2092329418]:  ---"Encode succeeded" len:673 571ms (04:00:16.852)
Trace[2092329418]:  ---"Txn call completed" 3326ms (04:00:20.185)
Trace[2092329418]:  ---"decode succeeded" len:673 300ms (04:00:20.486)]
Trace[2092329418]: ---"About to check admission control" 3345ms (04:00:12.901)
Trace[2092329418]: ---"Object stored in database" 7747ms (04:00:20.648)
Trace[2092329418]: ---"Writing http response done" 446ms (04:00:21.094)
Trace[2092329418]: [12.883859474s] [12.883859474s] END
I0322 04:00:20.883233    2297 trace.go:236] Trace[512195825]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:b8612938-70c5-4d47-9894-8996bc0749e8,client:127.0.0.1,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:POST (22-Mar-2024 04:00:11.439) (total time: 9440ms):
Trace[512195825]: ---"Conversion done" 325ms (04:00:11.771)
Trace[512195825]: ["Create etcd3" audit-id:b8612938-70c5-4d47-9894-8996bc0749e8,key:/events/kube-system/metrics-server-67c658944b-6v69k.17befa2f17f94614,type:*core.Event,resource:events 5395ms (04:00:15.484)
Trace[512195825]:  ---"Encode succeeded" len:763 946ms (04:00:16.430)
Trace[512195825]:  ---"Txn call succeeded" 3372ms (04:00:19.803)
Trace[512195825]:  ---"decode succeeded" len:763 484ms (04:00:20.288)]
Trace[512195825]: ---"Write to database call succeeded" len:456 120ms (04:00:20.408)
Trace[512195825]: ---"Writing http response done" 470ms (04:00:20.879)
Trace[512195825]: [9.440068474s] [9.440068474s] END
I0322 04:00:21.114130    2297 trace.go:236] Trace[65526356]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:cadfe9e3-7737-4819-a41c-a74122a4e5da,client:127.0.0.1,protocol:HTTP/2.0,resource:services,scope:resource,url:/api/v1/namespaces/kube-system/services/traefik,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/,verb:GET (22-Mar-2024 04:00:16.981) (total time: 4132ms):
Trace[65526356]: ---"About to Get from storage" 509ms (04:00:17.491)
Trace[65526356]: ---"About to write a response" 3186ms (04:00:20.677)
Trace[65526356]: ---"Writing http response done" 436ms (04:00:21.114)
Trace[65526356]: [4.13248662s] [4.13248662s] END
I0322 04:00:21.193014    2297 trace.go:236] Trace[972113814]: "List" accept:application/json, */*,audit-id:b11aa3f0-d0d4-4483-8b44-4f0f4d8cef23,client:10.42.0.8,protocol:HTTP/2.0,resource:secrets,scope:cluster,url:/api/v1/secrets,user-agent:traefik/2.10.5 (linux/amd64) kubernetes/ingress,verb:LIST (22-Mar-2024 04:00:19.226) (total time: 1570ms):
Trace[972113814]: ---"Writing http response done" count:5 1567ms (04:00:20.796)
Trace[972113814]: [1.57022861s] [1.57022861s] END
I0322 04:00:21.395493    2297 trace.go:236] Trace[92695600]: "List" accept:application/json, */*,audit-id:db599db9-d53e-4ced-9e5d-4c065e59d30f,client:10.42.0.8,protocol:HTTP/2.0,resource:ingressrouteudps,scope:cluster,url:/apis/traefik.containo.us/v1alpha1/ingressrouteudps,user-agent:traefik/2.10.5 (linux/amd64) kubernetes/crd,verb:LIST (22-Mar-2024 04:00:20.643) (total time: 740ms):
Trace[92695600]: ---"About to List from storage" 424ms (04:00:21.068)
Trace[92695600]: ---"Writing http response done" count:0 310ms (04:00:21.384)
Trace[92695600]: [740.904502ms] [740.904502ms] END
E0322 04:00:21.961987    2297 kubelet.go:2477] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="2.764s"
I0322 04:00:23.014983    2297 trace.go:236] Trace[633311498]: "Get" accept:application/json, */*,audit-id:fbe149e5-7d05-4bf4-899c-269a4e983c54,client:10.42.0.5,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps/extension-apiserver-authentication,user-agent:metrics-server/v0.6.3 (linux/amd64) kubernetes/a938798,verb:GET (22-Mar-2024 04:00:18.555) (total time: 4167ms):
Trace[633311498]: ---"About to write a response" 2895ms (04:00:21.453)
Trace[633311498]: ---"Writing http response done" 1270ms (04:00:22.723)
Trace[633311498]: [4.167779624s] [4.167779624s] END
I0322 04:00:24.430761    2297 trace.go:236] Trace[1540020851]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:9cd723a3-6500-46c7-9ae0-bc46372167cf,client:127.0.0.1,protocol:HTTP/2.0,resource:endpointslices,scope:resource,url:/apis/discovery.k8s.io/v1/namespaces/kube-system/endpointslices/metrics-server-fdhlb,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:endpointslice-controller,verb:PUT (22-Mar-2024 04:00:11.807) (total time: 12380ms):
Trace[1540020851]: ---"limitedReadBody succeeded" len:1247 79ms (04:00:11.886)
Trace[1540020851]: ---"About to convert to expected version" 310ms (04:00:12.197)
Trace[1540020851]: ---"About to store object in database" 172ms (04:00:12.376)
Trace[1540020851]: ["GuaranteedUpdate etcd3" audit-id:9cd723a3-6500-46c7-9ae0-bc46372167cf,key:/endpointslices/kube-system/metrics-server-fdhlb,type:*discovery.EndpointSlice,resource:endpointslices.discovery.k8s.io 11213ms (04:00:12.976)
Trace[1540020851]:  ---"initial value restored" 569ms (04:00:13.545)
Trace[1540020851]:  ---"About to Encode" 7134ms (04:00:20.679)
Trace[1540020851]:  ---"Txn call completed" 2287ms (04:00:22.968)
Trace[1540020851]:  ---"decode succeeded" len:1284 541ms (04:00:23.510)]
Trace[1540020851]: ---"Write to database call succeeded" len:1247 307ms (04:00:23.817)
Trace[1540020851]: ---"Writing http response done" 370ms (04:00:24.187)
Trace[1540020851]: [12.380896914s] [12.380896914s] END
I0322 04:00:24.605334    2297 trace.go:236] Trace[1898268897]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:7d5f2d67-6e32-465b-9ab4-2b4cb2e0654b,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/server,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PUT (22-Mar-2024 04:00:19.273) (total time: 5330ms):
Trace[1898268897]: ["GuaranteedUpdate etcd3" audit-id:7d5f2d67-6e32-465b-9ab4-2b4cb2e0654b,key:/leases/kube-node-lease/server,type:*coordination.Lease,resource:leases.coordination.k8s.io 5245ms (04:00:19.358)
Trace[1898268897]:  ---"About to Encode" 2037ms (04:00:21.400)
Trace[1898268897]:  ---"Encode succeeded" len:462 301ms (04:00:21.701)
Trace[1898268897]:  ---"Txn call completed" 2845ms (04:00:24.547)]
Trace[1898268897]: ---"Write to database call succeeded" len:465 50ms (04:00:24.601)
Trace[1898268897]: [5.330732973s] [5.330732973s] END
E0322 04:00:24.755272    2297 kubelet.go:2477] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="2.189s"
I0322 04:00:24.811659    2297 trace.go:236] Trace[908250557]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:9f4d7ed0-4dd5-4bd3-8daf-31aa56e3080b,client:127.0.0.1,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/metrics-server,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:endpoint-controller,verb:PUT (22-Mar-2024 04:00:12.328) (total time: 12257ms):
Trace[908250557]: ---"Conversion done" 490ms (04:00:12.831)
Trace[908250557]: ["GuaranteedUpdate etcd3" audit-id:9f4d7ed0-4dd5-4bd3-8daf-31aa56e3080b,key:/services/endpoints/kube-system/metrics-server,type:*core.Endpoints,resource:endpoints 10672ms (04:00:13.915)
Trace[908250557]:  ---"initial value restored" 685ms (04:00:14.601)
Trace[908250557]:  ---"About to Encode" 6015ms (04:00:20.616)
Trace[908250557]:  ---"Encode succeeded" len:761 706ms (04:00:21.323)
Trace[908250557]:  ---"Txn call completed" 2461ms (04:00:23.785)
Trace[908250557]:  ---"decode succeeded" len:761 386ms (04:00:24.171)]
Trace[908250557]: ---"Write to database call succeeded" len:764 94ms (04:00:24.266)
Trace[908250557]: ---"Writing http response done" 318ms (04:00:24.584)
Trace[908250557]: [12.257725373s] [12.257725373s] END
I0322 04:00:24.922075    2297 trace.go:236] Trace[1288672159]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:7b7da9c8-bf8b-4034-8c55-d341441de7ca,client:127.0.0.1,protocol:HTTP/2.0,resource:replicasets,scope:resource,url:/apis/apps/v1/namespaces/kube-system/replicasets/metrics-server-67c658944b/status,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:replicaset-controller,verb:PUT (22-Mar-2024 04:00:12.752) (total time: 11922ms):
Trace[1288672159]: ---"limitedReadBody succeeded" len:5357 219ms (04:00:12.972)
Trace[1288672159]: ---"About to convert to expected version" 537ms (04:00:13.510)
Trace[1288672159]: ---"Conversion done" 718ms (04:00:14.228)
Trace[1288672159]: ---"About to store object in database" 616ms (04:00:14.844)
Trace[1288672159]: ["GuaranteedUpdate etcd3" audit-id:7b7da9c8-bf8b-4034-8c55-d341441de7ca,key:/replicasets/kube-system/metrics-server-67c658944b,type:*apps.ReplicaSet,resource:replicasets.apps 9315ms (04:00:15.360)
Trace[1288672159]:  ---"initial value restored" 941ms (04:00:16.301)
Trace[1288672159]:  ---"About to Encode" 4121ms (04:00:20.423)
Trace[1288672159]:  ---"Encode succeeded" len:5306 474ms (04:00:20.897)
Trace[1288672159]:  ---"Txn call completed" 3235ms (04:00:24.132)
Trace[1288672159]:  ---"decode succeeded" len:5306 405ms (04:00:24.539)]
Trace[1288672159]: ---"Write to database call succeeded" len:5357 47ms (04:00:24.586)
Trace[1288672159]: ---"Writing http response done" 88ms (04:00:24.675)
Trace[1288672159]: [11.922184702s] [11.922184702s] END
I0322 04:00:25.341778    2297 status_manager.go:317] "Container readiness changed for unknown container" pod="kube-system/traefik-f4564c4f4-d2b8m" containerID="containerd://61d7fde647a627694f95932590ebf135feb434a150daf01220d491ed7230c443"
I0322 04:00:25.689902    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="16.028113852s"
I0322 04:00:26.197015    2297 trace.go:236] Trace[1726838341]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:2991ba26-0c32-443e-aaa4-d90fd0684f22,client:127.0.0.1,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/default/endpoints/kubernetes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 04:00:21.411) (total time: 4505ms):
Trace[1726838341]: ---"About to write a response" 3901ms (04:00:25.312)
Trace[1726838341]: ---"Writing http response done" 604ms (04:00:25.916)
Trace[1726838341]: [4.505279539s] [4.505279539s] END
I0322 04:00:26.300110    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="204.461441ms"
time="2024-03-22T04:00:26Z" level=info msg="certificate CN=serverworker signed by CN=k3s-server-ca@1711079624: notBefore=2024-03-22 03:53:44 +0000 UTC notAfter=2025-03-22 04:00:26 +0000 UTC"
W0322 04:00:26.455918    2297 controller.go:134] slow openapi aggregation of "ingressroutes.traefik.io": 11.55260027s
I0322 04:00:26.649312    2297 trace.go:236] Trace[387104306]: "List" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json,audit-id:09d0b41e-181e-4bea-8ab8-eb2a04931b40,client:10.42.0.5,protocol:HTTP/2.0,resource:pods,scope:cluster,url:/api/v1/pods,user-agent:metrics-server/v0.6.3 (linux/amd64) kubernetes/a938798,verb:LIST (22-Mar-2024 04:00:25.930) (total time: 718ms):
Trace[387104306]: ---"About to List from storage" 292ms (04:00:26.223)
Trace[387104306]: ---"Writing http response done" count:5 422ms (04:00:26.649)
Trace[387104306]: [718.453781ms] [718.453781ms] END
I0322 04:00:26.646097    2297 trace.go:236] Trace[2124328696]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:8fe387c1-da86-4cea-a472-7fa1c48b595d,client:10.42.0.5,protocol:HTTP/2.0,resource:nodes,scope:cluster,url:/api/v1/nodes,user-agent:metrics-server/v0.6.3 (linux/amd64) kubernetes/a938798,verb:LIST (22-Mar-2024 04:00:25.976) (total time: 664ms):
Trace[2124328696]: ---"Writing http response done" count:1 655ms (04:00:26.641)
Trace[2124328696]: [664.683421ms] [664.683421ms] END
E0322 04:00:27.081350    2297 kubelet.go:2477] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.526s"
I0322 04:00:27.407908    2297 trace.go:236] Trace[1331454302]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:78a5b925-11cc-49d4-9194-fcbe00a98c5d,client:127.0.0.1,protocol:HTTP/2.0,resource:apiservices,scope:resource,url:/apis/apiregistration.k8s.io/v1/apiservices/v1beta1.metrics.k8s.io/status,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PUT (22-Mar-2024 04:00:19.254) (total time: 7860ms):
Trace[1331454302]: ---"limitedReadBody succeeded" len:2063 57ms (04:00:19.311)
Trace[1331454302]: ["GuaranteedUpdate etcd3" audit-id:78a5b925-11cc-49d4-9194-fcbe00a98c5d,key:/apiregistration.k8s.io/apiservices/v1beta1.metrics.k8s.io,type:*apiregistration.APIService,resource:apiservices.apiregistration.k8s.io 7795ms (04:00:19.322)
Trace[1331454302]:  ---"initial value restored" 539ms (04:00:19.862)
Trace[1331454302]:  ---"About to Encode" 4140ms (04:00:24.002)
Trace[1331454302]:  ---"Txn call completed" 2524ms (04:00:26.536)]
Trace[1331454302]: ---"Write to database call succeeded" len:2063 194ms (04:00:26.741)
Trace[1331454302]: ---"Writing http response done" 374ms (04:00:27.115)
Trace[1331454302]: [7.860903093s] [7.860903093s] END
W0322 04:00:27.725118    2297 handler_proxy.go:93] no RequestInfo found in the context
E0322 04:00:27.728831    2297 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 04:00:27.770107    2297 trace.go:236] Trace[738091123]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:a5b1bc34-352c-47fe-b26b-a22f86f59c43,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-7zoch227uv4owxg75pgtnmv3jq,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PUT (22-Mar-2024 04:00:20.685) (total time: 6836ms):
Trace[738091123]: ["GuaranteedUpdate etcd3" audit-id:a5b1bc34-352c-47fe-b26b-a22f86f59c43,key:/leases/kube-system/apiserver-7zoch227uv4owxg75pgtnmv3jq,type:*coordination.Lease,resource:leases.coordination.k8s.io 6265ms (04:00:21.257)
Trace[738091123]:  ---"initial value restored" 297ms (04:00:21.554)
Trace[738091123]:  ---"About to Encode" 3707ms (04:00:25.261)
Trace[738091123]:  ---"Txn call completed" 1773ms (04:00:27.036)]
Trace[738091123]: ---"Write to database call succeeded" len:590 256ms (04:00:27.302)
Trace[738091123]: ---"Writing http response done" 218ms (04:00:27.521)
Trace[738091123]: [6.836377544s] [6.836377544s] END
E0322 04:00:27.815310    2297 available_controller.go:460] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.42.0.5:10250/apis/metrics.k8s.io/v1beta1: Get "https://10.42.0.5:10250/apis/metrics.k8s.io/v1beta1": proxy error from 127.0.0.1:6443 while dialing 10.42.0.5:10250, code 502: 502 Bad Gateway
E0322 04:00:27.733899    2297 handler_proxy.go:137] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
I0322 04:00:28.391982    2297 trace.go:236] Trace[944404608]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:5be04ebf-0a44-41a3-abc6-6b41ce6afa2c,client:127.0.0.1,protocol:HTTP/2.0,resource:services,scope:resource,url:/api/v1/namespaces/kube-system/services/metrics-server,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/,verb:GET (22-Mar-2024 04:00:26.524) (total time: 1600ms):
Trace[944404608]: ---"About to write a response" 1081ms (04:00:27.606)
Trace[944404608]: ---"Writing http response done" 518ms (04:00:28.124)
Trace[944404608]: [1.600159235s] [1.600159235s] END
I0322 04:00:28.444389    2297 trace.go:236] Trace[1960519065]: "List(recursive=true) etcd3" audit-id:,key:/masterleases/,resourceVersion:0,resourceVersionMatch:NotOlderThan,limit:0,continue: (22-Mar-2024 04:00:26.372) (total time: 1904ms):
Trace[1960519065]: [1.90407109s] [1.90407109s] END
I0322 04:00:28.740035    2297 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0322 04:00:28.741008    2297 shared_informer.go:318] Caches are synced for garbage collector
W0322 04:00:28.752578    2297 handler_proxy.go:93] no RequestInfo found in the context
E0322 04:00:28.757392    2297 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 04:00:28.757518    2297 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E0322 04:00:29.145347    2297 kubelet.go:2477] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.674s"
I0322 04:00:29.214286    2297 trace.go:236] Trace[866535561]: "Get" accept:application/json, */*,audit-id:9dcdc930-a284-4022-95b5-ea143805bceb,client:10.42.0.5,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps/extension-apiserver-authentication,user-agent:metrics-server/v0.6.3 (linux/amd64) kubernetes/a938798,verb:GET (22-Mar-2024 04:00:27.671) (total time: 1525ms):
Trace[866535561]: ---"About to write a response" 1294ms (04:00:28.967)
Trace[866535561]: ---"Writing http response done" 229ms (04:00:29.197)
Trace[866535561]: [1.525478901s] [1.525478901s] END
I0322 04:00:30.630253    2297 trace.go:236] Trace[936819560]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:929fb494-b652-40c0-a2ee-79d12a73191c,client:127.0.0.1,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/kube-system/pods/coredns-6799fbcd5-4tsrq/status,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PATCH (22-Mar-2024 04:00:21.466) (total time: 9156ms):
Trace[936819560]: ["GuaranteedUpdate etcd3" audit-id:929fb494-b652-40c0-a2ee-79d12a73191c,key:/pods/kube-system/coredns-6799fbcd5-4tsrq,type:*core.Pod,resource:pods 8028ms (04:00:22.594)
Trace[936819560]:  ---"initial value restored" 250ms (04:00:22.845)
Trace[936819560]:  ---"About to Encode" 5672ms (04:00:28.517)
Trace[936819560]:  ---"Txn call completed" 961ms (04:00:29.485)
Trace[936819560]:  ---"decode succeeded" len:5008 473ms (04:00:29.959)]
Trace[936819560]: ---"About to check admission control" 4531ms (04:00:27.376)
Trace[936819560]: ---"Object stored in database" 2586ms (04:00:29.963)
Trace[936819560]: ---"Writing http response done" 658ms (04:00:30.622)
Trace[936819560]: [9.156828993s] [9.156828993s] END
I0322 04:00:30.937403    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="420.486725ms"
I0322 04:00:30.981404    2297 trace.go:236] Trace[2015060048]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:f76aa3f3-b388-4ec7-b7b0-0967a1e9f3e4,client:127.0.0.1,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events/traefik-f4564c4f4-d2b8m.17befa2ef5b7d9f5,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PATCH (22-Mar-2024 04:00:24.776) (total time: 6122ms):
Trace[2015060048]: ["GuaranteedUpdate etcd3" audit-id:f76aa3f3-b388-4ec7-b7b0-0967a1e9f3e4,key:/events/kube-system/traefik-f4564c4f4-d2b8m.17befa2ef5b7d9f5,type:*core.Event,resource:events 5050ms (04:00:25.848)
Trace[2015060048]:  ---"initial value restored" 1259ms (04:00:27.108)
Trace[2015060048]:  ---"About to Encode" 2113ms (04:00:29.221)
Trace[2015060048]:  ---"Txn call completed" 1111ms (04:00:30.340)]
Trace[2015060048]: ---"About to check admission control" 1569ms (04:00:28.677)
Trace[2015060048]: ---"Object stored in database" 1944ms (04:00:30.622)
Trace[2015060048]: ---"Writing http response done" 276ms (04:00:30.898)
Trace[2015060048]: [6.122389016s] [6.122389016s] END
E0322 04:00:31.134745    2297 kubelet.go:2477] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.853s"
I0322 04:00:31.292775    2297 trace.go:236] Trace[610516678]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:d1aef661-36a1-42c5-baf8-1e8ef2d0d133,client:127.0.0.1,protocol:HTTP/2.0,resource:endpointslices,scope:resource,url:/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/kubernetes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 04:00:29.713) (total time: 1570ms):
Trace[610516678]: ---"About to write a response" 1473ms (04:00:31.192)
Trace[610516678]: [1.570662832s] [1.570662832s] END
I0322 04:00:31.851581    2297 trace.go:236] Trace[1577083926]: "List" accept:application/json, */*,audit-id:564b6bd2-e493-4796-92ee-bfcf76288983,client:10.42.0.5,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps,user-agent:metrics-server/v0.6.3 (linux/amd64) kubernetes/a938798,verb:GET (22-Mar-2024 04:00:31.270) (total time: 575ms):
Trace[1577083926]: ---"Writing http response done" count:1 389ms (04:00:31.846)
Trace[1577083926]: [575.792899ms] [575.792899ms] END
W0322 04:00:32.215270    2297 controller.go:134] slow openapi aggregation of "serverstransports.traefik.containo.us": 5.753871492s
E0322 04:00:32.300468    2297 kubelet.go:2477] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.058s"
I0322 04:00:32.512783    2297 trace.go:236] Trace[126496023]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:91cdc621-f618-44a1-b6ea-eee05b8b20bc,client:127.0.0.1,protocol:HTTP/2.0,resource:deployments,scope:resource,url:/apis/apps/v1/namespaces/kube-system/deployments/metrics-server/status,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:deployment-controller,verb:PUT (22-Mar-2024 04:00:27.646) (total time: 4862ms):
Trace[126496023]: ---"limitedReadBody succeeded" len:5736 131ms (04:00:27.777)
Trace[126496023]: ---"Conversion done" 206ms (04:00:27.986)
Trace[126496023]: ---"About to store object in database" 175ms (04:00:28.161)
Trace[126496023]: ["GuaranteedUpdate etcd3" audit-id:91cdc621-f618-44a1-b6ea-eee05b8b20bc,key:/deployments/kube-system/metrics-server,type:*apps.Deployment,resource:deployments.apps 4140ms (04:00:28.368)
Trace[126496023]:  ---"initial value restored" 225ms (04:00:28.594)
Trace[126496023]:  ---"About to Encode" 3175ms (04:00:31.769)
Trace[126496023]:  ---"Txn call completed" 601ms (04:00:32.379)
Trace[126496023]:  ---"decode succeeded" len:5714 100ms (04:00:32.480)]
Trace[126496023]: ---"Write to database call succeeded" len:5736 20ms (04:00:32.501)
Trace[126496023]: [4.862442657s] [4.862442657s] END
I0322 04:00:32.723856    2297 trace.go:236] Trace[963300950]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:aa546c26-9b06-4682-9c54-496f593e56d9,client:127.0.0.1,protocol:HTTP/2.0,resource:apiservices,scope:resource,url:/apis/apiregistration.k8s.io/v1/apiservices/v1beta1.metrics.k8s.io/status,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PUT (22-Mar-2024 04:00:29.163) (total time: 3513ms):
Trace[963300950]: ---"limitedReadBody succeeded" len:1919 48ms (04:00:29.212)
Trace[963300950]: ["GuaranteedUpdate etcd3" audit-id:aa546c26-9b06-4682-9c54-496f593e56d9,key:/apiregistration.k8s.io/apiservices/v1beta1.metrics.k8s.io,type:*apiregistration.APIService,resource:apiservices.apiregistration.k8s.io 3027ms (04:00:29.650)
Trace[963300950]:  ---"initial value restored" 367ms (04:00:30.018)
Trace[963300950]:  ---"About to Encode" 1884ms (04:00:31.902)
Trace[963300950]:  ---"Encode succeeded" len:2420 220ms (04:00:32.122)
Trace[963300950]:  ---"Txn call completed" 539ms (04:00:32.662)]
Trace[963300950]: [3.513992634s] [3.513992634s] END
I0322 04:00:32.756878    2297 trace.go:236] Trace[1746457750]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:2c25a172-7f2e-440c-93cb-379e54970a68,client:127.0.0.1,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/kube-system/pods/traefik-f4564c4f4-d2b8m,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 04:00:32.150) (total time: 605ms):
Trace[1746457750]: ---"About to write a response" 503ms (04:00:32.656)
Trace[1746457750]: [605.626416ms] [605.626416ms] END
W0322 04:00:32.816201    2297 handler_proxy.go:93] no RequestInfo found in the context
E0322 04:00:32.824285    2297 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
E0322 04:00:32.872715    2297 handler_proxy.go:137] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
I0322 04:00:33.109503    2297 trace.go:236] Trace[1138106414]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.56.110,type:*v1.Endpoints,resource:apiServerIPInfo (22-Mar-2024 04:00:31.872) (total time: 1236ms):
Trace[1138106414]: ---"initial value restored" 648ms (04:00:32.521)
Trace[1138106414]: ---"Transaction prepared" 256ms (04:00:32.778)
Trace[1138106414]: ---"Txn call completed" 326ms (04:00:33.104)
Trace[1138106414]: [1.2367958s] [1.2367958s] END
I0322 04:00:33.125261    2297 trace.go:236] Trace[1155402006]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:fdf19187-aef5-4f07-bfcd-e54f0c1feaed,client:127.0.0.1,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:POST (22-Mar-2024 04:00:32.454) (total time: 608ms):
Trace[1155402006]: ---"About to store object in database" 33ms (04:00:32.501)
Trace[1155402006]: ---"Writing http response done" 83ms (04:00:33.062)
Trace[1155402006]: [608.615346ms] [608.615346ms] END
W0322 04:00:33.832474    2297 handler_proxy.go:93] no RequestInfo found in the context
E0322 04:00:33.873702    2297 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 04:00:33.873840    2297 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0322 04:00:34.377972    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="901.37143ms"
I0322 04:00:34.382660    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="3.37845ms"
I0322 04:00:34.514922    2297 trace.go:236] Trace[328102436]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:68ee7070-b86b-458f-a560-3a36283f7956,client:127.0.0.1,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:POST (22-Mar-2024 04:00:33.833) (total time: 567ms):
Trace[328102436]: ---"Write to database call succeeded" len:429 33ms (04:00:34.232)
Trace[328102436]: ---"Writing http response done" 166ms (04:00:34.399)
Trace[328102436]: [567.186991ms] [567.186991ms] END
I0322 04:00:34.732785    2297 trace.go:236] Trace[1941633188]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:b34e9423-d0e5-4cce-b6bb-ded6ab96ccc0,client:127.0.0.1,protocol:HTTP/2.0,resource:endpointslices,scope:resource,url:/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/kubernetes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 04:00:33.985) (total time: 745ms):
Trace[1941633188]: ---"About to write a response" 681ms (04:00:34.666)
Trace[1941633188]: [745.419049ms] [745.419049ms] END
I0322 04:00:34.733042    2297 trace.go:236] Trace[1390965208]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:99a384cb-5dc4-4509-b8e3-268695047f14,client:127.0.0.1,protocol:HTTP/2.0,resource:endpointslices,scope:resource,url:/apis/discovery.k8s.io/v1/namespaces/kube-system/endpointslices/traefik-kzmq8,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:endpointslice-controller,verb:PUT (22-Mar-2024 04:00:34.016) (total time: 714ms):
Trace[1390965208]: ["GuaranteedUpdate etcd3" audit-id:99a384cb-5dc4-4509-b8e3-268695047f14,key:/endpointslices/kube-system/traefik-kzmq8,type:*discovery.EndpointSlice,resource:endpointslices.discovery.k8s.io 698ms (04:00:34.032)
Trace[1390965208]:  ---"About to Encode" 521ms (04:00:34.557)
Trace[1390965208]:  ---"Txn call completed" 166ms (04:00:34.728)]
Trace[1390965208]: [714.267377ms] [714.267377ms] END
I0322 04:00:35.219156    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="236.908s"
I0322 04:00:35.318659    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="112.633s"
time="2024-03-22T04:00:35Z" level=info msg="certificate CN=serverworker signed by CN=k3s-server-ca@1711079624: notBefore=2024-03-22 03:53:44 +0000 UTC notAfter=2025-03-22 04:00:35 +0000 UTC"
I0322 04:00:35.602791    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="162.513965ms"
I0322 04:00:35.631413    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="112.787s"
time="2024-03-22T04:00:35Z" level=info msg="certificate CN=system:node:serverworker,O=system:nodes signed by CN=k3s-client-ca@1711079624: notBefore=2024-03-22 03:53:44 +0000 UTC notAfter=2025-03-22 04:00:35 +0000 UTC"
time="2024-03-22T04:00:38Z" level=info msg="Handling backend connection request [serverworker]"
E0322 04:00:41.555900    2297 kubelet.go:2477] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.281s"
I0322 04:00:45.407731    2297 trace.go:236] Trace[1569494340]: "SerializeObject" audit-id:c4ccfa82-79f1-4126-a2fe-9b74535f7a95,method:GET,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/serverworker,protocol:HTTP/2.0,mediaType:application/vnd.kubernetes.protobuf,encoder:{"encodeGV":"coordination.k8s.io/v1","encoder":"protobuf","name":"versioning"} (22-Mar-2024 04:00:38.912) (total time: 6479ms):
Trace[1569494340]: ---"About to start writing response" size:157 2522ms (04:00:41.434)
Trace[1569494340]: [6.479996894s] [6.479996894s] END
I0322 04:00:45.507955    2297 request.go:697] Waited for 1.621581325s due to client-side throttling, not priority and fairness, request: GET:https://127.0.0.1:6444/api
I0322 04:00:47.227035    2297 trace.go:236] Trace[1203257043]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:c4ccfa82-79f1-4126-a2fe-9b74535f7a95,client:192.168.56.111,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 04:00:38.286) (total time: 8928ms):
Trace[1203257043]: [8.9288455s] [8.9288455s] END
time="2024-03-22T04:00:46Z" level=info msg="Slow SQL (started: 2024-03-22 04:00:39.405261092 +0000 UTC m=+415.142227310) (total time: 5.87644875s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC LIMIT 500 : [[% 756]]"
E0322 04:00:56.626501    2297 health_controller.go:162] Metrics Controller heartbeat missed
E0322 04:01:02.380711    2297 health_controller.go:162] Metrics Controller heartbeat missed
I0322 04:01:07.744452    2297 trace.go:236] Trace[1999520663]: "iptables ChainExists" (22-Mar-2024 04:00:48.564) (total time: 12458ms):
Trace[1999520663]: [12.458410667s] [12.458410667s] END
E0322 04:01:13.229697    2297 handler_proxy.go:137] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
I0322 04:01:20.386017    2297 trace.go:236] Trace[1826351955]: "SerializeObject" audit-id:1072067d-ff5d-438c-bda0-2f0a113db695,method:GET,url:/apis/storage.k8s.io/v1/csinodes/serverworker,protocol:HTTP/2.0,mediaType:application/vnd.kubernetes.protobuf,encoder:{"encodeGV":"storage.k8s.io/v1","encoder":"protobuf","name":"versioning"} (22-Mar-2024 04:00:43.950) (total time: 10919ms):
Trace[1826351955]: ---"About to start writing response" size:150 5991ms (04:00:49.941)
Trace[1826351955]: ---"Write call succeeded" writer:responsewriter.outerWithCloseNotifyAndFlush,size:150,firstWrite:true 4918ms (04:00:54.859)
Trace[1826351955]: [10.919127585s] [10.919127585s] END
I0322 04:01:20.777588    2297 request.go:697] Waited for 13.548061539s due to client-side throttling, not priority and fairness, request: GET:https://127.0.0.1:6444/api
time="2024-03-22T04:01:21Z" level=info msg="Slow SQL (started: 2024-03-22 04:00:50.119189134 +0000 UTC m=+425.856155338) (total time: 6.21477798s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC LIMIT 1 : [[/registry/masterleases/192.168.56.110 true]]"
I0322 04:01:13.266131    2297 trace.go:236] Trace[384861012]: "iptables ChainExists" (22-Mar-2024 04:00:48.650) (total time: 8531ms):
Trace[384861012]: [8.531292803s] [8.531292803s] END
W0322 04:01:22.417698    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.EndpointSlice ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:22.773160    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:34.761441    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ClusterRoleBinding ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:35.543240    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Namespace ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:23.800470    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PersistentVolume ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:36.602553    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:37.402520    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:22.836161    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Lease ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:22.837792    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ClusterRole ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:22.838649    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.VolumeAttachment ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:22.839523    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ResourceQuota ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:22.840059    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CustomResourceDefinition ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:22.841048    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Endpoints ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:22.841683    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:22.842322    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Namespace ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:22.842557    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1beta3.PriorityLevelConfiguration ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:22.843206    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.APIService ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:22.843280    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:22.843411    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.LimitRange ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:22.843593    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.RuntimeClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:23.214667    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:23.215333    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.DaemonSet ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:23.215694    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PersistentVolume ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:23.215865    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.EndpointSlice ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:23.216377    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.IngressClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:23.216852    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Role ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:23.217097    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:23.308302    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:23.522911    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:23.534959    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:23.567669    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:23.568074    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PriorityClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:23.612444    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:23.613263    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:23.614288    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ValidatingWebhookConfiguration ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:23.614790    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.MutatingWebhookConfiguration ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:23.797306    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CSIDriver ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:23.798039    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ReplicaSet ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:23.798218    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PersistentVolumeClaim ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:23.798741    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.StorageClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:23.799279    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Namespace ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:23.799349    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CSINode ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:23.799732    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.StatefulSet ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:23.800105    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PodDisruptionBudget ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:23.879346    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CSIStorageCapacity ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:23.880618    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:23.881147    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I0322 04:01:28.701621    2297 trace.go:236] Trace[954361825]: "iptables ChainExists" (22-Mar-2024 04:00:52.991) (total time: 24977ms):
Trace[954361825]: [24.977300232s] [24.977300232s] END
W0322 04:01:29.822680    2297 transport.go:301] Unable to cancel request for *otelhttp.Transport
E0322 04:01:21.432916    2297 health_controller.go:162] Metrics Controller heartbeat missed
W0322 04:01:37.925230    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.RoleBinding ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I0322 04:01:41.092059    2297 trace.go:236] Trace[204184956]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:1072067d-ff5d-438c-bda0-2f0a113db695,client:192.168.56.111,protocol:HTTP/2.0,resource:csinodes,scope:resource,url:/apis/storage.k8s.io/v1/csinodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 04:00:38.285) (total time: 42105ms):
Trace[204184956]: [42.105779868s] [42.105779868s] END
W0322 04:01:41.396946    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:30.127480    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:48.458861    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:48.676843    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:34.171105    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:33.485248    2297 reflector.go:458] object-"kube-system"/"kube-root-ca.crt": watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:49.916260    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:51.759710    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.NetworkPolicy ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:52.600658    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ValidatingWebhookConfiguration ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:35.347912    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:35.483563    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:54.628355    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Lease ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 04:01:56.954120    2297 health_controller.go:162] Metrics Controller heartbeat missed
I0322 04:01:59.982640    2297 trace.go:236] Trace[264930677]: "DeltaFIFO Pop Process" ID:v1.batch,Depth:24,Reason:slow event handlers blocking the queue (22-Mar-2024 04:01:25.969) (total time: 2585ms):
Trace[264930677]: [2.585799387s] [2.585799387s] END
W0322 04:02:01.723632    2297 transport.go:301] Unable to cancel request for *otelhttp.Transport
W0322 04:02:01.926046    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:02.903960    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:51.224493    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Endpoints ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 04:02:06.504782    2297 timeout.go:142] post-timeout activity - time-elapsed: 21.431848751s, GET "/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/serverworker" result: <nil>
E0322 04:01:50.604580    2297 controller.go:193] "Failed to update lease" err="Put \"https://127.0.0.1:6444/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-7zoch227uv4owxg75pgtnmv3jq\": http2: client connection lost"
W0322 04:02:07.924604    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v2.HorizontalPodAutoscaler ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:08.297251    2297 reflector.go:458] object-"kube-system"/"coredns": watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:08.542142    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Secret ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:52.529787    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:01:52.824711    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ServiceAccount ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:09.011532    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1beta3.FlowSchema ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 04:02:14.882948    2297 wrap.go:54] timeout or abort while handling: method=GET URI="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/serverworker?timeout=10s" audit-ID="c4ccfa82-79f1-4126-a2fe-9b74535f7a95"
W0322 04:01:59.944159    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.HelmChartConfig ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:00.457615    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Secret ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:01.549278    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I0322 04:02:19.405568    2297 trace.go:236] Trace[818748708]: "Calculate volume metrics of data for pod kube-system/traefik-f4564c4f4-d2b8m" (22-Mar-2024 04:01:28.410) (total time: 31273ms):
Trace[818748708]: [31.273641429s] [31.273641429s] END
W0322 04:02:21.345524    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PriorityClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:21.774399    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ServiceAccount ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:21.885849    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.LimitRange ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:21.940939    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Role ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:22.950961    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CronJob ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:23.637613    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PodDisruptionBudget ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:24.358941    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Job ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:24.379889    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:24.511119    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ClusterRoleBinding ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:24.511212    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:37.896375    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.RoleBinding ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:37.936494    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ResourceQuota ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:24.777618    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:25.005025    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:25.158593    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:38.736465    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 04:02:38.995251    2297 resource_quota_controller.go:440] failed to discover resources: Get "https://127.0.0.1:6444/api": http2: client connection lost
W0322 04:02:39.017444    2297 handler_proxy.go:93] no RequestInfo found in the context
W0322 04:02:24.380869    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CSIDriver ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:24.429722    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.RuntimeClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:39.596634    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ReplicationController ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:26.821843    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:24.429786    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:24.431687    2297 reflector.go:458] object-"kube-system"/"coredns-custom": watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:24.513664    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:24.825751    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1beta3.FlowSchema ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:06.951812    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Ingress ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:25.791739    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:26.323471    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I0322 04:02:26.435991    2297 trace.go:236] Trace[1948847186]: "iptables ChainExists" (22-Mar-2024 04:00:52.377) (total time: 75740ms):
Trace[1948847186]: [1m15.740813147s] [1m15.740813147s] END
E0322 04:02:12.729442    2297 health_controller.go:162] Metrics Controller heartbeat missed
E0322 04:02:39.930717    2297 health_controller.go:162] Metrics Controller heartbeat missed
E0322 04:02:39.931843    2297 health_controller.go:162] Metrics Controller heartbeat missed
W0322 04:02:28.614177    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.StorageClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I0322 04:02:30.502709    2297 trace.go:236] Trace[613526607]: "DeltaFIFO Pop Process" ID:v1.rbac.authorization.k8s.io,Depth:23,Reason:slow event handlers blocking the queue (22-Mar-2024 04:01:59.987) (total time: 16713ms):
Trace[613526607]: [16.713711068s] [16.713711068s] END
W0322 04:02:32.382564    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.RuntimeClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:32.443955    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 04:02:32.457467    2297 wrap.go:54] timeout or abort while handling: method=GET URI="/apis/storage.k8s.io/v1/csinodes/serverworker" audit-ID="1072067d-ff5d-438c-bda0-2f0a113db695"
W0322 04:02:32.958815    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:19.844170    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1beta3.PriorityLevelConfiguration ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:34.538101    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ClusterRole ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:21.989817    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ReplicaSet ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:35.294912    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CSIDriver ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:35.496047    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:35.531832    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:35.539164    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Addon ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:35.741141    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ReplicationController ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:35.741563    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:22.494562    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:36.354732    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:36.604122    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:23.336429    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:37.661755    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 04:02:41.557513    2297 server.go:310] "Unable to authenticate the request due to an error" err="context canceled"
time="2024-03-22T04:02:30Z" level=info msg="Slow SQL (started: 2024-03-22 04:01:06.458494447 +0000 UTC m=+442.195460623) (total time: 36.343468372s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC LIMIT 500 : [[% 756]]"
W0322 04:02:18.254365    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:45.906438    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.HelmChart ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I0322 04:02:45.971285    2297 trace.go:236] Trace[851000593]: "Calculate volume metrics of custom-config-volume for pod kube-system/coredns-6799fbcd5-4tsrq" (22-Mar-2024 04:02:31.500) (total time: 14463ms):
Trace[851000593]: [14.463692641s] [14.463692641s] END
W0322 04:02:45.971389    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.NetworkPolicy ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:33.405777    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Namespace ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:46.095088    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Job ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:33.687903    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:47.292773    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:34.137199    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Secret ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:47.780234    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ControllerRevision ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:32.506513    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Endpoints ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:48.051905    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:48.290228    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:34.721368    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PersistentVolume ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:48.921307    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.EndpointSlice ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:51.042632    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.StatefulSet ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:51.811757    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.DaemonSet ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:51.811844    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:51.813634    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:52.273983    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:53.537963    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CSINode ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:39.616183    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Deployment ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:45.654471    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:44.138985    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.VolumeAttachment ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:03:01.786243    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PodTemplate ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:46.726768    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:03:02.742113    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Lease ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:47.875664    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ClusterRoleBinding ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:03:07.398356    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.PersistentVolumeClaim ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:03:06.805322    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.ServiceAccount ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I0322 04:03:23.550934    2297 request.go:697] Waited for 15.205325291s due to client-side throttling, not priority and fairness, request: GET:https://127.0.0.1:6444/apis/scheduling.k8s.io/v1/priorityclasses?resourceVersion=665
W0322 04:03:25.292122    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CSIStorageCapacity ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:03:25.668415    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:02:52.030948    2297 reflector.go:458] object-"kube-system"/"local-path-config": watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I0322 04:03:27.417640    2297 trace.go:236] Trace[732621387]: "Calculate volume metrics of tmp-dir for pod kube-system/metrics-server-67c658944b-6v69k" (22-Mar-2024 04:02:55.088) (total time: 15768ms):
Trace[732621387]: [15.768202455s] [15.768202455s] END
time="2024-03-22T04:03:27Z" level=info msg="Slow SQL (started: 2024-03-22 04:02:08.706880925 +0000 UTC m=+504.443847102) (total time: 46.415617568s): INSERT INTO kine(name, created, deleted, create_revision, prev_revision, lease, value, old_value) values(?, ?, ?, ?, ?, ?, ?, ?) : [[/registry/masterleases/192.168.56.110 0 1 695 695 15 [107 56 115 0 10 15 10 2 118 49 18 9 69 110 100 112 111 105 110 116 115 18 40 10 16 10 0 18 0 26 0 34 0 42 0 50 0 56 1 66 0 18 20 10 18 10 14 49 57 50 46 49 54 56 46 53 54 46 49 49 48 26 0 26 0 34 0] [107 56 115 0 10 15 10 2 118 49 18 9 69 110 100 112 111 105 110 116 115 18 40 10 16 10 0 18 0 26 0 34 0 42 0 50 0 56 1 66 0 18 20 10 18 10 14 49 57 50 46 49 54 56 46 53 54 46 49 49 48 26 0 26 0 34 0]]]"
E0322 04:03:09.810444    2297 handler_proxy.go:137] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
I0322 04:03:14.382603    2297 trace.go:236] Trace[45494507]: "iptables ChainExists" (22-Mar-2024 04:01:53.563) (total time: 64568ms):
Trace[45494507]: [1m4.568148948s] [1m4.568148948s] END
E0322 04:03:01.227346    2297 kubelet_node_status.go:540] "Error updating node status, will retry" err="error getting node \"server\": Get \"https://127.0.0.1:6443/api/v1/nodes/server?resourceVersion=0&timeout=10s\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)"
I0322 04:03:01.668136    2297 trace.go:236] Trace[164418750]: "Calculate volume metrics of tmp for pod kube-system/traefik-f4564c4f4-d2b8m" (22-Mar-2024 04:02:19.411) (total time: 26646ms):
Trace[164418750]: [26.646059621s] [26.646059621s] END
W0322 04:03:21.415094    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.CertificateSigningRequest ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 04:03:21.439849    2297 health_controller.go:162] Metrics Controller heartbeat missed
W0322 04:03:06.265431    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.MutatingWebhookConfiguration ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 04:03:31.849969    2297 watch.go:287] unable to encode watch object *v1.WatchEvent: client disconnected (&streaming.encoderWithAllocator{writer:(*framer.lengthDelimitedFrameWriter)(0xc0072c9698), encoder:(*versioning.codec)(0xc010d7d220), memAllocator:(*runtime.Allocator)(0xc00849f398)})
I0322 04:03:17.396271    2297 trace.go:236] Trace[2118981953]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.56.110,type:*v1.Endpoints,resource:apiServerIPInfo (22-Mar-2024 04:00:44.458) (total time: 120898ms):
Trace[2118981953]: [2m0.898900094s] [2m0.898900094s] END
W0322 04:03:17.519223    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.StorageClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0322 04:03:39.178313    2297 reflector.go:458] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: watch of *v1.IngressClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0322 04:03:44.002719    2297 watch.go:287] unable to encode watch object *v1.WatchEvent: client disconnected (&streaming.encoderWithAllocator{writer:(*framer.lengthDelimitedFrameWriter)(0xc00aff7110), encoder:(*versioning.codec)(0xc015099680), memAllocator:(*runtime.Allocator)(0xc00849f6e0)})
E0322 04:03:27.856130    2297 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 04:03:44.539825    2297 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E0322 04:03:46.893441    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
I0322 04:03:48.748471    2297 request.go:697] Waited for 13.874673594s due to client-side throttling, not priority and fairness, request: GET:https://127.0.0.1:6444/apis/rbac.authorization.k8s.io/v1/clusterroles?resourceVersion=665
E0322 04:04:04.194328    2297 timeout.go:142] post-timeout activity - time-elapsed: 1m45.906448465s, GET "/apis/storage.k8s.io/v1/csinodes/serverworker" result: <nil>
I0322 04:03:49.171312    2297 trace.go:236] Trace[208066681]: "Calculate volume metrics of config-volume for pod kube-system/local-path-provisioner-6c86858495-hxd2h" (22-Mar-2024 04:02:33.789) (total time: 13076ms):
Trace[208066681]: [13.076072892s] [13.076072892s] END
E0322 04:03:50.356958    2297 watch.go:287] unable to encode watch object *v1.WatchEvent: client disconnected (&streaming.encoderWithAllocator{writer:(*framer.lengthDelimitedFrameWriter)(0xc002fe78a8), encoder:(*versioning.codec)(0xc010aab5e0), memAllocator:(*runtime.Allocator)(0xc00849f008)})
E0322 04:03:51.001288    2297 controller.go:193] "Failed to update lease" err="Put \"https://127.0.0.1:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/server?timeout=10s\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)"
I0322 04:03:56.389473    2297 trace.go:236] Trace[1425683990]: "iptables ChainExists" (22-Mar-2024 04:02:23.940) (total time: 60059ms):
Trace[1425683990]: [1m0.059443024s] [1m0.059443024s] END
I0322 04:03:56.762169    2297 garbagecollector.go:818] "failed to discover preferred resources" error="Get \"https://127.0.0.1:6444/api\": dial tcp 127.0.0.1:6444: i/o timeout"
E0322 04:03:56.786800    2297 handler_proxy.go:137] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
E0322 04:03:44.479458    2297 health_controller.go:162] Metrics Controller heartbeat missed
E0322 04:04:00.538651    2297 watch.go:287] unable to encode watch object *v1.WatchEvent: client disconnected (&streaming.encoderWithAllocator{writer:(*framer.lengthDelimitedFrameWriter)(0xc0072c8f00), encoder:(*versioning.codec)(0xc010d7caa0), memAllocator:(*runtime.Allocator)(0xc007ad67f8)})
E0322 04:04:05.252581    2297 controller.go:164] unable to sync kubernetes service: rpc error: code = Unavailable desc = keepalive ping failed to receive ACK within timeout
I0322 04:03:54.390004    2297 trace.go:236] Trace[20009522]: "iptables ChainExists" (22-Mar-2024 04:01:48.139) (total time: 112708ms):
Trace[20009522]: [1m52.708331317s] [1m52.708331317s] END
E0322 04:04:11.770644    2297 watch.go:287] unable to encode watch object *v1.WatchEvent: client disconnected (&streaming.encoderWithAllocator{writer:(*framer.lengthDelimitedFrameWriter)(0xc00ef1c120), encoder:(*versioning.codec)(0xc0106b4460), memAllocator:(*runtime.Allocator)(0xc00e026d50)})
I0322 04:03:58.782252    2297 trace.go:236] Trace[1569011093]: "Calculate volume metrics of kube-api-access-69spq for pod kube-system/metrics-server-67c658944b-6v69k" (22-Mar-2024 04:03:27.426) (total time: 16356ms):
Trace[1569011093]: [16.356569906s] [16.356569906s] END
I0322 04:04:12.389364    2297 log.go:245] http: TLS handshake error from 10.42.0.5:58820: EOF
E0322 04:04:12.764641    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
I0322 04:04:01.683254    2297 request.go:697] Waited for 14.504140591s due to client-side throttling, not priority and fairness, request: GET:https://127.0.0.1:6443/api/v1/nodes?fieldSelector=metadata.name%3Dserver&resourceVersion=679
E0322 04:04:09.369559    2297 watch.go:287] unable to encode watch object *v1.WatchEvent: client disconnected (&streaming.encoderWithAllocator{writer:(*framer.lengthDelimitedFrameWriter)(0xc002fe67f8), encoder:(*versioning.codec)(0xc010aaa3c0), memAllocator:(*runtime.Allocator)(0xc00d90a600)})
E0322 04:04:11.064033    2297 remote_image.go:128] "ListImages with filter from image service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" filter="nil"
I0322 04:04:22.022770    2297 trace.go:236] Trace[1421003335]: "iptables ChainExists" (22-Mar-2024 04:02:53.749) (total time: 51190ms):
Trace[1421003335]: [51.190746742s] [51.190746742s] END
I0322 04:04:12.508823    2297 request.go:697] Waited for 13.690511981s due to client-side throttling, not priority and fairness, request: GET:https://127.0.0.1:6444/api/v1/configmaps?resourceVersion=665
E0322 04:04:22.427542    2297 watch.go:287] unable to encode watch object *v1.WatchEvent: client disconnected (&streaming.encoderWithAllocator{writer:(*framer.lengthDelimitedFrameWriter)(0xc00b19a768), encoder:(*versioning.codec)(0xc013132f00), memAllocator:(*runtime.Allocator)(0xc00d760630)})
I0322 04:04:28.145938    2297 request.go:697] Waited for 11.531109695s due to client-side throttling, not priority and fairness, request: GET:https://127.0.0.1:6444/api/v1/endpoints?resourceVersion=713
E0322 04:04:29.336692    2297 health_controller.go:131] Network Policy Controller heartbeat missed
I0322 04:04:23.873351    2297 trace.go:236] Trace[555517243]: "Calculate volume metrics of kube-api-access-4b65q for pod kube-system/traefik-f4564c4f4-d2b8m" (22-Mar-2024 04:03:27.755) (total time: 46421ms):
Trace[555517243]: [46.421883654s] [46.421883654s] END
W0322 04:04:35.612158    2297 transport.go:301] Unable to cancel request for *otelhttp.Transport
E0322 04:04:35.774858    2297 remote_runtime.go:294] "ListPodSandbox with filter from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" filter="&PodSandboxFilter{Id:,State:&PodSandboxStateValue{State:SANDBOX_READY,},LabelSelector:map[string]string{},}"
I0322 04:04:43.583976    2297 log.go:245] http: TLS handshake error from 10.42.0.5:43110: EOF
I0322 04:04:43.595386    2297 log.go:245] http: TLS handshake error from 10.42.0.5:40856: write tcp 192.168.56.110:10250->10.42.0.5:40856: write: broken pipe
I0322 04:04:43.595444    2297 log.go:245] http: TLS handshake error from 10.42.0.5:38720: write tcp 192.168.56.110:10250->10.42.0.5:38720: write: broken pipe
E0322 04:04:36.460201    2297 watch.go:287] unable to encode watch object *v1.WatchEvent: client disconnected (&streaming.encoderWithAllocator{writer:(*framer.lengthDelimitedFrameWriter)(0xc008301890), encoder:(*versioning.codec)(0xc013bda0a0), memAllocator:(*runtime.Allocator)(0xc00849e5a0)})
E0322 04:04:36.497824    2297 watch.go:287] unable to encode watch object *v1.WatchEvent: client disconnected (&streaming.encoderWithAllocator{writer:(*framer.lengthDelimitedFrameWriter)(0xc008300168), encoder:(*versioning.codec)(0xc013837a40), memAllocator:(*runtime.Allocator)(0xc0047a0e58)})
I0322 04:04:39.802625    2297 trace.go:236] Trace[547166647]: "Calculate volume metrics of kube-api-access-vjtws for pod kube-system/coredns-6799fbcd5-4tsrq" (22-Mar-2024 04:03:14.881) (total time: 33545ms):
Trace[547166647]: [33.545118285s] [33.545118285s] END
E0322 04:04:40.604143    2297 watch.go:287] unable to encode watch object *v1.WatchEvent: client disconnected (&streaming.encoderWithAllocator{writer:(*framer.lengthDelimitedFrameWriter)(0xc002fe6e28), encoder:(*versioning.codec)(0xc010aaab40), memAllocator:(*runtime.Allocator)(0xc007617650)})
E0322 04:04:46.986655    2297 watch.go:287] unable to encode watch object *v1.WatchEvent: client disconnected (&streaming.encoderWithAllocator{writer:responsewriter.outerWithCloseNotifyAndFlush{UserProvidedDecorator:(*metrics.ResponseWriterDelegator)(0xc00e3d7380), InnerCloseNotifierFlusher:struct { httpsnoop.Unwrapper; http.ResponseWriter; http.Flusher; http.CloseNotifier; http.Pusher }{Unwrapper:(*httpsnoop.rw)(0xc0109d8640), ResponseWriter:(*httpsnoop.rw)(0xc0109d8640), Flusher:(*httpsnoop.rw)(0xc0109d8640), CloseNotifier:(*httpsnoop.rw)(0xc0109d8640), Pusher:(*httpsnoop.rw)(0xc0109d8640)}}, encoder:(*versioning.codec)(0xc012404d20), memAllocator:(*runtime.Allocator)(0xc00618def0)})
W0322 04:04:22.821516    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Lease: Get "https://127.0.0.1:6444/apis/coordination.k8s.io/v1/namespaces/kube-system/leases?labelSelector=k8s.io%2Fcomponent%3Dkube-apiserver&resourceVersion=668": dial tcp 127.0.0.1:6444: i/o timeout
I0322 04:04:48.616299    2297 log.go:245] http: TLS handshake error from 10.42.0.5:59710: EOF
W0322 04:04:37.412691    2297 handler_proxy.go:93] no RequestInfo found in the context
E0322 04:04:51.674459    2297 resource_quota_controller.go:440] failed to discover resources: Get "https://127.0.0.1:6444/api": dial tcp 127.0.0.1:6444: i/o timeout
E0322 04:04:41.013503    2297 server.go:310] "Unable to authenticate the request due to an error" err="context canceled"
time="2024-03-22T04:04:52Z" level=error msg="error while range on /registry/rolebindings/ /registry/rolebindings/: context canceled"
time="2024-03-22T04:04:52Z" level=error msg="error while range on /registry/replicasets/ /registry/replicasets/: context canceled"
W0322 04:04:52.773222    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Endpoints: Get "https://127.0.0.1:6443/api/v1/namespaces/default/endpoints?fieldSelector=metadata.name%3Dkubernetes&resourceVersion=667": dial tcp 127.0.0.1:6443: i/o timeout
E0322 04:04:52.774022    2297 health_controller.go:162] Metrics Controller heartbeat missed
E0322 04:04:52.777464    2297 health_controller.go:131] Network Policy Controller heartbeat missed
E0322 04:04:52.777878    2297 health_controller.go:162] Metrics Controller heartbeat missed
E0322 04:04:52.777889    2297 health_controller.go:131] Network Policy Controller heartbeat missed
E0322 04:04:52.777893    2297 health_controller.go:162] Metrics Controller heartbeat missed
E0322 04:04:52.777900    2297 health_controller.go:162] Metrics Controller heartbeat missed
I0322 04:04:52.779721    2297 trace.go:236] Trace[1982203396]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:27.515) (total time: 145259ms):
Trace[1982203396]: ---"Objects listed" error:Get "https://127.0.0.1:6443/api/v1/namespaces/default/endpoints?fieldSelector=metadata.name%3Dkubernetes&resourceVersion=667": dial tcp 127.0.0.1:6443: i/o timeout 145250ms (04:04:52.766)
Trace[1982203396]: [2m25.259593949s] [2m25.259593949s] END
W0322 04:04:53.013915    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/middlewares?resourceVersion=668": net/http: TLS handshake timeout
time="2024-03-22T04:04:53Z" level=error msg="error while range on /registry/cronjobs/ /registry/cronjobs/: context canceled"
time="2024-03-22T04:04:53Z" level=info msg="Slow SQL (started: 2024-03-22 04:04:24.517085461 +0000 UTC m=+640.254051662) (total time: 26.1674861s):  SELECT MAX(rkv.id) AS id FROM kine AS rkv : [[]]"
time="2024-03-22T04:04:53Z" level=error msg="error while range on /registry/poddisruptionbudgets/ /registry/poddisruptionbudgets/: context canceled"
E0322 04:04:54.205337    2297 watch.go:287] unable to encode watch object *v1.WatchEvent: client disconnected (&streaming.encoderWithAllocator{writer:(*framer.lengthDelimitedFrameWriter)(0xc007cd6540), encoder:(*versioning.codec)(0xc004a0b5e0), memAllocator:(*runtime.Allocator)(0xc004fae930)})
I0322 04:04:54.230179    2297 trace.go:236] Trace[727974451]: "Calculate volume metrics of kube-api-access-hp95s for pod kube-system/local-path-provisioner-6c86858495-hxd2h" (22-Mar-2024 04:04:29.131) (total time: 14614ms):
Trace[727974451]: [14.614289273s] [14.614289273s] END
time="2024-03-22T04:04:54Z" level=error msg="error while range on /registry/traefik.io/ingressroutetcps/ /registry/traefik.io/ingressroutetcps/: context canceled"
E0322 04:04:54.546401    2297 watch.go:287] unable to encode watch object *v1.WatchEvent: client disconnected (&streaming.encoderWithAllocator{writer:(*framer.lengthDelimitedFrameWriter)(0xc0063dfc08), encoder:(*versioning.codec)(0xc0138e6e60), memAllocator:(*runtime.Allocator)(0xc00618db30)})
E0322 04:04:46.410290    2297 watch.go:287] unable to encode watch object *v1.WatchEvent: client disconnected (&streaming.encoderWithAllocator{writer:(*framer.lengthDelimitedFrameWriter)(0xc008618810), encoder:(*versioning.codec)(0xc014009ae0), memAllocator:(*runtime.Allocator)(0xc0076162a0)})
time="2024-03-22T04:04:54Z" level=error msg="error while range on /registry/networkpolicies/ /registry/networkpolicies/: context canceled"
W0322 04:04:55.353789    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.RoleBinding: Get "https://127.0.0.1:6444/apis/rbac.authorization.k8s.io/v1/rolebindings?resourceVersion=668": dial tcp 127.0.0.1:6444: i/o timeout
I0322 04:04:55.361764    2297 trace.go:236] Trace[364123611]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:03.374) (total time: 171980ms):
Trace[364123611]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/rbac.authorization.k8s.io/v1/rolebindings?resourceVersion=668": dial tcp 127.0.0.1:6444: i/o timeout 163976ms (04:04:47.351)
Trace[364123611]: [2m51.980802852s] [2m51.980802852s] END
E0322 04:04:55.363567    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.RoleBinding: failed to list *v1.RoleBinding: Get "https://127.0.0.1:6444/apis/rbac.authorization.k8s.io/v1/rolebindings?resourceVersion=668": dial tcp 127.0.0.1:6444: i/o timeout
W0322 04:04:55.473543    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.StorageClass: Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/storageclasses?resourceVersion=667": dial tcp 127.0.0.1:6444: i/o timeout
I0322 04:04:55.478577    2297 trace.go:236] Trace[181066152]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:03.173) (total time: 172301ms):
Trace[181066152]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/storageclasses?resourceVersion=667": dial tcp 127.0.0.1:6444: i/o timeout 172297ms (04:04:55.471)
Trace[181066152]: [2m52.301431709s] [2m52.301431709s] END
W0322 04:04:55.541729    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.ConfigMap: Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=665": dial tcp 127.0.0.1:6444: i/o timeout
I0322 04:04:55.541840    2297 trace.go:236] Trace[1950306507]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:04.000) (total time: 171541ms):
Trace[1950306507]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=665": dial tcp 127.0.0.1:6444: i/o timeout 171541ms (04:04:55.541)
Trace[1950306507]: [2m51.541617646s] [2m51.541617646s] END
E0322 04:04:55.542147    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=665": dial tcp 127.0.0.1:6444: i/o timeout
E0322 04:04:55.968724    2297 node_lifecycle_controller.go:971] "Error updating node" err="Put \"https://127.0.0.1:6444/api/v1/nodes/server/status\": net/http: TLS handshake timeout" node="server"
I0322 04:04:56.027766    2297 trace.go:236] Trace[1447399355]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:02.885) (total time: 173138ms):
Trace[1447399355]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/coordination.k8s.io/v1/namespaces/kube-system/leases?labelSelector=k8s.io%2Fcomponent%3Dkube-apiserver&resourceVersion=668": dial tcp 127.0.0.1:6444: i/o timeout 139921ms (04:04:22.806)
Trace[1447399355]: [2m53.138366379s] [2m53.138366379s] END
E0322 04:04:56.035728    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Lease: failed to list *v1.Lease: Get "https://127.0.0.1:6444/apis/coordination.k8s.io/v1/namespaces/kube-system/leases?labelSelector=k8s.io%2Fcomponent%3Dkube-apiserver&resourceVersion=668": dial tcp 127.0.0.1:6444: i/o timeout
W0322 04:04:56.146240    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.ConfigMap: Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=665": dial tcp 127.0.0.1:6444: i/o timeout
E0322 04:04:48.936337    2297 remote_runtime.go:633] "Status from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded"
E0322 04:04:56.451286    2297 kubelet.go:2840] "Container runtime sanity check failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded"
W0322 04:04:56.542165    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.VolumeAttachment: Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/volumeattachments?resourceVersion=667": dial tcp 127.0.0.1:6444: i/o timeout
I0322 04:04:56.555385    2297 trace.go:236] Trace[1384932206]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:02.626) (total time: 173923ms):
Trace[1384932206]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/volumeattachments?resourceVersion=667": dial tcp 127.0.0.1:6444: i/o timeout 173913ms (04:04:56.540)
Trace[1384932206]: [2m53.923718284s] [2m53.923718284s] END
W0322 04:04:56.561854    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Node: Get "https://127.0.0.1:6444/api/v1/nodes?resourceVersion=679": dial tcp 127.0.0.1:6444: i/o timeout
I0322 04:04:56.562125    2297 trace.go:236] Trace[2007074287]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:03.313) (total time: 173248ms):
Trace[2007074287]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/nodes?resourceVersion=679": dial tcp 127.0.0.1:6444: i/o timeout 173247ms (04:04:56.561)
Trace[2007074287]: [2m53.248652357s] [2m53.248652357s] END
E0322 04:04:56.562229    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://127.0.0.1:6444/api/v1/nodes?resourceVersion=679": dial tcp 127.0.0.1:6444: i/o timeout
I0322 04:04:56.623296    2297 log.go:245] http: TLS handshake error from 10.42.0.5:53334: write tcp 192.168.56.110:10250->10.42.0.5:53334: write: broken pipe
time="2024-03-22T04:04:56Z" level=info msg="Slow SQL (started: 2024-03-22 04:04:49.258930228 +0000 UTC m=+664.995896414) (total time: 7.406963721s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/flowschemas/% false]]"
time="2024-03-22T04:04:56Z" level=error msg="error while range on /registry/flowschemas/ /registry/flowschemas/: context canceled"
W0322 04:04:56.680435    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.NetworkPolicy: Get "https://127.0.0.1:6443/apis/networking.k8s.io/v1/networkpolicies?resourceVersion=666": dial tcp 127.0.0.1:6443: i/o timeout
I0322 04:04:56.685250    2297 trace.go:236] Trace[531899295]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:03.423) (total time: 173258ms):
Trace[531899295]: ---"Objects listed" error:Get "https://127.0.0.1:6443/apis/networking.k8s.io/v1/networkpolicies?resourceVersion=666": dial tcp 127.0.0.1:6443: i/o timeout 173245ms (04:04:56.669)
Trace[531899295]: [2m53.258481552s] [2m53.258481552s] END
time="2024-03-22T04:04:57Z" level=info msg="Slow SQL (started: 2024-03-22 04:04:23.257685759 +0000 UTC m=+638.994651950) (total time: 34.025935535s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/roles/% false]]"
E0322 04:04:57.398657    2297 remote_runtime.go:407] "ListContainers with filter from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" filter="&ContainerFilter{Id:,State:nil,PodSandboxId:,LabelSelector:map[string]string{},}"
I0322 04:04:57.582782    2297 request.go:697] Waited for 6.522534466s due to client-side throttling, not priority and fairness, request: GET:https://127.0.0.1:6444/api/v1/nodes?resourceVersion=679
I0322 04:04:58.180310    2297 trace.go:236] Trace[868456729]: "iptables ChainExists" (22-Mar-2024 04:04:03.488) (total time: 48123ms):
Trace[868456729]: [48.123819515s] [48.123819515s] END
W0322 04:04:58.296657    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://127.0.0.1:6444/api/v1/services?resourceVersion=665": net/http: TLS handshake timeout
E0322 04:04:59.263262    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Endpoints: failed to list *v1.Endpoints: Get "https://127.0.0.1:6443/api/v1/namespaces/default/endpoints?fieldSelector=metadata.name%3Dkubernetes&resourceVersion=667": dial tcp 127.0.0.1:6443: i/o timeout
E0322 04:04:59.321610    2297 kuberuntime_sandbox.go:297] "Failed to list pod sandboxes" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded"
E0322 04:04:59.335097    2297 kubelet_pods.go:1090] "Error listing containers" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded"
E0322 04:04:59.339162    2297 kubelet.go:2473] "Failed cleaning pods" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded"
E0322 04:04:59.341138    2297 kubelet.go:2477] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="4m16.96s"
E0322 04:04:42.895498    2297 kuberuntime_image.go:103] "Failed to list images" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded"
I0322 04:04:59.470563    2297 image_gc_manager.go:210] "Failed to update image list" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded"
W0322 04:04:59.498227    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.ClusterRoleBinding: Get "https://127.0.0.1:6444/apis/rbac.authorization.k8s.io/v1/clusterrolebindings?resourceVersion=667": dial tcp 127.0.0.1:6444: i/o timeout
I0322 04:04:59.504351    2297 trace.go:236] Trace[2133349578]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:39.092) (total time: 140410ms):
Trace[2133349578]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/rbac.authorization.k8s.io/v1/clusterrolebindings?resourceVersion=667": dial tcp 127.0.0.1:6444: i/o timeout 140400ms (04:04:59.493)
Trace[2133349578]: [2m20.410963288s] [2m20.410963288s] END
E0322 04:04:59.507941    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ClusterRoleBinding: failed to list *v1.ClusterRoleBinding: Get "https://127.0.0.1:6444/apis/rbac.authorization.k8s.io/v1/clusterrolebindings?resourceVersion=667": dial tcp 127.0.0.1:6444: i/o timeout
W0322 04:04:59.789431    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Endpoints: Get "https://127.0.0.1:6444/api/v1/endpoints?resourceVersion=713": dial tcp 127.0.0.1:6444: i/o timeout
I0322 04:04:59.798263    2297 trace.go:236] Trace[1506498184]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:02.708) (total time: 177088ms):
Trace[1506498184]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/endpoints?resourceVersion=713": dial tcp 127.0.0.1:6444: i/o timeout 177079ms (04:04:59.787)
Trace[1506498184]: [2m57.088472815s] [2m57.088472815s] END
E0322 04:04:59.799502    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Endpoints: failed to list *v1.Endpoints: Get "https://127.0.0.1:6444/api/v1/endpoints?resourceVersion=713": dial tcp 127.0.0.1:6444: i/o timeout
W0322 04:04:59.799846    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.ClusterRole: Get "https://127.0.0.1:6444/apis/rbac.authorization.k8s.io/v1/clusterroles?resourceVersion=665": net/http: TLS handshake timeout
I0322 04:04:59.801194    2297 trace.go:236] Trace[440835120]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:43.598) (total time: 136203ms):
Trace[440835120]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/rbac.authorization.k8s.io/v1/clusterroles?resourceVersion=665": net/http: TLS handshake timeout 136201ms (04:04:59.799)
Trace[440835120]: [2m16.203096428s] [2m16.203096428s] END
E0322 04:04:59.801275    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ClusterRole: failed to list *v1.ClusterRole: Get "https://127.0.0.1:6444/apis/rbac.authorization.k8s.io/v1/clusterroles?resourceVersion=665": net/http: TLS handshake timeout
E0322 04:04:59.947331    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 04:04:53.033393    2297 trace.go:236] Trace[1351718136]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:03:02.441) (total time: 110579ms):
Trace[1351718136]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/middlewares?resourceVersion=668": net/http: TLS handshake timeout 110570ms (04:04:53.012)
Trace[1351718136]: [1m50.57968939s] [1m50.57968939s] END
W0322 04:05:00.748470    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.RuntimeClass: Get "https://127.0.0.1:6444/apis/node.k8s.io/v1/runtimeclasses?resourceVersion=668": net/http: TLS handshake timeout
I0322 04:05:00.750972    2297 trace.go:236] Trace[1819117126]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:03.697) (total time: 177052ms):
Trace[1819117126]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/node.k8s.io/v1/runtimeclasses?resourceVersion=668": net/http: TLS handshake timeout 177051ms (04:05:00.748)
Trace[1819117126]: [2m57.052427238s] [2m57.052427238s] END
E0322 04:05:00.753736    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: Get "https://127.0.0.1:6444/apis/node.k8s.io/v1/runtimeclasses?resourceVersion=668": net/http: TLS handshake timeout
E0322 04:05:00.940402    2297 handler_proxy.go:137] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
E0322 04:05:01.258483    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/storageclasses?resourceVersion=667": dial tcp 127.0.0.1:6444: i/o timeout
W0322 04:05:01.292876    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.CSIDriver: Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/csidrivers?resourceVersion=665": dial tcp 127.0.0.1:6444: i/o timeout
I0322 04:05:01.297577    2297 trace.go:236] Trace[1041087891]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:03.838) (total time: 177456ms):
Trace[1041087891]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/csidrivers?resourceVersion=665": dial tcp 127.0.0.1:6444: i/o timeout 177452ms (04:05:01.290)
Trace[1041087891]: [2m57.45669683s] [2m57.45669683s] END
W0322 04:04:53.840402    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Pod: Get "https://127.0.0.1:6444/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&resourceVersion=708": net/http: TLS handshake timeout
I0322 04:05:01.558117    2297 trace.go:236] Trace[1125520129]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:02.579) (total time: 178970ms):
Trace[1125520129]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&resourceVersion=708": net/http: TLS handshake timeout 171251ms (04:04:53.831)
Trace[1125520129]: [2m58.970380016s] [2m58.970380016s] END
E0322 04:05:01.562767    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://127.0.0.1:6444/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&resourceVersion=708": net/http: TLS handshake timeout
time="2024-03-22T04:05:01Z" level=info msg="Slow SQL (started: 2024-03-22 04:04:13.667231068 +0000 UTC m=+629.404197244) (total time: 34.762355148s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC LIMIT 500 : [[% 757]]"
W0322 04:04:55.776355    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.StatefulSet: Get "https://127.0.0.1:6444/apis/apps/v1/statefulsets?resourceVersion=667": dial tcp 127.0.0.1:6444: i/o timeout
I0322 04:05:01.682718    2297 trace.go:236] Trace[176180579]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:03.251) (total time: 178426ms):
Trace[176180579]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/apps/v1/statefulsets?resourceVersion=667": dial tcp 127.0.0.1:6444: i/o timeout 172520ms (04:04:55.772)
Trace[176180579]: [2m58.42631454s] [2m58.42631454s] END
time="2024-03-22T04:05:01Z" level=error msg="Compact failed: failed to get current revision: context deadline exceeded"
W0322 04:04:55.907722    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.ResourceQuota: Get "https://127.0.0.1:6444/api/v1/resourcequotas?resourceVersion=667": dial tcp 127.0.0.1:6444: i/o timeout
I0322 04:05:01.939578    2297 trace.go:236] Trace[2101069333]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:03.166) (total time: 178768ms):
Trace[2101069333]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/resourcequotas?resourceVersion=667": dial tcp 127.0.0.1:6444: i/o timeout 172726ms (04:04:55.893)
Trace[2101069333]: [2m58.768271512s] [2m58.768271512s] END
I0322 04:05:02.279188    2297 trace.go:236] Trace[316247757]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:03.217) (total time: 172929ms):
Trace[316247757]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=665": dial tcp 127.0.0.1:6444: i/o timeout 172927ms (04:04:56.144)
Trace[316247757]: [2m52.929186456s] [2m52.929186456s] END
E0322 04:05:02.290088    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=665": dial tcp 127.0.0.1:6444: i/o timeout
W0322 04:05:02.294990    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.ValidatingWebhookConfiguration: Get "https://127.0.0.1:6444/apis/admissionregistration.k8s.io/v1/validatingwebhookconfigurations?resourceVersion=665": dial tcp 127.0.0.1:6444: i/o timeout
I0322 04:05:02.299237    2297 trace.go:236] Trace[328416825]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:03.501) (total time: 178796ms):
Trace[328416825]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/admissionregistration.k8s.io/v1/validatingwebhookconfigurations?resourceVersion=665": dial tcp 127.0.0.1:6444: i/o timeout 178793ms (04:05:02.294)
Trace[328416825]: [2m58.796854772s] [2m58.796854772s] END
E0322 04:05:02.299336    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ValidatingWebhookConfiguration: failed to list *v1.ValidatingWebhookConfiguration: Get "https://127.0.0.1:6444/apis/admissionregistration.k8s.io/v1/validatingwebhookconfigurations?resourceVersion=665": dial tcp 127.0.0.1:6444: i/o timeout
E0322 04:05:02.346660    2297 watch.go:287] unable to encode watch object *v1.WatchEvent: client disconnected (&streaming.encoderWithAllocator{writer:responsewriter.outerWithCloseNotifyAndFlush{UserProvidedDecorator:(*metrics.ResponseWriterDelegator)(0xc00a3c4660), InnerCloseNotifierFlusher:struct { httpsnoop.Unwrapper; http.ResponseWriter; http.Flusher; http.CloseNotifier; http.Pusher }{Unwrapper:(*httpsnoop.rw)(0xc0032bd270), ResponseWriter:(*httpsnoop.rw)(0xc0032bd270), Flusher:(*httpsnoop.rw)(0xc0032bd270), CloseNotifier:(*httpsnoop.rw)(0xc0032bd270), Pusher:(*httpsnoop.rw)(0xc0032bd270)}}, encoder:(*versioning.codec)(0xc0084b1c20), memAllocator:(*runtime.Allocator)(0xc007617860)})
W0322 04:05:02.616472    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Node: Get "https://127.0.0.1:6444/api/v1/nodes?resourceVersion=679": dial tcp 127.0.0.1:6444: i/o timeout
I0322 04:05:02.624079    2297 trace.go:236] Trace[1007018213]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:02.693) (total time: 179927ms):
Trace[1007018213]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/nodes?resourceVersion=679": dial tcp 127.0.0.1:6444: i/o timeout 173683ms (04:04:56.377)
Trace[1007018213]: [2m59.927675597s] [2m59.927675597s] END
E0322 04:05:02.626650    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://127.0.0.1:6444/api/v1/nodes?resourceVersion=679": dial tcp 127.0.0.1:6444: i/o timeout
E0322 04:05:02.713533    2297 watch.go:287] unable to encode watch object *v1.WatchEvent: client disconnected (&streaming.encoderWithAllocator{writer:responsewriter.outerWithCloseNotifyAndFlush{UserProvidedDecorator:(*metrics.ResponseWriterDelegator)(0xc0047ba480), InnerCloseNotifierFlusher:struct { httpsnoop.Unwrapper; http.ResponseWriter; http.Flusher; http.CloseNotifier; http.Pusher }{Unwrapper:(*httpsnoop.rw)(0xc0042cf360), ResponseWriter:(*httpsnoop.rw)(0xc0042cf360), Flusher:(*httpsnoop.rw)(0xc0042cf360), CloseNotifier:(*httpsnoop.rw)(0xc0042cf360), Pusher:(*httpsnoop.rw)(0xc0042cf360)}}, encoder:(*versioning.codec)(0xc00a2f92c0), memAllocator:(*runtime.Allocator)(0xc008300d80)})
W0322 04:05:02.833136    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Node: Get "https://127.0.0.1:6443/api/v1/nodes?fieldSelector=metadata.name%3Dserver&resourceVersion=679": dial tcp 127.0.0.1:6443: i/o timeout
W0322 04:05:02.875831    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Pod: Get "https://127.0.0.1:6443/api/v1/pods?resourceVersion=708": dial tcp 127.0.0.1:6443: i/o timeout
I0322 04:05:02.885986    2297 trace.go:236] Trace[1177496038]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:01:58.248) (total time: 184631ms):
Trace[1177496038]: ---"Objects listed" error:Get "https://127.0.0.1:6443/api/v1/pods?resourceVersion=708": dial tcp 127.0.0.1:6443: i/o timeout 176675ms (04:04:54.923)
Trace[1177496038]: [3m4.631479993s] [3m4.631479993s] END
E0322 04:05:02.890108    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://127.0.0.1:6443/api/v1/pods?resourceVersion=708": dial tcp 127.0.0.1:6443: i/o timeout
E0322 04:05:03.056379    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/volumeattachments?resourceVersion=667": dial tcp 127.0.0.1:6444: i/o timeout
E0322 04:05:03.182554    2297 handler_proxy.go:137] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
E0322 04:05:03.338121    2297 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, context canceled]"
W0322 04:05:09.242116    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.ConfigMap: Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=665": dial tcp 127.0.0.1:6444: i/o timeout
I0322 04:05:09.249488    2297 trace.go:236] Trace[1695931113]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:15.376) (total time: 167459ms):
Trace[1695931113]: ---"Objects listed" error:Get "https://127.0.0.1:6443/api/v1/nodes?fieldSelector=metadata.name%3Dserver&resourceVersion=679": dial tcp 127.0.0.1:6443: i/o timeout 167451ms (04:05:02.828)
Trace[1695931113]: [2m47.459485683s] [2m47.459485683s] END
E0322 04:05:09.251969    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://127.0.0.1:6443/api/v1/nodes?fieldSelector=metadata.name%3Dserver&resourceVersion=679": dial tcp 127.0.0.1:6443: i/o timeout
W0322 04:05:04.393319    2297 transport.go:301] Unable to cancel request for *otelhttp.Transport
I0322 04:05:10.377535    2297 log.go:245] http: TLS handshake error from 10.42.0.5:38032: write tcp 192.168.56.110:10250->10.42.0.5:38032: write: broken pipe
E0322 04:05:10.408232    2297 kuberuntime_container.go:477] "ListContainers failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded"
W0322 04:05:04.891311    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/ingressroutetcps?resourceVersion=671": dial tcp 127.0.0.1:6444: i/o timeout
E0322 04:05:05.279720    2297 kubelet_node_status.go:540] "Error updating node status, will retry" err="error getting node \"server\": Get \"https://127.0.0.1:6443/api/v1/nodes/server?timeout=10s\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)"
time="2024-03-22T04:05:07Z" level=error msg="error while range on /registry/roles/ /registry/roles/: context canceled"
W0322 04:04:50.213847    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://127.0.0.1:6443/apis/discovery.k8s.io/v1/endpointslices?labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&resourceVersion=715": dial tcp 127.0.0.1:6443: i/o timeout
E0322 04:05:05.313069    2297 watch.go:287] unable to encode watch object *v1.WatchEvent: client disconnected (&streaming.encoderWithAllocator{writer:responsewriter.outerWithCloseNotifyAndFlush{UserProvidedDecorator:(*metrics.ResponseWriterDelegator)(0xc00a3c5380), InnerCloseNotifierFlusher:struct { httpsnoop.Unwrapper; http.ResponseWriter; http.Flusher; http.CloseNotifier; http.Pusher }{Unwrapper:(*httpsnoop.rw)(0xc0032bd3b0), ResponseWriter:(*httpsnoop.rw)(0xc0032bd3b0), Flusher:(*httpsnoop.rw)(0xc0032bd3b0), CloseNotifier:(*httpsnoop.rw)(0xc0032bd3b0), Pusher:(*httpsnoop.rw)(0xc0032bd3b0)}}, encoder:(*versioning.codec)(0xc0084b0b40), memAllocator:(*runtime.Allocator)(0xc00467dad0)})
W0322 04:05:06.186579    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://127.0.0.1:6443/api/v1/services?labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&resourceVersion=665": dial tcp 127.0.0.1:6443: i/o timeout
I0322 04:05:06.236245    2297 trace.go:236] Trace[1836687029]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:01:41.651) (total time: 196647ms):
Trace[1836687029]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/services?resourceVersion=665": net/http: TLS handshake timeout 196640ms (04:04:58.291)
Trace[1836687029]: [3m16.647940451s] [3m16.647940451s] END
E0322 04:05:11.284552    2297 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 04:05:11.291640    2297 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E0322 04:05:11.594175    2297 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" containerID="50900ef7d9e42c2a4625f6fb437eebbf4388a3e02fa58ab7d6836f44480c3bad"
time="2024-03-22T04:05:10Z" level=info msg="Slow SQL (started: 2024-03-22 04:04:24.575322002 +0000 UTC m=+640.312288212) (total time: 39.971190138s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/services/endpoints/% false]]"
W0322 04:05:06.427840    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Pod: Get "https://127.0.0.1:6444/api/v1/pods?resourceVersion=708": dial tcp 127.0.0.1:6444: i/o timeout
E0322 04:05:06.705790    2297 kubelet.go:2327] "Skipping pod synchronization" err="[container runtime is down, PLEG is not healthy: pleg was last seen active 4m20.954259554s ago; threshold is 3m0s]"
W0322 04:05:06.764537    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Secret: Get "https://127.0.0.1:6444/api/v1/secrets?resourceVersion=675": dial tcp 127.0.0.1:6444: i/o timeout
W0322 04:04:59.599211    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/k3s.cattle.io/v1/etcdsnapshotfiles?resourceVersion=665": dial tcp 127.0.0.1:6444: i/o timeout
W0322 04:05:12.598925    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Role: Get "https://127.0.0.1:6444/apis/rbac.authorization.k8s.io/v1/roles?resourceVersion=667": net/http: TLS handshake timeout
I0322 04:05:07.175204    2297 trace.go:236] Trace[104555070]: "SerializeObject" audit-id:b34f01f0-d6ec-40b6-87ee-57db1b8128c1,method:POST,url:/api/v1/namespaces/default/events,protocol:HTTP/2.0,mediaType:application/vnd.kubernetes.protobuf,encoder:{"encodeGV":"v1","encoder":"protobuf","name":"versioning"} (22-Mar-2024 04:02:01.612) (total time: 178346ms):
Trace[104555070]: ---"About to start writing response" size:154 31782ms (04:02:33.395)
Trace[104555070]: ---"Write call failed" writer:responsewriter.outerWithCloseNotifyAndFlush,size:154,firstWrite:true,err:http: Handler timeout 58418ms (04:03:31.814)
Trace[104555070]: ---"About to start writing response" size:69 68528ms (04:04:40.342)
Trace[104555070]: ---"Write call failed" writer:responsewriter.outerWithCloseNotifyAndFlush,size:69,firstWrite:false,err:http: Handler timeout 12659ms (04:04:53.001)
Trace[104555070]: [2m58.346379563s] [2m58.346379563s] END
I0322 04:05:12.632354    2297 trace.go:236] Trace[5130288]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:b34f01f0-d6ec-40b6-87ee-57db1b8128c1,client:192.168.56.111,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/default/events,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:POST (22-Mar-2024 04:00:38.289) (total time: 274336ms):
Trace[5130288]: ---"Write to database call failed" len:233,err:Timeout: request did not complete within requested timeout - context deadline exceeded 42269ms (04:01:20.599)
Trace[5130288]: [4m34.336844353s] [4m34.336844353s] END
W0322 04:05:12.784310    2297 reflector.go:535] object-"kube-system"/"coredns-custom": failed to list *v1.ConfigMap: Get "https://127.0.0.1:6443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dcoredns-custom&resourceVersion=665": net/http: TLS handshake timeout
W0322 04:05:07.448607    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PersistentVolumeClaim: Get "https://127.0.0.1:6444/api/v1/persistentvolumeclaims?resourceVersion=665": net/http: TLS handshake timeout
I0322 04:05:12.819626    2297 trace.go:236] Trace[1859207296]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:03.854) (total time: 188957ms):
Trace[1859207296]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/persistentvolumeclaims?resourceVersion=665": net/http: TLS handshake timeout 183572ms (04:05:07.426)
Trace[1859207296]: [3m8.957735448s] [3m8.957735448s] END
W0322 04:05:07.458663    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.RuntimeClass: Get "https://127.0.0.1:6444/apis/node.k8s.io/v1/runtimeclasses?resourceVersion=668": net/http: TLS handshake timeout
W0322 04:05:07.527248    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.ControllerRevision: Get "https://127.0.0.1:6444/apis/apps/v1/controllerrevisions?resourceVersion=665": net/http: TLS handshake timeout
W0322 04:05:00.711640    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Node: Get "https://127.0.0.1:6444/api/v1/nodes?resourceVersion=679": net/http: TLS handshake timeout
I0322 04:05:07.817672    2297 trace.go:236] Trace[1617551842]: "iptables ChainExists" (22-Mar-2024 04:04:53.807) (total time: 13999ms):
Trace[1617551842]: [13.999729496s] [13.999729496s] END
E0322 04:05:07.959109    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://127.0.0.1:6444/apis/apps/v1/statefulsets?resourceVersion=667": dial tcp 127.0.0.1:6444: i/o timeout
W0322 04:05:08.250599    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.LimitRange: Get "https://127.0.0.1:6444/api/v1/limitranges?resourceVersion=665": dial tcp 127.0.0.1:6444: i/o timeout
W0322 04:05:08.359080    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.ConfigMap: Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?resourceVersion=665": net/http: TLS handshake timeout
I0322 04:05:13.007143    2297 trace.go:236] Trace[2007364394]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:17.380) (total time: 175623ms):
Trace[2007364394]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?resourceVersion=665": net/http: TLS handshake timeout 170974ms (04:05:08.354)
Trace[2007364394]: [2m55.623815386s] [2m55.623815386s] END
I0322 04:05:13.009005    2297 trace.go:236] Trace[748133417]: "iptables ChainExists" (22-Mar-2024 04:04:51.193) (total time: 21815ms):
Trace[748133417]: [21.815417399s] [21.815417399s] END
E0322 04:05:08.885760    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.NetworkPolicy: failed to list *v1.NetworkPolicy: Get "https://127.0.0.1:6443/apis/networking.k8s.io/v1/networkpolicies?resourceVersion=666": dial tcp 127.0.0.1:6443: i/o timeout
time="2024-03-22T04:05:13Z" level=info msg="Slow SQL (started: 2024-03-22 04:04:58.664047922 +0000 UTC m=+674.401014106) (total time: 8.027807843s):  SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key' : [[]]"
E0322 04:05:13.332540    2297 watch.go:287] unable to encode watch object *v1.WatchEvent: client disconnected (&streaming.encoderWithAllocator{writer:responsewriter.outerWithCloseNotifyAndFlush{UserProvidedDecorator:(*metrics.ResponseWriterDelegator)(0xc00a3c4de0), InnerCloseNotifierFlusher:struct { httpsnoop.Unwrapper; http.ResponseWriter; http.Flusher; http.CloseNotifier; http.Pusher }{Unwrapper:(*httpsnoop.rw)(0xc0032bd180), ResponseWriter:(*httpsnoop.rw)(0xc0032bd180), Flusher:(*httpsnoop.rw)(0xc0032bd180), CloseNotifier:(*httpsnoop.rw)(0xc0032bd180), Pusher:(*httpsnoop.rw)(0xc0032bd180)}}, encoder:(*versioning.codec)(0xc0084b0820), memAllocator:(*runtime.Allocator)(0xc007ad65e8)})
E0322 04:05:08.220657    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ResourceQuota: failed to list *v1.ResourceQuota: Get "https://127.0.0.1:6444/api/v1/resourcequotas?resourceVersion=667": dial tcp 127.0.0.1:6444: i/o timeout
W0322 04:05:07.072254    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.RoleBinding: Get "https://127.0.0.1:6444/apis/rbac.authorization.k8s.io/v1/rolebindings?resourceVersion=668": dial tcp 127.0.0.1:6444: i/o timeout
I0322 04:05:13.722024    2297 trace.go:236] Trace[1390333736]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:39.560) (total time: 154153ms):
Trace[1390333736]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/rbac.authorization.k8s.io/v1/rolebindings?resourceVersion=668": dial tcp 127.0.0.1:6444: i/o timeout 140220ms (04:04:59.781)
Trace[1390333736]: [2m34.153331565s] [2m34.153331565s] END
E0322 04:05:07.189244    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/middlewares?resourceVersion=668": net/http: TLS handshake timeout
W0322 04:05:13.971427    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/helm.cattle.io/v1/helmcharts?resourceVersion=665": net/http: TLS handshake timeout
I0322 04:05:13.978880    2297 trace.go:236] Trace[85442801]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:41.428) (total time: 152547ms):
Trace[85442801]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/helm.cattle.io/v1/helmcharts?resourceVersion=665": net/http: TLS handshake timeout 145872ms (04:05:07.301)
Trace[85442801]: [2m32.547091767s] [2m32.547091767s] END
I0322 04:05:14.165065    2297 request.go:697] Waited for 6.768928776s due to client-side throttling, not priority and fairness, request: GET:https://127.0.0.1:6444/version
time="2024-03-22T04:05:14Z" level=info msg="Slow SQL (started: 2024-03-22 04:05:08.536928724 +0000 UTC m=+684.273894907) (total time: 5.714297154s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC LIMIT 500 : [[% 757]]"
W0322 04:05:14.356111    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.ConfigMap: Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dkube-apiserver-legacy-service-account-token-tracking&resourceVersion=665": net/http: TLS handshake timeout
W0322 04:05:14.356438    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PersistentVolume: Get "https://127.0.0.1:6444/api/v1/persistentvolumes?resourceVersion=667": net/http: TLS handshake timeout
I0322 04:05:14.362138    2297 trace.go:236] Trace[906573776]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:01:58.165) (total time: 196193ms):
Trace[906573776]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dkube-apiserver-legacy-service-account-token-tracking&resourceVersion=665": net/http: TLS handshake timeout 196188ms (04:05:14.353)
Trace[906573776]: [3m16.193214841s] [3m16.193214841s] END
time="2024-03-22T04:05:14Z" level=info msg="Slow SQL (started: 2024-03-22 04:04:58.901532235 +0000 UTC m=+674.638498419) (total time: 8.638627964s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/horizontalpodautoscalers/% false]]"
E0322 04:05:14.432702    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/csidrivers?resourceVersion=665": dial tcp 127.0.0.1:6444: i/o timeout
time="2024-03-22T04:05:14Z" level=info msg="Slow SQL (started: 2024-03-22 04:04:56.337276992 +0000 UTC m=+672.074243167) (total time: 18.40368855s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/traefik.containo.us/ingressroutes/% false]]"
E0322 04:05:14.950108    2297 watch.go:287] unable to encode watch object *v1.WatchEvent: client disconnected (&streaming.encoderWithAllocator{writer:responsewriter.outerWithCloseNotifyAndFlush{UserProvidedDecorator:(*metrics.ResponseWriterDelegator)(0xc0047bb260), InnerCloseNotifierFlusher:struct { httpsnoop.Unwrapper; http.ResponseWriter; http.Flusher; http.CloseNotifier; http.Pusher }{Unwrapper:(*httpsnoop.rw)(0xc0042cfb80), ResponseWriter:(*httpsnoop.rw)(0xc0042cfb80), Flusher:(*httpsnoop.rw)(0xc0042cfb80), CloseNotifier:(*httpsnoop.rw)(0xc0042cfb80), Pusher:(*httpsnoop.rw)(0xc0042cfb80)}}, encoder:(*versioning.codec)(0xc00a2f9540), memAllocator:(*runtime.Allocator)(0xc00838f0e0)})
W0322 04:05:07.913388    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.ValidatingWebhookConfiguration: Get "https://127.0.0.1:6444/apis/admissionregistration.k8s.io/v1/validatingwebhookconfigurations?resourceVersion=665": net/http: TLS handshake timeout
W0322 04:05:09.382403    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.ConfigMap: Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=665": net/http: TLS handshake timeout
W0322 04:05:15.671776    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://127.0.0.1:6444/api/v1/services?resourceVersion=665": net/http: TLS handshake timeout
I0322 04:05:15.677874    2297 trace.go:236] Trace[1301901054]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:41.079) (total time: 154594ms):
Trace[1301901054]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/services?resourceVersion=665": net/http: TLS handshake timeout 154586ms (04:05:15.666)
Trace[1301901054]: [2m34.594452173s] [2m34.594452173s] END
W0322 04:05:15.779499    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Deployment: Get "https://127.0.0.1:6444/apis/apps/v1/deployments?resourceVersion=719": net/http: TLS handshake timeout
I0322 04:05:15.784292    2297 trace.go:236] Trace[2045254893]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:03:02.841) (total time: 132941ms):
Trace[2045254893]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/apps/v1/deployments?resourceVersion=719": net/http: TLS handshake timeout 132933ms (04:05:15.774)
Trace[2045254893]: [2m12.941018533s] [2m12.941018533s] END
W0322 04:05:15.816353    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/tlsoptions?resourceVersion=668": net/http: TLS handshake timeout
I0322 04:05:15.820019    2297 trace.go:236] Trace[1329483231]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:03:19.904) (total time: 115914ms):
Trace[1329483231]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/tlsoptions?resourceVersion=668": net/http: TLS handshake timeout 115905ms (04:05:15.809)
Trace[1329483231]: [1m55.914805765s] [1m55.914805765s] END
E0322 04:05:15.821732    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/tlsoptions?resourceVersion=668": net/http: TLS handshake timeout
E0322 04:05:16.131456    2297 repair.go:83] unable to refresh the port allocations: rpc error: code = Unavailable desc = keepalive ping failed to receive ACK within timeout
W0322 04:05:16.289691    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.ResourceQuota: Get "https://127.0.0.1:6444/api/v1/resourcequotas?resourceVersion=667": net/http: TLS handshake timeout
W0322 04:05:09.931373    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v2.HorizontalPodAutoscaler: Get "https://127.0.0.1:6444/apis/autoscaling/v2/horizontalpodautoscalers?resourceVersion=665": net/http: TLS handshake timeout
I0322 04:05:16.418495    2297 trace.go:236] Trace[1989836263]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:01:36.538) (total time: 214698ms):
Trace[1989836263]: ---"Objects listed" error:Get "https://127.0.0.1:6443/apis/discovery.k8s.io/v1/endpointslices?labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&resourceVersion=715": dial tcp 127.0.0.1:6443: i/o timeout 193656ms (04:04:50.195)
Trace[1989836263]: [3m34.698550471s] [3m34.698550471s] END
I0322 04:05:16.476215    2297 trace.go:236] Trace[589406434]: "iptables ChainExists" (22-Mar-2024 04:04:53.795) (total time: 10388ms):
Trace[589406434]: [10.388911174s] [10.388911174s] END
E0322 04:05:16.484621    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://127.0.0.1:6444/api/v1/services?resourceVersion=665": net/http: TLS handshake timeout
E0322 04:05:16.920798    2297 container_log_manager.go:235] "Failed to get container status" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" containerID="50900ef7d9e42c2a4625f6fb437eebbf4388a3e02fa58ab7d6836f44480c3bad"
W0322 04:05:11.616311    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://127.0.0.1:6443/api/v1/namespaces?resourceVersion=667": net/http: TLS handshake timeout
I0322 04:05:17.020678    2297 trace.go:236] Trace[1833048262]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:43.967) (total time: 146532ms):
Trace[1833048262]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/ingressroutetcps?resourceVersion=671": dial tcp 127.0.0.1:6444: i/o timeout 140916ms (04:05:04.883)
Trace[1833048262]: [2m26.532340959s] [2m26.532340959s] END
W0322 04:05:22.781584    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/traefikservices?resourceVersion=665": net/http: TLS handshake timeout
I0322 04:05:22.828549    2297 trace.go:236] Trace[1700786933]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:09.359) (total time: 187003ms):
Trace[1700786933]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/autoscaling/v2/horizontalpodautoscalers?resourceVersion=665": net/http: TLS handshake timeout 174767ms (04:05:04.127)
Trace[1700786933]: [3m7.003371202s] [3m7.003371202s] END
E0322 04:05:22.834340    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v2.HorizontalPodAutoscaler: failed to list *v2.HorizontalPodAutoscaler: Get "https://127.0.0.1:6444/apis/autoscaling/v2/horizontalpodautoscalers?resourceVersion=665": net/http: TLS handshake timeout
W0322 04:05:22.881051    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Endpoints: Get "https://127.0.0.1:6444/api/v1/endpoints?resourceVersion=713": net/http: TLS handshake timeout
I0322 04:05:22.888539    2297 trace.go:236] Trace[1559103825]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:03:02.569) (total time: 140314ms):
Trace[1559103825]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/endpoints?resourceVersion=713": net/http: TLS handshake timeout 140309ms (04:05:22.879)
Trace[1559103825]: [2m20.314066129s] [2m20.314066129s] END
E0322 04:05:22.927938    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://127.0.0.1:6443/apis/discovery.k8s.io/v1/endpointslices?labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&resourceVersion=715": dial tcp 127.0.0.1:6443: i/o timeout
I0322 04:05:22.932001    2297 trace.go:236] Trace[1506478700]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:04.043) (total time: 192405ms):
Trace[1506478700]: ---"Objects listed" error:Get "https://127.0.0.1:6443/api/v1/services?labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&resourceVersion=665": dial tcp 127.0.0.1:6443: i/o timeout 174247ms (04:04:58.291)
Trace[1506478700]: [3m12.405174319s] [3m12.405174319s] END
E0322 04:05:22.933390    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://127.0.0.1:6443/api/v1/services?labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&resourceVersion=665": dial tcp 127.0.0.1:6443: i/o timeout
W0322 04:05:16.606612    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PriorityClass: Get "https://127.0.0.1:6444/apis/scheduling.k8s.io/v1/priorityclasses?resourceVersion=665": net/http: TLS handshake timeout
I0322 04:05:23.536605    2297 log.go:245] http: TLS handshake error from 10.42.0.5:59772: write tcp 192.168.56.110:10250->10.42.0.5:59772: write: broken pipe
W0322 04:05:23.635364    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.ReplicaSet: Get "https://127.0.0.1:6444/apis/apps/v1/replicasets?resourceVersion=717": net/http: TLS handshake timeout
W0322 04:05:23.641099    2297 reflector.go:535] object-"kube-system"/"local-path-config": failed to list *v1.ConfigMap: Get "https://127.0.0.1:6443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dlocal-path-config&resourceVersion=665": net/http: TLS handshake timeout
I0322 04:05:23.643808    2297 trace.go:236] Trace[1673988313]: "Reflector ListAndWatch" name:object-"kube-system"/"local-path-config" (22-Mar-2024 04:03:28.310) (total time: 115330ms):
Trace[1673988313]: ---"Objects listed" error:Get "https://127.0.0.1:6443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dlocal-path-config&resourceVersion=665": net/http: TLS handshake timeout 115328ms (04:05:23.639)
Trace[1673988313]: [1m55.330930125s] [1m55.330930125s] END
W0322 04:05:18.113781    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.ConfigMap: Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=665": net/http: TLS handshake timeout
I0322 04:05:28.374620    2297 trace.go:236] Trace[1569112083]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:03.279) (total time: 205090ms):
Trace[1569112083]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=665": net/http: TLS handshake timeout 194816ms (04:05:18.096)
Trace[1569112083]: [3m25.090693595s] [3m25.090693595s] END
E0322 04:05:28.379025    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=665": net/http: TLS handshake timeout
W0322 04:05:23.950838    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://127.0.0.1:6444/apis/discovery.k8s.io/v1/endpointslices?resourceVersion=715": net/http: TLS handshake timeout
I0322 04:05:28.380379    2297 trace.go:236] Trace[51951977]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:03:15.310) (total time: 133069ms):
Trace[51951977]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/discovery.k8s.io/v1/endpointslices?resourceVersion=715": net/http: TLS handshake timeout 128632ms (04:05:23.942)
Trace[51951977]: [2m13.069898067s] [2m13.069898067s] END
E0322 04:05:28.380410    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://127.0.0.1:6444/apis/discovery.k8s.io/v1/endpointslices?resourceVersion=715": net/http: TLS handshake timeout
E0322 04:05:17.544745    2297 webhook.go:154] Failed to make webhook authenticator request: client rate limiter Wait returned an error: context deadline exceeded
E0322 04:05:28.613220    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Endpoints: failed to list *v1.Endpoints: Get "https://127.0.0.1:6444/api/v1/endpoints?resourceVersion=713": net/http: TLS handshake timeout
time="2024-03-22T04:05:17Z" level=error msg="error while range on /registry/services/endpoints/ /registry/services/endpoints/: context canceled"
time="2024-03-22T04:05:19Z" level=info msg="Slow SQL (started: 2024-03-22 04:04:56.352248974 +0000 UTC m=+672.089215152) (total time: 17.728599067s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/ingressclasses/% false]]"
time="2024-03-22T04:05:19Z" level=info msg="Slow SQL (started: 2024-03-22 04:05:05.787093522 +0000 UTC m=+681.524059709) (total time: 1.711515752s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/prioritylevelconfigurations/% false]]"
time="2024-03-22T04:05:20Z" level=info msg="Slow SQL (started: 2024-03-22 04:04:55.109188731 +0000 UTC m=+670.846154907) (total time: 19.974000709s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/runtimeclasses/% false]]"
time="2024-03-22T04:05:33Z" level=info msg="Slow SQL (started: 2024-03-22 04:05:13.634979901 +0000 UTC m=+689.371946075) (total time: 19.721258468s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/traefik.containo.us/ingressroutetcps/% false]]"
I0322 04:05:29.068292    2297 trace.go:236] Trace[2010624064]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:22.402) (total time: 180866ms):
Trace[2010624064]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/scheduling.k8s.io/v1/priorityclasses?resourceVersion=665": net/http: TLS handshake timeout 167828ms (04:05:10.230)
Trace[2010624064]: [3m0.866833364s] [3m0.866833364s] END
E0322 04:05:35.305961    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PriorityClass: failed to list *v1.PriorityClass: Get "https://127.0.0.1:6444/apis/scheduling.k8s.io/v1/priorityclasses?resourceVersion=665": net/http: TLS handshake timeout
W0322 04:05:24.446837    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Node: Get "https://127.0.0.1:6443/api/v1/nodes?resourceVersion=679": net/http: TLS handshake timeout
I0322 04:05:35.311452    2297 trace.go:236] Trace[1062322615]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:32.401) (total time: 182907ms):
Trace[1062322615]: ---"Objects listed" error:Get "https://127.0.0.1:6443/api/v1/nodes?resourceVersion=679": net/http: TLS handshake timeout 166115ms (04:05:18.516)
Trace[1062322615]: [3m2.907164624s] [3m2.907164624s] END
I0322 04:05:17.666184    2297 trace.go:236] Trace[1002052504]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:39.437) (total time: 152902ms):
Trace[1002052504]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/k3s.cattle.io/v1/etcdsnapshotfiles?resourceVersion=665": dial tcp 127.0.0.1:6444: i/o timeout 140154ms (04:04:59.591)
Trace[1002052504]: [2m32.902852431s] [2m32.902852431s] END
E0322 04:05:35.313631    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/k3s.cattle.io/v1/etcdsnapshotfiles?resourceVersion=665": dial tcp 127.0.0.1:6444: i/o timeout
E0322 04:05:17.764038    2297 watch.go:287] unable to encode watch object *v1.WatchEvent: client disconnected (&streaming.encoderWithAllocator{writer:(*framer.lengthDelimitedFrameWriter)(0xc004faf020), encoder:(*versioning.codec)(0xc004559220), memAllocator:(*runtime.Allocator)(0xc005d06750)})
E0322 04:05:35.344439    2297 repair.go:125] unable to refresh the service IP block: rpc error: code = Unavailable desc = keepalive ping failed to receive ACK within timeout
W0322 04:05:36.010503    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.ReplicationController: Get "https://127.0.0.1:6444/api/v1/replicationcontrollers?resourceVersion=667": net/http: TLS handshake timeout
time="2024-03-22T04:05:19Z" level=info msg="Slow SQL (started: 2024-03-22 04:04:52.042409584 +0000 UTC m=+667.779375767) (total time: 21.317441336s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/volumeattachments/% false]]"
W0322 04:05:17.887837    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.containo.us/v1alpha1/tlsoptions?resourceVersion=668": net/http: TLS handshake timeout
W0322 04:05:17.890368    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/serverstransporttcps?resourceVersion=679": net/http: TLS handshake timeout
W0322 04:05:18.225440    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.CronJob: Get "https://127.0.0.1:6444/apis/batch/v1/cronjobs?resourceVersion=665": net/http: TLS handshake timeout
I0322 04:05:37.439972    2297 request.go:697] Waited for 5.364046485s due to client-side throttling, not priority and fairness, request: GET:https://127.0.0.1:6444/api
W0322 04:05:12.589133    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.ConfigMap: Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=665": net/http: TLS handshake timeout
I0322 04:05:18.568662    2297 trace.go:236] Trace[1353124558]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:37.549) (total time: 155051ms):
Trace[1353124558]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/rbac.authorization.k8s.io/v1/roles?resourceVersion=667": net/http: TLS handshake timeout 155049ms (04:05:12.598)
Trace[1353124558]: [2m35.051611356s] [2m35.051611356s] END
W0322 04:05:18.607415    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.ConfigMap: Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=665": net/http: TLS handshake timeout
E0322 04:05:18.644021    2297 kubelet.go:2327] "Skipping pod synchronization" err="[container runtime is down, PLEG is not healthy: pleg was last seen active 4m34.322129279s ago; threshold is 3m0s]"
E0322 04:05:18.798139    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get "https://127.0.0.1:6444/api/v1/persistentvolumeclaims?resourceVersion=665": net/http: TLS handshake timeout
W0322 04:05:12.866127    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/serverstransports?resourceVersion=671": net/http: TLS handshake timeout
I0322 04:05:37.544368    2297 trace.go:236] Trace[1568843453]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:22.767) (total time: 194774ms):
Trace[1568843453]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/serverstransports?resourceVersion=671": net/http: TLS handshake timeout 170093ms (04:05:12.860)
Trace[1568843453]: [3m14.774303528s] [3m14.774303528s] END
W0322 04:05:18.874601    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PodDisruptionBudget: Get "https://127.0.0.1:6444/apis/policy/v1/poddisruptionbudgets?resourceVersion=665": net/http: TLS handshake timeout
I0322 04:05:37.575258    2297 trace.go:236] Trace[227367169]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:03.753) (total time: 213822ms):
Trace[227367169]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/policy/v1/poddisruptionbudgets?resourceVersion=665": net/http: TLS handshake timeout 195116ms (04:05:18.869)
Trace[227367169]: [3m33.822187436s] [3m33.822187436s] END
E0322 04:05:37.576571    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get "https://127.0.0.1:6444/apis/policy/v1/poddisruptionbudgets?resourceVersion=665": net/http: TLS handshake timeout
I0322 04:05:18.961582    2297 trace.go:236] Trace[371424358]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:03:02.822) (total time: 130148ms):
Trace[371424358]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/apps/v1/controllerrevisions?resourceVersion=665": net/http: TLS handshake timeout 124694ms (04:05:07.517)
Trace[371424358]: [2m10.148077228s] [2m10.148077228s] END
E0322 04:05:37.577233    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ControllerRevision: failed to list *v1.ControllerRevision: Get "https://127.0.0.1:6444/apis/apps/v1/controllerrevisions?resourceVersion=665": net/http: TLS handshake timeout
I0322 04:05:18.961612    2297 trace.go:236] Trace[360592178]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:01:58.327) (total time: 194648ms):
Trace[360592178]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/nodes?resourceVersion=679": net/http: TLS handshake timeout 182382ms (04:05:00.709)
Trace[360592178]: [3m14.648371871s] [3m14.648371871s] END
E0322 04:05:37.577783    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://127.0.0.1:6444/api/v1/nodes?resourceVersion=679": net/http: TLS handshake timeout
I0322 04:05:18.963843    2297 trace.go:236] Trace[1796785323]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:02.380) (total time: 196583ms):
Trace[1796785323]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/limitranges?resourceVersion=665": dial tcp 127.0.0.1:6444: i/o timeout 185863ms (04:05:08.243)
Trace[1796785323]: [3m16.583192765s] [3m16.583192765s] END
E0322 04:05:37.578339    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.LimitRange: failed to list *v1.LimitRange: Get "https://127.0.0.1:6444/api/v1/limitranges?resourceVersion=665": dial tcp 127.0.0.1:6444: i/o timeout
E0322 04:05:18.966700    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?resourceVersion=665": net/http: TLS handshake timeout
E0322 04:05:19.003782    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/helm.cattle.io/v1/helmcharts?resourceVersion=665": net/http: TLS handshake timeout
E0322 04:05:19.233677    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
W0322 04:05:19.385977    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://127.0.0.1:6444/api/v1/namespaces?resourceVersion=667": net/http: TLS handshake timeout
I0322 04:05:37.630180    2297 trace.go:236] Trace[1256733523]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:03.661) (total time: 213964ms):
Trace[1256733523]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/namespaces?resourceVersion=667": net/http: TLS handshake timeout 189633ms (04:05:13.294)
Trace[1256733523]: [3m33.964938624s] [3m33.964938624s] END
E0322 04:05:20.061107    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.RoleBinding: failed to list *v1.RoleBinding: Get "https://127.0.0.1:6444/apis/rbac.authorization.k8s.io/v1/rolebindings?resourceVersion=668": dial tcp 127.0.0.1:6444: i/o timeout
W0322 04:05:13.824802    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.StatefulSet: Get "https://127.0.0.1:6444/apis/apps/v1/statefulsets?resourceVersion=667": net/http: TLS handshake timeout
I0322 04:05:37.639995    2297 trace.go:236] Trace[729466346]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:03:02.798) (total time: 154841ms):
Trace[729466346]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/apps/v1/statefulsets?resourceVersion=667": net/http: TLS handshake timeout 131015ms (04:05:13.813)
Trace[729466346]: [2m34.841768063s] [2m34.841768063s] END
E0322 04:05:37.642709    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://127.0.0.1:6444/apis/apps/v1/statefulsets?resourceVersion=667": net/http: TLS handshake timeout
E0322 04:05:20.309415    2297 controller.go:193] "Failed to update lease" err="Put \"https://127.0.0.1:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/server?timeout=10s\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
E0322 04:05:37.687290    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
W0322 04:05:20.529036    2297 reflector.go:535] object-"kube-system"/"kube-root-ca.crt": failed to list *v1.ConfigMap: Get "https://127.0.0.1:6443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dkube-root-ca.crt&resourceVersion=665": net/http: TLS handshake timeout
W0322 04:05:20.638905    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Pod: Get "https://127.0.0.1:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dserver&resourceVersion=708": net/http: TLS handshake timeout
I0322 04:05:20.701147    2297 trace.go:236] Trace[1397037624]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:03.683) (total time: 191320ms):
Trace[1397037624]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/admissionregistration.k8s.io/v1/validatingwebhookconfigurations?resourceVersion=665": net/http: TLS handshake timeout 184222ms (04:05:07.906)
Trace[1397037624]: [3m11.320891177s] [3m11.320891177s] END
W0322 04:05:20.764848    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/apiextensions.k8s.io/v1/customresourcedefinitions?resourceVersion=665": net/http: TLS handshake timeout
I0322 04:05:37.878898    2297 trace.go:236] Trace[73251808]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:40.866) (total time: 177006ms):
Trace[73251808]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/apiextensions.k8s.io/v1/customresourcedefinitions?resourceVersion=665": net/http: TLS handshake timeout 159893ms (04:05:20.760)
Trace[73251808]: [2m57.006764606s] [2m57.006764606s] END
I0322 04:05:21.010597    2297 trace.go:236] Trace[546640781]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:01:58.285) (total time: 196904ms):
Trace[546640781]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=665": dial tcp 127.0.0.1:6444: i/o timeout 190950ms (04:05:09.236)
Trace[546640781]: [3m16.904101596s] [3m16.904101596s] END
W0322 04:05:21.251565    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/middlewaretcps?resourceVersion=669": net/http: TLS handshake timeout
W0322 04:05:21.254390    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.LimitRange: Get "https://127.0.0.1:6444/api/v1/limitranges?resourceVersion=665": net/http: TLS handshake timeout
W0322 04:05:21.295662    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.NetworkPolicy: Get "https://127.0.0.1:6444/apis/networking.k8s.io/v1/networkpolicies?resourceVersion=666": net/http: TLS handshake timeout
I0322 04:05:37.989318    2297 trace.go:236] Trace[618199026]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:03:02.724) (total time: 155260ms):
Trace[618199026]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/networking.k8s.io/v1/networkpolicies?resourceVersion=666": net/http: TLS handshake timeout 138569ms (04:05:21.294)
Trace[618199026]: [2m35.260685941s] [2m35.260685941s] END
W0322 04:05:21.296190    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/helm.cattle.io/v1/helmchartconfigs?resourceVersion=756": net/http: TLS handshake timeout
W0322 04:05:21.296241    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.containo.us/v1alpha1/middlewaretcps?resourceVersion=671": net/http: TLS handshake timeout
W0322 04:05:21.295745    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.CSINode: Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/csinodes?resourceVersion=666": net/http: TLS handshake timeout
W0322 04:05:21.297964    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.StorageClass: Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/storageclasses?resourceVersion=667": net/http: TLS handshake timeout
W0322 04:05:21.298015    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/tlsstores?resourceVersion=665": net/http: TLS handshake timeout
I0322 04:05:38.194003    2297 trace.go:236] Trace[1690964148]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:55.830) (total time: 162345ms):
Trace[1690964148]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/tlsstores?resourceVersion=665": net/http: TLS handshake timeout 145467ms (04:05:21.298)
Trace[1690964148]: [2m42.345359799s] [2m42.345359799s] END
W0322 04:05:21.298047    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/ingressrouteudps?resourceVersion=671": net/http: TLS handshake timeout
I0322 04:05:38.218593    2297 trace.go:236] Trace[1890552365]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:39.775) (total time: 178432ms):
Trace[1890552365]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/ingressrouteudps?resourceVersion=671": net/http: TLS handshake timeout 161522ms (04:05:21.298)
Trace[1890552365]: [2m58.432173084s] [2m58.432173084s] END
W0322 04:05:21.303289    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.containo.us/v1alpha1/ingressroutes?resourceVersion=667": net/http: TLS handshake timeout
I0322 04:05:38.235782    2297 trace.go:236] Trace[1422879386]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:41.422) (total time: 176813ms):
Trace[1422879386]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/traefik.containo.us/v1alpha1/ingressroutes?resourceVersion=667": net/http: TLS handshake timeout 159880ms (04:05:21.303)
Trace[1422879386]: [2m56.813092096s] [2m56.813092096s] END
E0322 04:05:38.236244    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.containo.us/v1alpha1/ingressroutes?resourceVersion=667": net/http: TLS handshake timeout
W0322 04:05:21.344863    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Pod: Get "https://127.0.0.1:6444/api/v1/pods?resourceVersion=708": net/http: TLS handshake timeout
I0322 04:05:38.236833    2297 trace.go:236] Trace[7057392]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:40.877) (total time: 177359ms):
Trace[7057392]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/pods?resourceVersion=708": net/http: TLS handshake timeout 160466ms (04:05:21.344)
Trace[7057392]: [2m57.359347433s] [2m57.359347433s] END
E0322 04:05:38.236852    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://127.0.0.1:6444/api/v1/pods?resourceVersion=708": net/http: TLS handshake timeout
W0322 04:05:21.405492    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://127.0.0.1:6444/api/v1/namespaces?resourceVersion=667": net/http: TLS handshake timeout
I0322 04:05:38.237358    2297 trace.go:236] Trace[1052402818]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:03:02.682) (total time: 155554ms):
Trace[1052402818]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/namespaces?resourceVersion=667": net/http: TLS handshake timeout 138722ms (04:05:21.405)
Trace[1052402818]: [2m35.554940009s] [2m35.554940009s] END
E0322 04:05:38.237385    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://127.0.0.1:6444/api/v1/namespaces?resourceVersion=667": net/http: TLS handshake timeout
W0322 04:05:21.408273    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.containo.us/v1alpha1/traefikservices?resourceVersion=667": net/http: TLS handshake timeout
I0322 04:05:38.237427    2297 trace.go:236] Trace[1951422081]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:03:02.466) (total time: 155770ms):
Trace[1951422081]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/traefik.containo.us/v1alpha1/traefikservices?resourceVersion=667": net/http: TLS handshake timeout 138941ms (04:05:21.408)
Trace[1951422081]: [2m35.77050958s] [2m35.77050958s] END
E0322 04:05:38.237440    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.containo.us/v1alpha1/traefikservices?resourceVersion=667": net/http: TLS handshake timeout
W0322 04:05:21.432564    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Lease: Get "https://127.0.0.1:6444/apis/coordination.k8s.io/v1/leases?resourceVersion=712": net/http: TLS handshake timeout
I0322 04:05:38.237505    2297 trace.go:236] Trace[1493847154]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:03:03.901) (total time: 154335ms):
Trace[1493847154]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/coordination.k8s.io/v1/leases?resourceVersion=712": net/http: TLS handshake timeout 137528ms (04:05:21.429)
Trace[1493847154]: [2m34.335828086s] [2m34.335828086s] END
E0322 04:05:38.237516    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Lease: failed to list *v1.Lease: Get "https://127.0.0.1:6444/apis/coordination.k8s.io/v1/leases?resourceVersion=712": net/http: TLS handshake timeout
W0322 04:05:21.433183    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.DaemonSet: Get "https://127.0.0.1:6444/apis/apps/v1/daemonsets?resourceVersion=667": net/http: TLS handshake timeout
I0322 04:05:38.237542    2297 trace.go:236] Trace[1128816601]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:03:16.377) (total time: 141859ms):
Trace[1128816601]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/apps/v1/daemonsets?resourceVersion=667": net/http: TLS handshake timeout 125055ms (04:05:21.433)
Trace[1128816601]: [2m21.859703648s] [2m21.859703648s] END
E0322 04:05:38.237549    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.DaemonSet: failed to list *v1.DaemonSet: Get "https://127.0.0.1:6444/apis/apps/v1/daemonsets?resourceVersion=667": net/http: TLS handshake timeout
W0322 04:05:21.444792    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.ServiceAccount: Get "https://127.0.0.1:6444/api/v1/serviceaccounts?resourceVersion=665": net/http: TLS handshake timeout
I0322 04:05:38.238098    2297 trace.go:236] Trace[639448884]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:03:25.048) (total time: 133189ms):
Trace[639448884]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/serviceaccounts?resourceVersion=665": net/http: TLS handshake timeout 116395ms (04:05:21.444)
Trace[639448884]: [2m13.189273226s] [2m13.189273226s] END
E0322 04:05:38.238125    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ServiceAccount: failed to list *v1.ServiceAccount: Get "https://127.0.0.1:6444/api/v1/serviceaccounts?resourceVersion=665": net/http: TLS handshake timeout
W0322 04:05:21.445430    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/ingressroutes?resourceVersion=667": net/http: TLS handshake timeout
I0322 04:05:38.238167    2297 trace.go:236] Trace[286074891]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:59.996) (total time: 158241ms):
Trace[286074891]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/ingressroutes?resourceVersion=667": net/http: TLS handshake timeout 141448ms (04:05:21.445)
Trace[286074891]: [2m38.241698188s] [2m38.241698188s] END
E0322 04:05:38.238176    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/ingressroutes?resourceVersion=667": net/http: TLS handshake timeout
W0322 04:05:21.445466    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://127.0.0.1:6444/api/v1/services?resourceVersion=665": net/http: TLS handshake timeout
I0322 04:05:38.238782    2297 trace.go:236] Trace[983570603]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:03:02.672) (total time: 155566ms):
Trace[983570603]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/services?resourceVersion=665": net/http: TLS handshake timeout 138772ms (04:05:21.445)
Trace[983570603]: [2m35.56625668s] [2m35.56625668s] END
E0322 04:05:38.238804    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://127.0.0.1:6444/api/v1/services?resourceVersion=665": net/http: TLS handshake timeout
W0322 04:05:21.446016    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/apiregistration.k8s.io/v1/apiservices?resourceVersion=693": net/http: TLS handshake timeout
I0322 04:05:38.238846    2297 trace.go:236] Trace[1397217417]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:03:02.604) (total time: 155633ms):
Trace[1397217417]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/apiregistration.k8s.io/v1/apiservices?resourceVersion=693": net/http: TLS handshake timeout 138841ms (04:05:21.446)
Trace[1397217417]: [2m35.633964552s] [2m35.633964552s] END
E0322 04:05:38.238855    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/apiregistration.k8s.io/v1/apiservices?resourceVersion=693": net/http: TLS handshake timeout
W0322 04:05:21.448236    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.IngressClass: Get "https://127.0.0.1:6444/apis/networking.k8s.io/v1/ingressclasses?resourceVersion=665": net/http: TLS handshake timeout
I0322 04:05:38.239413    2297 trace.go:236] Trace[280471468]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:03:40.803) (total time: 117435ms):
Trace[280471468]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/networking.k8s.io/v1/ingressclasses?resourceVersion=665": net/http: TLS handshake timeout 100644ms (04:05:21.448)
Trace[280471468]: [1m57.435456401s] [1m57.435456401s] END
E0322 04:05:38.239436    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.IngressClass: failed to list *v1.IngressClass: Get "https://127.0.0.1:6444/apis/networking.k8s.io/v1/ingressclasses?resourceVersion=665": net/http: TLS handshake timeout
W0322 04:05:21.449499    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.MutatingWebhookConfiguration: Get "https://127.0.0.1:6444/apis/admissionregistration.k8s.io/v1/mutatingwebhookconfigurations?resourceVersion=665": net/http: TLS handshake timeout
I0322 04:05:38.240107    2297 trace.go:236] Trace[434105231]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:03:28.837) (total time: 129402ms):
Trace[434105231]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/admissionregistration.k8s.io/v1/mutatingwebhookconfigurations?resourceVersion=665": net/http: TLS handshake timeout 112611ms (04:05:21.449)
Trace[434105231]: [2m9.40247107s] [2m9.40247107s] END
E0322 04:05:38.240127    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.MutatingWebhookConfiguration: failed to list *v1.MutatingWebhookConfiguration: Get "https://127.0.0.1:6444/apis/admissionregistration.k8s.io/v1/mutatingwebhookconfigurations?resourceVersion=665": net/http: TLS handshake timeout
W0322 04:05:21.450990    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PodDisruptionBudget: Get "https://127.0.0.1:6444/apis/policy/v1/poddisruptionbudgets?resourceVersion=665": net/http: TLS handshake timeout
I0322 04:05:38.240732    2297 trace.go:236] Trace[1267456469]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:26.736) (total time: 191503ms):
Trace[1267456469]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/policy/v1/poddisruptionbudgets?resourceVersion=665": net/http: TLS handshake timeout 174714ms (04:05:21.450)
Trace[1267456469]: [3m11.503757006s] [3m11.503757006s] END
E0322 04:05:38.240754    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get "https://127.0.0.1:6444/apis/policy/v1/poddisruptionbudgets?resourceVersion=665": net/http: TLS handshake timeout
W0322 04:05:21.452401    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.CSIDriver: Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/csidrivers?resourceVersion=665": net/http: TLS handshake timeout
I0322 04:05:38.243153    2297 trace.go:236] Trace[632912829]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:41.422) (total time: 176820ms):
Trace[632912829]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/csidrivers?resourceVersion=665": net/http: TLS handshake timeout 160029ms (04:05:21.452)
Trace[632912829]: [2m56.820474596s] [2m56.820474596s] END
E0322 04:05:38.243174    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/csidrivers?resourceVersion=665": net/http: TLS handshake timeout
W0322 04:05:15.364798    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.containo.us/v1alpha1/middlewares?resourceVersion=671": net/http: TLS handshake timeout
I0322 04:05:38.243322    2297 trace.go:236] Trace[1541159202]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:53.996) (total time: 164246ms):
Trace[1541159202]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/traefik.containo.us/v1alpha1/middlewares?resourceVersion=671": net/http: TLS handshake timeout 141356ms (04:05:15.352)
Trace[1541159202]: [2m44.246874745s] [2m44.246874745s] END
E0322 04:05:38.243339    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.containo.us/v1alpha1/middlewares?resourceVersion=671": net/http: TLS handshake timeout
I0322 04:05:21.751923    2297 trace.go:236] Trace[1529201973]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:03.531) (total time: 191977ms):
Trace[1529201973]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=665": net/http: TLS handshake timeout 185846ms (04:05:09.377)
Trace[1529201973]: [3m11.977187677s] [3m11.977187677s] END
E0322 04:05:38.243594    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=665": net/http: TLS handshake timeout
W0322 04:05:21.751826    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.CSIStorageCapacity: Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/csistoragecapacities?resourceVersion=665": net/http: TLS handshake timeout
I0322 04:05:38.243645    2297 trace.go:236] Trace[226460939]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:03.015) (total time: 215227ms):
Trace[226460939]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/csistoragecapacities?resourceVersion=665": net/http: TLS handshake timeout 198729ms (04:05:21.745)
Trace[226460939]: [3m35.227669971s] [3m35.227669971s] END
E0322 04:05:38.243654    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/csistoragecapacities?resourceVersion=665": net/http: TLS handshake timeout
E0322 04:05:21.944052    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://127.0.0.1:6444/api/v1/services?resourceVersion=665": net/http: TLS handshake timeout
W0322 04:05:21.946541    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.ClusterRoleBinding: Get "https://127.0.0.1:6444/apis/rbac.authorization.k8s.io/v1/clusterrolebindings?resourceVersion=667": dial tcp 127.0.0.1:6444: i/o timeout
I0322 04:05:17.008467    2297 trace.go:236] Trace[734992860]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:01:38.486) (total time: 218502ms):
Trace[734992860]: ---"Objects listed" error:Get "https://127.0.0.1:6443/api/v1/namespaces?resourceVersion=667": net/http: TLS handshake timeout 213117ms (04:05:11.603)
Trace[734992860]: [3m38.502302246s] [3m38.502302246s] END
I0322 04:05:17.354109    2297 trace.go:236] Trace[1669332727]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:15.385) (total time: 176951ms):
Trace[1669332727]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/secrets?resourceVersion=675": dial tcp 127.0.0.1:6444: i/o timeout 163288ms (04:04:58.673)
Trace[1669332727]: [2m56.951131194s] [2m56.951131194s] END
E0322 04:05:38.303185    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Secret: failed to list *v1.Secret: Get "https://127.0.0.1:6444/api/v1/secrets?resourceVersion=675": dial tcp 127.0.0.1:6444: i/o timeout
I0322 04:05:24.733229    2297 trace.go:236] Trace[1197921367]: "Reflector ListAndWatch" name:object-"kube-system"/"coredns-custom" (22-Mar-2024 04:02:43.657) (total time: 149137ms):
Trace[1197921367]: ---"Objects listed" error:Get "https://127.0.0.1:6443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dcoredns-custom&resourceVersion=665": net/http: TLS handshake timeout 149123ms (04:05:12.780)
Trace[1197921367]: [2m29.137441548s] [2m29.137441548s] END
E0322 04:05:38.303879    2297 reflector.go:147] object-"kube-system"/"coredns-custom": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://127.0.0.1:6443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dcoredns-custom&resourceVersion=665": net/http: TLS handshake timeout
W0322 04:05:25.435668    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.CertificateSigningRequest: Get "https://127.0.0.1:6444/apis/certificates.k8s.io/v1/certificatesigningrequests?resourceVersion=667": net/http: TLS handshake timeout
E0322 04:05:25.479938    2297 timeout.go:142] post-timeout activity - time-elapsed: 1m33.632802385s, POST "/api/v1/namespaces/default/events" result: <nil>
W0322 04:05:25.621482    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.containo.us/v1alpha1/serverstransports?resourceVersion=671": net/http: TLS handshake timeout
I0322 04:05:26.167217    2297 trace.go:236] Trace[586048410]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:01:54.791) (total time: 199567ms):
Trace[586048410]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/persistentvolumes?resourceVersion=667": net/http: TLS handshake timeout 199561ms (04:05:14.353)
Trace[586048410]: [3m19.567178824s] [3m19.567178824s] END
E0322 04:05:38.416180    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get "https://127.0.0.1:6444/api/v1/persistentvolumes?resourceVersion=667": net/http: TLS handshake timeout
E0322 04:05:26.204143    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dkube-apiserver-legacy-service-account-token-tracking&resourceVersion=665": net/http: TLS handshake timeout
W0322 04:05:21.144078    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.ReplicaSet: Get "https://127.0.0.1:6444/apis/apps/v1/replicasets?resourceVersion=717": net/http: TLS handshake timeout
I0322 04:05:38.422305    2297 trace.go:236] Trace[1638342422]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:41.260) (total time: 177161ms):
Trace[1638342422]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/apps/v1/replicasets?resourceVersion=717": net/http: TLS handshake timeout 159875ms (04:05:21.136)
Trace[1638342422]: [2m57.161610037s] [2m57.161610037s] END
E0322 04:05:38.422853    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get "https://127.0.0.1:6444/apis/apps/v1/replicasets?resourceVersion=717": net/http: TLS handshake timeout
W0322 04:05:21.155718    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.containo.us/v1alpha1/ingressroutetcps?resourceVersion=668": net/http: TLS handshake timeout
I0322 04:05:38.423161    2297 trace.go:236] Trace[2073609624]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:03:02.563) (total time: 155859ms):
Trace[2073609624]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/traefik.containo.us/v1alpha1/ingressroutetcps?resourceVersion=668": net/http: TLS handshake timeout 138592ms (04:05:21.155)
Trace[2073609624]: [2m35.859898279s] [2m35.859898279s] END
E0322 04:05:38.423238    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.containo.us/v1alpha1/ingressroutetcps?resourceVersion=668": net/http: TLS handshake timeout
E0322 04:05:22.019281    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Deployment: failed to list *v1.Deployment: Get "https://127.0.0.1:6444/apis/apps/v1/deployments?resourceVersion=719": net/http: TLS handshake timeout
W0322 04:05:27.626667    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.CSIStorageCapacity: Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/csistoragecapacities?resourceVersion=665": net/http: TLS handshake timeout
I0322 04:05:38.424113    2297 trace.go:236] Trace[1369554265]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:03:26.373) (total time: 132049ms):
Trace[1369554265]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/csistoragecapacities?resourceVersion=665": net/http: TLS handshake timeout 121248ms (04:05:27.622)
Trace[1369554265]: [2m12.049961474s] [2m12.049961474s] END
E0322 04:05:38.424215    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/csistoragecapacities?resourceVersion=665": net/http: TLS handshake timeout
E0322 04:05:22.282534    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/ingressroutetcps?resourceVersion=671": dial tcp 127.0.0.1:6444: i/o timeout
I0322 04:05:27.886044    2297 log.go:245] http: TLS handshake error from 10.42.0.5:39082: EOF
I0322 04:05:27.984787    2297 trace.go:236] Trace[1006852048]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:01:58.408) (total time: 205230ms):
Trace[1006852048]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/apps/v1/replicasets?resourceVersion=717": net/http: TLS handshake timeout 199444ms (04:05:17.852)
Trace[1006852048]: [3m25.230492249s] [3m25.230492249s] END
E0322 04:05:38.469289    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get "https://127.0.0.1:6444/apis/apps/v1/replicasets?resourceVersion=717": net/http: TLS handshake timeout
E0322 04:05:27.987604    2297 reflector.go:147] object-"kube-system"/"local-path-config": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://127.0.0.1:6443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dlocal-path-config&resourceVersion=665": net/http: TLS handshake timeout
W0322 04:05:28.156358    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/k3s.cattle.io/v1/addons?resourceVersion=665": net/http: TLS handshake timeout
I0322 04:05:38.479053    2297 trace.go:236] Trace[732442524]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:39.664) (total time: 178811ms):
Trace[732442524]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/k3s.cattle.io/v1/addons?resourceVersion=665": net/http: TLS handshake timeout 162859ms (04:05:22.523)
Trace[732442524]: [2m58.811567139s] [2m58.811567139s] END
E0322 04:05:38.479165    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/k3s.cattle.io/v1/addons?resourceVersion=665": net/http: TLS handshake timeout
I0322 04:05:38.504114    2297 log.go:245] http: TLS handshake error from 10.42.0.5:52402: write tcp 192.168.56.110:10250->10.42.0.5:52402: write: broken pipe
I0322 04:05:28.363245    2297 trace.go:236] Trace[747466374]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:55.158) (total time: 147514ms):
Trace[747466374]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/resourcequotas?resourceVersion=667": net/http: TLS handshake timeout 136008ms (04:05:11.167)
Trace[747466374]: [2m27.514363094s] [2m27.514363094s] END
E0322 04:05:38.518385    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ResourceQuota: failed to list *v1.ResourceQuota: Get "https://127.0.0.1:6444/api/v1/resourcequotas?resourceVersion=667": net/http: TLS handshake timeout
I0322 04:05:17.606070    2297 trace.go:236] Trace[1635990559]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:03.028) (total time: 188972ms):
Trace[1635990559]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/pods?resourceVersion=708": dial tcp 127.0.0.1:6444: i/o timeout 183376ms (04:05:06.405)
Trace[1635990559]: [3m8.972888545s] [3m8.972888545s] END
E0322 04:05:38.520014    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://127.0.0.1:6444/api/v1/pods?resourceVersion=708": dial tcp 127.0.0.1:6444: i/o timeout
I0322 04:05:30.589490    2297 trace.go:236] Trace[510290405]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:40.901) (total time: 158031ms):
Trace[510290405]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/node.k8s.io/v1/runtimeclasses?resourceVersion=668": net/http: TLS handshake timeout 146557ms (04:05:07.458)
Trace[510290405]: [2m38.031036855s] [2m38.031036855s] END
E0322 04:05:38.520219    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: Get "https://127.0.0.1:6444/apis/node.k8s.io/v1/runtimeclasses?resourceVersion=668": net/http: TLS handshake timeout
W0322 04:05:25.424675    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.ConfigMap: Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=665": net/http: TLS handshake timeout
I0322 04:05:38.521169    2297 trace.go:236] Trace[947729109]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:21.497) (total time: 197022ms):
Trace[947729109]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=665": net/http: TLS handshake timeout 177752ms (04:05:19.250)
Trace[947729109]: [3m17.022967402s] [3m17.022967402s] END
E0322 04:05:38.521231    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=665": net/http: TLS handshake timeout
I0322 04:05:38.529461    2297 log.go:245] http: TLS handshake error from 10.42.0.5:54738: EOF
I0322 04:05:38.529657    2297 log.go:245] http: TLS handshake error from 10.42.0.5:38854: write tcp 192.168.56.110:10250->10.42.0.5:38854: write: broken pipe
I0322 04:05:38.529733    2297 log.go:245] http: TLS handshake error from 10.42.0.5:36486: EOF
I0322 04:05:38.529849    2297 log.go:245] http: TLS handshake error from 10.42.0.5:34312: EOF
W0322 04:05:32.690944    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PersistentVolume: Get "https://127.0.0.1:6444/api/v1/persistentvolumes?resourceVersion=667": net/http: TLS handshake timeout
I0322 04:05:38.530568    2297 trace.go:236] Trace[2075480625]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:03:15.503) (total time: 143027ms):
Trace[2075480625]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/persistentvolumes?resourceVersion=667": net/http: TLS handshake timeout 137184ms (04:05:32.687)
Trace[2075480625]: [2m23.027517702s] [2m23.027517702s] END
E0322 04:05:38.530645    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get "https://127.0.0.1:6444/api/v1/persistentvolumes?resourceVersion=667": net/http: TLS handshake timeout
W0322 04:05:32.691015    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://127.0.0.1:6443/api/v1/services?resourceVersion=665": net/http: TLS handshake timeout
I0322 04:05:38.531130    2297 trace.go:236] Trace[797648217]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:40.737) (total time: 177793ms):
Trace[797648217]: ---"Objects listed" error:Get "https://127.0.0.1:6443/api/v1/services?resourceVersion=665": net/http: TLS handshake timeout 171949ms (04:05:32.687)
Trace[797648217]: [2m57.793293925s] [2m57.793293925s] END
E0322 04:05:38.531210    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://127.0.0.1:6443/api/v1/services?resourceVersion=665": net/http: TLS handshake timeout
W0322 04:05:26.180304    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1beta3.PriorityLevelConfiguration: Get "https://127.0.0.1:6444/apis/flowcontrol.apiserver.k8s.io/v1beta3/prioritylevelconfigurations?resourceVersion=668": net/http: TLS handshake timeout
I0322 04:05:38.531960    2297 trace.go:236] Trace[2083142676]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:41.242) (total time: 177289ms):
Trace[2083142676]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/flowcontrol.apiserver.k8s.io/v1beta3/prioritylevelconfigurations?resourceVersion=668": net/http: TLS handshake timeout 159722ms (04:05:20.964)
Trace[2083142676]: [2m57.289904915s] [2m57.289904915s] END
E0322 04:05:38.532085    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1beta3.PriorityLevelConfiguration: failed to list *v1beta3.PriorityLevelConfiguration: Get "https://127.0.0.1:6444/apis/flowcontrol.apiserver.k8s.io/v1beta3/prioritylevelconfigurations?resourceVersion=668": net/http: TLS handshake timeout
W0322 04:05:33.320460    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Ingress: Get "https://127.0.0.1:6444/apis/networking.k8s.io/v1/ingresses?resourceVersion=668": net/http: TLS handshake timeout
I0322 04:05:38.532942    2297 trace.go:236] Trace[651332699]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:41.236) (total time: 177296ms):
Trace[651332699]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/networking.k8s.io/v1/ingresses?resourceVersion=668": net/http: TLS handshake timeout 166090ms (04:05:27.326)
Trace[651332699]: [2m57.296686864s] [2m57.296686864s] END
E0322 04:05:38.533026    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Ingress: failed to list *v1.Ingress: Get "https://127.0.0.1:6444/apis/networking.k8s.io/v1/ingresses?resourceVersion=668": net/http: TLS handshake timeout
W0322 04:05:34.001332    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Job: Get "https://127.0.0.1:6444/apis/batch/v1/jobs?resourceVersion=665": net/http: TLS handshake timeout
I0322 04:05:38.533155    2297 trace.go:236] Trace[1628722150]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:38.725) (total time: 179807ms):
Trace[1628722150]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/batch/v1/jobs?resourceVersion=665": net/http: TLS handshake timeout 169213ms (04:05:27.938)
Trace[1628722150]: [2m59.807483614s] [2m59.807483614s] END
E0322 04:05:38.533312    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Job: failed to list *v1.Job: Get "https://127.0.0.1:6444/apis/batch/v1/jobs?resourceVersion=665": net/http: TLS handshake timeout
E0322 04:05:34.918783    2297 kubelet_node_status.go:540] "Error updating node status, will retry" err="error getting node \"server\": Get \"https://127.0.0.1:6443/api/v1/nodes/server?timeout=10s\": context deadline exceeded"
W0322 04:05:30.346584    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.containo.us/v1alpha1/ingressrouteudps?resourceVersion=668": net/http: TLS handshake timeout
W0322 04:05:35.097443    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.VolumeAttachment: Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/volumeattachments?resourceVersion=667": net/http: TLS handshake timeout
W0322 04:05:33.840758    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.RuntimeClass: Get "https://127.0.0.1:6443/apis/node.k8s.io/v1/runtimeclasses?resourceVersion=668": net/http: TLS handshake timeout
W0322 04:05:40.457269    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Node: Get "https://127.0.0.1:6443/api/v1/nodes?fieldSelector=metadata.name%3Dserver&resourceVersion=679": net/http: TLS handshake timeout
I0322 04:05:40.636754    2297 trace.go:236] Trace[2125495381]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:39.676) (total time: 163108ms):
Trace[2125495381]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/traefikservices?resourceVersion=665": net/http: TLS handshake timeout 163100ms (04:05:22.776)
Trace[2125495381]: [2m43.108485432s] [2m43.108485432s] END
W0322 04:05:34.593971    2297 reflector.go:535] object-"kube-system"/"coredns": failed to list *v1.ConfigMap: Get "https://127.0.0.1:6443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dcoredns&resourceVersion=665": net/http: TLS handshake timeout
I0322 04:05:34.983514    2297 trace.go:236] Trace[1534754688]: "iptables ChainExists" (22-Mar-2024 04:05:24.226) (total time: 4654ms):
Trace[1534754688]: [4.654404251s] [4.654404251s] END
I0322 04:05:42.611331    2297 trace.go:236] Trace[1280212050]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:56.006) (total time: 160011ms):
Trace[1280212050]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/replicationcontrollers?resourceVersion=667": net/http: TLS handshake timeout 149222ms (04:05:25.228)
Trace[1280212050]: [2m40.011794174s] [2m40.011794174s] END
W0322 04:05:36.345075    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Secret: Get "https://127.0.0.1:6444/api/v1/secrets?resourceVersion=675": net/http: TLS handshake timeout
I0322 04:05:44.104408    2297 trace.go:236] Trace[1129227529]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:39.867) (total time: 177466ms):
Trace[1129227529]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/serverstransporttcps?resourceVersion=679": net/http: TLS handshake timeout 158022ms (04:05:17.890)
Trace[1129227529]: [2m57.466819453s] [2m57.466819453s] END
E0322 04:05:44.405496    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Role: failed to list *v1.Role: Get "https://127.0.0.1:6444/apis/rbac.authorization.k8s.io/v1/roles?resourceVersion=667": net/http: TLS handshake timeout
I0322 04:05:37.502660    2297 trace.go:236] Trace[1796269034]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:01:58.303) (total time: 219183ms):
Trace[1796269034]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=665": net/http: TLS handshake timeout 194342ms (04:05:12.645)
Trace[1796269034]: [3m39.183864598s] [3m39.183864598s] END
E0322 04:05:41.167680    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://127.0.0.1:6443/api/v1/nodes?resourceVersion=679": net/http: TLS handshake timeout
W0322 04:05:42.522157    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.CSIDriver: Get "https://127.0.0.1:6443/apis/storage.k8s.io/v1/csidrivers?resourceVersion=665": net/http: TLS handshake timeout
time="2024-03-22T04:05:42Z" level=error msg="error while range on /registry/volumeattachments/ /registry/volumeattachments/: context canceled"
I0322 04:05:44.444271    2297 trace.go:236] Trace[2026529377]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:34.931) (total time: 183469ms):
Trace[2026529377]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/traefik.containo.us/v1alpha1/serverstransports?resourceVersion=671": net/http: TLS handshake timeout 165659ms (04:05:20.591)
Trace[2026529377]: [3m3.469529984s] [3m3.469529984s] END
time="2024-03-22T04:05:43Z" level=info msg="Slow SQL (started: 2024-03-22 04:05:20.389882103 +0000 UTC m=+696.126848296) (total time: 15.913130218s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC LIMIT 500 : [[% 757]]"
E0322 04:05:44.447179    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/serverstransports?resourceVersion=671": net/http: TLS handshake timeout
I0322 04:05:44.729035    2297 trace.go:236] Trace[357753171]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:03:03.098) (total time: 155481ms):
Trace[357753171]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/traefik.containo.us/v1alpha1/ingressrouteudps?resourceVersion=668": net/http: TLS handshake timeout 147227ms (04:05:30.325)
Trace[357753171]: [2m35.481337128s] [2m35.481337128s] END
time="2024-03-22T04:05:46Z" level=info msg="Slow SQL (started: 2024-03-22 04:05:23.818623362 +0000 UTC m=+699.555589549) (total time: 22.990125432s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/clusterroles/% false]]"
I0322 04:05:44.811521    2297 trace.go:236] Trace[1289725895]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:03:35.569) (total time: 129235ms):
Trace[1289725895]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/volumeattachments?resourceVersion=667": net/http: TLS handshake timeout 119522ms (04:05:35.091)
Trace[1289725895]: [2m9.235406958s] [2m9.235406958s] END
I0322 04:05:44.877695    2297 trace.go:236] Trace[973784723]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:01:58.760) (total time: 219028ms):
Trace[973784723]: ---"Objects listed" error:Get "https://127.0.0.1:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dserver&resourceVersion=708": net/http: TLS handshake timeout 201875ms (04:05:20.635)
Trace[973784723]: [3m39.028250449s] [3m39.028250449s] END
E0322 04:05:45.084817    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=665": dial tcp 127.0.0.1:6444: i/o timeout
W0322 04:05:45.158340    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.containo.us/v1alpha1/tlsstores?resourceVersion=669": net/http: TLS handshake timeout
I0322 04:05:45.261606    2297 trace.go:236] Trace[374819345]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:24.744) (total time: 193188ms):
Trace[374819345]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/limitranges?resourceVersion=665": net/http: TLS handshake timeout 176510ms (04:05:21.254)
Trace[374819345]: [3m13.188404942s] [3m13.188404942s] END
I0322 04:05:45.416550    2297 trace.go:236] Trace[623182863]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:43.781) (total time: 174282ms):
Trace[623182863]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/traefik.containo.us/v1alpha1/middlewaretcps?resourceVersion=671": net/http: TLS handshake timeout 157514ms (04:05:21.296)
Trace[623182863]: [2m54.282906999s] [2m54.282906999s] END
I0322 04:05:45.445948    2297 trace.go:236] Trace[286152552]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:03.354) (total time: 214767ms):
Trace[286152552]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/csinodes?resourceVersion=666": net/http: TLS handshake timeout 197941ms (04:05:21.295)
Trace[286152552]: [3m34.767681822s] [3m34.767681822s] END
E0322 04:05:45.609881    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/ingressrouteudps?resourceVersion=671": net/http: TLS handshake timeout
E0322 04:05:45.672861    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://127.0.0.1:6443/api/v1/namespaces?resourceVersion=667": net/http: TLS handshake timeout
I0322 04:05:45.676948    2297 trace.go:236] Trace[42510835]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:03:46.109) (total time: 112195ms):
Trace[42510835]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/certificates.k8s.io/v1/certificatesigningrequests?resourceVersion=667": net/http: TLS handshake timeout 94336ms (04:05:20.446)
Trace[42510835]: [1m52.195652686s] [1m52.195652686s] END
E0322 04:05:46.751512    2297 server.go:310] "Unable to authenticate the request due to an error" err="context canceled"
I0322 04:05:47.159263    2297 trace.go:236] Trace[16006648]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:56.033) (total time: 164427ms):
Trace[16006648]: ---"Objects listed" error:Get "https://127.0.0.1:6443/api/v1/nodes?fieldSelector=metadata.name%3Dserver&resourceVersion=679": net/http: TLS handshake timeout 158227ms (04:05:34.260)
Trace[16006648]: [2m44.427590664s] [2m44.427590664s] END
E0322 04:05:47.546633    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/traefikservices?resourceVersion=665": net/http: TLS handshake timeout
I0322 04:05:51.783690    2297 trace.go:236] Trace[1085221627]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:43.561) (total time: 179955ms):
Trace[1085221627]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/traefik.containo.us/v1alpha1/tlsoptions?resourceVersion=668": net/http: TLS handshake timeout 154322ms (04:05:17.884)
Trace[1085221627]: [2m59.955824818s] [2m59.955824818s] END
I0322 04:05:52.349136    2297 trace.go:236] Trace[795526631]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:01:36.674) (total time: 247707ms):
Trace[795526631]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=665": net/http: TLS handshake timeout 215911ms (04:05:12.585)
Trace[795526631]: [4m7.707236678s] [4m7.707236678s] END
I0322 04:05:52.639914    2297 trace.go:236] Trace[729226900]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:03:36.670) (total time: 121501ms):
Trace[729226900]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/storageclasses?resourceVersion=667": net/http: TLS handshake timeout 104627ms (04:05:21.297)
Trace[729226900]: [2m1.501904144s] [2m1.501904144s] END
E0322 04:05:52.690562    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://127.0.0.1:6444/api/v1/namespaces?resourceVersion=667": net/http: TLS handshake timeout
E0322 04:05:52.741485    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/tlsstores?resourceVersion=665": net/http: TLS handshake timeout
I0322 04:05:44.780241    2297 trace.go:236] Trace[1234641486]: "Reflector ListAndWatch" name:object-"kube-system"/"kube-root-ca.crt" (22-Mar-2024 04:01:58.665) (total time: 219065ms):
Trace[1234641486]: ---"Objects listed" error:Get "https://127.0.0.1:6443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dkube-root-ca.crt&resourceVersion=665": net/http: TLS handshake timeout 201861ms (04:05:20.526)
Trace[1234641486]: [3m39.065835994s] [3m39.065835994s] END
E0322 04:05:53.156732    2297 kubelet.go:2327] "Skipping pod synchronization" err="[container runtime is down, PLEG is not healthy: pleg was last seen active 4m59.35763579s ago; threshold is 3m0s]"
W0322 04:05:38.704073    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1beta3.FlowSchema: Get "https://127.0.0.1:6444/apis/flowcontrol.apiserver.k8s.io/v1beta3/flowschemas?resourceVersion=665": net/http: TLS handshake timeout
E0322 04:05:44.942023    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ValidatingWebhookConfiguration: failed to list *v1.ValidatingWebhookConfiguration: Get "https://127.0.0.1:6444/apis/admissionregistration.k8s.io/v1/validatingwebhookconfigurations?resourceVersion=665": net/http: TLS handshake timeout
I0322 04:05:45.206761    2297 trace.go:236] Trace[1351938486]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:57.173) (total time: 168010ms):
Trace[1351938486]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/middlewaretcps?resourceVersion=669": net/http: TLS handshake timeout 144069ms (04:05:21.243)
Trace[1351938486]: [2m48.01084158s] [2m48.01084158s] END
E0322 04:05:53.651382    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/middlewaretcps?resourceVersion=669": net/http: TLS handshake timeout
E0322 04:05:53.841840    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.NetworkPolicy: failed to list *v1.NetworkPolicy: Get "https://127.0.0.1:6444/apis/networking.k8s.io/v1/networkpolicies?resourceVersion=666": net/http: TLS handshake timeout
I0322 04:05:53.901952    2297 trace.go:236] Trace[676924441]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:56.288) (total time: 161748ms):
Trace[676924441]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/helm.cattle.io/v1/helmchartconfigs?resourceVersion=756": net/http: TLS handshake timeout 145007ms (04:05:21.296)
Trace[676924441]: [2m41.748591259s] [2m41.748591259s] END
W0322 04:05:53.939817    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.ConfigMap: Get "https://127.0.0.1:6444/api/v1/configmaps?resourceVersion=665": net/http: TLS handshake timeout
I0322 04:05:54.373567    2297 trace.go:236] Trace[1365600428]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:41.150) (total time: 178457ms):
Trace[1365600428]: ---"Objects listed" error:Get "https://127.0.0.1:6443/apis/node.k8s.io/v1/runtimeclasses?resourceVersion=668": net/http: TLS handshake timeout 172684ms (04:05:33.834)
Trace[1365600428]: [2m58.457063407s] [2m58.457063407s] END
I0322 04:05:54.669491    2297 request.go:697] Waited for 7.779026069s due to client-side throttling, not priority and fairness, request: GET:https://127.0.0.1:6444/apis/traefik.containo.us/v1alpha1/ingressroutetcps?resourceVersion=668
E0322 04:05:55.617275    2297 health_controller.go:162] Metrics Controller heartbeat missed
W0322 04:05:55.773287    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Node: Get "https://127.0.0.1:6444/api/v1/nodes?resourceVersion=679": net/http: TLS handshake timeout
E0322 04:05:55.874300    2297 server.go:310] "Unable to authenticate the request due to an error" err="context canceled"
I0322 04:05:56.033057    2297 trace.go:236] Trace[316542271]: "Calculate volume metrics of tmp-dir for pod kube-system/metrics-server-67c658944b-6v69k" (22-Mar-2024 04:05:35.720) (total time: 6255ms):
Trace[316542271]: [6.255402985s] [6.255402985s] END
I0322 04:05:57.122586    2297 trace.go:236] Trace[1431314063]: "iptables ChainExists" (22-Mar-2024 04:05:24.190) (total time: 25166ms):
Trace[1431314063]: [25.166788293s] [25.166788293s] END
I0322 04:05:57.176439    2297 trace.go:236] Trace[857280342]: "Reflector ListAndWatch" name:object-"kube-system"/"coredns" (22-Mar-2024 04:02:15.685) (total time: 212657ms):
Trace[857280342]: ---"Objects listed" error:Get "https://127.0.0.1:6443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dcoredns&resourceVersion=665": net/http: TLS handshake timeout 194092ms (04:05:29.778)
Trace[857280342]: [3m32.657577161s] [3m32.657577161s] END
E0322 04:05:57.194260    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get "https://127.0.0.1:6444/api/v1/replicationcontrollers?resourceVersion=667": net/http: TLS handshake timeout
E0322 04:05:57.261803    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/serverstransporttcps?resourceVersion=679": net/http: TLS handshake timeout
E0322 04:05:58.240111    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.containo.us/v1alpha1/ingressrouteudps?resourceVersion=668": net/http: TLS handshake timeout
E0322 04:05:58.494863    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/volumeattachments?resourceVersion=667": net/http: TLS handshake timeout
I0322 04:05:58.669739    2297 trace.go:236] Trace[1819875448]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:03:14.919) (total time: 163744ms):
Trace[1819875448]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/traefik.containo.us/v1alpha1/tlsstores?resourceVersion=669": net/http: TLS handshake timeout 143909ms (04:05:38.829)
Trace[1819875448]: [2m43.744721849s] [2m43.744721849s] END
E0322 04:05:58.702030    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.LimitRange: failed to list *v1.LimitRange: Get "https://127.0.0.1:6444/api/v1/limitranges?resourceVersion=665": net/http: TLS handshake timeout
E0322 04:05:58.948932    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://127.0.0.1:6443/api/v1/nodes?fieldSelector=metadata.name%3Dserver&resourceVersion=679": net/http: TLS handshake timeout
E0322 04:05:59.379894    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:05:59.936229    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.containo.us/v1alpha1/tlsoptions?resourceVersion=668": net/http: TLS handshake timeout
E0322 04:06:00.331019    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/storageclasses?resourceVersion=667": net/http: TLS handshake timeout
time="2024-03-22T04:06:00Z" level=info msg="Slow SQL (started: 2024-03-22 04:04:54.953502576 +0000 UTC m=+670.690468764) (total time: 49.664266164s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/services/specs/% false]]"
time="2024-03-22T04:06:00Z" level=error msg="error while range on /registry/services/specs/ /registry/services/specs/: context canceled"
W0322 04:05:52.109371    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PersistentVolumeClaim: Get "https://127.0.0.1:6444/api/v1/persistentvolumeclaims?resourceVersion=665": net/http: TLS handshake timeout
W0322 04:06:00.568071    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Node: Get "https://127.0.0.1:6444/api/v1/nodes?resourceVersion=679": net/http: TLS handshake timeout
I0322 04:06:00.637637    2297 trace.go:236] Trace[207811191]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:37.809) (total time: 194425ms):
Trace[207811191]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/batch/v1/cronjobs?resourceVersion=665": net/http: TLS handshake timeout 160411ms (04:05:18.220)
Trace[207811191]: [3m14.425192125s] [3m14.425192125s] END
I0322 04:06:00.639847    2297 trace.go:236] Trace[1132849645]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:01:55.004) (total time: 223252ms):
Trace[1132849645]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/rbac.authorization.k8s.io/v1/clusterrolebindings?resourceVersion=667": dial tcp 127.0.0.1:6444: i/o timeout 200698ms (04:05:15.702)
Trace[1132849645]: [3m43.252459389s] [3m43.252459389s] END
E0322 04:06:00.891265    2297 reflector.go:147] object-"kube-system"/"kube-root-ca.crt": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://127.0.0.1:6443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dkube-root-ca.crt&resourceVersion=665": net/http: TLS handshake timeout
time="2024-03-22T04:06:01Z" level=info msg="Slow SQL (started: 2024-03-22 04:05:14.195835743 +0000 UTC m=+689.932801930) (total time: 39.172621839s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/configmaps/% false]]"
W0322 04:06:02.227654    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.ConfigMap: Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=665": net/http: TLS handshake timeout
E0322 04:06:02.885793    2297 kubelet.go:2327] "Skipping pod synchronization" err="[container runtime is down, PLEG is not healthy: pleg was last seen active 5m16.150776908s ago; threshold is 3m0s]"
W0322 04:05:54.594315    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.ConfigMap: Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=665": net/http: TLS handshake timeout
E0322 04:06:04.324089    2297 health_controller.go:162] Metrics Controller heartbeat missed
time="2024-03-22T04:06:10Z" level=info msg="Slow SQL (started: 2024-03-22 04:05:26.275081442 +0000 UTC m=+702.012047666) (total time: 20.045368781s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? AND mkv.id <= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC LIMIT 1000 : [[/registry/horizontalpodautoscalers/% 757 false]]"
E0322 04:06:05.443141    2297 handler_proxy.go:137] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
E0322 04:06:14.496610    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.containo.us/v1alpha1/serverstransports?resourceVersion=671": net/http: TLS handshake timeout
I0322 04:06:15.175111    2297 garbagecollector.go:818] "failed to discover preferred resources" error="Get \"https://127.0.0.1:6444/api\": net/http: TLS handshake timeout"
E0322 04:06:07.440863    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.containo.us/v1alpha1/middlewaretcps?resourceVersion=671": net/http: TLS handshake timeout
E0322 04:06:05.588699    2297 reflector.go:147] object-"kube-system"/"coredns": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://127.0.0.1:6443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dcoredns&resourceVersion=665": net/http: TLS handshake timeout
I0322 04:05:57.206084    2297 trace.go:236] Trace[1332336007]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:03:02.520) (total time: 174676ms):
Trace[1332336007]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/secrets?resourceVersion=675": net/http: TLS handshake timeout 147154ms (04:05:29.674)
Trace[1332336007]: [2m54.676832581s] [2m54.676832581s] END
time="2024-03-22T04:06:07Z" level=info msg="Slow SQL (started: 2024-03-22 04:05:38.995745134 +0000 UTC m=+714.732711314) (total time: 19.79966681s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/secrets/% false]]"
I0322 04:06:05.627185    2297 trace.go:236] Trace[225232486]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:40.305) (total time: 197423ms):
Trace[225232486]: ---"Objects listed" error:Get "https://127.0.0.1:6443/apis/storage.k8s.io/v1/csidrivers?resourceVersion=665": net/http: TLS handshake timeout 176333ms (04:05:36.638)
Trace[225232486]: [3m17.423196342s] [3m17.423196342s] END
E0322 04:06:05.794521    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=665": net/http: TLS handshake timeout
E0322 04:06:07.273010    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://127.0.0.1:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dserver&resourceVersion=708": net/http: TLS handshake timeout
W0322 04:06:07.275228    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.ConfigMap: Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=665": net/http: TLS handshake timeout
E0322 04:05:58.786209    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/csinodes?resourceVersion=666": net/http: TLS handshake timeout
E0322 04:06:07.757197    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.CertificateSigningRequest: failed to list *v1.CertificateSigningRequest: Get "https://127.0.0.1:6444/apis/certificates.k8s.io/v1/certificatesigningrequests?resourceVersion=667": net/http: TLS handshake timeout
W0322 04:06:01.255191    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.StorageClass: Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/storageclasses?resourceVersion=667": net/http: TLS handshake timeout
W0322 04:06:08.544816    2297 handler_proxy.go:93] no RequestInfo found in the context
I0322 04:06:08.914389    2297 trace.go:236] Trace[942687376]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:03:26.772) (total time: 153796ms):
Trace[942687376]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/nodes?resourceVersion=679": net/http: TLS handshake timeout 153786ms (04:06:00.559)
Trace[942687376]: [2m33.796996588s] [2m33.796996588s] END
E0322 04:06:09.020162    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ClusterRoleBinding: failed to list *v1.ClusterRoleBinding: Get "https://127.0.0.1:6444/apis/rbac.authorization.k8s.io/v1/clusterrolebindings?resourceVersion=667": dial tcp 127.0.0.1:6444: i/o timeout
I0322 04:06:09.027178    2297 trace.go:236] Trace[54272632]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:40.800) (total time: 200549ms):
Trace[54272632]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/flowcontrol.apiserver.k8s.io/v1beta3/flowschemas?resourceVersion=665": net/http: TLS handshake timeout 172642ms (04:05:33.443)
Trace[54272632]: [3m20.549083185s] [3m20.549083185s] END
W0322 04:06:01.451150    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Endpoints: Get "https://127.0.0.1:6443/api/v1/namespaces/default/endpoints?fieldSelector=metadata.name%3Dkubernetes&resourceVersion=667": net/http: TLS handshake timeout
E0322 04:06:09.184188    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 04:06:10.431711    2297 trace.go:236] Trace[1753129487]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:09.005) (total time: 53225ms):
Trace[1753129487]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=665": net/http: TLS handshake timeout 45205ms (04:05:54.210)
Trace[1753129487]: [53.225174555s] [53.225174555s] END
E0322 04:06:10.487155    2297 compact.go:124] etcd: endpoint ([unix://kine.sock]) compact failed: rpc error: code = Unavailable desc = keepalive ping failed to receive ACK within timeout
E0322 04:06:10.885126    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: Get "https://127.0.0.1:6443/apis/node.k8s.io/v1/runtimeclasses?resourceVersion=668": net/http: TLS handshake timeout
E0322 04:06:11.374032    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/helm.cattle.io/v1/helmchartconfigs?resourceVersion=756": net/http: TLS handshake timeout
I0322 04:06:11.383143    2297 trace.go:236] Trace[1469087417]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:41.372) (total time: 192573ms):
Trace[1469087417]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/configmaps?resourceVersion=665": net/http: TLS handshake timeout 185114ms (04:05:46.486)
Trace[1469087417]: [3m12.57397854s] [3m12.57397854s] END
I0322 04:06:11.731209    2297 trace.go:236] Trace[161535231]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:04:56.774) (total time: 66167ms):
Trace[161535231]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=665": net/http: TLS handshake timeout 57810ms (04:05:54.585)
Trace[161535231]: [1m6.167469231s] [1m6.167469231s] END
E0322 04:06:16.788490    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=665": net/http: TLS handshake timeout
I0322 04:06:13.226916    2297 request.go:697] Waited for 8.686952882s due to client-side throttling, not priority and fairness, request: GET:https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=665
E0322 04:06:16.815922    2297 node_lifecycle_controller.go:713] "Failed while getting a Node to retry updating node health. Probably Node was deleted" node="server"
E0322 04:06:04.337578    2297 health_controller.go:162] Metrics Controller heartbeat missed
E0322 04:06:16.831071    2297 health_controller.go:162] Metrics Controller heartbeat missed
E0322 04:06:16.831823    2297 health_controller.go:162] Metrics Controller heartbeat missed
E0322 04:06:16.832518    2297 node_lifecycle_controller.go:718] "Update health of Node from Controller error, Skipping - no pods will be evicted" err="Get \"https://127.0.0.1:6444/api/v1/nodes/server\": net/http: TLS handshake timeout" node=""
E0322 04:06:17.681846    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.CronJob: failed to list *v1.CronJob: Get "https://127.0.0.1:6444/apis/batch/v1/cronjobs?resourceVersion=665": net/http: TLS handshake timeout
time="2024-03-22T04:06:22Z" level=error msg="error while range on /registry/traefik.io/serverstransports/ /registry/traefik.io/serverstransports/: context canceled"
time="2024-03-22T04:06:22Z" level=info msg="Slow SQL (started: 2024-03-22 04:05:56.006078952 +0000 UTC m=+731.743045170) (total time: 7.621025474s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/traefik.io/tlsoptions/% false]]"
E0322 04:06:18.222858    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=665": net/http: TLS handshake timeout
E0322 04:06:18.810267    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/apiextensions.k8s.io/v1/customresourcedefinitions?resourceVersion=665": net/http: TLS handshake timeout
W0322 04:06:19.025910    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.ReplicationController: Get "https://127.0.0.1:6444/api/v1/replicationcontrollers?resourceVersion=667": net/http: TLS handshake timeout
E0322 04:06:36.019246    2297 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, context deadline exceeded]"
I0322 04:06:18.744138    2297 trace.go:236] Trace[1876806976]: "iptables ChainExists" (22-Mar-2024 04:05:52.368) (total time: 17251ms):
Trace[1876806976]: [17.251963606s] [17.251963606s] END
W0322 04:06:02.493170    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PodTemplate: Get "https://127.0.0.1:6444/api/v1/podtemplates?resourceVersion=668": net/http: TLS handshake timeout
W0322 04:06:36.808006    2297 transport.go:301] Unable to cancel request for *otelhttp.Transport
E0322 04:06:23.263151    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:06:36.930649    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
I0322 04:06:24.792696    2297 trace.go:236] Trace[1368460911]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:04:57.382) (total time: 67147ms):
Trace[1368460911]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/nodes?resourceVersion=679": net/http: TLS handshake timeout 50253ms (04:05:47.636)
Trace[1368460911]: [1m7.147859131s] [1m7.147859131s] END
E0322 04:06:27.969576    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://127.0.0.1:6444/api/v1/nodes?resourceVersion=679": net/http: TLS handshake timeout
E0322 04:06:28.029164    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1beta3.FlowSchema: failed to list *v1beta3.FlowSchema: Get "https://127.0.0.1:6444/apis/flowcontrol.apiserver.k8s.io/v1beta3/flowschemas?resourceVersion=665": net/http: TLS handshake timeout
I0322 04:06:28.297010    2297 trace.go:236] Trace[527519523]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:00.807) (total time: 75810ms):
Trace[527519523]: ---"Objects listed" error:Get "https://127.0.0.1:6443/api/v1/namespaces/default/endpoints?fieldSelector=metadata.name%3Dkubernetes&resourceVersion=667": net/http: TLS handshake timeout 60636ms (04:06:01.443)
Trace[527519523]: [1m15.810686029s] [1m15.810686029s] END
E0322 04:06:15.390579    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
I0322 04:06:28.419506    2297 request.go:697] Waited for 8.650843683s due to client-side throttling, not priority and fairness, request: GET:https://127.0.0.1:6444/apis/discovery.k8s.io/v1/endpointslices?resourceVersion=715
E0322 04:06:28.646826    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://127.0.0.1:6444/api/v1/configmaps?resourceVersion=665": net/http: TLS handshake timeout
E0322 04:06:28.978290    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.containo.us/v1alpha1/tlsstores?resourceVersion=669": net/http: TLS handshake timeout
I0322 04:06:29.515737    2297 trace.go:236] Trace[1363106726]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:03:41.782) (total time: 147054ms):
Trace[1363106726]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/persistentvolumeclaims?resourceVersion=665": net/http: TLS handshake timeout 122426ms (04:05:44.208)
Trace[1363106726]: [2m27.054365347s] [2m27.054365347s] END
time="2024-03-22T04:06:24Z" level=error msg="error while range on /registry/certificatesigningrequests/ /registry/certificatesigningrequests/: context canceled"
W0322 04:06:38.647491    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://127.0.0.1:6444/api/v1/namespaces?resourceVersion=667": http2: client connection lost
E0322 04:06:27.530708    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://127.0.0.1:6443/apis/storage.k8s.io/v1/csidrivers?resourceVersion=665": net/http: TLS handshake timeout
W0322 04:06:41.681102    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Secret: Get "https://127.0.0.1:6444/api/v1/secrets?resourceVersion=675": http2: client connection lost
I0322 04:06:42.196155    2297 trace.go:236] Trace[300050977]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:04.590) (total time: 71843ms):
Trace[300050977]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/storageclasses?resourceVersion=667": net/http: TLS handshake timeout 56637ms (04:06:01.227)
Trace[300050977]: [1m11.843307271s] [1m11.843307271s] END
I0322 04:06:43.256355    2297 trace.go:236] Trace[1227087154]: "SerializeObject" audit-id:e52c76aa-747b-46c2-82fb-ee5df92b0631,method:POST,url:/api/v1/nodes,protocol:HTTP/2.0,mediaType:application/vnd.kubernetes.protobuf,encoder:{"encodeGV":"v1","encoder":"protobuf","name":"versioning"} (22-Mar-2024 04:05:06.763) (total time: 69917ms):
Trace[1227087154]: ---"About to start writing response" size:154 6464ms (04:05:13.227)
Trace[1227087154]: ---"About to start writing response" size:69 39676ms (04:05:52.908)
Trace[1227087154]: ---"Write call failed" writer:responsewriter.outerWithCloseNotifyAndFlush,size:69,firstWrite:false,err:http: Handler timeout 7801ms (04:06:00.710)
Trace[1227087154]: [1m9.91744081s] [1m9.91744081s] END
W0322 04:06:43.627511    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Pod: Get "https://127.0.0.1:6444/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&resourceVersion=708": net/http: TLS handshake timeout
I0322 04:06:43.771051    2297 trace.go:236] Trace[2053172835]: "Reflector ListAndWatch" name:object-"kube-system"/"coredns-custom" (22-Mar-2024 04:05:39.880) (total time: 51458ms):
Trace[2053172835]: [51.458775624s] [51.458775624s] END
E0322 04:06:29.353465    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Secret: failed to list *v1.Secret: Get "https://127.0.0.1:6444/api/v1/secrets?resourceVersion=675": net/http: TLS handshake timeout
I0322 04:06:45.519781    2297 request.go:697] Waited for 15.904776549s due to client-side throttling, not priority and fairness, request: GET:https://127.0.0.1:6444/version
I0322 04:06:46.664038    2297 request.go:697] Waited for 13.207384191s due to client-side throttling, not priority and fairness, request: GET:https://127.0.0.1:6444/api/v1/namespaces?resourceVersion=667
E0322 04:06:10.340841    2297 event.go:289] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"traefik-f4564c4f4-d2b8m.17befa2f9b5d2f8f", GenerateName:"", Namespace:"kube-system", SelfLink:"", UID:"", ResourceVersion:"726", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"traefik-f4564c4f4-d2b8m", UID:"071534ad-b847-4778-843c-690278292cda", APIVersion:"v1", ResourceVersion:"612", FieldPath:"spec.containers{traefik}"}, Reason:"Unhealthy", Message:"Readiness probe failed: Get \"http://10.42.0.8:9000/ping\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)", Source:v1.EventSource{Component:"kubelet", Host:"server"}, FirstTimestamp:time.Date(2024, time.March, 22, 3, 59, 50, 0, time.Local), LastTimestamp:time.Date(2024, time.March, 22, 4, 1, 1, 21098764, time.Local), Count:4, Type:"Warning", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"kubelet", ReportingInstance:"server"}': 'Patch "https://127.0.0.1:6443/api/v1/namespaces/kube-system/events/traefik-f4564c4f4-d2b8m.17befa2f9b5d2f8f": net/http: TLS handshake timeout'(may retry after sleeping)
E0322 04:07:01.109249    2297 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, context deadline exceeded]"
E0322 04:06:34.886720    2297 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, context deadline exceeded]"
E0322 04:07:01.802004    2297 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, context deadline exceeded]"
E0322 04:07:02.429609    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://127.0.0.1:6444/api/v1/nodes?resourceVersion=679": net/http: TLS handshake timeout
E0322 04:07:02.541008    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Endpoints: failed to list *v1.Endpoints: Get "https://127.0.0.1:6443/api/v1/namespaces/default/endpoints?fieldSelector=metadata.name%3Dkubernetes&resourceVersion=667": net/http: TLS handshake timeout
E0322 04:07:02.734174    2297 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, context deadline exceeded]"
E0322 04:07:02.849269    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get "https://127.0.0.1:6444/api/v1/persistentvolumeclaims?resourceVersion=665": net/http: TLS handshake timeout
time="2024-03-22T04:06:27Z" level=error msg="Failed to list /registry/k3s.cattle.io/addons/ for revision 756: context canceled"
time="2024-03-22T04:06:53Z" level=info msg="Slow SQL (started: 2024-03-22 04:05:34.302365321 +0000 UTC m=+710.039331502) (total time: 1m18.960561858s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/traefik.io/middlewaretcps/% false]]"
W0322 04:06:20.929163    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.ClusterRoleBinding: Get "https://127.0.0.1:6444/apis/rbac.authorization.k8s.io/v1/clusterrolebindings?resourceVersion=667": net/http: TLS handshake timeout
time="2024-03-22T04:06:37Z" level=info msg="Slow SQL (started: 2024-03-22 04:05:39.958631099 +0000 UTC m=+715.695597303) (total time: 22.995152233s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/horizontalpodautoscalers/% 756]]"
time="2024-03-22T04:06:47Z" level=error msg="error while range on /registry/traefik.containo.us/middlewaretcps/ /registry/traefik.containo.us/middlewaretcps/: context canceled"
time="2024-03-22T04:06:48Z" level=info msg="Slow SQL (started: 2024-03-22 04:05:39.292894776 +0000 UTC m=+715.029861011) (total time: 13.798260876s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? AND mkv.id <= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC LIMIT 1000 : [[/registry/traefik.containo.us/ingressroutes/% 757 false]]"
I0322 04:06:51.906663    2297 trace.go:236] Trace[1020740617]: "iptables ChainExists" (22-Mar-2024 04:05:48.340) (total time: 50012ms):
Trace[1020740617]: [50.012911143s] [50.012911143s] END
E0322 04:06:53.801935    2297 finisher.go:157] FinishRequest: post-timeout activity, waited for 5m11.157691673s, child goroutine has not returned yet
W0322 04:06:39.514845    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Node: Get "https://127.0.0.1:6444/api/v1/nodes?resourceVersion=679": net/http: TLS handshake timeout
W0322 04:06:39.586112    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.ClusterRole: Get "https://127.0.0.1:6444/apis/rbac.authorization.k8s.io/v1/clusterroles?resourceVersion=665": net/http: TLS handshake timeout
E0322 04:06:54.709890    2297 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, context canceled]"
I0322 04:06:54.861282    2297 trace.go:236] Trace[2139780800]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:42.205) (total time: 59479ms):
Trace[2139780800]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/secrets?resourceVersion=675": http2: client connection lost 59470ms (04:06:41.675)
Trace[2139780800]: [59.479447311s] [59.479447311s] END
E0322 04:07:03.955702    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Secret: failed to list *v1.Secret: Get "https://127.0.0.1:6444/api/v1/secrets?resourceVersion=675": http2: client connection lost
W0322 04:06:54.907378    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.RuntimeClass: Get "https://127.0.0.1:6444/apis/node.k8s.io/v1/runtimeclasses?resourceVersion=668": http2: client connection lost
E0322 04:06:41.840248    2297 health_controller.go:162] Metrics Controller heartbeat missed
E0322 04:07:03.969907    2297 health_controller.go:162] Metrics Controller heartbeat missed
E0322 04:07:03.973417    2297 health_controller.go:162] Metrics Controller heartbeat missed
I0322 04:06:56.207233    2297 trace.go:236] Trace[1025066967]: "Reflector ListAndWatch" name:object-"kube-system"/"kube-root-ca.crt" (22-Mar-2024 04:06:10.641) (total time: 20888ms):
Trace[1025066967]: [20.888900022s] [20.888900022s] END
I0322 04:06:59.330244    2297 request.go:697] Waited for 13.396077515s due to client-side throttling, not priority and fairness, request: GET:https://127.0.0.1:6444/api
E0322 04:07:04.250709    2297 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, context deadline exceeded]"
W0322 04:07:04.643028    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Endpoints: Get "https://127.0.0.1:6444/api/v1/endpoints?resourceVersion=713": http2: client connection lost
E0322 04:07:05.367630    2297 kubelet.go:2327] "Skipping pod synchronization" err="[container runtime is down, PLEG is not healthy: pleg was last seen active 5m33.610941712s ago; threshold is 3m0s]"
W0322 04:06:51.970883    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://127.0.0.1:6444/api/v1/services?resourceVersion=665": http2: client connection lost
I0322 04:07:09.069055    2297 trace.go:236] Trace[572834691]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:16.594) (total time: 239819ms):
Trace[572834691]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=665": net/http: TLS handshake timeout 222028ms (04:05:58.622)
Trace[572834691]: [3m59.819435605s] [3m59.819435605s] END
time="2024-03-22T04:07:09Z" level=info msg="Slow SQL (started: 2024-03-22 04:06:16.34155504 +0000 UTC m=+752.078521241) (total time: 26.67075236s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/k3s.cattle.io/addons/% false]]"
time="2024-03-22T04:07:09Z" level=error msg="error while range on /registry/k3s.cattle.io/addons/ /registry/k3s.cattle.io/addons/: context canceled"
E0322 04:07:09.477468    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/storageclasses?resourceVersion=667": net/http: TLS handshake timeout
W0322 04:07:09.660204    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Pod: Get "https://127.0.0.1:6443/api/v1/pods?resourceVersion=708": net/http: TLS handshake timeout
I0322 04:07:09.842107    2297 trace.go:236] Trace[1533279662]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:e52c76aa-747b-46c2-82fb-ee5df92b0631,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:POST (22-Mar-2024 04:02:07.610) (total time: 290968ms):
Trace[1533279662]: ---"limitedReadBody succeeded" len:1432 35975ms (04:02:43.585)
Trace[1533279662]: ---"Conversion done" 51081ms (04:03:34.685)
Trace[1533279662]: ---"About to store object in database" 29131ms (04:04:03.817)
Trace[1533279662]: ---"Write to database call failed" len:1432,err:Timeout: request did not complete within requested timeout - context deadline exceeded 36182ms (04:04:39.999)
Trace[1533279662]: [4m50.968323879s] [4m50.968323879s] END
time="2024-03-22T04:07:09Z" level=error msg="error while range on /registry/traefik.containo.us/tlsoptions/ /registry/traefik.containo.us/tlsoptions/: context canceled"
I0322 04:07:11.159112    2297 trace.go:236] Trace[23893779]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:02.461) (total time: 101170ms):
Trace[23893779]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&resourceVersion=708": net/http: TLS handshake timeout 86159ms (04:06:28.620)
Trace[23893779]: [1m41.170707373s] [1m41.170707373s] END
time="2024-03-22T04:07:11Z" level=info msg="Slow SQL (started: 2024-03-22 04:06:14.013161026 +0000 UTC m=+749.750127225) (total time: 45.353177651s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/traefik.io/middlewaretcps/% 756]]"
E0322 04:07:11.461616    2297 resource_quota_controller.go:440] failed to discover resources: Get "https://127.0.0.1:6444/api": dial tcp 127.0.0.1:6444: i/o timeout
W0322 04:07:12.956146    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.ValidatingWebhookConfiguration: Get "https://127.0.0.1:6444/apis/admissionregistration.k8s.io/v1/validatingwebhookconfigurations?resourceVersion=665": net/http: TLS handshake timeout
I0322 04:07:13.312668    2297 trace.go:236] Trace[1963993103]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:03:02.998) (total time: 225554ms):
Trace[1963993103]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/podtemplates?resourceVersion=668": net/http: TLS handshake timeout 170786ms (04:05:53.785)
Trace[1963993103]: [3m45.554505934s] [3m45.554505934s] END
I0322 04:07:02.333987    2297 trace.go:236] Trace[445171770]: "Reflector ListAndWatch" name:object-"kube-system"/"coredns" (22-Mar-2024 04:06:20.593) (total time: 13539ms):
Trace[445171770]: [13.539101017s] [13.539101017s] END
W0322 04:07:00.849082    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.CSINode: Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/csinodes?resourceVersion=666": net/http: TLS handshake timeout
I0322 04:07:13.740583    2297 trace.go:236] Trace[1659644611]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:55.876) (total time: 246546ms):
Trace[1659644611]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/replicationcontrollers?resourceVersion=667": net/http: TLS handshake timeout 185896ms (04:06:01.773)
Trace[1659644611]: [4m6.546687967s] [4m6.546687967s] END
E0322 04:07:14.113308    2297 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, context deadline exceeded]"
I0322 04:07:01.408351    2297 node_lifecycle_controller.go:1225] "Initializing eviction metric for zone" zone=""
I0322 04:07:15.525648    2297 trace.go:236] Trace[1113347089]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:03.842) (total time: 120052ms):
Trace[1113347089]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/rbac.authorization.k8s.io/v1/clusterroles?resourceVersion=665": net/http: TLS handshake timeout 70122ms (04:06:13.964)
Trace[1113347089]: [2m0.052883146s] [2m0.052883146s] END
I0322 04:07:15.755321    2297 trace.go:236] Trace[1887686853]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:03.885) (total time: 119876ms):
Trace[1887686853]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/nodes?resourceVersion=679": net/http: TLS handshake timeout 95620ms (04:06:39.505)
Trace[1887686853]: [1m59.876631107s] [1m59.876631107s] END
I0322 04:07:16.789111    2297 trace.go:236] Trace[598309751]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:00.615) (total time: 124030ms):
Trace[598309751]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/endpoints?resourceVersion=713": http2: client connection lost 124014ms (04:07:04.630)
Trace[598309751]: [2m4.03068363s] [2m4.03068363s] END
W0322 04:07:17.654438    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1beta3.FlowSchema: Get "https://127.0.0.1:6444/apis/flowcontrol.apiserver.k8s.io/v1beta3/flowschemas?resourceVersion=665": http2: client connection lost
W0322 04:07:18.929187    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PriorityClass: Get "https://127.0.0.1:6444/apis/scheduling.k8s.io/v1/priorityclasses?resourceVersion=665": http2: client connection lost
W0322 04:07:19.633778    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.ConfigMap: Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=665": dial tcp 127.0.0.1:6444: i/o timeout
W0322 04:07:19.647641    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.VolumeAttachment: Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/volumeattachments?resourceVersion=667": http2: client connection lost
I0322 04:07:28.836126    2297 trace.go:236] Trace[929879363]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:04.122) (total time: 144712ms):
Trace[929879363]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/volumeattachments?resourceVersion=667": http2: client connection lost 125994ms (04:07:10.117)
Trace[929879363]: [2m24.71296905s] [2m24.71296905s] END
time="2024-03-22T04:07:16Z" level=info msg="Slow SQL (started: 2024-03-22 04:05:57.586228579 +0000 UTC m=+733.323194798) (total time: 42.072897397s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/traefik.io/ingressrouteudps/% false]]"
time="2024-03-22T04:07:18Z" level=info msg="Slow SQL (started: 2024-03-22 04:06:51.631820072 +0000 UTC m=+787.368786270) (total time: 15.549832313s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/traefik.io/tlsstores/% false]]"
time="2024-03-22T04:07:18Z" level=info msg="Slow SQL (started: 2024-03-22 04:07:05.738289222 +0000 UTC m=+801.475255432) (total time: 12.970348536s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/limitranges/% false]]"
time="2024-03-22T04:07:25Z" level=info msg="Slow SQL (started: 2024-03-22 04:05:46.960050124 +0000 UTC m=+722.697016358) (total time: 1m0.046362977s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/traefik.containo.us/tlsstores/% false]]"
time="2024-03-22T04:07:25Z" level=info msg="Slow SQL (started: 2024-03-22 04:06:02.775142786 +0000 UTC m=+738.512108990) (total time: 59.420420627s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/endpointslices/% false]]"
time="2024-03-22T04:07:27Z" level=info msg="Slow SQL (started: 2024-03-22 04:05:33.684607618 +0000 UTC m=+709.421573800) (total time: 1m2.226465525s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC  : [[/registry/masterleases/192.168.56.110 false]]"
time="2024-03-22T04:07:28Z" level=info msg="Slow SQL (started: 2024-03-22 04:06:46.929442268 +0000 UTC m=+782.666408459) (total time: 27.021329758s):  SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key' : [[]]"
time="2024-03-22T04:07:41Z" level=error msg="error while range on /registry/traefik.io/middlewaretcps/ /registry/traefik.io/middlewaretcps/: context canceled"
E0322 04:07:19.760455    2297 health_controller.go:162] Metrics Controller heartbeat missed
E0322 04:07:42.645920    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:42.646291    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:42.646569    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get "https://127.0.0.1:6444/api/v1/replicationcontrollers?resourceVersion=667": net/http: TLS handshake timeout
E0322 04:07:42.648508    2297 health_controller.go:162] Metrics Controller heartbeat missed
E0322 04:07:42.648996    2297 health_controller.go:162] Metrics Controller heartbeat missed
time="2024-03-22T04:07:42Z" level=info msg="Slow SQL (started: 2024-03-22 04:05:55.027245303 +0000 UTC m=+730.764211494) (total time: 1m47.629439036s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/controllers/% false]]"
I0322 04:07:42.659600    2297 trace.go:236] Trace[457914841]: "Calculate volume metrics of data for pod kube-system/traefik-f4564c4f4-d2b8m" (22-Mar-2024 04:07:29.134) (total time: 13524ms):
Trace[457914841]: [13.524750756s] [13.524750756s] END
time="2024-03-22T04:07:42Z" level=info msg="Slow SQL (started: 2024-03-22 04:05:35.438017698 +0000 UTC m=+711.174983893) (total time: 1m52.248713373s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/storageclasses/% 756]]"
time="2024-03-22T04:07:42Z" level=error msg="error while range on /registry/persistentvolumes/ /registry/persistentvolumes/: context canceled"
time="2024-03-22T04:07:42Z" level=error msg="error while range on /registry/traefik.io/tlsstores/ /registry/traefik.io/tlsstores/: context canceled"
W0322 04:07:42.674063    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://127.0.0.1:6443/api/v1/services?labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&resourceVersion=665": net/http: TLS handshake timeout
I0322 04:07:42.674635    2297 trace.go:236] Trace[1495875917]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:26.562) (total time: 136111ms):
Trace[1495875917]: ---"Objects listed" error:Get "https://127.0.0.1:6443/api/v1/services?labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&resourceVersion=665": net/http: TLS handshake timeout 136111ms (04:07:42.674)
Trace[1495875917]: [2m16.111709441s] [2m16.111709441s] END
E0322 04:07:42.674784    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://127.0.0.1:6443/api/v1/services?labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&resourceVersion=665": net/http: TLS handshake timeout
time="2024-03-22T04:07:42Z" level=info msg="Slow SQL (started: 2024-03-22 04:05:34.764862161 +0000 UTC m=+710.501828347) (total time: 1m29.61486144s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/horizontalpodautoscalers/% false]]"
time="2024-03-22T04:07:42Z" level=error msg="error while range on /registry/limitranges/ /registry/limitranges/: context canceled"
time="2024-03-22T04:07:42Z" level=info msg="Slow SQL (started: 2024-03-22 04:07:05.577259233 +0000 UTC m=+801.314225445) (total time: 25.692634501s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? AND mkv.id <= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC LIMIT 1000 : [[/registry/traefik.io/tlsoptions/% 757 false]]"
E0322 04:07:42.701671    2297 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, context deadline exceeded]"
E0322 04:07:42.702008    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:42.702142    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
I0322 04:07:42.703941    2297 trace.go:236] Trace[1540856579]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:03.613) (total time: 301845ms):
Trace[1540856579]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/services?resourceVersion=665": http2: client connection lost 288352ms (04:06:51.966)
Trace[1540856579]: [5m1.845590456s] [5m1.845590456s] END
E0322 04:07:42.704760    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://127.0.0.1:6444/api/v1/services?resourceVersion=665": http2: client connection lost
E0322 04:07:42.704128    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:20.776734    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=665": net/http: TLS handshake timeout
W0322 04:07:09.412429    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.APIService: Get "https://127.0.0.1:6444/apis/apiregistration.k8s.io/v1/apiservices?resourceVersion=693": http2: client connection lost
I0322 04:07:42.705388    2297 trace.go:236] Trace[1964432689]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:02.612) (total time: 340093ms):
Trace[1964432689]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/apiregistration.k8s.io/v1/apiservices?resourceVersion=693": http2: client connection lost 306794ms (04:07:09.406)
Trace[1964432689]: [5m40.093245371s] [5m40.093245371s] END
E0322 04:07:42.705504    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.APIService: failed to list *v1.APIService: Get "https://127.0.0.1:6444/apis/apiregistration.k8s.io/v1/apiservices?resourceVersion=693": http2: client connection lost
E0322 04:07:10.002724    2297 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
W0322 04:07:07.077044    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.CustomResourceDefinition: Get "https://127.0.0.1:6444/apis/apiextensions.k8s.io/v1/customresourcedefinitions?resourceVersion=665": http2: client connection lost
I0322 04:07:42.706601    2297 trace.go:236] Trace[1074006811]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:01:58.002) (total time: 344703ms):
Trace[1074006811]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/apiextensions.k8s.io/v1/customresourcedefinitions?resourceVersion=665": http2: client connection lost 296503ms (04:06:54.506)
Trace[1074006811]: [5m44.70358655s] [5m44.70358655s] END
E0322 04:07:42.706703    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.CustomResourceDefinition: failed to list *v1.CustomResourceDefinition: Get "https://127.0.0.1:6444/apis/apiextensions.k8s.io/v1/customresourcedefinitions?resourceVersion=665": http2: client connection lost
E0322 04:07:42.708055    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:42.708241    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
I0322 04:07:42.710147    2297 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0322 04:07:42.712738    2297 log.go:245] http: TLS handshake error from 10.42.0.5:38558: EOF
W0322 04:07:42.713577    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.RoleBinding: Get "https://127.0.0.1:6444/apis/rbac.authorization.k8s.io/v1/rolebindings?resourceVersion=668": http2: client connection lost
I0322 04:07:42.716489    2297 trace.go:236] Trace[370775996]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:04:56.347) (total time: 166368ms):
Trace[370775996]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/rbac.authorization.k8s.io/v1/rolebindings?resourceVersion=668": http2: client connection lost 166365ms (04:07:42.713)
Trace[370775996]: [2m46.368875544s] [2m46.368875544s] END
E0322 04:07:42.716584    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.RoleBinding: failed to list *v1.RoleBinding: Get "https://127.0.0.1:6444/apis/rbac.authorization.k8s.io/v1/rolebindings?resourceVersion=668": http2: client connection lost
E0322 04:07:42.719312    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
E0322 04:07:42.719610    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:42.720153    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
E0322 04:07:42.720945    2297 server.go:310] "Unable to authenticate the request due to an error" err="context canceled"
E0322 04:07:42.726382    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:42.726662    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:42.727809    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:42.728145    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:42.728403    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:42.730239    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:42.730425    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:42.731186    2297 timeout.go:142] post-timeout activity - time-elapsed: 246.177406ms, GET "/apis/networking.k8s.io/v1/ingressclasses" result: <nil>
E0322 04:07:42.731675    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:42.752187    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:43.054001    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:42.714000    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:42.771174    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
W0322 04:07:14.588068    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.StatefulSet: Get "https://127.0.0.1:6444/apis/apps/v1/statefulsets?resourceVersion=667": net/http: TLS handshake timeout
I0322 04:07:43.054067    2297 trace.go:236] Trace[1934128957]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:15.524) (total time: 147529ms):
Trace[1934128957]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/apps/v1/statefulsets?resourceVersion=667": net/http: TLS handshake timeout 105815ms (04:07:01.339)
Trace[1934128957]: [2m27.529336718s] [2m27.529336718s] END
E0322 04:07:43.054084    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://127.0.0.1:6444/apis/apps/v1/statefulsets?resourceVersion=667": net/http: TLS handshake timeout
W0322 04:07:43.055784    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://127.0.0.1:6443/apis/discovery.k8s.io/v1/endpointslices?labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&resourceVersion=715": dial tcp 127.0.0.1:6443: i/o timeout
I0322 04:07:43.055899    2297 trace.go:236] Trace[302109267]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:24.243) (total time: 138812ms):
Trace[302109267]: ---"Objects listed" error:Get "https://127.0.0.1:6443/apis/discovery.k8s.io/v1/endpointslices?labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&resourceVersion=715": dial tcp 127.0.0.1:6443: i/o timeout 138812ms (04:07:43.055)
Trace[302109267]: [2m18.812461729s] [2m18.812461729s] END
E0322 04:07:43.055913    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://127.0.0.1:6443/apis/discovery.k8s.io/v1/endpointslices?labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&resourceVersion=715": dial tcp 127.0.0.1:6443: i/o timeout
E0322 04:07:43.056531    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
W0322 04:07:41.564379    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.NetworkPolicy: Get "https://127.0.0.1:6443/apis/networking.k8s.io/v1/networkpolicies?resourceVersion=666": net/http: TLS handshake timeout
I0322 04:07:43.056772    2297 trace.go:236] Trace[1984156914]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:14.123) (total time: 148932ms):
Trace[1984156914]: ---"Objects listed" error:Get "https://127.0.0.1:6443/apis/networking.k8s.io/v1/networkpolicies?resourceVersion=666": net/http: TLS handshake timeout 147438ms (04:07:41.562)
Trace[1984156914]: [2m28.932923654s] [2m28.932923654s] END
E0322 04:07:43.056784    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.NetworkPolicy: failed to list *v1.NetworkPolicy: Get "https://127.0.0.1:6443/apis/networking.k8s.io/v1/networkpolicies?resourceVersion=666": net/http: TLS handshake timeout
E0322 04:07:43.057376    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.058097    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.058522    2297 timeout.go:142] post-timeout activity - time-elapsed: 3.00644ms, GET "/apis/traefik.containo.us/v1alpha1/ingressroutetcps" result: <nil>
E0322 04:07:41.829531    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.059342    2297 finisher.go:175] FinishRequest: post-timeout activity - time-elapsed: 3m3.055412201s, panicked: false, err: context deadline exceeded, panic-reason: <nil>
I0322 04:07:43.061941    2297 trace.go:236] Trace[363104743]: "SerializeObject" audit-id:da83444e-0c06-4507-bb73-4636157d4c7f,method:GET,url:/apis/node.k8s.io/v1/runtimeclasses,protocol:HTTP/2.0,mediaType:application/vnd.kubernetes.protobuf,encoder:{"encodeGV":"node.k8s.io/v1","encoder":"protobuf","name":"versioning"} (22-Mar-2024 04:05:48.587) (total time: 114474ms):
Trace[363104743]: ---"About to start writing response" size:11186 15122ms (04:06:03.709)
Trace[363104743]: ---"Write call failed" writer:responsewriter.outerWithCloseNotifyAndFlush,size:11186,firstWrite:true,err:http: Handler timeout 8417ms (04:06:12.127)
Trace[363104743]: ---"About to start writing response" size:69 49797ms (04:07:01.924)
Trace[363104743]: [1m54.47415768s] [1m54.47415768s] END
I0322 04:07:43.062093    2297 trace.go:236] Trace[213369708]: "List" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:da83444e-0c06-4507-bb73-4636157d4c7f,client:192.168.56.111,protocol:HTTP/2.0,resource:runtimeclasses,scope:cluster,url:/apis/node.k8s.io/v1/runtimeclasses,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 04:04:50.330) (total time: 172731ms):
Trace[213369708]: ---"About to List from storage" 7358ms (04:04:57.689)
Trace[213369708]: ["cacher list" audit-id:da83444e-0c06-4507-bb73-4636157d4c7f,type:runtimeclasses.node.k8s.io 157713ms (04:05:05.348)
Trace[213369708]:  ---"watchCache locked acquired" 5617ms (04:05:10.968)]
Trace[213369708]: ---"Listing from storage done" 5664ms (04:05:23.305)
Trace[213369708]: ---"Writing http response done" count:10 139756ms (04:07:43.061)
Trace[213369708]: [2m52.731505383s] [2m52.731505383s] END
E0322 04:07:43.062263    2297 timeout.go:142] post-timeout activity - time-elapsed: 4m46.998474654s, GET "/apis/node.k8s.io/v1/runtimeclasses" result: <nil>
E0322 04:07:43.064748    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:43.064794    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.065843    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.065956    2297 timeout.go:142] post-timeout activity - time-elapsed: 363.573827ms, GET "/api/v1/namespaces/kube-system/configmaps" result: <nil>
E0322 04:07:43.076551    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:43.076580    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.077714    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.077820    2297 timeout.go:142] post-timeout activity - time-elapsed: 500.410347ms, GET "/apis/admissionregistration.k8s.io/v1/mutatingwebhookconfigurations" result: <nil>
E0322 04:07:43.083244    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:43.083290    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.084406    2297 wrap.go:54] timeout or abort while handling: method=GET URI="/api/v1/services?resourceVersion=679" audit-ID="ad414e75-bc8a-4b8a-9f39-e34e853643c7"
E0322 04:07:43.084520    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.085489    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:43.085786    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.091768    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:43.092345    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.093342    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.188058    2297 timeout.go:142] post-timeout activity - time-elapsed: 121.715145ms, GET "/apis/rbac.authorization.k8s.io/v1/clusterroles" result: <nil>
time="2024-03-22T04:07:42Z" level=info msg="Slow SQL (started: 2024-03-22 04:05:49.585592832 +0000 UTC m=+725.322559025) (total time: 1m53.129715669s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/apiextensions.k8s.io/customresourcedefinitions/% false]]"
time="2024-03-22T04:07:43Z" level=error msg="error while range on /registry/apiextensions.k8s.io/customresourcedefinitions/ /registry/apiextensions.k8s.io/customresourcedefinitions/: context canceled"
time="2024-03-22T04:07:42Z" level=info msg="Slow SQL (started: 2024-03-22 04:07:18.776215826 +0000 UTC m=+814.513182027) (total time: 24.019134264s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/roles/% 756]]"
time="2024-03-22T04:07:43Z" level=error msg="Failed to list /registry/roles/ for revision 756: context canceled"
W0322 04:07:43.093839    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Deployment: Get "https://127.0.0.1:6444/apis/apps/v1/deployments?resourceVersion=719": net/http: TLS handshake timeout
I0322 04:07:43.245448    2297 trace.go:236] Trace[1137543241]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:42.742) (total time: 120502ms):
Trace[1137543241]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/apps/v1/deployments?resourceVersion=719": net/http: TLS handshake timeout 120351ms (04:07:43.093)
Trace[1137543241]: [2m0.502126357s] [2m0.502126357s] END
E0322 04:07:43.245473    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Deployment: failed to list *v1.Deployment: Get "https://127.0.0.1:6444/apis/apps/v1/deployments?resourceVersion=719": net/http: TLS handshake timeout
E0322 04:07:25.973908    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PodTemplate: failed to list *v1.PodTemplate: Get "https://127.0.0.1:6444/api/v1/podtemplates?resourceVersion=668": net/http: TLS handshake timeout
I0322 04:07:27.463884    2297 trace.go:236] Trace[2094543385]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:04.381) (total time: 131234ms):
Trace[2094543385]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/node.k8s.io/v1/runtimeclasses?resourceVersion=668": http2: client connection lost 97363ms (04:06:41.744)
Trace[2094543385]: [2m11.234673899s] [2m11.234673899s] END
E0322 04:07:43.245498    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: Get "https://127.0.0.1:6444/apis/node.k8s.io/v1/runtimeclasses?resourceVersion=668": http2: client connection lost
W0322 04:07:28.902088    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PersistentVolume: Get "https://127.0.0.1:6444/api/v1/persistentvolumes?resourceVersion=667": http2: client connection lost
I0322 04:07:43.245524    2297 trace.go:236] Trace[135883148]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:02.672) (total time: 340572ms):
Trace[135883148]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/persistentvolumes?resourceVersion=667": http2: client connection lost 326225ms (04:07:28.898)
Trace[135883148]: [5m40.572926974s] [5m40.572926974s] END
E0322 04:07:43.245530    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get "https://127.0.0.1:6444/api/v1/persistentvolumes?resourceVersion=667": http2: client connection lost
E0322 04:07:18.343196    2297 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, context deadline exceeded]"
E0322 04:07:43.245656    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
W0322 04:07:19.032663    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.StorageClass: Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/storageclasses?resourceVersion=667": http2: client connection lost
I0322 04:07:43.245680    2297 trace.go:236] Trace[91514256]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:43.640) (total time: 299605ms):
Trace[91514256]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/storageclasses?resourceVersion=667": http2: client connection lost 275368ms (04:07:19.008)
Trace[91514256]: [4m59.605468214s] [4m59.605468214s] END
E0322 04:07:43.245687    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/storageclasses?resourceVersion=667": http2: client connection lost
W0322 04:07:19.091404    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.LimitRange: Get "https://127.0.0.1:6444/api/v1/limitranges?resourceVersion=665": http2: client connection lost
I0322 04:07:43.245709    2297 trace.go:236] Trace[1686277028]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:45.486) (total time: 117759ms):
Trace[1686277028]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/limitranges?resourceVersion=665": http2: client connection lost 93592ms (04:07:19.079)
Trace[1686277028]: [1m57.759152696s] [1m57.759152696s] END
E0322 04:07:43.245716    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.LimitRange: failed to list *v1.LimitRange: Get "https://127.0.0.1:6444/api/v1/limitranges?resourceVersion=665": http2: client connection lost
E0322 04:07:35.815961    2297 controller.go:193] "Failed to update lease" err="Put \"https://127.0.0.1:6444/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-7zoch227uv4owxg75pgtnmv3jq\": http2: client connection lost"
time="2024-03-22T04:07:42Z" level=info msg="Slow SQL (started: 2024-03-22 04:07:26.704524627 +0000 UTC m=+822.441490807) (total time: 16.09210984s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/traefik.io/traefikservices/% false]]"
time="2024-03-22T04:07:43Z" level=error msg="error while range on /registry/traefik.io/traefikservices/ /registry/traefik.io/traefikservices/: context canceled"
E0322 04:07:38.765458    2297 kubelet_node_status.go:540] "Error updating node status, will retry" err="error getting node \"server\": Get \"https://127.0.0.1:6443/api/v1/nodes/server?timeout=10s\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)"
E0322 04:07:43.250557    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.250590    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
time="2024-03-22T04:07:42Z" level=info msg="Slow SQL (started: 2024-03-22 04:07:27.40649335 +0000 UTC m=+823.143459565) (total time: 15.390208532s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/traefik.io/ingressroutes/% false]]"
time="2024-03-22T04:07:43Z" level=error msg="error while range on /registry/traefik.io/ingressroutes/ /registry/traefik.io/ingressroutes/: context canceled"
E0322 04:07:39.835880    2297 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, context deadline exceeded]"
E0322 04:07:43.251571    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:43.251597    2297 timeout.go:142] post-timeout activity - time-elapsed: 19.087718135s, GET "/api/v1/endpoints" result: <nil>
E0322 04:07:43.251605    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
time="2024-03-22T04:07:42Z" level=info msg="Slow SQL (started: 2024-03-22 04:05:56.310529705 +0000 UTC m=+732.047495930) (total time: 1m46.486428788s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/traefik.containo.us/tlsstores/% 756]]"
time="2024-03-22T04:07:43Z" level=error msg="Failed to list /registry/traefik.containo.us/tlsstores/ for revision 756: context canceled"
E0322 04:07:40.446394    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:43.266699    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
I0322 04:07:40.483841    2297 trace.go:236] Trace[334005898]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:04.213) (total time: 125464ms):
Trace[334005898]: ---"Objects listed" error:Get "https://127.0.0.1:6443/api/v1/pods?resourceVersion=708": net/http: TLS handshake timeout 98056ms (04:06:42.269)
Trace[334005898]: [2m5.464046211s] [2m5.464046211s] END
E0322 04:07:43.266731    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://127.0.0.1:6443/api/v1/pods?resourceVersion=708": net/http: TLS handshake timeout
W0322 04:07:41.386660    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Lease: Get "https://127.0.0.1:6444/apis/coordination.k8s.io/v1/namespaces/kube-system/leases?labelSelector=apiserver.kubernetes.io%2Fidentity%3Dkube-apiserver&resourceVersion=688": http2: client connection lost
I0322 04:07:43.266814    2297 trace.go:236] Trace[153701847]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:14.221) (total time: 329045ms):
Trace[153701847]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/coordination.k8s.io/v1/namespaces/kube-system/leases?labelSelector=apiserver.kubernetes.io%2Fidentity%3Dkube-apiserver&resourceVersion=688": http2: client connection lost 327155ms (04:07:41.377)
Trace[153701847]: [5m29.045227144s] [5m29.045227144s] END
E0322 04:07:43.266823    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Lease: failed to list *v1.Lease: Get "https://127.0.0.1:6444/apis/coordination.k8s.io/v1/namespaces/kube-system/leases?labelSelector=apiserver.kubernetes.io%2Fidentity%3Dkube-apiserver&resourceVersion=688": http2: client connection lost
I0322 04:07:41.539434    2297 trace.go:236] Trace[853486766]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:09.206) (total time: 123765ms):
Trace[853486766]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/admissionregistration.k8s.io/v1/validatingwebhookconfigurations?resourceVersion=665": net/http: TLS handshake timeout 111152ms (04:07:00.358)
Trace[853486766]: [2m3.765575398s] [2m3.765575398s] END
E0322 04:07:43.266856    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ValidatingWebhookConfiguration: failed to list *v1.ValidatingWebhookConfiguration: Get "https://127.0.0.1:6444/apis/admissionregistration.k8s.io/v1/validatingwebhookconfigurations?resourceVersion=665": net/http: TLS handshake timeout
W0322 04:07:24.640535    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Pod: Get "https://127.0.0.1:6444/api/v1/pods?resourceVersion=708": dial tcp 127.0.0.1:6444: i/o timeout
I0322 04:07:43.266916    2297 trace.go:236] Trace[1826567290]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:41.943) (total time: 121323ms):
Trace[1826567290]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/pods?resourceVersion=708": dial tcp 127.0.0.1:6444: i/o timeout 90836ms (04:07:12.779)
Trace[1826567290]: [2m1.32388454s] [2m1.32388454s] END
E0322 04:07:43.266963    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://127.0.0.1:6444/api/v1/pods?resourceVersion=708": dial tcp 127.0.0.1:6444: i/o timeout
I0322 04:07:41.879303    2297 trace.go:236] Trace[1153752208]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:00.495) (total time: 122494ms):
Trace[1153752208]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/rbac.authorization.k8s.io/v1/clusterrolebindings?resourceVersion=667": net/http: TLS handshake timeout 70521ms (04:06:11.017)
Trace[1153752208]: [2m2.494238636s] [2m2.494238636s] END
E0322 04:07:43.266981    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ClusterRoleBinding: failed to list *v1.ClusterRoleBinding: Get "https://127.0.0.1:6444/apis/rbac.authorization.k8s.io/v1/clusterrolebindings?resourceVersion=667": net/http: TLS handshake timeout
I0322 04:07:42.040445    2297 trace.go:236] Trace[1136754952]: "iptables ChainExists" (22-Mar-2024 04:05:48.382) (total time: 84494ms):
Trace[1136754952]: [1m24.494976694s] [1m24.494976694s] END
W0322 04:07:42.482882    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/tlsstores?resourceVersion=665": net/http: TLS handshake timeout
I0322 04:07:43.267051    2297 trace.go:236] Trace[1352141525]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:06:01.837) (total time: 101429ms):
Trace[1352141525]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/tlsstores?resourceVersion=665": net/http: TLS handshake timeout 100642ms (04:07:42.479)
Trace[1352141525]: [1m41.429363141s] [1m41.429363141s] END
E0322 04:07:43.267057    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/tlsstores?resourceVersion=665": net/http: TLS handshake timeout
W0322 04:07:42.486236    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.CSIDriver: Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/csidrivers?resourceVersion=665": net/http: TLS handshake timeout
I0322 04:07:43.267117    2297 trace.go:236] Trace[787594867]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:15.778) (total time: 147488ms):
Trace[787594867]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/csidrivers?resourceVersion=665": net/http: TLS handshake timeout 146707ms (04:07:42.486)
Trace[787594867]: [2m27.488477591s] [2m27.488477591s] END
E0322 04:07:43.267125    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/csidrivers?resourceVersion=665": net/http: TLS handshake timeout
E0322 04:07:42.537016    2297 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, context deadline exceeded]"
E0322 04:07:43.267339    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
W0322 04:07:42.537548    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.CSIStorageCapacity: Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/csistoragecapacities?resourceVersion=665": dial tcp 127.0.0.1:6444: i/o timeout
I0322 04:07:43.267369    2297 trace.go:236] Trace[1795998137]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:41.968) (total time: 121298ms):
Trace[1795998137]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/csistoragecapacities?resourceVersion=665": dial tcp 127.0.0.1:6444: i/o timeout 120569ms (04:07:42.537)
Trace[1795998137]: [2m1.298844288s] [2m1.298844288s] END
E0322 04:07:43.267401    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/csistoragecapacities?resourceVersion=665": dial tcp 127.0.0.1:6444: i/o timeout
E0322 04:07:42.581065    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:42.609467    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:42.611987    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:42.612038    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
I0322 04:07:42.631398    2297 trace.go:236] Trace[1771615576]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:03:02.522) (total time: 251134ms):
Trace[1771615576]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/csinodes?resourceVersion=666": net/http: TLS handshake timeout 226144ms (04:06:48.667)
Trace[1771615576]: [4m11.134110938s] [4m11.134110938s] END
E0322 04:07:43.267470    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/csinodes?resourceVersion=666": net/http: TLS handshake timeout
E0322 04:07:42.642484    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ClusterRole: failed to list *v1.ClusterRole: Get "https://127.0.0.1:6444/apis/rbac.authorization.k8s.io/v1/clusterroles?resourceVersion=665": net/http: TLS handshake timeout
E0322 04:07:42.642645    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://127.0.0.1:6444/api/v1/nodes?resourceVersion=679": net/http: TLS handshake timeout
E0322 04:07:42.642682    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Endpoints: failed to list *v1.Endpoints: Get "https://127.0.0.1:6444/api/v1/endpoints?resourceVersion=713": http2: client connection lost
I0322 04:07:42.644635    2297 trace.go:236] Trace[599209238]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:27.392) (total time: 315251ms):
Trace[599209238]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/flowcontrol.apiserver.k8s.io/v1beta3/flowschemas?resourceVersion=665": http2: client connection lost 290245ms (04:07:17.637)
Trace[599209238]: [5m15.251722801s] [5m15.251722801s] END
E0322 04:07:43.267498    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1beta3.FlowSchema: failed to list *v1beta3.FlowSchema: Get "https://127.0.0.1:6444/apis/flowcontrol.apiserver.k8s.io/v1beta3/flowschemas?resourceVersion=665": http2: client connection lost
I0322 04:07:42.644659    2297 trace.go:236] Trace[186051094]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:01:57.880) (total time: 344763ms):
Trace[186051094]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/scheduling.k8s.io/v1/priorityclasses?resourceVersion=665": http2: client connection lost 309771ms (04:07:07.652)
Trace[186051094]: [5m44.763731198s] [5m44.763731198s] END
E0322 04:07:43.267527    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PriorityClass: failed to list *v1.PriorityClass: Get "https://127.0.0.1:6444/apis/scheduling.k8s.io/v1/priorityclasses?resourceVersion=665": http2: client connection lost
I0322 04:07:42.645195    2297 trace.go:236] Trace[1803334461]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:39.559) (total time: 109264ms):
Trace[1803334461]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=665": dial tcp 127.0.0.1:6444: i/o timeout 90557ms (04:07:10.117)
Trace[1803334461]: [1m49.264154721s] [1m49.264154721s] END
E0322 04:07:43.267579    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=665": dial tcp 127.0.0.1:6444: i/o timeout
E0322 04:07:42.645212    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: Get "https://127.0.0.1:6444/apis/storage.k8s.io/v1/volumeattachments?resourceVersion=667": http2: client connection lost
E0322 04:07:42.714129    2297 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, context deadline exceeded]"
E0322 04:07:43.267665    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:43.267698    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
time="2024-03-22T04:07:42Z" level=info msg="Slow SQL (started: 2024-03-22 04:06:06.903281333 +0000 UTC m=+742.640247546) (total time: 1m35.893946637s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/helm.cattle.io/helmcharts/% 756]]"
time="2024-03-22T04:07:43Z" level=error msg="Failed to list /registry/helm.cattle.io/helmcharts/ for revision 756: context canceled"
E0322 04:07:43.281710    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.281869    2297 timeout.go:142] post-timeout activity - time-elapsed: 788.342936ms, GET "/apis/traefik.containo.us/v1alpha1/serverstransports" result: <nil>
W0322 04:07:42.714623    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v2.HorizontalPodAutoscaler: Get "https://127.0.0.1:6444/apis/autoscaling/v2/horizontalpodautoscalers?resourceVersion=665": net/http: TLS handshake timeout
I0322 04:07:43.282710    2297 trace.go:236] Trace[989975431]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:26.580) (total time: 136702ms):
Trace[989975431]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/autoscaling/v2/horizontalpodautoscalers?resourceVersion=665": net/http: TLS handshake timeout 136134ms (04:07:42.714)
Trace[989975431]: [2m16.70241535s] [2m16.70241535s] END
E0322 04:07:43.282763    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v2.HorizontalPodAutoscaler: failed to list *v2.HorizontalPodAutoscaler: Get "https://127.0.0.1:6444/apis/autoscaling/v2/horizontalpodautoscalers?resourceVersion=665": net/http: TLS handshake timeout
E0322 04:07:42.714954    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
time="2024-03-22T04:07:42Z" level=info msg="Slow SQL (started: 2024-03-22 04:07:37.794823015 +0000 UTC m=+833.531789222) (total time: 5.002515609s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/priorityclasses/% 756]]"
time="2024-03-22T04:07:43Z" level=error msg="Failed to list /registry/priorityclasses/ for revision 756: context canceled"
E0322 04:07:43.300133    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.300348    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 04:07:43.300518    2297 trace.go:236] Trace[89356246]: "SerializeObject" audit-id:12004f6a-d9d2-4355-8f56-dda84c7a2440,method:GET,url:/api,protocol:HTTP/2.0,mediaType:application/json,encoder:{"encoder":"{\"name\":\"json\",\"pretty\":\"false\",\"strict\":\"false\",\"yaml\":\"false\"}","name":"versioning"} (22-Mar-2024 04:05:20.467) (total time: 142832ms):
Trace[89356246]: ---"About to start writing response" size:129 31002ms (04:05:51.470)
Trace[89356246]: ---"About to start writing response" size:114 96652ms (04:07:28.125)
Trace[89356246]: [2m22.832502182s] [2m22.832502182s] END
E0322 04:07:43.300657    2297 timeout.go:142] post-timeout activity - time-elapsed: 3m9.02566839s, GET "/api" result: <nil>
E0322 04:07:43.300741    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.300834    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.300866    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.300979    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
I0322 04:07:43.301036    2297 trace.go:236] Trace[2053382903]: "SerializeObject" audit-id:fdb25b84-ba0a-40c0-a304-87b848f41bf6,method:PUT,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-7zoch227uv4owxg75pgtnmv3jq,protocol:HTTP/2.0,mediaType:application/vnd.kubernetes.protobuf,encoder:{"encodeGV":"coordination.k8s.io/v1","encoder":"protobuf","name":"versioning"} (22-Mar-2024 04:06:34.383) (total time: 68917ms):
Trace[2053382903]: ---"About to start writing response" size:154 25598ms (04:06:59.982)
Trace[2053382903]: ---"Write call failed" writer:responsewriter.outerWithCloseNotifyAndFlush,size:154,firstWrite:true,err:http: Handler timeout 23666ms (04:07:23.648)
Trace[2053382903]: ---"About to start writing response" size:69 19618ms (04:07:43.267)
Trace[2053382903]: [1m8.917341291s] [1m8.917341291s] END
I0322 04:07:43.301258    2297 trace.go:236] Trace[1729757070]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:fdb25b84-ba0a-40c0-a304-87b848f41bf6,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-7zoch227uv4owxg75pgtnmv3jq,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PUT (22-Mar-2024 04:05:25.758) (total time: 137542ms):
Trace[1729757070]: ---"About to convert to expected version" 12116ms (04:05:37.884)
Trace[1729757070]: ---"Conversion done" 15639ms (04:05:53.524)
Trace[1729757070]: ---"Write to database call failed" len:590,err:Timeout: request did not complete within requested timeout - context deadline exceeded 8280ms (04:06:01.811)
Trace[1729757070]: ["GuaranteedUpdate etcd3" audit-id:fdb25b84-ba0a-40c0-a304-87b848f41bf6,key:/leases/kube-system/apiserver-7zoch227uv4owxg75pgtnmv3jq,type:*coordination.Lease,resource:leases.coordination.k8s.io 68755ms (04:06:34.545)
Trace[1729757070]:  ---"initial value restored" 38842ms (04:07:13.388)
Trace[1729757070]:  ---"About to Encode" 29756ms (04:07:43.144)]
Trace[1729757070]: [2m17.542813619s] [2m17.542813619s] END
E0322 04:07:43.301984    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.302112    2297 timeout.go:142] post-timeout activity - time-elapsed: 904.047104ms, GET "/api/v1/secrets" result: <nil>
E0322 04:07:42.772359    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:42.773499    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:42.774177    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:42.774652    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:42.775371    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:42.775859    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:42.778174    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:42.778835    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:42.783080    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:42.791900    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:42.792474    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:42.792829    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:42.795448    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:42.795502    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:42.795526    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:42.795536    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:42.795544    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
W0322 04:07:42.797829    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/ingressroutetcps?resourceVersion=671": dial tcp 127.0.0.1:6444: i/o timeout
I0322 04:07:43.302611    2297 trace.go:236] Trace[274087143]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:42.391) (total time: 120911ms):
Trace[274087143]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/ingressroutetcps?resourceVersion=671": dial tcp 127.0.0.1:6444: i/o timeout 120406ms (04:07:42.797)
Trace[274087143]: [2m0.911181648s] [2m0.911181648s] END
E0322 04:07:43.302738    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/ingressroutetcps?resourceVersion=671": dial tcp 127.0.0.1:6444: i/o timeout
W0322 04:07:42.797913    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.IngressClass: Get "https://127.0.0.1:6444/apis/networking.k8s.io/v1/ingressclasses?resourceVersion=665": http2: client connection lost
I0322 04:07:43.302851    2297 trace.go:236] Trace[653123420]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:03.737) (total time: 339565ms):
Trace[653123420]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/networking.k8s.io/v1/ingressclasses?resourceVersion=665": http2: client connection lost 339060ms (04:07:42.797)
Trace[653123420]: [5m39.565736731s] [5m39.565736731s] END
E0322 04:07:43.302927    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.IngressClass: failed to list *v1.IngressClass: Get "https://127.0.0.1:6444/apis/networking.k8s.io/v1/ingressclasses?resourceVersion=665": http2: client connection lost
W0322 04:07:16.682027    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.MutatingWebhookConfiguration: Get "https://127.0.0.1:6444/apis/admissionregistration.k8s.io/v1/mutatingwebhookconfigurations?resourceVersion=665": http2: client connection lost
I0322 04:07:43.303083    2297 trace.go:236] Trace[1377062261]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:01:58.376) (total time: 344926ms):
Trace[1377062261]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/admissionregistration.k8s.io/v1/mutatingwebhookconfigurations?resourceVersion=665": http2: client connection lost 306194ms (04:07:04.570)
Trace[1377062261]: [5m44.926616927s] [5m44.926616927s] END
E0322 04:07:43.303139    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.MutatingWebhookConfiguration: failed to list *v1.MutatingWebhookConfiguration: Get "https://127.0.0.1:6444/apis/admissionregistration.k8s.io/v1/mutatingwebhookconfigurations?resourceVersion=665": http2: client connection lost
E0322 04:07:43.303223    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.303281    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.303395    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
time="2024-03-22T04:07:42Z" level=info msg="Slow SQL (started: 2024-03-22 04:07:38.478488441 +0000 UTC m=+834.215454631) (total time: 4.319027218s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/traefik.io/tlsoptions/% 756]]"
time="2024-03-22T04:07:43Z" level=error msg="Failed to list /registry/traefik.io/tlsoptions/ for revision 756: context canceled"
E0322 04:07:43.320874    2297 timeout.go:142] post-timeout activity - time-elapsed: 33.990396415s, GET "/api/v1/nodes" result: <nil>
E0322 04:07:43.321110    2297 timeout.go:142] post-timeout activity - time-elapsed: 3m7.120206662s, PUT "/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-7zoch227uv4owxg75pgtnmv3jq" result: <nil>
E0322 04:07:43.321333    2297 timeout.go:142] post-timeout activity - time-elapsed: 946.914897ms, GET "/apis/traefik.io/v1alpha1/ingressroutetcps" result: <nil>
E0322 04:07:43.321986    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.322110    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.322257    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
W0322 04:07:42.798428    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1beta3.PriorityLevelConfiguration: Get "https://127.0.0.1:6444/apis/flowcontrol.apiserver.k8s.io/v1beta3/prioritylevelconfigurations?resourceVersion=668": http2: client connection lost
I0322 04:07:43.322496    2297 trace.go:236] Trace[569590487]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:16.988) (total time: 326334ms):
Trace[569590487]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/flowcontrol.apiserver.k8s.io/v1beta3/prioritylevelconfigurations?resourceVersion=668": http2: client connection lost 325810ms (04:07:42.798)
Trace[569590487]: [5m26.334133748s] [5m26.334133748s] END
E0322 04:07:43.322610    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1beta3.PriorityLevelConfiguration: failed to list *v1beta3.PriorityLevelConfiguration: Get "https://127.0.0.1:6444/apis/flowcontrol.apiserver.k8s.io/v1beta3/prioritylevelconfigurations?resourceVersion=668": http2: client connection lost
E0322 04:07:43.322863    2297 timeout.go:142] post-timeout activity - time-elapsed: 19.909318214s, GET "/apis/traefik.containo.us/v1alpha1/ingressrouteudps" result: <nil>
E0322 04:07:43.322975    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.323052    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.323140    2297 timeout.go:142] post-timeout activity - time-elapsed: 809.170418ms, GET "/api/v1/services" result: <nil>
E0322 04:07:43.323234    2297 timeout.go:142] post-timeout activity - time-elapsed: 305.925213ms, GET "/apis/traefik.containo.us/v1alpha1/traefikservices" result: <nil>
E0322 04:07:43.323324    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.323378    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.323447    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.323500    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.323569    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.323645    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.323721    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.323791    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.323840    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.323943    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.324060    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 04:07:43.324184    2297 trace.go:236] Trace[1095150051]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:6c86093b-bded-45c6-8f87-6ded290cc81f,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:namespace,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 04:07:42.727) (total time: 596ms):
Trace[1095150051]: [596.787504ms] [596.787504ms] END
E0322 04:07:43.324408    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.324510    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.324583    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.324708    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.324801    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.324841    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.324937    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 04:07:43.324982    2297 trace.go:236] Trace[1756255042]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:0b37d9e4-f543-4ddc-bb19-ce1bc9dff472,client:127.0.0.1,protocol:HTTP/2.0,resource:services,scope:cluster,url:/api/v1/services,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 04:07:42.771) (total time: 553ms):
Trace[1756255042]: [553.217292ms] [553.217292ms] END
I0322 04:07:43.325241    2297 trace.go:236] Trace[1782561733]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:eed57b76-f765-4025-b7d4-645cb3f66238,client:127.0.0.1,protocol:HTTP/2.0,resource:prioritylevelconfigurations,scope:cluster,url:/apis/flowcontrol.apiserver.k8s.io/v1beta3/prioritylevelconfigurations,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 04:07:42.773) (total time: 551ms):
Trace[1782561733]: [551.312743ms] [551.312743ms] END
E0322 04:07:43.325408    2297 timeout.go:142] post-timeout activity - time-elapsed: 746.950441ms, GET "/apis/flowcontrol.apiserver.k8s.io/v1beta3/prioritylevelconfigurations" result: <nil>
I0322 04:07:43.325518    2297 trace.go:236] Trace[751047779]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:0ad62ad2-3541-4dbd-87ab-e7f4a360181d,client:127.0.0.1,protocol:HTTP/2.0,resource:flowschemas,scope:cluster,url:/apis/flowcontrol.apiserver.k8s.io/v1beta3/flowschemas,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 04:07:42.728) (total time: 596ms):
Trace[751047779]: ---"Writing http response done" count:13 596ms (04:07:43.325)
Trace[751047779]: [596.83455ms] [596.83455ms] END
I0322 04:07:43.325654    2297 trace.go:236] Trace[1129971389]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:4da6e05e-d484-4351-9f31-8052e528e71d,client:127.0.0.1,protocol:HTTP/2.0,resource:secrets,scope:cluster,url:/api/v1/secrets,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 04:07:42.783) (total time: 542ms):
Trace[1129971389]: ---"Writing http response done" count:7 542ms (04:07:43.325)
Trace[1129971389]: [542.29705ms] [542.29705ms] END
I0322 04:07:43.325774    2297 trace.go:236] Trace[1197757478]: "List" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:c8f57c0f-38ce-418e-8e3d-e8b32abfb051,client:192.168.56.111,protocol:HTTP/2.0,resource:pods,scope:cluster,url:/api/v1/pods,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 04:07:42.792) (total time: 533ms):
Trace[1197757478]: ---"Writing http response done" count:0 533ms (04:07:43.325)
Trace[1197757478]: [533.48528ms] [533.48528ms] END
I0322 04:07:43.325909    2297 trace.go:236] Trace[1883554908]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:9ec25740-722a-4d3e-8986-f1c37e651f10,client:192.168.56.111,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 04:07:42.720) (total time: 605ms):
Trace[1883554908]: [605.833855ms] [605.833855ms] END
E0322 04:07:43.326045    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
time="2024-03-22T04:07:42Z" level=info msg="Slow SQL (started: 2024-03-22 04:07:39.383310291 +0000 UTC m=+835.120276535) (total time: 3.414334075s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/daemonsets/% false]]"
time="2024-03-22T04:07:43Z" level=error msg="error while range on /registry/daemonsets/ /registry/daemonsets/: context canceled"
E0322 04:07:43.326430    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 04:07:43.326539    2297 trace.go:236] Trace[1602802546]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:2e2ead9d-2e41-4e94-9b79-ea44b355ccdd,client:127.0.0.1,protocol:HTTP/2.0,resource:persistentvolumes,scope:cluster,url:/api/v1/persistentvolumes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 04:07:42.731) (total time: 594ms):
Trace[1602802546]: [594.911118ms] [594.911118ms] END
E0322 04:07:43.326722    2297 timeout.go:142] post-timeout activity - time-elapsed: 749.253539ms, GET "/api/v1/persistentvolumes" result: <nil>
E0322 04:07:43.326796    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 04:07:43.326918    2297 trace.go:236] Trace[1812398331]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:95ff6867-230b-491a-ab11-5c22df002d40,client:192.168.56.111,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 04:07:42.719) (total time: 607ms):
Trace[1812398331]: [607.704182ms] [607.704182ms] END
E0322 04:07:43.327053    2297 timeout.go:142] post-timeout activity - time-elapsed: 2m3.126713586s, GET "/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/serverworker" result: <nil>
I0322 04:07:43.327137    2297 trace.go:236] Trace[527590506]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:f645b0fe-c237-4069-a6c8-aa55f7f0eccd,client:127.0.0.1,protocol:HTTP/2.0,resource:endpoints,scope:cluster,url:/api/v1/endpoints,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 04:07:42.775) (total time: 551ms):
Trace[527590506]: [551.362075ms] [551.362075ms] END
E0322 04:07:43.327228    2297 timeout.go:142] post-timeout activity - time-elapsed: 749.792149ms, GET "/api/v1/endpoints" result: <nil>
E0322 04:07:42.800744    2297 remote_runtime.go:294] "ListPodSandbox with filter from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" filter="nil"
E0322 04:07:43.329780    2297 kuberuntime_sandbox.go:297] "Failed to list pod sandboxes" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded"
E0322 04:07:43.339169    2297 timeout.go:142] post-timeout activity - time-elapsed: 18.787278014s, GET "/apis/coordination.k8s.io/v1/namespaces/kube-system/leases" result: <nil>
E0322 04:07:43.339546    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.339679    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.339771    2297 timeout.go:142] post-timeout activity - time-elapsed: 693.392401ms, GET "/api/v1/services" result: <nil>
I0322 04:07:43.339922    2297 trace.go:236] Trace[861402604]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:dd7523f3-ae82-4e49-98ee-2963ff9d51d2,client:127.0.0.1,protocol:HTTP/2.0,resource:storageclasses,scope:cluster,url:/apis/storage.k8s.io/v1/storageclasses,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 04:07:42.773) (total time: 566ms):
Trace[861402604]: [566.726762ms] [566.726762ms] END
E0322 04:07:43.340188    2297 timeout.go:142] post-timeout activity - time-elapsed: 762.676911ms, GET "/apis/storage.k8s.io/v1/storageclasses" result: <nil>
I0322 04:07:43.340298    2297 trace.go:236] Trace[705493275]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:5a15563a-785d-449f-8fee-efb148d827d3,client:127.0.0.1,protocol:HTTP/2.0,resource:rolebindings,scope:cluster,url:/apis/rbac.authorization.k8s.io/v1/rolebindings,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 04:07:42.774) (total time: 565ms):
Trace[705493275]: [565.839156ms] [565.839156ms] END
E0322 04:07:43.340533    2297 timeout.go:142] post-timeout activity - time-elapsed: 762.99654ms, GET "/apis/rbac.authorization.k8s.io/v1/rolebindings" result: <nil>
E0322 04:07:43.340597    2297 timeout.go:142] post-timeout activity - time-elapsed: 762.068708ms, GET "/apis/flowcontrol.apiserver.k8s.io/v1beta3/flowschemas" result: <nil>
I0322 04:07:43.340753    2297 trace.go:236] Trace[1801204156]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:96f023e5-331a-4205-8cdf-d7d2158cfd35,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:namespace,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 04:07:42.774) (total time: 565ms):
Trace[1801204156]: ---"Writing http response done" count:0 565ms (04:07:43.340)
Trace[1801204156]: [565.821482ms] [565.821482ms] END
I0322 04:07:43.340990    2297 trace.go:236] Trace[1019217623]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:fe8fb392-081a-4bc5-aff0-bda8f3bc2535,client:127.0.0.1,protocol:HTTP/2.0,resource:volumeattachments,scope:cluster,url:/apis/storage.k8s.io/v1/volumeattachments,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 04:07:42.778) (total time: 562ms):
Trace[1019217623]: [562.644791ms] [562.644791ms] END
E0322 04:07:43.341114    2297 timeout.go:142] post-timeout activity - time-elapsed: 762.633901ms, GET "/apis/storage.k8s.io/v1/volumeattachments" result: <nil>
E0322 04:07:43.341250    2297 timeout.go:142] post-timeout activity - time-elapsed: 762.822624ms, GET "/api/v1/secrets" result: <nil>
E0322 04:07:43.341337    2297 timeout.go:142] post-timeout activity - time-elapsed: 19.431889627s, GET "/api/v1/pods" result: <nil>
I0322 04:07:43.341463    2297 trace.go:236] Trace[1655411013]: "List" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:33f31dc3-c2fe-4e7d-97b1-99587e054267,client:192.168.56.111,protocol:HTTP/2.0,resource:services,scope:cluster,url:/api/v1/services,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 04:07:42.792) (total time: 548ms):
Trace[1655411013]: [548.669957ms] [548.669957ms] END
E0322 04:07:43.341666    2297 timeout.go:142] post-timeout activity - time-elapsed: 1m57.423026397s, GET "/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/serverworker" result: <nil>
time="2024-03-22T04:07:42Z" level=error msg="error while range on /registry/namespaces/ /registry/namespaces/: context canceled"
I0322 04:07:43.342447    2297 trace.go:236] Trace[1203899291]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:aa52c4cb-2fed-4168-9131-f1a9b2b84f0e,client:127.0.0.1,protocol:HTTP/2.0,resource:runtimeclasses,scope:cluster,url:/apis/node.k8s.io/v1/runtimeclasses,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 04:07:42.775) (total time: 566ms):
Trace[1203899291]: ---"Writing http response done" count:10 566ms (04:07:43.342)
Trace[1203899291]: [566.461717ms] [566.461717ms] END
E0322 04:07:43.342628    2297 timeout.go:142] post-timeout activity - time-elapsed: 765.04225ms, GET "/apis/node.k8s.io/v1/runtimeclasses" result: <nil>
W0322 04:07:42.800925    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.ServiceAccount: Get "https://127.0.0.1:6444/api/v1/serviceaccounts?resourceVersion=665": http2: client connection lost
I0322 04:07:43.342846    2297 trace.go:236] Trace[59609586]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:15.297) (total time: 328045ms):
Trace[59609586]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/serviceaccounts?resourceVersion=665": http2: client connection lost 327503ms (04:07:42.800)
Trace[59609586]: [5m28.045007066s] [5m28.045007066s] END
E0322 04:07:43.342932    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ServiceAccount: failed to list *v1.ServiceAccount: Get "https://127.0.0.1:6444/api/v1/serviceaccounts?resourceVersion=665": http2: client connection lost
E0322 04:07:43.343030    2297 timeout.go:142] post-timeout activity - time-elapsed: 763.345828ms, GET "/apis/coordination.k8s.io/v1/namespaces/kube-system/leases" result: <nil>
I0322 04:07:43.343130    2297 trace.go:236] Trace[950597191]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:f86917a0-1ef1-4363-9b99-249801b8c149,client:127.0.0.1,protocol:HTTP/2.0,resource:resourcequotas,scope:cluster,url:/api/v1/resourcequotas,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 04:07:42.779) (total time: 563ms):
Trace[950597191]: ---"Writing http response done" count:0 563ms (04:07:43.343)
Trace[950597191]: [563.931199ms] [563.931199ms] END
E0322 04:07:43.343257    2297 timeout.go:142] post-timeout activity - time-elapsed: 764.752204ms, GET "/api/v1/resourcequotas" result: <nil>
E0322 04:07:43.343350    2297 timeout.go:142] post-timeout activity - time-elapsed: 617.572851ms, GET "/api/v1/services" result: <nil>
time="2024-03-22T04:07:42Z" level=info msg="Slow SQL (started: 2024-03-22 04:05:35.730158509 +0000 UTC m=+711.467124719) (total time: 1m30.522087152s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/helm.cattle.io/helmcharts/% false]]"
time="2024-03-22T04:07:43Z" level=error msg="error while range on /registry/helm.cattle.io/helmcharts/ /registry/helm.cattle.io/helmcharts/: context canceled"
time="2024-03-22T04:07:42Z" level=info msg="Slow SQL (started: 2024-03-22 04:06:51.846594049 +0000 UTC m=+787.583560235) (total time: 15.787612976s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/replicasets/% 756]]"
time="2024-03-22T04:07:43Z" level=error msg="Failed to list /registry/replicasets/ for revision 756: context canceled"
I0322 04:07:42.836618    2297 log.go:245] http: TLS handshake error from 10.42.0.5:60008: EOF
I0322 04:07:43.356386    2297 log.go:245] http: TLS handshake error from 10.42.0.5:46124: write tcp 192.168.56.110:10250->10.42.0.5:46124: write: broken pipe
I0322 04:07:43.356419    2297 log.go:245] http: TLS handshake error from 10.42.0.5:53432: EOF
I0322 04:07:43.356521    2297 log.go:245] http: TLS handshake error from 10.42.0.5:43892: EOF
I0322 04:07:43.356558    2297 log.go:245] http: TLS handshake error from 10.42.0.5:60986: EOF
time="2024-03-22T04:07:42Z" level=info msg="Slow SQL (started: 2024-03-22 04:05:41.408067732 +0000 UTC m=+717.145033915) (total time: 1m23.52479864s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/storageclasses/% false]]"
time="2024-03-22T04:07:43Z" level=error msg="error while range on /registry/storageclasses/ /registry/storageclasses/: context canceled"
time="2024-03-22T04:07:42Z" level=info msg="Slow SQL (started: 2024-03-22 04:06:14.622327686 +0000 UTC m=+750.359293943) (total time: 1m20.452092564s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC LIMIT 500 : [[% 757]]"
E0322 04:07:42.844214    2297 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, context deadline exceeded]"
E0322 04:07:43.358267    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:43.358292    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
W0322 04:07:42.844406    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/tlsoptions?resourceVersion=668": net/http: TLS handshake timeout
I0322 04:07:43.358486    2297 trace.go:236] Trace[540021191]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:17.217) (total time: 146140ms):
Trace[540021191]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/tlsoptions?resourceVersion=668": net/http: TLS handshake timeout 145626ms (04:07:42.844)
Trace[540021191]: [2m26.140942459s] [2m26.140942459s] END
E0322 04:07:43.358509    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/tlsoptions?resourceVersion=668": net/http: TLS handshake timeout
W0322 04:07:19.860824    2297 transport.go:301] Unable to cancel request for *otelhttp.Transport
E0322 04:07:13.066786    2297 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, context deadline exceeded]"
E0322 04:07:43.358689    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:42.889382    2297 timeout.go:142] post-timeout activity - time-elapsed: 385.464203ms, GET "/apis/traefik.io/v1alpha1/traefikservices" result: <nil>
E0322 04:07:42.890095    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:42.891106    2297 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, context deadline exceeded]"
E0322 04:07:43.358967    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:42.893651    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
W0322 04:07:42.897269    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Endpoints: Get "https://127.0.0.1:6444/api/v1/endpoints?resourceVersion=713": dial tcp 127.0.0.1:6444: i/o timeout
I0322 04:07:43.359033    2297 trace.go:236] Trace[1734294132]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:29.873) (total time: 133485ms):
Trace[1734294132]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/endpoints?resourceVersion=713": dial tcp 127.0.0.1:6444: i/o timeout 133024ms (04:07:42.897)
Trace[1734294132]: [2m13.485967738s] [2m13.485967738s] END
E0322 04:07:43.359047    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Endpoints: failed to list *v1.Endpoints: Get "https://127.0.0.1:6444/api/v1/endpoints?resourceVersion=713": dial tcp 127.0.0.1:6444: i/o timeout
E0322 04:07:42.897919    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
W0322 04:07:42.899771    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Lease: Get "https://127.0.0.1:6444/apis/coordination.k8s.io/v1/namespaces/kube-system/leases?labelSelector=k8s.io%2Fcomponent%3Dkube-apiserver&resourceVersion=668": http2: client connection lost
I0322 04:07:43.359177    2297 trace.go:236] Trace[924425705]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:04:57.278) (total time: 166080ms):
Trace[924425705]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/coordination.k8s.io/v1/namespaces/kube-system/leases?labelSelector=k8s.io%2Fcomponent%3Dkube-apiserver&resourceVersion=668": http2: client connection lost 165621ms (04:07:42.899)
Trace[924425705]: [2m46.080799313s] [2m46.080799313s] END
E0322 04:07:43.359190    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Lease: failed to list *v1.Lease: Get "https://127.0.0.1:6444/apis/coordination.k8s.io/v1/namespaces/kube-system/leases?labelSelector=k8s.io%2Fcomponent%3Dkube-apiserver&resourceVersion=668": http2: client connection lost
E0322 04:07:42.899895    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
W0322 04:07:42.904691    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PersistentVolume: Get "https://127.0.0.1:6444/api/v1/persistentvolumes?resourceVersion=667": dial tcp 127.0.0.1:6444: i/o timeout
I0322 04:07:43.359302    2297 trace.go:236] Trace[53908941]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:42.256) (total time: 121102ms):
Trace[53908941]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/persistentvolumes?resourceVersion=667": dial tcp 127.0.0.1:6444: i/o timeout 120648ms (04:07:42.904)
Trace[53908941]: [2m1.102584201s] [2m1.102584201s] END
E0322 04:07:43.359331    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get "https://127.0.0.1:6444/api/v1/persistentvolumes?resourceVersion=667": dial tcp 127.0.0.1:6444: i/o timeout
time="2024-03-22T04:07:42Z" level=info msg="Slow SQL (started: 2024-03-22 04:06:12.637874822 +0000 UTC m=+748.374840998) (total time: 1m30.286643404s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/secrets/% 756]]"
E0322 04:07:43.359749    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
W0322 04:07:42.904799    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.NetworkPolicy: Get "https://127.0.0.1:6444/apis/networking.k8s.io/v1/networkpolicies?resourceVersion=666": net/http: TLS handshake timeout
I0322 04:07:43.361442    2297 trace.go:236] Trace[101205678]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:07:06.949) (total time: 36412ms):
Trace[101205678]: ---"Objects listed" error:<nil> 36412ms (04:07:43.361)
Trace[101205678]: [36.412285029s] [36.412285029s] END
I0322 04:07:43.362961    2297 trace.go:236] Trace[588271701]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:54.827) (total time: 108532ms):
Trace[588271701]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/networking.k8s.io/v1/networkpolicies?resourceVersion=666": net/http: TLS handshake timeout 108077ms (04:07:42.904)
Trace[588271701]: [1m48.532395954s] [1m48.532395954s] END
E0322 04:07:43.363000    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.NetworkPolicy: failed to list *v1.NetworkPolicy: Get "https://127.0.0.1:6444/apis/networking.k8s.io/v1/networkpolicies?resourceVersion=666": net/http: TLS handshake timeout
E0322 04:07:43.364517    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.364979    2297 timeout.go:142] post-timeout activity - time-elapsed: 850.305717ms, GET "/apis/traefik.io/v1alpha1/ingressrouteudps" result: <nil>
time="2024-03-22T04:07:42Z" level=info msg="Slow SQL (started: 2024-03-22 04:06:47.679339924 +0000 UTC m=+783.416306156) (total time: 55.309232626s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/flowschemas/% false]]"
time="2024-03-22T04:07:43Z" level=error msg="error while range on /registry/flowschemas/ /registry/flowschemas/: context canceled"
E0322 04:07:43.366428    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.370760    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.370794    2297 timeout.go:142] post-timeout activity - time-elapsed: 668.413547ms, GET "/apis/networking.k8s.io/v1/ingressclasses" result: <nil>
E0322 04:07:43.370807    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
W0322 04:07:42.905829    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.ClusterRole: Get "https://127.0.0.1:6444/apis/rbac.authorization.k8s.io/v1/clusterroles?resourceVersion=665": http2: client connection lost
I0322 04:07:43.370960    2297 trace.go:236] Trace[1936575975]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:02.803) (total time: 340567ms):
Trace[1936575975]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/rbac.authorization.k8s.io/v1/clusterroles?resourceVersion=665": http2: client connection lost 340102ms (04:07:42.905)
Trace[1936575975]: [5m40.567318208s] [5m40.567318208s] END
E0322 04:07:43.371000    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ClusterRole: failed to list *v1.ClusterRole: Get "https://127.0.0.1:6444/apis/rbac.authorization.k8s.io/v1/clusterroles?resourceVersion=665": http2: client connection lost
W0322 04:07:33.726525    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://127.0.0.1:6444/api/v1/services?resourceVersion=665": dial tcp 127.0.0.1:6444: i/o timeout
I0322 04:07:43.371118    2297 trace.go:236] Trace[1303291208]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:17.667) (total time: 145703ms):
Trace[1303291208]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/services?resourceVersion=665": dial tcp 127.0.0.1:6444: i/o timeout 121468ms (04:07:19.135)
Trace[1303291208]: [2m25.703895649s] [2m25.703895649s] END
E0322 04:07:43.371150    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://127.0.0.1:6444/api/v1/services?resourceVersion=665": dial tcp 127.0.0.1:6444: i/o timeout
E0322 04:07:42.909749    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
I0322 04:07:42.909958    2297 trace.go:236] Trace[565163620]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:02.540) (total time: 276114ms):
Trace[565163620]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/namespaces?resourceVersion=667": http2: client connection lost 262505ms (04:06:25.046)
Trace[565163620]: [4m36.114950035s] [4m36.114950035s] END
E0322 04:07:43.371168    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://127.0.0.1:6444/api/v1/namespaces?resourceVersion=667": http2: client connection lost
E0322 04:07:42.910220    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
E0322 04:07:42.910504    2297 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, context deadline exceeded]"
E0322 04:07:43.371511    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
W0322 04:07:42.924351    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.ConfigMap: Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dkube-apiserver-legacy-service-account-token-tracking&resourceVersion=665": dial tcp 127.0.0.1:6444: i/o timeout
I0322 04:07:43.371564    2297 trace.go:236] Trace[674242234]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:46.242) (total time: 117128ms):
Trace[674242234]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dkube-apiserver-legacy-service-account-token-tracking&resourceVersion=665": dial tcp 127.0.0.1:6444: i/o timeout 116681ms (04:07:42.924)
Trace[674242234]: [1m57.128767911s] [1m57.128767911s] END
E0322 04:07:43.371597    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dkube-apiserver-legacy-service-account-token-tracking&resourceVersion=665": dial tcp 127.0.0.1:6444: i/o timeout
E0322 04:07:42.928524    2297 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, context deadline exceeded]"
E0322 04:07:43.371853    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:43.371971    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:43.372038    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:43.372152    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:43.372184    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.372282    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
time="2024-03-22T04:07:42Z" level=error msg="error while range on /registry/traefik.io/ingressrouteudps/ /registry/traefik.io/ingressrouteudps/: context canceled"
time="2024-03-22T04:07:42Z" level=error msg="error while range on /registry/traefik.containo.us/ingressroutes/ /registry/traefik.containo.us/ingressroutes/: context canceled"
W0322 04:07:42.930162    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://127.0.0.1:6443/api/v1/services?resourceVersion=665": dial tcp 127.0.0.1:6443: i/o timeout
I0322 04:07:43.372636    2297 trace.go:236] Trace[1147992006]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:42.287) (total time: 121084ms):
Trace[1147992006]: ---"Objects listed" error:Get "https://127.0.0.1:6443/api/v1/services?resourceVersion=665": dial tcp 127.0.0.1:6443: i/o timeout 120641ms (04:07:42.929)
Trace[1147992006]: [2m1.084781472s] [2m1.084781472s] END
E0322 04:07:43.372651    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://127.0.0.1:6443/api/v1/services?resourceVersion=665": dial tcp 127.0.0.1:6443: i/o timeout
E0322 04:07:43.372915    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.372938    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.373027    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.373041    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.373062    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.373066    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.373944    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.374182    2297 timeout.go:142] post-timeout activity - time-elapsed: 872.568224ms, GET "/apis/traefik.containo.us/v1alpha1/tlsstores" result: <nil>
E0322 04:07:43.374451    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
time="2024-03-22T04:07:42Z" level=error msg="error while range on /registry/traefik.io/tlsoptions/ /registry/traefik.io/tlsoptions/: context canceled"
time="2024-03-22T04:07:42Z" level=error msg="error while range on /registry/traefik.containo.us/tlsstores/ /registry/traefik.containo.us/tlsstores/: context canceled"
time="2024-03-22T04:07:42Z" level=info msg="Slow SQL (started: 2024-03-22 04:06:14.738636237 +0000 UTC m=+750.475602445) (total time: 1m28.251225636s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/clusterroles/% 756]]"
time="2024-03-22T04:07:43Z" level=error msg="Failed to list /registry/clusterroles/ for revision 756: context canceled"
E0322 04:07:43.399867    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.399972    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:42.936195    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
W0322 04:07:42.936388    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/helm.cattle.io/v1/helmcharts?resourceVersion=665": net/http: TLS handshake timeout
I0322 04:07:43.400262    2297 trace.go:236] Trace[1166836907]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:38.634) (total time: 124765ms):
Trace[1166836907]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/helm.cattle.io/v1/helmcharts?resourceVersion=665": net/http: TLS handshake timeout 124302ms (04:07:42.936)
Trace[1166836907]: [2m4.765774022s] [2m4.765774022s] END
E0322 04:07:43.400288    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/helm.cattle.io/v1/helmcharts?resourceVersion=665": net/http: TLS handshake timeout
I0322 04:07:42.936502    2297 request.go:697] Waited for 16.13718025s due to client-side throttling, not priority and fairness, request: GET:https://127.0.0.1:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dserver&resourceVersion=708
E0322 04:07:43.401117    2297 timeout.go:142] post-timeout activity - time-elapsed: 509.245594ms, GET "/api/v1/persistentvolumes" result: <nil>
E0322 04:07:43.401159    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.401197    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.401350    2297 timeout.go:142] post-timeout activity - time-elapsed: 689.730182ms, GET "/api/v1/namespaces" result: <nil>
E0322 04:07:43.401394    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.401409    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.401505    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
time="2024-03-22T04:07:42Z" level=info msg="Slow SQL (started: 2024-03-22 04:06:57.886684357 +0000 UTC m=+793.623650540) (total time: 45.103618248s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/traefik.io/ingressrouteudps/% 756]]"
time="2024-03-22T04:07:43Z" level=error msg="Failed to list /registry/traefik.io/ingressrouteudps/ for revision 756: context canceled"
E0322 04:07:42.937573    2297 webhook.go:154] Failed to make webhook authenticator request: Post "https://127.0.0.1:6443/apis/authentication.k8s.io/v1/tokenreviews": context deadline exceeded
E0322 04:07:43.442685    2297 server.go:310] "Unable to authenticate the request due to an error" err="Post \"https://127.0.0.1:6443/apis/authentication.k8s.io/v1/tokenreviews\": context deadline exceeded"
I0322 04:07:43.443819    2297 trace.go:236] Trace[56980229]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:cdacc8d3-d24d-47f4-a47d-47ddc1b891b9,client:192.168.56.111,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 04:07:42.908) (total time: 534ms):
Trace[56980229]: [534.806774ms] [534.806774ms] END
E0322 04:07:43.444081    2297 timeout.go:142] post-timeout activity - time-elapsed: 537.019853ms, GET "/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/serverworker" result: <nil>
E0322 04:07:43.444153    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
time="2024-03-22T04:07:42Z" level=info msg="Slow SQL (started: 2024-03-22 04:06:03.606274784 +0000 UTC m=+739.343240992) (total time: 1m39.384401994s):  SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? AND mkv.id <= ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC LIMIT 1000 : [[/registry/traefik.containo.us/ingressroutetcps/% 757 false]]"
time="2024-03-22T04:07:43Z" level=error msg="error while range on /registry/traefik.containo.us/ingressroutetcps/ /registry/traefik.containo.us/ingressroutetcps/: context canceled"
E0322 04:07:43.448634    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 04:07:43.448803    2297 trace.go:236] Trace[2030612379]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:f7e0a688-7624-42ed-8cdb-55d6ce2842d9,client:127.0.0.1,protocol:HTTP/2.0,resource:serviceaccounts,scope:cluster,url:/api/v1/serviceaccounts,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 04:07:42.935) (total time: 512ms):
Trace[2030612379]: ---"Writing http response done" count:41 512ms (04:07:43.448)
Trace[2030612379]: [512.906958ms] [512.906958ms] END
E0322 04:07:43.449024    2297 timeout.go:142] post-timeout activity - time-elapsed: 514.953448ms, GET "/api/v1/serviceaccounts" result: <nil>
E0322 04:07:43.449074    2297 timeout.go:142] post-timeout activity - time-elapsed: 538.734534ms, GET "/apis/traefik.io/v1alpha1/serverstransports" result: <nil>
I0322 04:07:43.449117    2297 trace.go:236] Trace[1494287774]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:28560b34-3bb7-4732-92be-85ed4c40d5d4,client:192.168.56.111,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 04:07:42.724) (total time: 724ms):
Trace[1494287774]: [724.458352ms] [724.458352ms] END
E0322 04:07:43.449302    2297 timeout.go:142] post-timeout activity - time-elapsed: 555.741152ms, GET "/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/serverworker" result: <nil>
I0322 04:07:43.449425    2297 trace.go:236] Trace[1459903653]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:595f2674-9598-4e89-9c0c-359f118ff252,client:192.168.56.111,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 04:07:42.895) (total time: 553ms):
Trace[1459903653]: [553.517108ms] [553.517108ms] END
E0322 04:07:43.449629    2297 timeout.go:142] post-timeout activity - time-elapsed: 555.741287ms, GET "/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/serverworker" result: <nil>
I0322 04:07:43.449747    2297 trace.go:236] Trace[2058378137]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:4d3193f3-367d-42f8-b433-4afbd0b9efbe,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-7zoch227uv4owxg75pgtnmv3jq,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:PUT (22-Mar-2024 04:07:42.906) (total time: 543ms):
Trace[2058378137]: [543.024247ms] [543.024247ms] END
I0322 04:07:43.449924    2297 trace.go:236] Trace[809518606]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:5666470f-46f7-453b-8144-9d8b7f95bde7,client:192.168.56.111,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 04:05:41.658) (total time: 121791ms):
Trace[809518606]: ---"About to Get from storage" 15335ms (04:05:56.994)
Trace[809518606]: [2m1.791458353s] [2m1.791458353s] END
I0322 04:07:42.937679    2297 trace.go:236] Trace[57445257]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.56.110,type:*v1.Endpoints,resource:apiServerIPInfo (22-Mar-2024 04:04:54.486) (total time: 168450ms):
Trace[57445257]: [2m48.450773043s] [2m48.450773043s] END
E0322 04:07:43.450189    2297 controller.go:164] unable to sync kubernetes service: rpc error: code = Unavailable desc = keepalive ping failed to receive ACK within timeout
time="2024-03-22T04:07:42Z" level=info msg="Slow SQL (started: 2024-03-22 04:06:04.756443167 +0000 UTC m=+740.493409371) (total time: 1m38.234658409s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/traefik.io/middlewares/% false]]"
time="2024-03-22T04:07:43Z" level=error msg="error while range on /registry/traefik.io/middlewares/ /registry/traefik.io/middlewares/: context canceled"
E0322 04:07:43.451320    2297 timeout.go:142] post-timeout activity - time-elapsed: 545.177225ms, PUT "/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-7zoch227uv4owxg75pgtnmv3jq" result: <nil>
E0322 04:07:43.451341    2297 timeout.go:142] post-timeout activity - time-elapsed: 2m34.957583033s, GET "/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/serverworker" result: <nil>
E0322 04:07:42.938498    2297 status.go:71] apiserver received an error that is not an metav1.Status: context.deadlineExceededError{}: context deadline exceeded
E0322 04:07:42.938516    2297 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, context canceled]"
E0322 04:07:43.456144    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:43.456186    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.456228    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
time="2024-03-22T04:07:42Z" level=info msg="Slow SQL (started: 2024-03-22 04:06:42.795139106 +0000 UTC m=+778.532105289) (total time: 1m0.196295993s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/apiextensions.k8s.io/customresourcedefinitions/% 756]]"
time="2024-03-22T04:07:43Z" level=error msg="Failed to list /registry/apiextensions.k8s.io/customresourcedefinitions/ for revision 756: context canceled"
W0322 04:07:42.938728    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/traefikservices?resourceVersion=665": net/http: TLS handshake timeout
I0322 04:07:43.482490    2297 trace.go:236] Trace[2073114406]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:54.343) (total time: 109139ms):
Trace[2073114406]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/traefikservices?resourceVersion=665": net/http: TLS handshake timeout 108595ms (04:07:42.938)
Trace[2073114406]: [1m49.139309038s] [1m49.139309038s] END
E0322 04:07:43.482533    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/traefikservices?resourceVersion=665": net/http: TLS handshake timeout
W0322 04:07:18.076791    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Node: Get "https://127.0.0.1:6443/api/v1/nodes?fieldSelector=metadata.name%3Dserver&resourceVersion=679": dial tcp 127.0.0.1:6443: i/o timeout
I0322 04:07:43.482585    2297 trace.go:236] Trace[813203685]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:10.394) (total time: 153087ms):
Trace[813203685]: ---"Objects listed" error:Get "https://127.0.0.1:6443/api/v1/nodes?fieldSelector=metadata.name%3Dserver&resourceVersion=679": dial tcp 127.0.0.1:6443: i/o timeout 116550ms (04:07:06.945)
Trace[813203685]: [2m33.087688765s] [2m33.087688765s] END
E0322 04:07:43.482836    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://127.0.0.1:6443/api/v1/nodes?fieldSelector=metadata.name%3Dserver&resourceVersion=679": dial tcp 127.0.0.1:6443: i/o timeout
W0322 04:07:42.939251    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PersistentVolumeClaim: Get "https://127.0.0.1:6444/api/v1/persistentvolumeclaims?resourceVersion=665": dial tcp 127.0.0.1:6444: i/o timeout
I0322 04:07:43.482924    2297 trace.go:236] Trace[822023463]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:38.739) (total time: 124743ms):
Trace[822023463]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/persistentvolumeclaims?resourceVersion=665": dial tcp 127.0.0.1:6444: i/o timeout 124199ms (04:07:42.939)
Trace[822023463]: [2m4.743183478s] [2m4.743183478s] END
E0322 04:07:43.482932    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get "https://127.0.0.1:6444/api/v1/persistentvolumeclaims?resourceVersion=665": dial tcp 127.0.0.1:6444: i/o timeout
E0322 04:07:42.943647    2297 status.go:71] apiserver received an error that is not an metav1.Status: context.deadlineExceededError{}: context deadline exceeded
E0322 04:07:42.944408    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:42.945394    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
E0322 04:07:42.945699    2297 controller.go:193] "Failed to update lease" err="Put \"https://127.0.0.1:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/server?timeout=10s\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)"
time="2024-03-22T04:07:43Z" level=error msg="error while range on /registry/serviceaccounts/ /registry/serviceaccounts/: context canceled"
E0322 04:07:43.483520    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.483538    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
W0322 04:07:42.945984    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Role: Get "https://127.0.0.1:6444/apis/rbac.authorization.k8s.io/v1/roles?resourceVersion=667": http2: client connection lost
I0322 04:07:43.483605    2297 trace.go:236] Trace[480547824]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:02.869) (total time: 340614ms):
Trace[480547824]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/rbac.authorization.k8s.io/v1/roles?resourceVersion=667": http2: client connection lost 340076ms (04:07:42.945)
Trace[480547824]: [5m40.614202347s] [5m40.614202347s] END
E0322 04:07:43.483706    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Role: failed to list *v1.Role: Get "https://127.0.0.1:6444/apis/rbac.authorization.k8s.io/v1/roles?resourceVersion=667": http2: client connection lost
E0322 04:07:42.946996    2297 server.go:310] "Unable to authenticate the request due to an error" err="context canceled"
E0322 04:07:42.947050    2297 server.go:310] "Unable to authenticate the request due to an error" err="context canceled"
E0322 04:07:42.980737    2297 finisher.go:175] FinishRequest: post-timeout activity - time-elapsed: 70.922316ms, panicked: false, err: context deadline exceeded, panic-reason: <nil>
W0322 04:07:42.982347    2297 handler_proxy.go:93] no RequestInfo found in the context
E0322 04:07:43.484475    2297 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0322 04:07:43.485336    2297 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
time="2024-03-22T04:07:43Z" level=info msg="Slow SQL (started: 2024-03-22 04:05:51.275740269 +0000 UTC m=+727.012706511) (total time: 1m48.25105426s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/csidrivers/% false]]"
time="2024-03-22T04:07:43Z" level=error msg="error while range on /registry/csidrivers/ /registry/csidrivers/: context canceled"
time="2024-03-22T04:07:43Z" level=info msg="Slow SQL (started: 2024-03-22 04:07:11.405799369 +0000 UTC m=+807.142765559) (total time: 10.070018798s):  SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key' : [[]]"
time="2024-03-22T04:07:43Z" level=error msg="Failed to list /registry/traefik.io/middlewaretcps/ for revision 756: context canceled"
E0322 04:07:42.988049    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:43.499998    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:42.991909    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=665": net/http: TLS handshake timeout
E0322 04:07:42.991966    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
W0322 04:07:42.992768    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://127.0.0.1:6444/api/v1/services?resourceVersion=665": net/http: TLS handshake timeout
I0322 04:07:43.500070    2297 trace.go:236] Trace[1043301521]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:39.737) (total time: 123762ms):
Trace[1043301521]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/services?resourceVersion=665": net/http: TLS handshake timeout 123255ms (04:07:42.992)
Trace[1043301521]: [2m3.762487056s] [2m3.762487056s] END
E0322 04:07:43.500088    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://127.0.0.1:6444/api/v1/services?resourceVersion=665": net/http: TLS handshake timeout
E0322 04:07:42.993866    2297 kubelet.go:2327] "Skipping pod synchronization" err="[container runtime is down, PLEG is not healthy: pleg was last seen active 6m39.847061192s ago; threshold is 3m0s]"
E0322 04:07:42.994176    2297 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, context deadline exceeded]"
E0322 04:07:43.500194    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:42.997227    2297 handler_proxy.go:137] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
I0322 04:07:42.997255    2297 trace.go:236] Trace[1511615006]: "iptables ChainExists" (22-Mar-2024 04:06:23.839) (total time: 73397ms):
Trace[1511615006]: [1m13.397935491s] [1m13.397935491s] END
E0322 04:07:42.999744    2297 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, context deadline exceeded]"
E0322 04:07:43.500291    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:43.000637    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:43.001273    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
E0322 04:07:43.008295    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:43.011933    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:43.013915    2297 timeout.go:142] post-timeout activity - time-elapsed: 5m12.839581292s, POST "/api/v1/nodes" result: <nil>
I0322 04:07:38.978218    2297 trace.go:236] Trace[1912054046]: "Calculate volume metrics of config-volume for pod kube-system/coredns-6799fbcd5-4tsrq" (22-Mar-2024 04:07:09.194) (total time: 11699ms):
Trace[1912054046]: [11.69981375s] [11.69981375s] END
E0322 04:07:43.014124    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://127.0.0.1:6444/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&resourceVersion=708": net/http: TLS handshake timeout
W0322 04:07:43.015871    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.ResourceQuota: Get "https://127.0.0.1:6444/api/v1/resourcequotas?resourceVersion=667": http2: client connection lost
I0322 04:07:43.500461    2297 trace.go:236] Trace[1012843441]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:15.225) (total time: 148274ms):
Trace[1012843441]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/resourcequotas?resourceVersion=667": http2: client connection lost 147790ms (04:07:43.015)
Trace[1012843441]: [2m28.274583807s] [2m28.274583807s] END
E0322 04:07:43.500509    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ResourceQuota: failed to list *v1.ResourceQuota: Get "https://127.0.0.1:6444/api/v1/resourcequotas?resourceVersion=667": http2: client connection lost
E0322 04:07:43.016341    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:43.017258    2297 status.go:71] apiserver received an error that is not an metav1.Status: context.deadlineExceededError{}: context deadline exceeded
E0322 04:07:43.017542    2297 remote_image.go:128] "ListImages with filter from image service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" filter="nil"
E0322 04:07:43.500580    2297 kuberuntime_image.go:103] "Failed to list images" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded"
I0322 04:07:43.500635    2297 image_gc_manager.go:210] "Failed to update image list" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded"
E0322 04:07:43.018451    2297 writers.go:122] apiserver was unable to write a JSON response: client disconnected
E0322 04:07:43.023942    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
W0322 04:07:43.025773    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.ReplicaSet: Get "https://127.0.0.1:6444/apis/apps/v1/replicasets?resourceVersion=717": dial tcp 127.0.0.1:6444: i/o timeout
I0322 04:07:43.500706    2297 trace.go:236] Trace[2093771985]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:42.002) (total time: 121498ms):
Trace[2093771985]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/apps/v1/replicasets?resourceVersion=717": dial tcp 127.0.0.1:6444: i/o timeout 121023ms (04:07:43.025)
Trace[2093771985]: [2m1.49812888s] [2m1.49812888s] END
E0322 04:07:43.500717    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get "https://127.0.0.1:6444/apis/apps/v1/replicasets?resourceVersion=717": dial tcp 127.0.0.1:6444: i/o timeout
E0322 04:07:43.030691    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:43.031804    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
E0322 04:07:43.034052    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:43.035411    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:43.035882    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:43.037327    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
W0322 04:07:43.039008    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.containo.us/v1alpha1/middlewares?resourceVersion=671": dial tcp 127.0.0.1:6444: i/o timeout
I0322 04:07:43.500831    2297 trace.go:236] Trace[588688836]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:39.627) (total time: 123873ms):
Trace[588688836]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/traefik.containo.us/v1alpha1/middlewares?resourceVersion=671": dial tcp 127.0.0.1:6444: i/o timeout 123411ms (04:07:43.038)
Trace[588688836]: [2m3.873567937s] [2m3.873567937s] END
E0322 04:07:43.500841    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.containo.us/v1alpha1/middlewares?resourceVersion=671": dial tcp 127.0.0.1:6444: i/o timeout
E0322 04:07:43.040767    2297 finisher.go:175] FinishRequest: post-timeout activity - time-elapsed: 24.401122ms, panicked: false, err: context deadline exceeded, panic-reason: <nil>
W0322 04:07:43.041421    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/k3s.cattle.io/v1/etcdsnapshotfiles?resourceVersion=665": dial tcp 127.0.0.1:6444: i/o timeout
I0322 04:07:43.500873    2297 trace.go:236] Trace[961299362]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:36.151) (total time: 127349ms):
Trace[961299362]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/k3s.cattle.io/v1/etcdsnapshotfiles?resourceVersion=665": dial tcp 127.0.0.1:6444: i/o timeout 126890ms (04:07:43.041)
Trace[961299362]: [2m7.349554273s] [2m7.349554273s] END
E0322 04:07:43.500920    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/k3s.cattle.io/v1/etcdsnapshotfiles?resourceVersion=665": dial tcp 127.0.0.1:6444: i/o timeout
W0322 04:07:43.041754    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/apiregistration.k8s.io/v1/apiservices?resourceVersion=693": dial tcp 127.0.0.1:6444: i/o timeout
I0322 04:07:43.500944    2297 trace.go:236] Trace[281999077]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:42.187) (total time: 121313ms):
Trace[281999077]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/apiregistration.k8s.io/v1/apiservices?resourceVersion=693": dial tcp 127.0.0.1:6444: i/o timeout 120853ms (04:07:43.041)
Trace[281999077]: [2m1.313069035s] [2m1.313069035s] END
E0322 04:07:43.500989    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/apiregistration.k8s.io/v1/apiservices?resourceVersion=693": dial tcp 127.0.0.1:6444: i/o timeout
E0322 04:07:43.501014    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.501124    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:43.501142    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.501150    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.501218    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:43.501227    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.501241    2297 timeout.go:142] post-timeout activity - time-elapsed: 20.087864224s, GET "/api/v1/namespaces/kube-system/configmaps" result: <nil>
E0322 04:07:43.501311    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.501361    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:43.502032    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
time="2024-03-22T04:07:43Z" level=info msg="Slow SQL (started: 2024-03-22 04:06:39.328578142 +0000 UTC m=+775.065544328) (total time: 44.920914799s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/endpointslices/% 756]]"
time="2024-03-22T04:07:43Z" level=error msg="Failed to list /registry/endpointslices/ for revision 756: context canceled"
E0322 04:07:43.515256    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 04:07:43.515315    2297 trace.go:236] Trace[212647488]: "Get" accept:application/json, */*,audit-id:19cc864a-fea1-4a5f-9434-6a7dc57dc9b7,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 04:05:44.750) (total time: 118765ms):
Trace[212647488]: ---"About to Get from storage" 8192ms (04:05:52.943)
Trace[212647488]: [1m58.765096402s] [1m58.765096402s] END
E0322 04:07:43.515459    2297 timeout.go:142] post-timeout activity - time-elapsed: 5m2.912917487s, GET "/api/v1/nodes/serverworker" result: <nil>
W0322 04:07:43.041874    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/middlewares?resourceVersion=668": net/http: TLS handshake timeout
I0322 04:07:43.515800    2297 trace.go:236] Trace[139003726]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:20.969) (total time: 142546ms):
Trace[139003726]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/middlewares?resourceVersion=668": net/http: TLS handshake timeout 142072ms (04:07:43.041)
Trace[139003726]: [2m22.54635481s] [2m22.54635481s] END
E0322 04:07:43.515837    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.io/v1alpha1/middlewares?resourceVersion=668": net/http: TLS handshake timeout
W0322 04:07:43.042193    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.containo.us/v1alpha1/traefikservices?resourceVersion=667": dial tcp 127.0.0.1:6444: i/o timeout
I0322 04:07:43.515868    2297 trace.go:236] Trace[207737343]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:39.371) (total time: 124144ms):
Trace[207737343]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/traefik.containo.us/v1alpha1/traefikservices?resourceVersion=667": dial tcp 127.0.0.1:6444: i/o timeout 123670ms (04:07:43.042)
Trace[207737343]: [2m4.144021934s] [2m4.144021934s] END
E0322 04:07:43.515929    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.containo.us/v1alpha1/traefikservices?resourceVersion=667": dial tcp 127.0.0.1:6444: i/o timeout
W0322 04:07:43.043264    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Job: Get "https://127.0.0.1:6444/apis/batch/v1/jobs?resourceVersion=665": dial tcp 127.0.0.1:6444: i/o timeout
I0322 04:07:43.515974    2297 trace.go:236] Trace[458180765]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:42.137) (total time: 121378ms):
Trace[458180765]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/batch/v1/jobs?resourceVersion=665": dial tcp 127.0.0.1:6444: i/o timeout 120905ms (04:07:43.043)
Trace[458180765]: [2m1.378383296s] [2m1.378383296s] END
E0322 04:07:43.515983    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Job: failed to list *v1.Job: Get "https://127.0.0.1:6444/apis/batch/v1/jobs?resourceVersion=665": dial tcp 127.0.0.1:6444: i/o timeout
W0322 04:07:43.044564    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://127.0.0.1:6444/api/v1/services?resourceVersion=665": net/http: TLS handshake timeout
I0322 04:07:43.516061    2297 trace.go:236] Trace[1670817225]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:47.161) (total time: 116354ms):
Trace[1670817225]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/services?resourceVersion=665": net/http: TLS handshake timeout 115883ms (04:07:43.044)
Trace[1670817225]: [1m56.354636202s] [1m56.354636202s] END
E0322 04:07:43.516071    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://127.0.0.1:6444/api/v1/services?resourceVersion=665": net/http: TLS handshake timeout
E0322 04:07:43.044690    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:43.050168    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:43.105708    2297 timeout.go:142] post-timeout activity - time-elapsed: 22.368307ms, GET "/apis/traefik.containo.us/v1alpha1/middlewares" result: <nil>
E0322 04:07:43.105995    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.116516    2297 finisher.go:175] FinishRequest: post-timeout activity - time-elapsed: 78.420069ms, panicked: false, err: context deadline exceeded, panic-reason: <nil>
E0322 04:07:43.118939    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
I0322 04:07:25.613590    2297 request.go:697] Waited for 13.18550995s due to client-side throttling, not priority and fairness, request: GET:https://127.0.0.1:6444/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=665
E0322 04:07:43.516423    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:43.516445    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.516455    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.516461    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.517057    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.517176    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.517278    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:43.517294    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"client disconnected"}: client disconnected
E0322 04:07:43.517301    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.517311    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.517384    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:43.517432    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.517448    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.517460    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.517471    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.517542    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.517555    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.517561    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.517567    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
I0322 04:07:43.517824    2297 trace.go:236] Trace[1993264361]: "List" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:54bab70d-3a38-4da1-a3ec-641bf741a247,client:192.168.56.111,protocol:HTTP/2.0,resource:csidrivers,scope:cluster,url:/apis/storage.k8s.io/v1/csidrivers,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 04:07:42.985) (total time: 531ms):
Trace[1993264361]: ---"Writing http response done" count:0 531ms (04:07:43.517)
Trace[1993264361]: [531.823447ms] [531.823447ms] END
time="2024-03-22T04:07:43Z" level=error msg="Failed to list /registry/storageclasses/ for revision 756: context canceled"
E0322 04:07:43.544135    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.544174    2297 timeout.go:142] post-timeout activity - time-elapsed: 19.094843617s, GET "/apis/traefik.io/v1alpha1/middlewaretcps" result: <nil>
E0322 04:07:43.544192    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.544204    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.544219    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 04:07:43.544271    2297 trace.go:236] Trace[1954115643]: "Get" accept:application/json, */*,audit-id:2f48bce2-ac7b-48d3-81b4-56ccf5dc0f8d,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 04:07:42.938) (total time: 605ms):
Trace[1954115643]: [605.919709ms] [605.919709ms] END
E0322 04:07:43.544377    2297 timeout.go:142] post-timeout activity - time-elapsed: 20.25616244s, GET "/api/v1/nodes/serverworker" result: <nil>
E0322 04:07:43.544408    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
I0322 04:07:43.544811    2297 trace.go:236] Trace[1851949819]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:54.296) (total time: 109248ms):
Trace[1851949819]: ---"Objects listed" error:<nil> 109248ms (04:07:43.544)
Trace[1851949819]: [1m49.248196702s] [1m49.248196702s] END
E0322 04:07:43.129863    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.130142    2297 timeout.go:142] post-timeout activity - time-elapsed: 787.479706ms, GET "/api/v1/nodes" result: <nil>
E0322 04:07:43.130155    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.143285    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:43.143656    2297 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0322 04:07:43.145003    2297 finisher.go:175] FinishRequest: post-timeout activity - time-elapsed: 1m41.320381175s, panicked: false, err: context deadline exceeded, panic-reason: <nil>
I0322 04:07:43.145127    2297 trace.go:236] Trace[1147278999]: "iptables ChainExists" (22-Mar-2024 04:07:05.835) (total time: 37309ms):
Trace[1147278999]: [37.309158019s] [37.309158019s] END
W0322 04:07:43.180576    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.containo.us/v1alpha1/ingressroutetcps?resourceVersion=668": net/http: TLS handshake timeout
I0322 04:07:43.545096    2297 trace.go:236] Trace[1450911390]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:39.697) (total time: 123847ms):
Trace[1450911390]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/traefik.containo.us/v1alpha1/ingressroutetcps?resourceVersion=668": net/http: TLS handshake timeout 123483ms (04:07:43.180)
Trace[1450911390]: [2m3.847597679s] [2m3.847597679s] END
E0322 04:07:43.545151    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: Get "https://127.0.0.1:6444/apis/traefik.containo.us/v1alpha1/ingressroutetcps?resourceVersion=668": net/http: TLS handshake timeout
I0322 04:07:43.545219    2297 trace.go:236] Trace[2038041586]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:0dc44329-acdf-4ddc-bb13-1964340fde76,client:127.0.0.1,protocol:HTTP/2.0,resource:priorityclasses,scope:cluster,url:/apis/scheduling.k8s.io/v1/priorityclasses,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 04:07:42.944) (total time: 600ms):
Trace[2038041586]: [600.91186ms] [600.91186ms] END
I0322 04:07:43.545837    2297 trace.go:236] Trace[994393044]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:fcb75751-85af-4872-ad86-b6997936d3ea,client:127.0.0.1,protocol:HTTP/2.0,resource:customresourcedefinitions,scope:cluster,url:/apis/apiextensions.k8s.io/v1/customresourcedefinitions,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 04:07:42.731) (total time: 813ms):
Trace[994393044]: ---"Writing http response done" count:23 805ms (04:07:43.545)
Trace[994393044]: [813.897166ms] [813.897166ms] END
E0322 04:07:43.545905    2297 timeout.go:142] post-timeout activity - time-elapsed: 968.395988ms, GET "/apis/apiextensions.k8s.io/v1/customresourcedefinitions" result: <nil>
E0322 04:07:43.545924    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.545932    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.545981    2297 timeout.go:142] post-timeout activity - time-elapsed: 460.150409ms, GET "/apis/storage.k8s.io/v1/storageclasses" result: <nil>
E0322 04:07:43.545992    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.546113    2297 timeout.go:142] post-timeout activity - time-elapsed: 600.546727ms, GET "/apis/storage.k8s.io/v1/csidrivers" result: <nil>
time="2024-03-22T04:07:43Z" level=info msg="Slow SQL (started: 2024-03-22 04:05:48.301547747 +0000 UTC m=+724.038513941) (total time: 1m54.756647271s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/statefulsets/% false]]"
time="2024-03-22T04:07:43Z" level=error msg="error while range on /registry/statefulsets/ /registry/statefulsets/: context canceled"
E0322 04:07:43.546301    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.546331    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.546455    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.546479    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.546494    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.546503    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.546554    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 04:07:43.546610    2297 trace.go:236] Trace[979606579]: "List" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:6b58b09f-24dd-4f7b-a5b5-545e5644713c,client:192.168.56.111,protocol:HTTP/2.0,resource:pods,scope:cluster,url:/api/v1/pods,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 04:05:19.757) (total time: 143788ms):
Trace[979606579]: ---"About to List from storage" 41468ms (04:06:01.226)
Trace[979606579]: ["cacher list" audit-id:6b58b09f-24dd-4f7b-a5b5-545e5644713c,type:pods 86996ms (04:06:16.549)
Trace[979606579]:  ---"watchCache locked acquired" 11428ms (04:06:27.985)
Trace[979606579]:  ---"watchCache fresh enough" 14533ms (04:06:42.518)]
Trace[979606579]: ---"Listing from storage done" 13381ms (04:07:10.635)
Trace[979606579]: ---"Writing http response done" count:0 32911ms (04:07:43.546)
Trace[979606579]: [2m23.788656206s] [2m23.788656206s] END
E0322 04:07:43.546750    2297 timeout.go:142] post-timeout activity - time-elapsed: 5m7.637191647s, GET "/api/v1/pods" result: <nil>
I0322 04:07:43.546849    2297 trace.go:236] Trace[364751084]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:5442cb1f-24b9-493f-91db-3e6700b66e94,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:POST (22-Mar-2024 04:07:43.003) (total time: 543ms):
Trace[364751084]: [543.247277ms] [543.247277ms] END
E0322 04:07:43.546972    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 04:07:43.547013    2297 trace.go:236] Trace[405170059]: "List" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:243301a4-6774-44ca-965e-ae365925b43c,client:192.168.56.111,protocol:HTTP/2.0,resource:runtimeclasses,scope:cluster,url:/apis/node.k8s.io/v1/runtimeclasses,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 04:07:43.011) (total time: 535ms):
Trace[405170059]: ---"Writing http response done" count:10 535ms (04:07:43.546)
Trace[405170059]: [535.288776ms] [535.288776ms] END
I0322 04:07:43.547232    2297 trace.go:236] Trace[1549600295]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:e57f05c0-0baa-4022-8945-772b2bbf9b5f,client:192.168.56.111,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 04:05:44.903) (total time: 118643ms):
Trace[1549600295]: ---"About to Get from storage" 16489ms (04:06:01.392)
Trace[1549600295]: [1m58.6439791s] [1m58.6439791s] END
E0322 04:07:43.547277    2297 timeout.go:142] post-timeout activity - time-elapsed: 3m5.533941767s, GET "/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/serverworker" result: <nil>
I0322 04:07:43.547301    2297 trace.go:236] Trace[1985014641]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:998bf0ec-f6f4-4f79-8565-5b17d50caaa5,client:192.168.56.111,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/default/events,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:POST (22-Mar-2024 04:07:43.016) (total time: 531ms):
Trace[1985014641]: [531.116612ms] [531.116612ms] END
I0322 04:07:43.547423    2297 trace.go:236] Trace[1306590832]: "List" accept:application/json, */*,audit-id:ad414e75-bc8a-4b8a-9f39-e34e853643c7,client:10.42.0.8,protocol:HTTP/2.0,resource:services,scope:cluster,url:/api/v1/services,user-agent:traefik/2.10.5 (linux/amd64) kubernetes/crd,verb:LIST (22-Mar-2024 04:07:43.018) (total time: 529ms):
Trace[1306590832]: ---"Writing http response done" count:4 529ms (04:07:43.547)
Trace[1306590832]: [529.280817ms] [529.280817ms] END
I0322 04:07:43.547547    2297 trace.go:236] Trace[1986065223]: "List" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:34029f74-857f-4123-ad3d-f95521a10177,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 04:07:43.023) (total time: 523ms):
Trace[1986065223]: ---"Writing http response done" count:0 523ms (04:07:43.547)
Trace[1986065223]: [523.96196ms] [523.96196ms] END
E0322 04:07:43.547657    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.547782    2297 timeout.go:142] post-timeout activity - time-elapsed: 2.576944199s, GET "/apis/traefik.io/v1alpha1/ingressroutes" result: <nil>
E0322 04:07:43.547800    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.547816    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.548006    2297 timeout.go:142] post-timeout activity - time-elapsed: 749.640766ms, GET "/apis/scheduling.k8s.io/v1/priorityclasses" result: <nil>
E0322 04:07:43.548027    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.548040    2297 timeout.go:142] post-timeout activity - time-elapsed: 525.875127ms, GET "/apis/traefik.containo.us/v1alpha1/ingressroutes" result: <nil>
E0322 04:07:43.548060    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.548147    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.548157    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0322 04:07:43.548163    2297 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
time="2024-03-22T04:07:43Z" level=error msg="error while range on /registry/controllers/ /registry/controllers/: context canceled"
I0322 04:07:43.548677    2297 trace.go:236] Trace[111370202]: "List" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1,application/json,audit-id:a2f709ca-c565-4999-b392-2c00ad662993,client:10.42.0.5,protocol:HTTP/2.0,resource:pods,scope:cluster,url:/api/v1/pods,user-agent:metrics-server/v0.6.3 (linux/amd64) kubernetes/a938798,verb:LIST (22-Mar-2024 04:07:43.035) (total time: 513ms):
Trace[111370202]: [513.485909ms] [513.485909ms] END
E0322 04:07:43.548731    2297 timeout.go:142] post-timeout activity - time-elapsed: 534.668617ms, GET "/api/v1/pods" result: <nil>
I0322 04:07:43.553549    2297 trace.go:236] Trace[1919414971]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:57.727) (total time: 105826ms):
Trace[1919414971]: ---"Objects listed" error:<nil> 105826ms (04:07:43.553)
Trace[1919414971]: [1m45.826358922s] [1m45.826358922s] END
I0322 04:07:43.553989    2297 trace.go:236] Trace[923317127]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:38.451) (total time: 125102ms):
Trace[923317127]: ---"Objects listed" error:<nil> 125102ms (04:07:43.553)
Trace[923317127]: [2m5.102334805s] [2m5.102334805s] END
I0322 04:07:43.555681    2297 trace.go:236] Trace[26219820]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:06:31.158) (total time: 72397ms):
Trace[26219820]: ---"Objects listed" error:<nil> 72397ms (04:07:43.555)
Trace[26219820]: [1m12.397156601s] [1m12.397156601s] END
E0322 04:07:43.556476    2297 timeout.go:142] post-timeout activity - time-elapsed: 43.989720577s, POST "/api/v1/nodes" result: <nil>
E0322 04:07:43.556531    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.556542    2297 timeout.go:142] post-timeout activity - time-elapsed: 645.641482ms, GET "/apis/node.k8s.io/v1/runtimeclasses" result: <nil>
E0322 04:07:43.556561    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.556570    2297 timeout.go:142] post-timeout activity - time-elapsed: 20.308644324s, POST "/api/v1/namespaces/default/events" result: <nil>
E0322 04:07:43.556582    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.556589    2297 timeout.go:142] post-timeout activity - time-elapsed: 450.965162ms, GET "/api/v1/services" result: <nil>
E0322 04:07:43.556594    2297 timeout.go:142] post-timeout activity - time-elapsed: 556.346639ms, GET "/api/v1/nodes" result: <nil>
I0322 04:07:43.556617    2297 trace.go:236] Trace[969473551]: "SerializeObject" audit-id:55e58ae3-6861-4fe2-9af4-ae197b27786c,method:GET,url:/api/v1/secrets,protocol:HTTP/2.0,mediaType:application/json,encoder:{"encodeGV":"v1","encoder":"{\"name\":\"json\",\"pretty\":\"false\",\"strict\":\"false\",\"yaml\":\"false\"}","name":"versioning"} (22-Mar-2024 04:07:23.922) (total time: 19634ms):
Trace[969473551]: ---"About to start writing response" size:129 19108ms (04:07:43.030)
Trace[969473551]: [19.634087323s] [19.634087323s] END
E0322 04:07:43.556632    2297 timeout.go:142] post-timeout activity - time-elapsed: 1.053180416s, GET "/api/v1/secrets" result: <nil>
E0322 04:07:43.556644    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 04:07:43.556665    2297 trace.go:236] Trace[340168911]: "List" accept:application/json, */*,audit-id:35993320-8953-41a7-b630-ecfee02ba09e,client:10.42.0.5,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/kube-system/configmaps,user-agent:metrics-server/v0.6.3 (linux/amd64) kubernetes/a938798,verb:GET (22-Mar-2024 04:07:43.032) (total time: 523ms):
Trace[340168911]: ---"Writing http response done" count:1 523ms (04:07:43.556)
Trace[340168911]: [523.729392ms] [523.729392ms] END
E0322 04:07:43.556736    2297 timeout.go:142] post-timeout activity - time-elapsed: 564.989131ms, GET "/api/v1/namespaces/kube-system/configmaps" result: <nil>
time="2024-03-22T04:07:43Z" level=info msg="Slow SQL (started: 2024-03-22 04:06:12.850586382 +0000 UTC m=+748.587552579) (total time: 1m30.235808952s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), COUNT(c.theid) FROM ( SELECT * FROM ( SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv JOIN ( SELECT MAX(mkv.id) AS id FROM kine AS mkv WHERE mkv.name LIKE ? GROUP BY mkv.name) AS maxkv ON maxkv.id = kv.id WHERE kv.deleted = 0 OR ? ) AS lkv ORDER BY lkv.theid ASC ) c : [[/registry/csistoragecapacities/% false]]"
time="2024-03-22T04:07:43Z" level=error msg="error while range on /registry/csistoragecapacities/ /registry/csistoragecapacities/: context canceled"
I0322 04:07:43.557540    2297 trace.go:236] Trace[1052488012]: "List" accept:application/json, */*,audit-id:4c01d2d1-8427-40df-bc31-1881150df017,client:10.42.0.8,protocol:HTTP/2.0,resource:endpoints,scope:cluster,url:/api/v1/endpoints,user-agent:traefik/2.10.5 (linux/amd64) kubernetes/ingress,verb:LIST (22-Mar-2024 04:07:43.050) (total time: 507ms):
Trace[1052488012]: [507.489954ms] [507.489954ms] END
I0322 04:07:43.557691    2297 trace.go:236] Trace[245721321]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:dc7fd796-8869-4477-8496-5115f8bfe86f,client:192.168.56.111,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 04:07:43.031) (total time: 526ms):
Trace[245721321]: [526.112518ms] [526.112518ms] END
I0322 04:07:43.558248    2297 trace.go:236] Trace[960398158]: "List" accept:application/json, */*,audit-id:baf19569-1ec2-45f1-8985-35b3134fff4b,client:10.42.0.2,protocol:HTTP/2.0,resource:persistentvolumeclaims,scope:cluster,url:/api/v1/persistentvolumeclaims,user-agent:local-path-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:LIST (22-Mar-2024 04:07:43.035) (total time: 522ms):
Trace[960398158]: ---"Writing http response done" count:0 522ms (04:07:43.558)
Trace[960398158]: [522.483422ms] [522.483422ms] END
E0322 04:07:43.558321    2297 timeout.go:142] post-timeout activity - time-elapsed: 522.419286ms, GET "/api/v1/persistentvolumeclaims" result: <nil>
I0322 04:07:43.558344    2297 trace.go:236] Trace[713841964]: "List" accept:application/json, */*,audit-id:9db31c88-315c-4662-be54-eae2c4e5a106,client:192.168.56.111,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/default/endpoints,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 04:07:43.036) (total time: 521ms):
Trace[713841964]: [521.394008ms] [521.394008ms] END
E0322 04:07:43.558441    2297 timeout.go:142] post-timeout activity - time-elapsed: 542.360385ms, GET "/api/v1/namespaces/default/endpoints" result: <nil>
I0322 04:07:43.558475    2297 trace.go:236] Trace[1756495229]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:1bea6191-45bb-40c6-a964-ad8cc6c350d2,client:192.168.56.111,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 04:07:42.945) (total time: 613ms):
Trace[1756495229]: [613.220477ms] [613.220477ms] END
E0322 04:07:43.558552    2297 timeout.go:142] post-timeout activity - time-elapsed: 721.480157ms, GET "/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/serverworker" result: <nil>
I0322 04:07:43.559012    2297 trace.go:236] Trace[381973242]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:2d85349e-32b0-49d8-8013-8c996c5cdbb1,client:192.168.56.111,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 04:05:53.354) (total time: 110204ms):
Trace[381973242]: ---"About to Get from storage" 24553ms (04:06:17.907)
Trace[381973242]: [1m50.204743339s] [1m50.204743339s] END
E0322 04:07:43.559064    2297 timeout.go:142] post-timeout activity - time-elapsed: 2m36.441042805s, GET "/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/serverworker" result: <nil>
I0322 04:07:43.559177    2297 trace.go:236] Trace[1576167180]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:645e5c43-12d2-49f2-a9b9-4fbfde87e5f6,client:127.0.0.1,protocol:HTTP/2.0,resource:roles,scope:cluster,url:/apis/rbac.authorization.k8s.io/v1/roles,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:LIST (22-Mar-2024 04:07:42.768) (total time: 790ms):
Trace[1576167180]: ---"Writing http response done" count:7 790ms (04:07:43.559)
Trace[1576167180]: [790.807782ms] [790.807782ms] END
E0322 04:07:43.559264    2297 timeout.go:142] post-timeout activity - time-elapsed: 980.716056ms, GET "/apis/rbac.authorization.k8s.io/v1/roles" result: <nil>
E0322 04:07:43.559348    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0322 04:07:43.559367    2297 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0322 04:07:43.559769    2297 trace.go:236] Trace[1707154831]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:07:18.643) (total time: 24916ms):
Trace[1707154831]: ---"Objects listed" error:<nil> 24916ms (04:07:43.559)
Trace[1707154831]: [24.916084253s] [24.916084253s] END
I0322 04:07:43.560129    2297 trace.go:236] Trace[1236079123]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:45.431) (total time: 118128ms):
Trace[1236079123]: ---"Objects listed" error:<nil> 118128ms (04:07:43.560)
Trace[1236079123]: [1m58.128537696s] [1m58.128537696s] END
I0322 04:07:43.561738    2297 trace.go:236] Trace[802354127]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:42.165) (total time: 121395ms):
Trace[802354127]: ---"Objects listed" error:<nil> 121395ms (04:07:43.561)
Trace[802354127]: [2m1.395743682s] [2m1.395743682s] END
I0322 04:07:43.562773    2297 trace.go:236] Trace[1892520063]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:29.615) (total time: 133947ms):
Trace[1892520063]: ---"Objects listed" error:<nil> 133947ms (04:07:43.562)
Trace[1892520063]: [2m13.947727911s] [2m13.947727911s] END
I0322 04:07:43.563518    2297 trace.go:236] Trace[613859694]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:52.133) (total time: 111430ms):
Trace[613859694]: ---"Objects listed" error:<nil> 111430ms (04:07:43.563)
Trace[613859694]: [1m51.430294606s] [1m51.430294606s] END
time="2024-03-22T04:07:43Z" level=info msg="Slow SQL (started: 2024-03-22 04:06:27.289607842 +0000 UTC m=+763.026574082) (total time: 1m15.839051477s):  SELECT ( SELECT MAX(rkv.id) AS id FROM kine AS rkv), ( SELECT MAX(crkv.prev_revision) AS prev_revision FROM kine AS crkv WHERE crkv.name = 'compact_rev_key'), kv.id AS theid, kv.name, kv.created, kv.deleted, kv.create_revision, kv.prev_revision, kv.lease, kv.value, kv.old_value FROM kine AS kv WHERE kv.name LIKE ? AND kv.id > ? ORDER BY kv.id ASC : [[/registry/csidrivers/% 756]]"
time="2024-03-22T04:07:43Z" level=error msg="Failed to list /registry/csidrivers/ for revision 756: context canceled"
E0322 04:07:43.581603    2297 timeout.go:142] post-timeout activity - time-elapsed: 553.674471ms, GET "/api/v1/endpoints" result: <nil>
E0322 04:07:43.581720    2297 timeout.go:142] post-timeout activity - time-elapsed: 735.225739ms, GET "/apis/apiregistration.k8s.io/v1/apiservices" result: <nil>
I0322 04:07:43.581761    2297 trace.go:236] Trace[1201548891]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:8118828e-4321-4761-b9dc-45baa41f5ef7,client:192.168.56.111,protocol:HTTP/2.0,resource:csinodes,scope:resource,url:/apis/storage.k8s.io/v1/csinodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 04:07:43.017) (total time: 564ms):
Trace[1201548891]: [564.656181ms] [564.656181ms] END
E0322 04:07:43.581859    2297 timeout.go:142] post-timeout activity - time-elapsed: 1m34.733594579s, GET "/apis/storage.k8s.io/v1/csinodes/serverworker" result: <nil>
E0322 04:07:43.581938    2297 timeout.go:142] post-timeout activity - time-elapsed: 1m9.093775324s, GET "/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/serverworker" result: <nil>
E0322 04:07:43.582687    2297 timeout.go:142] post-timeout activity - time-elapsed: 31.456909505s, GET "/apis/traefik.containo.us/v1alpha1/middlewaretcps" result: <nil>
I0322 04:07:43.582734    2297 trace.go:236] Trace[650212877]: "List" accept:application/json, */*,audit-id:313753ae-1751-4a9e-94c7-1d53a42b1a0a,client:10.42.0.8,protocol:HTTP/2.0,resource:ingresses,scope:cluster,url:/apis/networking.k8s.io/v1/ingresses,user-agent:traefik/2.10.5 (linux/amd64) kubernetes/ingress,verb:LIST (22-Mar-2024 04:07:43.067) (total time: 515ms):
Trace[650212877]: ---"Writing http response done" count:0 515ms (04:07:43.582)
Trace[650212877]: [515.114171ms] [515.114171ms] END
E0322 04:07:43.583066    2297 timeout.go:142] post-timeout activity - time-elapsed: 520.469105ms, GET "/apis/networking.k8s.io/v1/ingresses" result: <nil>
I0322 04:07:43.587148    2297 trace.go:236] Trace[1612408206]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:06:21.280) (total time: 82307ms):
Trace[1612408206]: ---"Objects listed" error:<nil> 82307ms (04:07:43.587)
Trace[1612408206]: [1m22.307073753s] [1m22.307073753s] END
I0322 04:07:43.587447    2297 trace.go:236] Trace[670688065]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:06:17.702) (total time: 85884ms):
Trace[670688065]: ---"Objects listed" error:<nil> 85884ms (04:07:43.587)
Trace[670688065]: [1m25.884625139s] [1m25.884625139s] END
E0322 04:07:43.613996    2297 kubelet.go:1402] "Container garbage collection failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded"
I0322 04:07:43.618341    2297 trace.go:236] Trace[1969104371]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:06:20.702) (total time: 82916ms):
Trace[1969104371]: ---"Objects listed" error:<nil> 82916ms (04:07:43.618)
Trace[1969104371]: [1m22.916053015s] [1m22.916053015s] END
I0322 04:07:43.618671    2297 trace.go:236] Trace[1899379200]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:39.579) (total time: 124039ms):
Trace[1899379200]: ---"Objects listed" error:<nil> 124039ms (04:07:43.618)
Trace[1899379200]: [2m4.039621483s] [2m4.039621483s] END
I0322 04:07:43.619362    2297 trace.go:236] Trace[1766749300]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:42.660) (total time: 120958ms):
Trace[1766749300]: ---"Objects listed" error:<nil> 120958ms (04:07:43.619)
Trace[1766749300]: [2m0.958436176s] [2m0.958436176s] END
I0322 04:07:43.619617    2297 trace.go:236] Trace[1678281383]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:06:01.501) (total time: 102117ms):
Trace[1678281383]: ---"Objects listed" error:<nil> 102117ms (04:07:43.619)
Trace[1678281383]: [1m42.117961762s] [1m42.117961762s] END
I0322 04:07:43.619820    2297 trace.go:236] Trace[1003751724]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:52.781) (total time: 110838ms):
Trace[1003751724]: ---"Objects listed" error:<nil> 110838ms (04:07:43.619)
Trace[1003751724]: [1m50.838435252s] [1m50.838435252s] END
I0322 04:07:43.620550    2297 trace.go:236] Trace[895651024]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:40.971) (total time: 122648ms):
Trace[895651024]: ---"Objects listed" error:<nil> 122648ms (04:07:43.619)
Trace[895651024]: [2m2.648989544s] [2m2.648989544s] END
I0322 04:07:43.620825    2297 trace.go:236] Trace[1361497979]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:39.900) (total time: 123720ms):
Trace[1361497979]: ---"Objects listed" error:<nil> 123720ms (04:07:43.620)
Trace[1361497979]: [2m3.720724479s] [2m3.720724479s] END
I0322 04:07:43.621007    2297 trace.go:236] Trace[556323072]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:06:01.847) (total time: 101773ms):
Trace[556323072]: ---"Objects listed" error:<nil> 101773ms (04:07:43.620)
Trace[556323072]: [1m41.773461749s] [1m41.773461749s] END
I0322 04:07:43.621202    2297 trace.go:236] Trace[817897429]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:39.781) (total time: 123839ms):
Trace[817897429]: ---"Objects listed" error:<nil> 123839ms (04:07:43.621)
Trace[817897429]: [2m3.839742105s] [2m3.839742105s] END
I0322 04:07:43.621447    2297 trace.go:236] Trace[743797624]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:42.636) (total time: 120985ms):
Trace[743797624]: ---"Objects listed" error:<nil> 120985ms (04:07:43.621)
Trace[743797624]: [2m0.985231659s] [2m0.985231659s] END
I0322 04:07:43.621668    2297 trace.go:236] Trace[11294666]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:06:00.886) (total time: 102734ms):
Trace[11294666]: ---"Objects listed" error:<nil> 102734ms (04:07:43.621)
Trace[11294666]: [1m42.734785644s] [1m42.734785644s] END
I0322 04:07:43.621845    2297 trace.go:236] Trace[519292248]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:38.479) (total time: 125142ms):
Trace[519292248]: ---"Objects listed" error:<nil> 125142ms (04:07:43.621)
Trace[519292248]: [2m5.142521752s] [2m5.142521752s] END
I0322 04:07:43.622033    2297 trace.go:236] Trace[1314210006]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:39.981) (total time: 123640ms):
Trace[1314210006]: ---"Objects listed" error:<nil> 123640ms (04:07:43.622)
Trace[1314210006]: [2m3.640137968s] [2m3.640137968s] END
I0322 04:07:43.622263    2297 trace.go:236] Trace[1378217132]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:06:00.301) (total time: 103321ms):
Trace[1378217132]: ---"Objects listed" error:<nil> 103321ms (04:07:43.622)
Trace[1378217132]: [1m43.321058003s] [1m43.321058003s] END
I0322 04:07:43.622454    2297 trace.go:236] Trace[1201067989]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:39.426) (total time: 124196ms):
Trace[1201067989]: ---"Objects listed" error:<nil> 124196ms (04:07:43.622)
Trace[1201067989]: [2m4.196168607s] [2m4.196168607s] END
I0322 04:07:43.622649    2297 trace.go:236] Trace[511010323]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:06:20.787) (total time: 82835ms):
Trace[511010323]: ---"Objects listed" error:<nil> 82835ms (04:07:43.622)
Trace[511010323]: [1m22.835222156s] [1m22.835222156s] END
I0322 04:07:43.622822    2297 trace.go:236] Trace[398635669]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:39.501) (total time: 124121ms):
Trace[398635669]: ---"Objects listed" error:<nil> 124121ms (04:07:43.622)
Trace[398635669]: [2m4.121525313s] [2m4.121525313s] END
I0322 04:07:43.622974    2297 trace.go:236] Trace[1599688540]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:07:04.239) (total time: 39383ms):
Trace[1599688540]: ---"Objects listed" error:<nil> 39383ms (04:07:43.622)
Trace[1599688540]: [39.38335349s] [39.38335349s] END
I0322 04:07:43.623191    2297 trace.go:236] Trace[1831129869]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:46.889) (total time: 116733ms):
Trace[1831129869]: ---"Objects listed" error:<nil> 116733ms (04:07:43.623)
Trace[1831129869]: [1m56.733398304s] [1m56.733398304s] END
I0322 04:07:43.623363    2297 trace.go:236] Trace[352297533]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:37.053) (total time: 126569ms):
Trace[352297533]: ---"Objects listed" error:<nil> 126569ms (04:07:43.623)
Trace[352297533]: [2m6.569439961s] [2m6.569439961s] END
I0322 04:07:43.623641    2297 trace.go:236] Trace[753083515]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:41.808) (total time: 121814ms):
Trace[753083515]: ---"Objects listed" error:<nil> 121814ms (04:07:43.623)
Trace[753083515]: [2m1.81491247s] [2m1.81491247s] END
I0322 04:07:43.623880    2297 trace.go:236] Trace[1366966573]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:31.331) (total time: 132291ms):
Trace[1366966573]: ---"Objects listed" error:<nil> 132291ms (04:07:43.623)
Trace[1366966573]: [2m12.291871098s] [2m12.291871098s] END
I0322 04:07:43.624054    2297 trace.go:236] Trace[430728329]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:39.815) (total time: 123808ms):
Trace[430728329]: ---"Objects listed" error:<nil> 123808ms (04:07:43.624)
Trace[430728329]: [2m3.808526518s] [2m3.808526518s] END
I0322 04:07:43.624274    2297 trace.go:236] Trace[781014455]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:39.855) (total time: 123768ms):
Trace[781014455]: ---"Objects listed" error:<nil> 123768ms (04:07:43.624)
Trace[781014455]: [2m3.768897413s] [2m3.768897413s] END
I0322 04:07:43.624563    2297 trace.go:236] Trace[1086211517]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:42.134) (total time: 121490ms):
Trace[1086211517]: ---"Objects listed" error:<nil> 121490ms (04:07:43.624)
Trace[1086211517]: [2m1.490475737s] [2m1.490475737s] END
I0322 04:07:43.624770    2297 trace.go:236] Trace[2054001036]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:38.817) (total time: 124806ms):
Trace[2054001036]: ---"Objects listed" error:<nil> 124806ms (04:07:43.624)
Trace[2054001036]: [2m4.806884814s] [2m4.806884814s] END
I0322 04:07:43.625266    2297 trace.go:236] Trace[1670974007]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:06:38.749) (total time: 64875ms):
Trace[1670974007]: ---"Objects listed" error:<nil> 64875ms (04:07:43.625)
Trace[1670974007]: [1m4.875553375s] [1m4.875553375s] END
I0322 04:07:43.625584    2297 trace.go:236] Trace[1081545619]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:06:52.491) (total time: 51133ms):
Trace[1081545619]: ---"Objects listed" error:<nil> 51133ms (04:07:43.625)
Trace[1081545619]: [51.133661127s] [51.133661127s] END
I0322 04:07:43.625793    2297 trace.go:236] Trace[1157175124]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:39.933) (total time: 123692ms):
Trace[1157175124]: ---"Objects listed" error:<nil> 123692ms (04:07:43.625)
Trace[1157175124]: [2m3.692275951s] [2m3.692275951s] END
I0322 04:07:43.625970    2297 trace.go:236] Trace[5753614]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:06:38.199) (total time: 65426ms):
Trace[5753614]: ---"Objects listed" error:<nil> 65426ms (04:07:43.625)
Trace[5753614]: [1m5.426686967s] [1m5.426686967s] END
I0322 04:07:43.626147    2297 trace.go:236] Trace[1867396461]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:06:06.839) (total time: 96786ms):
Trace[1867396461]: ---"Objects listed" error:<nil> 96786ms (04:07:43.626)
Trace[1867396461]: [1m36.786868962s] [1m36.786868962s] END
I0322 04:07:43.626297    2297 trace.go:236] Trace[487182638]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:06:15.444) (total time: 88181ms):
Trace[487182638]: ---"Objects listed" error:<nil> 88181ms (04:07:43.626)
Trace[487182638]: [1m28.181521182s] [1m28.181521182s] END
I0322 04:07:43.626486    2297 trace.go:236] Trace[822023520]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:51.780) (total time: 111845ms):
Trace[822023520]: ---"Objects listed" error:<nil> 111845ms (04:07:43.626)
Trace[822023520]: [1m51.845798036s] [1m51.845798036s] END
I0322 04:07:43.626758    2297 trace.go:236] Trace[2121284986]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:06:01.499) (total time: 102126ms):
Trace[2121284986]: ---"Objects listed" error:<nil> 102126ms (04:07:43.626)
Trace[2121284986]: [1m42.126862379s] [1m42.126862379s] END
I0322 04:07:43.626989    2297 trace.go:236] Trace[1430592794]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:06:17.297) (total time: 86329ms):
Trace[1430592794]: ---"Objects listed" error:<nil> 86329ms (04:07:43.626)
Trace[1430592794]: [1m26.329799214s] [1m26.329799214s] END
I0322 04:07:43.627151    2297 trace.go:236] Trace[1907131606]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:06:08.127) (total time: 95499ms):
Trace[1907131606]: ---"Objects listed" error:<nil> 95499ms (04:07:43.627)
Trace[1907131606]: [1m35.499226532s] [1m35.499226532s] END
I0322 04:07:43.627314    2297 trace.go:236] Trace[918857094]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:06:18.630) (total time: 84997ms):
Trace[918857094]: ---"Objects listed" error:<nil> 84997ms (04:07:43.627)
Trace[918857094]: [1m24.997033905s] [1m24.997033905s] END
I0322 04:07:43.627497    2297 trace.go:236] Trace[1667113518]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:06:03.408) (total time: 100219ms):
Trace[1667113518]: ---"Objects listed" error:<nil> 100219ms (04:07:43.627)
Trace[1667113518]: [1m40.219126261s] [1m40.219126261s] END
I0322 04:07:43.627727    2297 trace.go:236] Trace[637264380]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:06:38.166) (total time: 65461ms):
Trace[637264380]: ---"Objects listed" error:<nil> 65461ms (04:07:43.627)
Trace[637264380]: [1m5.461523653s] [1m5.461523653s] END
I0322 04:07:43.627985    2297 trace.go:236] Trace[1773627028]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:44.906) (total time: 118721ms):
Trace[1773627028]: ---"Objects listed" error:<nil> 118721ms (04:07:43.627)
Trace[1773627028]: [1m58.721235636s] [1m58.721235636s] END
I0322 04:07:43.628245    2297 trace.go:236] Trace[1258703231]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:46.016) (total time: 117611ms):
Trace[1258703231]: ---"Objects listed" error:<nil> 117611ms (04:07:43.628)
Trace[1258703231]: [1m57.611723611s] [1m57.611723611s] END
I0322 04:07:43.628973    2297 trace.go:236] Trace[815049437]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:06:46.183) (total time: 57445ms):
Trace[815049437]: ---"Objects listed" error:<nil> 57445ms (04:07:43.628)
Trace[815049437]: [57.445623441s] [57.445623441s] END
I0322 04:07:43.632784    2297 trace.go:236] Trace[1218138077]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:59.873) (total time: 103759ms):
Trace[1218138077]: ---"Objects listed" error:<nil> 103759ms (04:07:43.632)
Trace[1218138077]: [1m43.759488719s] [1m43.759488719s] END
I0322 04:07:43.633153    2297 trace.go:236] Trace[608065193]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:06:42.652) (total time: 60980ms):
Trace[608065193]: ---"Objects listed" error:<nil> 60980ms (04:07:43.633)
Trace[608065193]: [1m0.980760134s] [1m0.980760134s] END
I0322 04:07:43.633886    2297 trace.go:236] Trace[1735245385]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:50.396) (total time: 113237ms):
Trace[1735245385]: ---"Objects listed" error:<nil> 113237ms (04:07:43.633)
Trace[1735245385]: [1m53.237018221s] [1m53.237018221s] END
I0322 04:07:43.634238    2297 trace.go:236] Trace[783512247]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:06:32.620) (total time: 71014ms):
Trace[783512247]: ---"Objects listed" error:<nil> 71013ms (04:07:43.634)
Trace[783512247]: [1m11.014017037s] [1m11.014017037s] END
I0322 04:07:43.637639    2297 trace.go:236] Trace[1332161410]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:39.546) (total time: 124091ms):
Trace[1332161410]: ---"Objects listed" error:<nil> 124091ms (04:07:43.637)
Trace[1332161410]: [2m4.091513094s] [2m4.091513094s] END
I0322 04:07:43.639305    2297 trace.go:236] Trace[223333760]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:06:35.394) (total time: 68244ms):
Trace[223333760]: ---"Objects listed" error:<nil> 68244ms (04:07:43.639)
Trace[223333760]: [1m8.244372404s] [1m8.244372404s] END
I0322 04:07:43.644990    2297 trace.go:236] Trace[688832878]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:07:03.894) (total time: 39750ms):
Trace[688832878]: ---"Objects listed" error:<nil> 39750ms (04:07:43.644)
Trace[688832878]: [39.750145341s] [39.750145341s] END
I0322 04:07:43.645638    2297 trace.go:236] Trace[1767359817]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:06:36.724) (total time: 66921ms):
Trace[1767359817]: ---"Objects listed" error:<nil> 66921ms (04:07:43.645)
Trace[1767359817]: [1m6.921611681s] [1m6.921611681s] END
I0322 04:07:43.646095    2297 trace.go:236] Trace[1214635538]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:42.578) (total time: 121067ms):
Trace[1214635538]: ---"Objects listed" error:<nil> 121067ms (04:07:43.646)
Trace[1214635538]: [2m1.067095261s] [2m1.067095261s] END
I0322 04:07:43.678798    2297 trace.go:236] Trace[870770719]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:06:32.626) (total time: 71051ms):
Trace[870770719]: ---"Objects listed" error:<nil> 71051ms (04:07:43.678)
Trace[870770719]: [1m11.05189441s] [1m11.05189441s] END
W0322 04:07:43.679197    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Addon: Get "https://127.0.0.1:6444/apis/k3s.cattle.io/v1/addons?resourceVersion=665": net/http: TLS handshake timeout
I0322 04:07:43.679355    2297 trace.go:236] Trace[1782804838]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:43.506) (total time: 300172ms):
Trace[1782804838]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/k3s.cattle.io/v1/addons?resourceVersion=665": net/http: TLS handshake timeout 300172ms (04:07:43.679)
Trace[1782804838]: [5m0.172449419s] [5m0.172449419s] END
E0322 04:07:43.679522    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Addon: failed to list *v1.Addon: Get "https://127.0.0.1:6444/apis/k3s.cattle.io/v1/addons?resourceVersion=665": net/http: TLS handshake timeout
W0322 04:07:43.679703    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.ClusterRoleBinding: Get "https://127.0.0.1:6444/apis/rbac.authorization.k8s.io/v1/clusterrolebindings?resourceVersion=667": net/http: TLS handshake timeout
I0322 04:07:43.679829    2297 trace.go:236] Trace[1951758909]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:03:04.577) (total time: 279101ms):
Trace[1951758909]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/rbac.authorization.k8s.io/v1/clusterrolebindings?resourceVersion=667": net/http: TLS handshake timeout 279101ms (04:07:43.679)
Trace[1951758909]: [4m39.101875355s] [4m39.101875355s] END
E0322 04:07:43.679908    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ClusterRoleBinding: failed to list *v1.ClusterRoleBinding: Get "https://127.0.0.1:6444/apis/rbac.authorization.k8s.io/v1/clusterrolebindings?resourceVersion=667": net/http: TLS handshake timeout
I0322 04:07:43.726089    2297 trace.go:236] Trace[809726279]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:06:20.734) (total time: 82991ms):
Trace[809726279]: ---"Objects listed" error:<nil> 82991ms (04:07:43.726)
Trace[809726279]: [1m22.991562141s] [1m22.991562141s] END
I0322 04:07:43.734751    2297 garbagecollector.go:816] "failed to discover some groups" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"
I0322 04:07:43.726792    2297 trace.go:236] Trace[2103284554]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:45.623) (total time: 118102ms):
Trace[2103284554]: ---"Objects listed" error:<nil> 118102ms (04:07:43.726)
Trace[2103284554]: [1m58.102841725s] [1m58.102841725s] END
I0322 04:07:43.726830    2297 trace.go:236] Trace[127874005]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:58.285) (total time: 105441ms):
Trace[127874005]: ---"Objects listed" error:<nil> 105441ms (04:07:43.726)
Trace[127874005]: [1m45.441227099s] [1m45.441227099s] END
I0322 04:07:43.726896    2297 trace.go:236] Trace[1727188507]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:53.757) (total time: 109969ms):
Trace[1727188507]: ---"Objects listed" error:<nil> 109969ms (04:07:43.726)
Trace[1727188507]: [1m49.969704054s] [1m49.969704054s] END
I0322 04:07:43.727159    2297 trace.go:236] Trace[2096091239]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:05:42.008) (total time: 121719ms):
Trace[2096091239]: ---"Objects listed" error:<nil> 121719ms (04:07:43.727)
Trace[2096091239]: [2m1.719128021s] [2m1.719128021s] END
I0322 04:07:44.080908    2297 trace.go:236] Trace[1404819096]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.56.110,type:*v1.Endpoints,resource:apiServerIPInfo (22-Mar-2024 04:07:43.450) (total time: 629ms):
Trace[1404819096]: ---"initial value restored" 605ms (04:07:44.056)
Trace[1404819096]: [629.885739ms] [629.885739ms] END
I0322 04:07:44.733596    2297 trace.go:236] Trace[1244452717]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:ed468e4e-03ca-420a-a9e3-00872db047c9,client:192.168.56.111,protocol:HTTP/2.0,resource:csinodes,scope:resource,url:/apis/storage.k8s.io/v1/csinodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 04:07:43.238) (total time: 1495ms):
Trace[1244452717]: [1.495526354s] [1.495526354s] END
I0322 04:07:44.773016    2297 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0322 04:07:44.773338    2297 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0322 04:07:45.164678    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="46.022465ms"
I0322 04:07:45.164671    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="287.187s"
I0322 04:07:45.165127    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="42.430237ms"
I0322 04:07:45.165732    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="614.735s"
I0322 04:07:45.367853    2297 trace.go:236] Trace[1787045726]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:a20141b7-afed-4cc9-a09a-60c2eef38e53,client:127.0.0.1,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/default/endpoints/kubernetes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 04:07:44.082) (total time: 1285ms):
Trace[1787045726]: ---"About to write a response" 1284ms (04:07:45.367)
Trace[1787045726]: [1.28504786s] [1.28504786s] END
E0322 04:07:46.435544    2297 handler_proxy.go:137] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
I0322 04:07:46.776877    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="58.799s"
I0322 04:07:46.790916    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="58.315s"
I0322 04:07:47.022246    2297 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/traefik-f4564c4f4-d2b8m" containerName="traefik"
I0322 04:07:47.038414    2297 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/coredns-6799fbcd5-4tsrq" containerName="coredns"
I0322 04:07:47.049767    2297 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/metrics-server-67c658944b-6v69k" containerName="metrics-server"
I0322 04:07:47.297200    2297 scope.go:117] "RemoveContainer" containerID="dc08329bb326840153edd7095108a9e41fe4c4e85dee454d915edfaa94b6ff0d"
I0322 04:07:47.347903    2297 scope.go:117] "RemoveContainer" containerID="61d7fde647a627694f95932590ebf135feb434a150daf01220d491ed7230c443"
I0322 04:07:47.364314    2297 scope.go:117] "RemoveContainer" containerID="288e3d733a7ad05f7708717be6a74d4db5ec51fc68316f03757dbd762548a192"
I0322 04:07:47.517002    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="132.129954ms"
I0322 04:07:47.523551    2297 trace.go:236] Trace[791602706]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:e674036f-d46f-4b78-9be6-7a29f4b902b0,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 04:07:44.739) (total time: 2782ms):
Trace[791602706]: [2.782762243s] [2.782762243s] END
I0322 04:07:47.524834    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="279.648s"
I0322 04:07:47.536201    2297 trace.go:236] Trace[1665774402]: "Get" accept:application/json, */*,audit-id:91194024-2163-4284-8c21-45162d2172c2,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 04:07:43.191) (total time: 4338ms):
Trace[1665774402]: [4.338316265s] [4.338316265s] END
I0322 04:07:47.565854    2297 trace.go:236] Trace[1893555886]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:0fce22c4-81a8-4f83-9be9-50ebd5693af0,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:POST (22-Mar-2024 04:07:45.882) (total time: 1683ms):
Trace[1893555886]: ["Create etcd3" audit-id:0fce22c4-81a8-4f83-9be9-50ebd5693af0,key:/minions/serverworker,type:*core.Node,resource:nodes 1679ms (04:07:45.886)
Trace[1893555886]:  ---"Txn call succeeded" 1673ms (04:07:47.560)]
Trace[1893555886]: [1.683786363s] [1.683786363s] END
I0322 04:07:47.571805    2297 trace.go:236] Trace[1759501075]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:0740aa40-c19b-4076-960b-1ad00e916202,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 04:07:45.177) (total time: 2393ms):
Trace[1759501075]: [2.393878402s] [2.393878402s] END
I0322 04:07:47.573392    2297 trace.go:236] Trace[1636977767]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:497ae8da-993e-4bcc-a604-68dd3bf985c9,client:192.168.56.111,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/serverworker,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 04:07:44.052) (total time: 3521ms):
Trace[1636977767]: [3.521040229s] [3.521040229s] END
I0322 04:07:47.575999    2297 trace.go:236] Trace[2080059399]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:d644e63c-56a1-46a8-b9a1-77d0bca3a443,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/server,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b,verb:GET (22-Mar-2024 04:07:43.436) (total time: 4139ms):
Trace[2080059399]: ---"About to write a response" 4137ms (04:07:47.574)
Trace[2080059399]: [4.139087645s] [4.139087645s] END
I0322 04:07:47.599306    2297 trace.go:236] Trace[1505256812]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:276322c7-3cac-4392-9748-6c654f2c08da,client:127.0.0.1,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/server/status,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:node-controller,verb:PUT (22-Mar-2024 04:07:43.439) (total time: 4159ms):
Trace[1505256812]: ---"limitedReadBody succeeded" len:4863 37ms (04:07:43.477)
Trace[1505256812]: ["GuaranteedUpdate etcd3" audit-id:276322c7-3cac-4392-9748-6c654f2c08da,key:/minions/server,type:*core.Node,resource:nodes 4121ms (04:07:43.477)
Trace[1505256812]:  ---"Txn call completed" 4118ms (04:07:47.597)]
Trace[1505256812]: [4.159643211s] [4.159643211s] END
I0322 04:07:47.644076    2297 node_controller.go:431] Initializing node serverworker with cloud provider
E0322 04:07:47.647622    2297 node_controller.go:240] error syncing 'serverworker': failed to get instance metadata for node serverworker: address annotations not yet set, requeuing
I0322 04:07:47.638025    2297 actual_state_of_world.go:547] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"serverworker\" does not exist"
I0322 04:07:47.668708    2297 node_controller.go:431] Initializing node serverworker with cloud provider
E0322 04:07:47.669190    2297 node_controller.go:240] error syncing 'serverworker': failed to get instance metadata for node serverworker: address annotations not yet set, requeuing
I0322 04:07:47.675031    2297 event.go:307] "Event occurred" object="server" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="NodeNotReady" message="Node server status is now: NodeNotReady"
I0322 04:07:47.679214    2297 node_controller.go:431] Initializing node serverworker with cloud provider
E0322 04:07:47.679492    2297 node_controller.go:240] error syncing 'serverworker': failed to get instance metadata for node serverworker: address annotations not yet set, requeuing
I0322 04:07:47.699515    2297 node_controller.go:431] Initializing node serverworker with cloud provider
E0322 04:07:47.699751    2297 node_controller.go:240] error syncing 'serverworker': failed to get instance metadata for node serverworker: address annotations not yet set, requeuing
I0322 04:07:47.743846    2297 node_controller.go:431] Initializing node serverworker with cloud provider
E0322 04:07:47.744165    2297 node_controller.go:240] error syncing 'serverworker': failed to get instance metadata for node serverworker: address annotations not yet set, requeuing
I0322 04:07:47.747709    2297 node_controller.go:431] Initializing node serverworker with cloud provider
E0322 04:07:47.748450    2297 node_controller.go:240] error syncing 'serverworker': failed to get instance metadata for node serverworker: address annotations not yet set, requeuing
I0322 04:07:47.777294    2297 event.go:307] "Event occurred" object="kube-system/local-path-provisioner-6c86858495-hxd2h" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0322 04:07:47.798335    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="235.792873ms"
I0322 04:07:47.802164    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="386.285s"
I0322 04:07:47.824196    2297 node_controller.go:431] Initializing node serverworker with cloud provider
E0322 04:07:47.839980    2297 node_controller.go:240] error syncing 'serverworker': failed to get instance metadata for node serverworker: address annotations not yet set, requeuing
I0322 04:07:47.836810    2297 range_allocator.go:380] "Set node PodCIDR" node="serverworker" podCIDRs=["10.42.1.0/24"]
I0322 04:07:47.855107    2297 node_controller.go:431] Initializing node serverworker with cloud provider
E0322 04:07:47.855159    2297 node_controller.go:240] error syncing 'serverworker': failed to get instance metadata for node serverworker: address annotations not yet set, requeuing
I0322 04:07:47.856981    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="34.727283ms"
I0322 04:07:47.857058    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="46.929s"
I0322 04:07:47.862937    2297 node_lifecycle_controller.go:1029] "Controller detected that all Nodes are not-Ready. Entering master disruption mode"
I0322 04:07:47.863325    2297 event.go:307] "Event occurred" object="kube-system/svclb-traefik-2c133370-x894w" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0322 04:07:47.879532    2297 node_controller.go:431] Initializing node serverworker with cloud provider
E0322 04:07:47.882899    2297 node_controller.go:240] error syncing 'serverworker': failed to get instance metadata for node serverworker: address annotations not yet set, requeuing
I0322 04:07:48.160054    2297 node_controller.go:431] Initializing node serverworker with cloud provider
E0322 04:07:48.160516    2297 node_controller.go:240] error syncing 'serverworker': failed to get instance metadata for node serverworker: address annotations not yet set, requeuing
I0322 04:07:48.167776    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="11.677464ms"
I0322 04:07:48.168096    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="53.47s"
I0322 04:07:48.501432    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="53.044652ms"
I0322 04:07:48.501873    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="373.33s"
I0322 04:07:48.522690    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="469.515s"
I0322 04:07:49.637422    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="50.942723ms"
I0322 04:07:49.637574    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="98.632s"
I0322 04:07:50.735336    2297 node_controller.go:431] Initializing node serverworker with cloud provider
E0322 04:07:50.746820    2297 node_controller.go:240] error syncing 'serverworker': failed to get instance metadata for node serverworker: address annotations not yet set, requeuing
E0322 04:07:51.748036    2297 resource_quota_controller.go:440] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I0322 04:07:52.881449    2297 event.go:307] "Event occurred" object="serverworker" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node serverworker event: Registered Node serverworker in Controller"
I0322 04:07:52.884430    2297 node_lifecycle_controller.go:877] "Missing timestamp for Node. Assuming now as a timestamp" node="serverworker"
I0322 04:07:55.866217    2297 node_controller.go:431] Initializing node serverworker with cloud provider
E0322 04:07:55.867366    2297 node_controller.go:240] error syncing 'serverworker': failed to get instance metadata for node serverworker: address annotations not yet set, requeuing
W0322 04:07:57.563992    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Pod: Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/pods?resourceVersion=708": net/http: TLS handshake timeout
I0322 04:07:57.565505    2297 trace.go:236] Trace[1615884357]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:03.796) (total time: 353767ms):
Trace[1615884357]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/pods?resourceVersion=708": net/http: TLS handshake timeout 353765ms (04:07:57.562)
Trace[1615884357]: [5m53.767674267s] [5m53.767674267s] END
E0322 04:07:57.565931    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://127.0.0.1:6444/api/v1/namespaces/kube-system/pods?resourceVersion=708": net/http: TLS handshake timeout
W0322 04:07:57.597727    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Node: Get "https://127.0.0.1:6444/api/v1/nodes?resourceVersion=679": dial tcp 127.0.0.1:6444: i/o timeout
I0322 04:07:57.598396    2297 trace.go:236] Trace[1128996371]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:02.314) (total time: 355283ms):
Trace[1128996371]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/nodes?resourceVersion=679": dial tcp 127.0.0.1:6444: i/o timeout 355283ms (04:07:57.597)
Trace[1128996371]: [5m55.283407361s] [5m55.283407361s] END
E0322 04:07:57.598807    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://127.0.0.1:6444/api/v1/nodes?resourceVersion=679": dial tcp 127.0.0.1:6444: i/o timeout
W0322 04:07:57.666759    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Job: Get "https://127.0.0.1:6444/apis/batch/v1/jobs?resourceVersion=665": net/http: TLS handshake timeout
I0322 04:07:57.667454    2297 trace.go:236] Trace[855762120]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:03:02.387) (total time: 295279ms):
Trace[855762120]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/batch/v1/jobs?resourceVersion=665": net/http: TLS handshake timeout 295278ms (04:07:57.666)
Trace[855762120]: [4m55.27920377s] [4m55.27920377s] END
E0322 04:07:57.667964    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Job: failed to list *v1.Job: Get "https://127.0.0.1:6444/apis/batch/v1/jobs?resourceVersion=665": net/http: TLS handshake timeout
W0322 04:07:57.851856    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://127.0.0.1:6444/apis/discovery.k8s.io/v1/endpointslices?resourceVersion=715": dial tcp 127.0.0.1:6444: i/o timeout
I0322 04:07:57.852550    2297 trace.go:236] Trace[2081415328]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:02.949) (total time: 354903ms):
Trace[2081415328]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/discovery.k8s.io/v1/endpointslices?resourceVersion=715": dial tcp 127.0.0.1:6444: i/o timeout 354902ms (04:07:57.851)
Trace[2081415328]: [5m54.90302715s] [5m54.90302715s] END
E0322 04:07:57.852918    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://127.0.0.1:6444/apis/discovery.k8s.io/v1/endpointslices?resourceVersion=715": dial tcp 127.0.0.1:6444: i/o timeout
W0322 04:07:57.939327    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.DaemonSet: Get "https://127.0.0.1:6444/apis/apps/v1/namespaces/kube-system/daemonsets?resourceVersion=667": dial tcp 127.0.0.1:6444: i/o timeout
I0322 04:07:57.939776    2297 trace.go:236] Trace[512579419]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:01:58.102) (total time: 359837ms):
Trace[512579419]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/apps/v1/namespaces/kube-system/daemonsets?resourceVersion=667": dial tcp 127.0.0.1:6444: i/o timeout 359837ms (04:07:57.939)
Trace[512579419]: [5m59.837461248s] [5m59.837461248s] END
E0322 04:07:57.940248    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.DaemonSet: failed to list *v1.DaemonSet: Get "https://127.0.0.1:6444/apis/apps/v1/namespaces/kube-system/daemonsets?resourceVersion=667": dial tcp 127.0.0.1:6444: i/o timeout
W0322 04:07:57.941237    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.HelmChartConfig: Get "https://127.0.0.1:6444/apis/helm.cattle.io/v1/helmchartconfigs?resourceVersion=668": dial tcp 127.0.0.1:6444: i/o timeout
I0322 04:07:57.941551    2297 trace.go:236] Trace[860967478]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:21.067) (total time: 336874ms):
Trace[860967478]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/helm.cattle.io/v1/helmchartconfigs?resourceVersion=668": dial tcp 127.0.0.1:6444: i/o timeout 336873ms (04:07:57.941)
Trace[860967478]: [5m36.874084005s] [5m36.874084005s] END
E0322 04:07:57.941884    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.HelmChartConfig: failed to list *v1.HelmChartConfig: Get "https://127.0.0.1:6444/apis/helm.cattle.io/v1/helmchartconfigs?resourceVersion=668": dial tcp 127.0.0.1:6444: i/o timeout
W0322 04:07:57.945577    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.HelmChart: Get "https://127.0.0.1:6444/apis/helm.cattle.io/v1/helmcharts?resourceVersion=665": net/http: TLS handshake timeout
I0322 04:07:57.946029    2297 trace.go:236] Trace[611314201]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:59.784) (total time: 298161ms):
Trace[611314201]: ---"Objects listed" error:Get "https://127.0.0.1:6444/apis/helm.cattle.io/v1/helmcharts?resourceVersion=665": net/http: TLS handshake timeout 298161ms (04:07:57.945)
Trace[611314201]: [4m58.161730544s] [4m58.161730544s] END
E0322 04:07:57.946227    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.HelmChart: failed to list *v1.HelmChart: Get "https://127.0.0.1:6444/apis/helm.cattle.io/v1/helmcharts?resourceVersion=665": net/http: TLS handshake timeout
W0322 04:07:58.021385    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Secret: Get "https://127.0.0.1:6444/api/v1/secrets?resourceVersion=675": dial tcp 127.0.0.1:6444: i/o timeout
I0322 04:07:58.021538    2297 trace.go:236] Trace[69514271]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:18.592) (total time: 339429ms):
Trace[69514271]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/secrets?resourceVersion=675": dial tcp 127.0.0.1:6444: i/o timeout 339429ms (04:07:58.021)
Trace[69514271]: [5m39.429375281s] [5m39.429375281s] END
E0322 04:07:58.021574    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Secret: failed to list *v1.Secret: Get "https://127.0.0.1:6444/api/v1/secrets?resourceVersion=675": dial tcp 127.0.0.1:6444: i/o timeout
W0322 04:07:58.024586    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.ConfigMap: Get "https://127.0.0.1:6444/api/v1/configmaps?resourceVersion=665": net/http: TLS handshake timeout
I0322 04:07:58.024734    2297 trace.go:236] Trace[1574104558]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:41.076) (total time: 316948ms):
Trace[1574104558]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/configmaps?resourceVersion=665": net/http: TLS handshake timeout 316948ms (04:07:58.024)
Trace[1574104558]: [5m16.94845141s] [5m16.94845141s] END
E0322 04:07:58.024933    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://127.0.0.1:6444/api/v1/configmaps?resourceVersion=665": net/http: TLS handshake timeout
W0322 04:07:58.028716    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.Node: Get "https://127.0.0.1:6444/api/v1/nodes?resourceVersion=679": net/http: TLS handshake timeout
I0322 04:07:58.029018    2297 trace.go:236] Trace[2081380691]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:03:02.775) (total time: 295253ms):
Trace[2081380691]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/nodes?resourceVersion=679": net/http: TLS handshake timeout 295252ms (04:07:58.028)
Trace[2081380691]: [4m55.2531026s] [4m55.2531026s] END
E0322 04:07:58.029135    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://127.0.0.1:6444/api/v1/nodes?resourceVersion=679": net/http: TLS handshake timeout
W0322 04:07:58.031151    2297 reflector.go:535] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: failed to list *v1.ServiceAccount: Get "https://127.0.0.1:6444/api/v1/serviceaccounts?resourceVersion=665": net/http: TLS handshake timeout
I0322 04:07:58.031437    2297 trace.go:236] Trace[1298583086]: "Reflector ListAndWatch" name:k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229 (22-Mar-2024 04:02:37.055) (total time: 320976ms):
Trace[1298583086]: ---"Objects listed" error:Get "https://127.0.0.1:6444/api/v1/serviceaccounts?resourceVersion=665": net/http: TLS handshake timeout 320975ms (04:07:58.031)
Trace[1298583086]: [5m20.976127227s] [5m20.976127227s] END
E0322 04:07:58.031686    2297 reflector.go:147] k8s.io/client-go@v1.28.7-k3s1/tools/cache/reflector.go:229: Failed to watch *v1.ServiceAccount: failed to list *v1.ServiceAccount: Get "https://127.0.0.1:6444/api/v1/serviceaccounts?resourceVersion=665": net/http: TLS handshake timeout
I0322 04:07:58.099865    2297 topologycache.go:237] "Can't get CPU or zone information for node" node="serverworker"
I0322 04:07:58.100004    2297 node_controller.go:431] Initializing node serverworker with cloud provider
E0322 04:07:58.100096    2297 node_controller.go:240] error syncing 'serverworker': failed to get instance metadata for node serverworker: address annotations not yet set, requeuing
I0322 04:07:58.113080    2297 node_controller.go:431] Initializing node serverworker with cloud provider
E0322 04:07:58.113192    2297 node_controller.go:240] error syncing 'serverworker': failed to get instance metadata for node serverworker: address annotations not yet set, requeuing
time="2024-03-22T04:07:58Z" level=info msg="Updated coredns node hosts entry [192.168.56.111 serverworker]"
I0322 04:07:58.895925    2297 node_controller.go:431] Initializing node serverworker with cloud provider
E0322 04:07:58.896059    2297 node_controller.go:240] error syncing 'serverworker': failed to get instance metadata for node serverworker: address annotations not yet set, requeuing
I0322 04:07:59.238868    2297 event.go:307] "Event occurred" object="kube-system/traefik-crd" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik-crd"
I0322 04:07:59.239019    2297 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
time="2024-03-22T04:07:59Z" level=info msg="Updating TLS secret for kube-system/k3s-serving (count: 10): map[listener.cattle.io/cn-10.43.0.1:10.43.0.1 listener.cattle.io/cn-127.0.0.1:127.0.0.1 listener.cattle.io/cn-192.168.56.110:192.168.56.110 listener.cattle.io/cn-__1-f16284:::1 listener.cattle.io/cn-kubernetes:kubernetes listener.cattle.io/cn-kubernetes.default:kubernetes.default listener.cattle.io/cn-kubernetes.default.svc:kubernetes.default.svc listener.cattle.io/cn-kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local listener.cattle.io/cn-localhost:localhost listener.cattle.io/cn-server:server listener.cattle.io/fingerprint:SHA1=57817D08CEF13EAC5FA6EA63398833E2B858E1D0]"
I0322 04:08:00.295954    2297 node_controller.go:431] Initializing node serverworker with cloud provider
I0322 04:08:00.305922    2297 node_controller.go:502] Successfully initialized node serverworker with cloud provider
I0322 04:08:00.308069    2297 event.go:307] "Event occurred" object="serverworker" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="Synced" message="Node synced successfully"
I0322 04:08:00.332974    2297 event.go:307] "Event occurred" object="kube-system/svclb-traefik-2c133370" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: svclb-traefik-2c133370-gbs2d"
I0322 04:08:01.340841    2297 kube.go:482] Creating the node lease for IPv4. This is the n.Spec.PodCIDRs: [10.42.1.0/24]
I0322 04:08:01.342243    2297 subnet.go:160] Batch elem [0] is { lease.Event{Type:0, Lease:lease.Lease{EnableIPv4:true, EnableIPv6:false, Subnet:ip.IP4Net{IP:0xa2a0100, PrefixLen:0x18}, IPv6Subnet:ip.IP6Net{IP:(*ip.IP6)(nil), PrefixLen:0x0}, Attrs:lease.LeaseAttrs{PublicIP:0xa00020f, PublicIPv6:(*ip.IP6)(nil), BackendType:"vxlan", BackendData:json.RawMessage{0x7b, 0x22, 0x56, 0x4e, 0x49, 0x22, 0x3a, 0x31, 0x2c, 0x22, 0x56, 0x74, 0x65, 0x70, 0x4d, 0x41, 0x43, 0x22, 0x3a, 0x22, 0x61, 0x36, 0x3a, 0x33, 0x63, 0x3a, 0x66, 0x39, 0x3a, 0x30, 0x35, 0x3a, 0x38, 0x66, 0x3a, 0x39, 0x64, 0x22, 0x7d}, BackendV6Data:json.RawMessage(nil)}, Expiration:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Asof:0}} }
I0322 04:08:02.888816    2297 node_lifecycle_controller.go:1048] "Controller detected that some Nodes are Ready. Exiting master disruption mode"
I0322 04:08:05.681481    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="35.358502ms"
I0322 04:08:05.683683    2297 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="130s"
I0322 04:08:05.850643    2297 handler.go:275] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0322 04:08:10.257732    2297 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Service" apiVersion="v1" type="Normal" reason="UpdatedLoadBalancer" message="Updated LoadBalancer with new IPs: [192.168.56.110] -> [192.168.56.110 192.168.56.111]"
time="2024-03-22T04:08:44Z" level=info msg="COMPACT revision 0 has already been compacted"
time="2024-03-22T04:13:44Z" level=info msg="COMPACT compactRev=0 targetCompactRev=30 currentRev=1030"
time="2024-03-22T04:13:44Z" level=info msg="COMPACT deleted 0 rows from 30 revisions in 8.073955ms - compacted to 30/1030"
time="2024-03-22T04:18:44Z" level=info msg="COMPACT compactRev=30 targetCompactRev=150 currentRev=1150"
time="2024-03-22T04:18:44Z" level=info msg="COMPACT deleted 14 rows from 120 revisions in 21.12783ms - compacted to 150/1150"
time="2024-03-22T04:22:44Z" level=info msg="error in remotedialer server [400]: websocket: close 1006 (abnormal closure): unexpected EOF"
I0322 04:23:23.869123    2297 event.go:307] "Event occurred" object="serverworker" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="NodeNotReady" message="Node serverworker status is now: NodeNotReady"
I0322 04:23:23.888824    2297 event.go:307] "Event occurred" object="kube-system/svclb-traefik-2c133370-gbs2d" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0322 04:23:23.936979    2297 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Service" apiVersion="v1" type="Normal" reason="UpdatedLoadBalancer" message="Updated LoadBalancer with new IPs: [192.168.56.110 192.168.56.111] -> [192.168.56.110]"
time="2024-03-22T04:23:44Z" level=info msg="COMPACT compactRev=150 targetCompactRev=273 currentRev=1273"
time="2024-03-22T04:23:44Z" level=info msg="COMPACT deleted 21 rows from 123 revisions in 23.18966ms - compacted to 273/1273"
time="2024-03-22T04:28:44Z" level=info msg="COMPACT compactRev=273 targetCompactRev=364 currentRev=1364"
time="2024-03-22T04:28:44Z" level=info msg="COMPACT deleted 26 rows from 91 revisions in 25.248013ms - compacted to 364/1364"
time="2024-03-22T04:33:44Z" level=info msg="COMPACT compactRev=364 targetCompactRev=453 currentRev=1453"
time="2024-03-22T04:33:44Z" level=info msg="COMPACT deleted 33 rows from 89 revisions in 11.688872ms - compacted to 453/1453"
time="2024-03-22T04:38:44Z" level=info msg="COMPACT compactRev=453 targetCompactRev=543 currentRev=1543"
time="2024-03-22T04:38:44Z" level=info msg="COMPACT deleted 52 rows from 90 revisions in 180.56354ms - compacted to 543/1543"
time="2024-03-22T04:43:44Z" level=info msg="COMPACT compactRev=543 targetCompactRev=632 currentRev=1632"
time="2024-03-22T04:43:44Z" level=info msg="COMPACT deleted 57 rows from 89 revisions in 79.675159ms - compacted to 632/1632"
time="2024-03-22T04:48:44Z" level=info msg="COMPACT compactRev=632 targetCompactRev=720 currentRev=1720"
time="2024-03-22T04:48:44Z" level=info msg="COMPACT deleted 67 rows from 88 revisions in 76.88353ms - compacted to 720/1720"
time="2024-03-22T04:53:44Z" level=info msg="COMPACT compactRev=720 targetCompactRev=810 currentRev=1810"
time="2024-03-22T04:53:44Z" level=info msg="COMPACT deleted 72 rows from 90 revisions in 65.6972ms - compacted to 810/1810"
time="2024-03-22T04:58:44Z" level=info msg="COMPACT compactRev=810 targetCompactRev=983 currentRev=1983"
time="2024-03-22T04:58:44Z" level=info msg="COMPACT deleted 153 rows from 173 revisions in 20.524628ms - compacted to 983/1983"
time="2024-03-22T05:03:44Z" level=info msg="COMPACT compactRev=983 targetCompactRev=1094 currentRev=2094"
time="2024-03-22T05:03:44Z" level=info msg="COMPACT deleted 111 rows from 111 revisions in 25.19197ms - compacted to 1094/2094"
E0322 06:09:38.027351    2297 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
I0322 06:09:38.035463    2297 garbagecollector.go:818] "failed to discover preferred resources" error="the server has asked for the client to provide credentials"
E0322 06:09:46.153323    2297 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0322 06:09:46.163738    2297 resource_quota_controller.go:440] failed to discover resources: the server has asked for the client to provide credentials
time="2024-03-22T06:13:35Z" level=info msg="COMPACT compactRev=1094 targetCompactRev=1190 currentRev=2190"
time="2024-03-22T06:13:35Z" level=info msg="COMPACT deleted 96 rows from 96 revisions in 13.546492ms - compacted to 1190/2190"
time="2024-03-22T06:18:35Z" level=info msg="COMPACT compactRev=1190 targetCompactRev=1308 currentRev=2308"
time="2024-03-22T06:18:35Z" level=info msg="COMPACT deleted 115 rows from 118 revisions in 20.594656ms - compacted to 1308/2308"
time="2024-03-22T06:23:35Z" level=info msg="COMPACT compactRev=1308 targetCompactRev=1397 currentRev=2397"
time="2024-03-22T06:23:35Z" level=info msg="COMPACT deleted 89 rows from 89 revisions in 19.501057ms - compacted to 1397/2397"
time="2024-03-22T06:28:35Z" level=info msg="COMPACT compactRev=1397 targetCompactRev=1487 currentRev=2487"
time="2024-03-22T06:28:35Z" level=info msg="COMPACT deleted 90 rows from 90 revisions in 12.527433ms - compacted to 1487/2487"
time="2024-03-22T06:33:35Z" level=info msg="COMPACT compactRev=1487 targetCompactRev=1580 currentRev=2580"
time="2024-03-22T06:33:35Z" level=info msg="COMPACT deleted 93 rows from 93 revisions in 16.987507ms - compacted to 1580/2580"
time="2024-03-22T06:38:35Z" level=info msg="COMPACT compactRev=1580 targetCompactRev=1669 currentRev=2669"
time="2024-03-22T06:38:35Z" level=info msg="COMPACT deleted 89 rows from 89 revisions in 43.389692ms - compacted to 1669/2669"
time="2024-03-22T06:43:35Z" level=info msg="COMPACT compactRev=1669 targetCompactRev=1759 currentRev=2759"
time="2024-03-22T06:43:35Z" level=info msg="COMPACT deleted 90 rows from 90 revisions in 50.608304ms - compacted to 1759/2759"
time="2024-03-22T06:48:35Z" level=info msg="COMPACT compactRev=1759 targetCompactRev=1849 currentRev=2849"
time="2024-03-22T06:48:35Z" level=info msg="COMPACT deleted 108 rows from 90 revisions in 60.411851ms - compacted to 1849/2849"
time="2024-03-22T06:53:35Z" level=info msg="COMPACT compactRev=1849 targetCompactRev=1938 currentRev=2938"
time="2024-03-22T06:53:35Z" level=info msg="COMPACT deleted 154 rows from 89 revisions in 12.652551ms - compacted to 1938/2938"
time="2024-03-22T06:58:35Z" level=info msg="COMPACT compactRev=1938 targetCompactRev=2028 currentRev=3028"
time="2024-03-22T06:58:35Z" level=info msg="COMPACT deleted 90 rows from 90 revisions in 15.26194ms - compacted to 2028/3028"
time="2024-03-22T07:03:35Z" level=info msg="COMPACT compactRev=2028 targetCompactRev=2118 currentRev=3118"
time="2024-03-22T07:03:35Z" level=info msg="COMPACT deleted 112 rows from 90 revisions in 59.05241ms - compacted to 2118/3118"
time="2024-03-22T07:08:35Z" level=info msg="COMPACT compactRev=2118 targetCompactRev=2207 currentRev=3207"
time="2024-03-22T07:08:35Z" level=info msg="COMPACT deleted 107 rows from 89 revisions in 6.521415ms - compacted to 2207/3207"
time="2024-03-22T07:13:35Z" level=info msg="COMPACT compactRev=2207 targetCompactRev=2296 currentRev=3296"
time="2024-03-22T07:13:35Z" level=info msg="COMPACT deleted 105 rows from 89 revisions in 16.078189ms - compacted to 2296/3296"
time="2024-03-22T07:18:35Z" level=info msg="COMPACT compactRev=2296 targetCompactRev=2387 currentRev=3387"
time="2024-03-22T07:18:35Z" level=info msg="COMPACT deleted 91 rows from 91 revisions in 1.840722ms - compacted to 2387/3387"
time="2024-03-22T07:23:35Z" level=info msg="COMPACT compactRev=2387 targetCompactRev=2476 currentRev=3476"
time="2024-03-22T07:23:35Z" level=info msg="COMPACT deleted 89 rows from 89 revisions in 58.902528ms - compacted to 2476/3476"
time="2024-03-22T07:28:35Z" level=info msg="COMPACT compactRev=2476 targetCompactRev=2565 currentRev=3565"
time="2024-03-22T07:28:35Z" level=info msg="COMPACT deleted 92 rows from 89 revisions in 14.749735ms - compacted to 2565/3565"
time="2024-03-22T07:33:35Z" level=info msg="COMPACT compactRev=2565 targetCompactRev=2656 currentRev=3656"
time="2024-03-22T07:33:35Z" level=info msg="COMPACT deleted 91 rows from 91 revisions in 15.189624ms - compacted to 2656/3656"
time="2024-03-22T07:38:35Z" level=info msg="COMPACT compactRev=2656 targetCompactRev=2745 currentRev=3745"
time="2024-03-22T07:38:35Z" level=info msg="COMPACT deleted 89 rows from 89 revisions in 57.738024ms - compacted to 2745/3745"
time="2024-03-22T07:43:35Z" level=info msg="COMPACT compactRev=2745 targetCompactRev=2835 currentRev=3835"
time="2024-03-22T07:43:35Z" level=info msg="COMPACT deleted 90 rows from 90 revisions in 8.523503ms - compacted to 2835/3835"
time="2024-03-22T07:48:35Z" level=info msg="COMPACT compactRev=2835 targetCompactRev=2925 currentRev=3925"
time="2024-03-22T07:48:35Z" level=info msg="COMPACT deleted 90 rows from 90 revisions in 7.573238ms - compacted to 2925/3925"
time="2024-03-22T07:53:35Z" level=info msg="COMPACT compactRev=2925 targetCompactRev=3014 currentRev=4014"
time="2024-03-22T07:53:35Z" level=info msg="COMPACT deleted 89 rows from 89 revisions in 10.886442ms - compacted to 3014/4014"
time="2024-03-22T07:58:35Z" level=info msg="COMPACT compactRev=3014 targetCompactRev=3104 currentRev=4104"
time="2024-03-22T07:58:35Z" level=info msg="COMPACT deleted 90 rows from 90 revisions in 7.929268ms - compacted to 3104/4104"
time="2024-03-22T08:03:35Z" level=info msg="COMPACT compactRev=3104 targetCompactRev=3194 currentRev=4194"
time="2024-03-22T08:03:35Z" level=info msg="COMPACT deleted 90 rows from 90 revisions in 23.086256ms - compacted to 3194/4194"
time="2024-03-22T08:08:35Z" level=info msg="COMPACT compactRev=3194 targetCompactRev=3283 currentRev=4283"
time="2024-03-22T08:08:35Z" level=info msg="COMPACT deleted 89 rows from 89 revisions in 8.428406ms - compacted to 3283/4283"
time="2024-03-22T08:13:35Z" level=info msg="COMPACT compactRev=3283 targetCompactRev=3373 currentRev=4373"
time="2024-03-22T08:13:35Z" level=info msg="COMPACT deleted 90 rows from 90 revisions in 7.949945ms - compacted to 3373/4373"
time="2024-03-22T08:18:35Z" level=info msg="COMPACT compactRev=3373 targetCompactRev=3463 currentRev=4463"
time="2024-03-22T08:18:35Z" level=info msg="COMPACT deleted 90 rows from 90 revisions in 7.099965ms - compacted to 3463/4463"
time="2024-03-22T08:23:35Z" level=info msg="COMPACT compactRev=3463 targetCompactRev=3552 currentRev=4552"
time="2024-03-22T08:23:35Z" level=info msg="COMPACT deleted 89 rows from 89 revisions in 7.286464ms - compacted to 3552/4552"
time="2024-03-22T08:28:35Z" level=info msg="COMPACT compactRev=3552 targetCompactRev=3642 currentRev=4642"
time="2024-03-22T08:28:35Z" level=info msg="COMPACT deleted 90 rows from 90 revisions in 2.427763ms - compacted to 3642/4642"
time="2024-03-25T05:31:10Z" level=info msg="Starting k3s v1.28.7+k3s1 (051b14b2)"
time="2024-03-25T05:31:10Z" level=info msg="Configuring sqlite3 database connection pooling: maxIdleConns=2, maxOpenConns=0, connMaxLifetime=0s"
time="2024-03-25T05:31:10Z" level=info msg="Configuring database table schema and indexes, this may take a moment..."
time="2024-03-25T05:31:10Z" level=info msg="Database tables and indexes are up to date"
time="2024-03-25T05:31:10Z" level=info msg="Kine available at unix://kine.sock"
time="2024-03-25T05:31:10Z" level=info msg="generated self-signed CA certificate CN=k3s-client-ca@1711344670: notBefore=2024-03-25 05:31:10.057377142 +0000 UTC notAfter=2034-03-23 05:31:10.057377142 +0000 UTC"
time="2024-03-25T05:31:10Z" level=info msg="certificate CN=system:admin,O=system:masters signed by CN=k3s-client-ca@1711344670: notBefore=2024-03-25 05:31:10 +0000 UTC notAfter=2025-03-25 05:31:10 +0000 UTC"
time="2024-03-25T05:31:10Z" level=info msg="certificate CN=system:k3s-supervisor,O=system:masters signed by CN=k3s-client-ca@1711344670: notBefore=2024-03-25 05:31:10 +0000 UTC notAfter=2025-03-25 05:31:10 +0000 UTC"
time="2024-03-25T05:31:10Z" level=info msg="certificate CN=system:kube-controller-manager signed by CN=k3s-client-ca@1711344670: notBefore=2024-03-25 05:31:10 +0000 UTC notAfter=2025-03-25 05:31:10 +0000 UTC"
time="2024-03-25T05:31:10Z" level=info msg="certificate CN=system:kube-scheduler signed by CN=k3s-client-ca@1711344670: notBefore=2024-03-25 05:31:10 +0000 UTC notAfter=2025-03-25 05:31:10 +0000 UTC"
time="2024-03-25T05:31:10Z" level=info msg="certificate CN=system:apiserver,O=system:masters signed by CN=k3s-client-ca@1711344670: notBefore=2024-03-25 05:31:10 +0000 UTC notAfter=2025-03-25 05:31:10 +0000 UTC"
time="2024-03-25T05:31:10Z" level=info msg="certificate CN=system:kube-proxy signed by CN=k3s-client-ca@1711344670: notBefore=2024-03-25 05:31:10 +0000 UTC notAfter=2025-03-25 05:31:10 +0000 UTC"
time="2024-03-25T05:31:10Z" level=info msg="certificate CN=system:k3s-controller signed by CN=k3s-client-ca@1711344670: notBefore=2024-03-25 05:31:10 +0000 UTC notAfter=2025-03-25 05:31:10 +0000 UTC"
time="2024-03-25T05:31:10Z" level=info msg="certificate CN=k3s-cloud-controller-manager signed by CN=k3s-client-ca@1711344670: notBefore=2024-03-25 05:31:10 +0000 UTC notAfter=2025-03-25 05:31:10 +0000 UTC"
time="2024-03-25T05:31:10Z" level=info msg="generated self-signed CA certificate CN=k3s-server-ca@1711344670: notBefore=2024-03-25 05:31:10.062343912 +0000 UTC notAfter=2034-03-23 05:31:10.062343912 +0000 UTC"
time="2024-03-25T05:31:10Z" level=info msg="certificate CN=kube-apiserver signed by CN=k3s-server-ca@1711344670: notBefore=2024-03-25 05:31:10 +0000 UTC notAfter=2025-03-25 05:31:10 +0000 UTC"
time="2024-03-25T05:31:10Z" level=info msg="generated self-signed CA certificate CN=k3s-request-header-ca@1711344670: notBefore=2024-03-25 05:31:10.063402538 +0000 UTC notAfter=2034-03-23 05:31:10.063402538 +0000 UTC"
time="2024-03-25T05:31:10Z" level=info msg="certificate CN=system:auth-proxy signed by CN=k3s-request-header-ca@1711344670: notBefore=2024-03-25 05:31:10 +0000 UTC notAfter=2025-03-25 05:31:10 +0000 UTC"
time="2024-03-25T05:31:10Z" level=info msg="generated self-signed CA certificate CN=etcd-server-ca@1711344670: notBefore=2024-03-25 05:31:10.064367865 +0000 UTC notAfter=2034-03-23 05:31:10.064367865 +0000 UTC"
time="2024-03-25T05:31:10Z" level=info msg="certificate CN=etcd-client signed by CN=etcd-server-ca@1711344670: notBefore=2024-03-25 05:31:10 +0000 UTC notAfter=2025-03-25 05:31:10 +0000 UTC"
time="2024-03-25T05:31:10Z" level=info msg="generated self-signed CA certificate CN=etcd-peer-ca@1711344670: notBefore=2024-03-25 05:31:10.065332001 +0000 UTC notAfter=2034-03-23 05:31:10.065332001 +0000 UTC"
time="2024-03-25T05:31:10Z" level=info msg="certificate CN=etcd-peer signed by CN=etcd-peer-ca@1711344670: notBefore=2024-03-25 05:31:10 +0000 UTC notAfter=2025-03-25 05:31:10 +0000 UTC"
time="2024-03-25T05:31:10Z" level=info msg="certificate CN=etcd-server signed by CN=etcd-server-ca@1711344670: notBefore=2024-03-25 05:31:10 +0000 UTC notAfter=2025-03-25 05:31:10 +0000 UTC"
time="2024-03-25T05:31:10Z" level=info msg="Saving cluster bootstrap data to datastore"
time="2024-03-25T05:31:10Z" level=info msg="certificate CN=k3s,O=k3s signed by CN=k3s-server-ca@1711344670: notBefore=2024-03-25 05:31:10 +0000 UTC notAfter=2025-03-25 05:31:10 +0000 UTC"
time="2024-03-25T05:31:10Z" level=warning msg="dynamiclistener [::]:6443: no cached certificate available for preload - deferring certificate load until storage initialization or first client request"
time="2024-03-25T05:31:10Z" level=info msg="Active TLS secret / (ver=) (count 10): map[listener.cattle.io/cn-10.43.0.1:10.43.0.1 listener.cattle.io/cn-127.0.0.1:127.0.0.1 listener.cattle.io/cn-192.168.56.110:192.168.56.110 listener.cattle.io/cn-__1-f16284:::1 listener.cattle.io/cn-kubernetes:kubernetes listener.cattle.io/cn-kubernetes.default:kubernetes.default listener.cattle.io/cn-kubernetes.default.svc:kubernetes.default.svc listener.cattle.io/cn-kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local listener.cattle.io/cn-localhost:localhost listener.cattle.io/cn-server:server listener.cattle.io/fingerprint:SHA1=94E17A61242B431FB41127365AC892EB2FC94C07]"
time="2024-03-25T05:31:10Z" level=info msg="Running kube-apiserver --advertise-address=192.168.56.110 --advertise-port=6443 --allow-privileged=true --anonymous-auth=false --api-audiences=https://kubernetes.default.svc.cluster.local,k3s --authorization-mode=Node,RBAC --bind-address=127.0.0.1 --cert-dir=/var/lib/rancher/k3s/server/tls/temporary-certs --client-ca-file=/var/lib/rancher/k3s/server/tls/client-ca.crt --egress-selector-config-file=/var/lib/rancher/k3s/server/etc/egress-selector-config.yaml --enable-admission-plugins=NodeRestriction --enable-aggregator-routing=true --enable-bootstrap-token-auth=true --etcd-servers=unix://kine.sock --kubelet-certificate-authority=/var/lib/rancher/k3s/server/tls/server-ca.crt --kubelet-client-certificate=/var/lib/rancher/k3s/server/tls/client-kube-apiserver.crt --kubelet-client-key=/var/lib/rancher/k3s/server/tls/client-kube-apiserver.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --profiling=false --proxy-client-cert-file=/var/lib/rancher/k3s/server/tls/client-auth-proxy.crt --proxy-client-key-file=/var/lib/rancher/k3s/server/tls/client-auth-proxy.key --requestheader-allowed-names=system:auth-proxy --requestheader-client-ca-file=/var/lib/rancher/k3s/server/tls/request-header-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6444 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/var/lib/rancher/k3s/server/tls/service.key --service-account-signing-key-file=/var/lib/rancher/k3s/server/tls/service.current.key --service-cluster-ip-range=10.43.0.0/16 --service-node-port-range=30000-32767 --storage-backend=etcd3 --tls-cert-file=/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.crt --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 --tls-private-key-file=/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.key"
time="2024-03-25T05:31:10Z" level=info msg="Running kube-scheduler --authentication-kubeconfig=/var/lib/rancher/k3s/server/cred/scheduler.kubeconfig --authorization-kubeconfig=/var/lib/rancher/k3s/server/cred/scheduler.kubeconfig --bind-address=127.0.0.1 --kubeconfig=/var/lib/rancher/k3s/server/cred/scheduler.kubeconfig --leader-elect=false --profiling=false --secure-port=10259"
I0325 05:31:10.362377    2321 options.go:220] external host was not specified, using 192.168.56.110
I0325 05:31:10.362959    2321 server.go:156] Version: v1.28.7+k3s1
I0325 05:31:10.362976    2321 server.go:158] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
time="2024-03-25T05:31:10Z" level=info msg="Running kube-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/var/lib/rancher/k3s/server/cred/controller.kubeconfig --authorization-kubeconfig=/var/lib/rancher/k3s/server/cred/controller.kubeconfig --bind-address=127.0.0.1 --cluster-cidr=10.42.0.0/16 --cluster-signing-kube-apiserver-client-cert-file=/var/lib/rancher/k3s/server/tls/client-ca.nochain.crt --cluster-signing-kube-apiserver-client-key-file=/var/lib/rancher/k3s/server/tls/client-ca.key --cluster-signing-kubelet-client-cert-file=/var/lib/rancher/k3s/server/tls/client-ca.nochain.crt --cluster-signing-kubelet-client-key-file=/var/lib/rancher/k3s/server/tls/client-ca.key --cluster-signing-kubelet-serving-cert-file=/var/lib/rancher/k3s/server/tls/server-ca.nochain.crt --cluster-signing-kubelet-serving-key-file=/var/lib/rancher/k3s/server/tls/server-ca.key --cluster-signing-legacy-unknown-cert-file=/var/lib/rancher/k3s/server/tls/server-ca.nochain.crt --cluster-signing-legacy-unknown-key-file=/var/lib/rancher/k3s/server/tls/server-ca.key --configure-cloud-routes=false --controllers=*,tokencleaner,-service,-route,-cloud-node-lifecycle --kubeconfig=/var/lib/rancher/k3s/server/cred/controller.kubeconfig --leader-elect=false --profiling=false --root-ca-file=/var/lib/rancher/k3s/server/tls/server-ca.crt --secure-port=10257 --service-account-private-key-file=/var/lib/rancher/k3s/server/tls/service.current.key --service-cluster-ip-range=10.43.0.0/16 --use-service-account-credentials=true"
time="2024-03-25T05:31:10Z" level=info msg="Running cloud-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/var/lib/rancher/k3s/server/cred/cloud-controller.kubeconfig --authorization-kubeconfig=/var/lib/rancher/k3s/server/cred/cloud-controller.kubeconfig --bind-address=127.0.0.1 --cloud-config=/var/lib/rancher/k3s/server/etc/cloud-config.yaml --cloud-provider=k3s --cluster-cidr=10.42.0.0/16 --configure-cloud-routes=false --controllers=*,-route --feature-gates=CloudDualStackNodeIPs=true --kubeconfig=/var/lib/rancher/k3s/server/cred/cloud-controller.kubeconfig --leader-elect=false --leader-elect-resource-name=k3s-cloud-controller-manager --node-status-update-frequency=1m0s --profiling=false"
time="2024-03-25T05:31:10Z" level=info msg="Server node token is available at /var/lib/rancher/k3s/server/token"
time="2024-03-25T05:31:10Z" level=info msg="To join server node to cluster: k3s server -s https://10.0.2.15:6443 -t ${SERVER_NODE_TOKEN}"
time="2024-03-25T05:31:10Z" level=info msg="Agent node token is available at /var/lib/rancher/k3s/server/agent-token"
time="2024-03-25T05:31:10Z" level=info msg="To join agent node to cluster: k3s agent -s https://10.0.2.15:6443 -t ${AGENT_NODE_TOKEN}"
time="2024-03-25T05:31:10Z" level=info msg="Wrote kubeconfig /etc/rancher/k3s/k3s.yaml"
time="2024-03-25T05:31:10Z" level=info msg="Run: k3s kubectl"
time="2024-03-25T05:31:10Z" level=info msg="Waiting for API server to become available"
I0325 05:31:10.720264    2321 plugins.go:158] Loaded 12 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,TaintNodesByCondition,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,MutatingAdmissionWebhook.
I0325 05:31:10.720285    2321 plugins.go:161] Loaded 13 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,ClusterTrustBundleAttest,CertificateSubjectRestriction,ValidatingAdmissionPolicy,ValidatingAdmissionWebhook,ResourceQuota.
I0325 05:31:10.720967    2321 instance.go:298] Using reconciler: lease
I0325 05:31:10.722143    2321 shared_informer.go:311] Waiting for caches to sync for node_authorizer
I0325 05:31:10.734011    2321 handler.go:275] Adding GroupVersion apiextensions.k8s.io v1 to ResourceManager
W0325 05:31:10.734042    2321 genericapiserver.go:744] Skipping API apiextensions.k8s.io/v1beta1 because it has no resources.
I0325 05:31:10.816229    2321 handler.go:275] Adding GroupVersion  v1 to ResourceManager
I0325 05:31:10.816506    2321 instance.go:709] API group "internal.apiserver.k8s.io" is not enabled, skipping.
I0325 05:31:10.947743    2321 instance.go:709] API group "resource.k8s.io" is not enabled, skipping.
I0325 05:31:10.955737    2321 handler.go:275] Adding GroupVersion authentication.k8s.io v1 to ResourceManager
W0325 05:31:10.955759    2321 genericapiserver.go:744] Skipping API authentication.k8s.io/v1beta1 because it has no resources.
W0325 05:31:10.955764    2321 genericapiserver.go:744] Skipping API authentication.k8s.io/v1alpha1 because it has no resources.
I0325 05:31:10.956278    2321 handler.go:275] Adding GroupVersion authorization.k8s.io v1 to ResourceManager
W0325 05:31:10.956290    2321 genericapiserver.go:744] Skipping API authorization.k8s.io/v1beta1 because it has no resources.
I0325 05:31:10.957067    2321 handler.go:275] Adding GroupVersion autoscaling v2 to ResourceManager
I0325 05:31:10.957872    2321 handler.go:275] Adding GroupVersion autoscaling v1 to ResourceManager
W0325 05:31:10.957887    2321 genericapiserver.go:744] Skipping API autoscaling/v2beta1 because it has no resources.
W0325 05:31:10.957891    2321 genericapiserver.go:744] Skipping API autoscaling/v2beta2 because it has no resources.
I0325 05:31:10.959369    2321 handler.go:275] Adding GroupVersion batch v1 to ResourceManager
W0325 05:31:10.959382    2321 genericapiserver.go:744] Skipping API batch/v1beta1 because it has no resources.
I0325 05:31:10.960570    2321 handler.go:275] Adding GroupVersion certificates.k8s.io v1 to ResourceManager
W0325 05:31:10.960584    2321 genericapiserver.go:744] Skipping API certificates.k8s.io/v1beta1 because it has no resources.
W0325 05:31:10.960588    2321 genericapiserver.go:744] Skipping API certificates.k8s.io/v1alpha1 because it has no resources.
I0325 05:31:10.961252    2321 handler.go:275] Adding GroupVersion coordination.k8s.io v1 to ResourceManager
W0325 05:31:10.961265    2321 genericapiserver.go:744] Skipping API coordination.k8s.io/v1beta1 because it has no resources.
W0325 05:31:10.961386    2321 genericapiserver.go:744] Skipping API discovery.k8s.io/v1beta1 because it has no resources.
I0325 05:31:10.961895    2321 handler.go:275] Adding GroupVersion discovery.k8s.io v1 to ResourceManager
I0325 05:31:10.963313    2321 handler.go:275] Adding GroupVersion networking.k8s.io v1 to ResourceManager
W0325 05:31:10.963324    2321 genericapiserver.go:744] Skipping API networking.k8s.io/v1beta1 because it has no resources.
W0325 05:31:10.963328    2321 genericapiserver.go:744] Skipping API networking.k8s.io/v1alpha1 because it has no resources.
I0325 05:31:10.963903    2321 handler.go:275] Adding GroupVersion node.k8s.io v1 to ResourceManager
W0325 05:31:10.963915    2321 genericapiserver.go:744] Skipping API node.k8s.io/v1beta1 because it has no resources.
W0325 05:31:10.963919    2321 genericapiserver.go:744] Skipping API node.k8s.io/v1alpha1 because it has no resources.
time="2024-03-25T05:31:10Z" level=info msg="Password verified locally for node server"
I0325 05:31:10.964935    2321 handler.go:275] Adding GroupVersion policy v1 to ResourceManager
W0325 05:31:10.964947    2321 genericapiserver.go:744] Skipping API policy/v1beta1 because it has no resources.
time="2024-03-25T05:31:10Z" level=info msg="certificate CN=server signed by CN=k3s-server-ca@1711344670: notBefore=2024-03-25 05:31:10 +0000 UTC notAfter=2025-03-25 05:31:10 +0000 UTC"
I0325 05:31:10.966867    2321 handler.go:275] Adding GroupVersion rbac.authorization.k8s.io v1 to ResourceManager
W0325 05:31:10.966896    2321 genericapiserver.go:744] Skipping API rbac.authorization.k8s.io/v1beta1 because it has no resources.
W0325 05:31:10.966922    2321 genericapiserver.go:744] Skipping API rbac.authorization.k8s.io/v1alpha1 because it has no resources.
I0325 05:31:10.967533    2321 handler.go:275] Adding GroupVersion scheduling.k8s.io v1 to ResourceManager
W0325 05:31:10.967545    2321 genericapiserver.go:744] Skipping API scheduling.k8s.io/v1beta1 because it has no resources.
W0325 05:31:10.967549    2321 genericapiserver.go:744] Skipping API scheduling.k8s.io/v1alpha1 because it has no resources.
I0325 05:31:10.987376    2321 handler.go:275] Adding GroupVersion storage.k8s.io v1 to ResourceManager
W0325 05:31:10.987407    2321 genericapiserver.go:744] Skipping API storage.k8s.io/v1beta1 because it has no resources.
W0325 05:31:10.987413    2321 genericapiserver.go:744] Skipping API storage.k8s.io/v1alpha1 because it has no resources.
I0325 05:31:10.989638    2321 handler.go:275] Adding GroupVersion flowcontrol.apiserver.k8s.io v1beta3 to ResourceManager
I0325 05:31:10.991736    2321 handler.go:275] Adding GroupVersion flowcontrol.apiserver.k8s.io v1beta2 to ResourceManager
W0325 05:31:10.991756    2321 genericapiserver.go:744] Skipping API flowcontrol.apiserver.k8s.io/v1beta1 because it has no resources.
W0325 05:31:10.991841    2321 genericapiserver.go:744] Skipping API flowcontrol.apiserver.k8s.io/v1alpha1 because it has no resources.
I0325 05:31:10.998247    2321 handler.go:275] Adding GroupVersion apps v1 to ResourceManager
W0325 05:31:10.998267    2321 genericapiserver.go:744] Skipping API apps/v1beta2 because it has no resources.
W0325 05:31:10.998285    2321 genericapiserver.go:744] Skipping API apps/v1beta1 because it has no resources.
I0325 05:31:10.999843    2321 handler.go:275] Adding GroupVersion admissionregistration.k8s.io v1 to ResourceManager
W0325 05:31:10.999904    2321 genericapiserver.go:744] Skipping API admissionregistration.k8s.io/v1beta1 because it has no resources.
W0325 05:31:10.999913    2321 genericapiserver.go:744] Skipping API admissionregistration.k8s.io/v1alpha1 because it has no resources.
I0325 05:31:11.000911    2321 handler.go:275] Adding GroupVersion events.k8s.io v1 to ResourceManager
W0325 05:31:11.000963    2321 genericapiserver.go:744] Skipping API events.k8s.io/v1beta1 because it has no resources.
I0325 05:31:11.005834    2321 handler.go:275] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
W0325 05:31:11.005851    2321 genericapiserver.go:744] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
time="2024-03-25T05:31:11Z" level=info msg="certificate CN=system:node:server,O=system:nodes signed by CN=k3s-client-ca@1711344670: notBefore=2024-03-25 05:31:10 +0000 UTC notAfter=2025-03-25 05:31:11 +0000 UTC"
I0325 05:31:11.441692    2321 secure_serving.go:213] Serving securely on 127.0.0.1:6444
I0325 05:31:11.441919    2321 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/rancher/k3s/server/tls/request-header-ca.crt"
I0325 05:31:11.442137    2321 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.crt::/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.key"
I0325 05:31:11.442322    2321 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0325 05:31:11.442983    2321 available_controller.go:423] Starting AvailableConditionController
I0325 05:31:11.443024    2321 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0325 05:31:11.443436    2321 controller.go:116] Starting legacy_token_tracking_controller
I0325 05:31:11.443484    2321 shared_informer.go:311] Waiting for caches to sync for configmaps
I0325 05:31:11.443511    2321 apf_controller.go:374] Starting API Priority and Fairness config controller
I0325 05:31:11.443576    2321 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/rancher/k3s/server/tls/client-auth-proxy.crt::/var/lib/rancher/k3s/server/tls/client-auth-proxy.key"
I0325 05:31:11.443753    2321 system_namespaces_controller.go:67] Starting system namespaces controller
I0325 05:31:11.443882    2321 handler_discovery.go:412] Starting ResourceDiscoveryManager
I0325 05:31:11.444052    2321 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0325 05:31:11.444095    2321 shared_informer.go:311] Waiting for caches to sync for cluster_authentication_trust_controller
I0325 05:31:11.444299    2321 gc_controller.go:78] Starting apiserver lease garbage collector
I0325 05:31:11.444718    2321 gc_controller.go:78] Starting apiserver lease garbage collector
I0325 05:31:11.444832    2321 aggregator.go:164] waiting for initial CRD sync...
I0325 05:31:11.445170    2321 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0325 05:31:11.445223    2321 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0325 05:31:11.445398    2321 customresource_discovery_controller.go:289] Starting DiscoveryController
I0325 05:31:11.445683    2321 controller.go:78] Starting OpenAPI AggregationController
I0325 05:31:11.445794    2321 controller.go:80] Starting OpenAPI V3 AggregationController
I0325 05:31:11.445879    2321 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/rancher/k3s/server/tls/client-ca.crt"
I0325 05:31:11.446408    2321 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/rancher/k3s/server/tls/client-ca.crt"
I0325 05:31:11.446534    2321 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/rancher/k3s/server/tls/request-header-ca.crt"
I0325 05:31:11.447246    2321 crdregistration_controller.go:111] Starting crd-autoregister controller
I0325 05:31:11.447297    2321 shared_informer.go:311] Waiting for caches to sync for crd-autoregister
I0325 05:31:11.447329    2321 controller.go:134] Starting OpenAPI controller
I0325 05:31:11.447414    2321 controller.go:85] Starting OpenAPI V3 controller
I0325 05:31:11.447471    2321 naming_controller.go:291] Starting NamingConditionController
I0325 05:31:11.447577    2321 establishing_controller.go:76] Starting EstablishingController
I0325 05:31:11.447632    2321 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0325 05:31:11.447688    2321 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0325 05:31:11.447754    2321 crd_finalizer.go:266] Starting CRDFinalizer
I0325 05:31:11.567456    2321 cache.go:39] Caches are synced for AvailableConditionController controller
I0325 05:31:11.567565    2321 shared_informer.go:318] Caches are synced for configmaps
I0325 05:31:11.567799    2321 apf_controller.go:379] Running API Priority and Fairness config worker
I0325 05:31:11.567846    2321 apf_controller.go:382] Running API Priority and Fairness periodic rebalancing process
I0325 05:31:11.568204    2321 shared_informer.go:318] Caches are synced for cluster_authentication_trust_controller
I0325 05:31:11.568316    2321 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0325 05:31:11.568588    2321 shared_informer.go:318] Caches are synced for crd-autoregister
I0325 05:31:11.569012    2321 aggregator.go:166] initial CRD sync complete...
I0325 05:31:11.569058    2321 autoregister_controller.go:141] Starting autoregister controller
I0325 05:31:11.569066    2321 cache.go:32] Waiting for caches to sync for autoregister controller
I0325 05:31:11.569108    2321 cache.go:39] Caches are synced for autoregister controller
time="2024-03-25T05:31:11Z" level=info msg="Module overlay was already loaded"
I0325 05:31:11.587975    2321 controller.go:624] quota admission added evaluator for: namespaces
time="2024-03-25T05:31:11Z" level=info msg="Module br_netfilter was already loaded"
I0325 05:31:11.622205    2321 shared_informer.go:318] Caches are synced for node_authorizer
time="2024-03-25T05:31:11Z" level=info msg="Set sysctl 'net/ipv4/conf/all/forwarding' to 1"
time="2024-03-25T05:31:11Z" level=info msg="Set sysctl 'net/netfilter/nf_conntrack_max' to 131072"
time="2024-03-25T05:31:11Z" level=info msg="Set sysctl 'net/netfilter/nf_conntrack_tcp_timeout_established' to 86400"
time="2024-03-25T05:31:11Z" level=info msg="Set sysctl 'net/netfilter/nf_conntrack_tcp_timeout_close_wait' to 3600"
time="2024-03-25T05:31:11Z" level=info msg="Logging containerd to /var/lib/rancher/k3s/agent/containerd/containerd.log"
time="2024-03-25T05:31:11Z" level=info msg="Running containerd -c /var/lib/rancher/k3s/agent/etc/containerd/config.toml -a /run/k3s/containerd/containerd.sock --state /run/k3s/containerd --root /var/lib/rancher/k3s/agent/containerd"
E0325 05:31:11.642245    2321 controller.go:95] Unable to perform initial Kubernetes service initialization: namespaces "default" not found
I0325 05:31:11.663878    2321 controller.go:624] quota admission added evaluator for: leases.coordination.k8s.io
I0325 05:31:12.450921    2321 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0325 05:31:12.456413    2321 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0325 05:31:12.465784    2321 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
time="2024-03-25T05:31:12Z" level=info msg="containerd is now running"
time="2024-03-25T05:31:12Z" level=info msg="Running kubelet --address=0.0.0.0 --allowed-unsafe-sysctls=net.ipv4.ip_forward,net.ipv6.conf.all.forwarding --anonymous-auth=false --authentication-token-webhook=true --authorization-mode=Webhook --cgroup-driver=systemd --client-ca-file=/var/lib/rancher/k3s/agent/client-ca.crt --cloud-provider=external --cluster-dns=10.43.0.10 --cluster-domain=cluster.local --container-runtime-endpoint=unix:///run/k3s/containerd/containerd.sock --containerd=/run/k3s/containerd/containerd.sock --eviction-hard=imagefs.available<5%,nodefs.available<5% --eviction-minimum-reclaim=imagefs.available=10%,nodefs.available=10% --fail-swap-on=false --feature-gates=CloudDualStackNodeIPs=true --healthz-bind-address=127.0.0.1 --hostname-override=server --kubeconfig=/var/lib/rancher/k3s/agent/kubelet.kubeconfig --node-ip=192.168.56.110 --node-labels= --pod-infra-container-image=rancher/mirrored-pause:3.6 --pod-manifest-path=/var/lib/rancher/k3s/agent/pod-manifests --read-only-port=0 --resolv-conf=/run/systemd/resolve/resolv.conf --serialize-image-pulls=false --tls-cert-file=/var/lib/rancher/k3s/agent/serving-kubelet.crt --tls-private-key-file=/var/lib/rancher/k3s/agent/serving-kubelet.key"
time="2024-03-25T05:31:12Z" level=info msg="Connecting to proxy" url="wss://127.0.0.1:6443/v1-k3s/connect"
time="2024-03-25T05:31:12Z" level=info msg="Handling backend connection request [server]"
time="2024-03-25T05:31:12Z" level=info msg="Waiting to retrieve kube-proxy configuration; server is not ready: https://127.0.0.1:6443/v1-k3s/readyz: 500 Internal Server Error"
I0325 05:31:12.776849    2321 controller.go:624] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0325 05:31:12.803556    2321 controller.go:624] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0325 05:31:12.858471    2321 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.43.0.1"}
W0325 05:31:12.863939    2321 lease.go:263] Resetting endpoints for master service "kubernetes" to [192.168.56.110]
I0325 05:31:12.865075    2321 controller.go:624] quota admission added evaluator for: endpoints
I0325 05:31:12.869086    2321 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
time="2024-03-25T05:31:13Z" level=info msg="Kube API server is now running"
time="2024-03-25T05:31:13Z" level=info msg="ETCD server is now running"
time="2024-03-25T05:31:13Z" level=info msg="k3s is up and running"
time="2024-03-25T05:31:13Z" level=info msg="Waiting for cloud-controller-manager privileges to become available"
time="2024-03-25T05:31:13Z" level=info msg="Creating k3s-supervisor event broadcaster"
time="2024-03-25T05:31:13Z" level=info msg="Applying CRD addons.k3s.cattle.io"
time="2024-03-25T05:31:13Z" level=info msg="Applying CRD etcdsnapshotfiles.k3s.cattle.io"
time="2024-03-25T05:31:13Z" level=info msg="Applying CRD helmcharts.helm.cattle.io"
I0325 05:31:13.689306    2321 handler.go:275] Adding GroupVersion k3s.cattle.io v1 to ResourceManager
I0325 05:31:13.745926    2321 handler.go:275] Adding GroupVersion k3s.cattle.io v1 to ResourceManager
I0325 05:31:13.793312    2321 handler.go:275] Adding GroupVersion helm.cattle.io v1 to ResourceManager
Flag --cloud-provider has been deprecated, will be removed in 1.25 or later, in favor of removing cloud provider code from Kubelet.
Flag --containerd has been deprecated, This is a cadvisor flag that was mistakenly registered with the Kubelet. Due to legacy concerns, it will follow the standard CLI deprecation timeline before being removed.
Flag --pod-infra-container-image has been deprecated, will be removed in a future release. Image garbage collector will get sandbox image information from CRI.
I0325 05:31:13.794836    2321 server.go:202] "--pod-infra-container-image will not be pruned by the image garbage collector in kubelet and should also be set in the remote runtime"
I0325 05:31:13.796662    2321 server.go:462] "Kubelet version" kubeletVersion="v1.28.7+k3s1"
I0325 05:31:13.796679    2321 server.go:464] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0325 05:31:13.798717    2321 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/rancher/k3s/agent/client-ca.crt"
time="2024-03-25T05:31:13Z" level=info msg="Applying CRD helmchartconfigs.helm.cattle.io"
I0325 05:31:13.809004    2321 server.go:720] "--cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /"
I0325 05:31:13.809978    2321 container_manager_linux.go:265] "Container manager verified user specified cgroup-root exists" cgroupRoot=[]
I0325 05:31:13.810371    2321 container_manager_linux.go:270] "Creating Container Manager object based on Node Config" nodeConfig={"RuntimeCgroupsName":"","SystemCgroupsName":"","KubeletCgroupsName":"","KubeletOOMScoreAdj":-999,"ContainerRuntime":"","CgroupsPerQOS":true,"CgroupRoot":"/","CgroupDriver":"systemd","KubeletRootDir":"/var/lib/kubelet","ProtectKernelDefaults":false,"KubeReservedCgroupName":"","SystemReservedCgroupName":"","ReservedSystemCPUs":{},"EnforceNodeAllocatable":{"pods":{}},"KubeReserved":null,"SystemReserved":null,"HardEvictionThresholds":[{"Signal":"imagefs.available","Operator":"LessThan","Value":{"Quantity":null,"Percentage":0.05},"GracePeriod":0,"MinReclaim":null},{"Signal":"nodefs.available","Operator":"LessThan","Value":{"Quantity":null,"Percentage":0.05},"GracePeriod":0,"MinReclaim":null}],"QOSReserved":{},"CPUManagerPolicy":"none","CPUManagerPolicyOptions":null,"TopologyManagerScope":"container","CPUManagerReconcilePeriod":10000000000,"ExperimentalMemoryManagerPolicy":"None","ExperimentalMemoryManagerReservedMemory":null,"PodPidsLimit":-1,"EnforceCPULimits":true,"CPUCFSQuotaPeriod":100000000,"TopologyManagerPolicy":"none","TopologyManagerPolicyOptions":null}
I0325 05:31:13.810535    2321 topology_manager.go:138] "Creating topology manager with none policy"
I0325 05:31:13.810655    2321 container_manager_linux.go:301] "Creating device plugin manager"
I0325 05:31:13.811262    2321 state_mem.go:36] "Initialized new in-memory state store"
I0325 05:31:13.811518    2321 kubelet.go:393] "Attempting to sync node with API server"
I0325 05:31:13.811637    2321 kubelet.go:298] "Adding static pod path" path="/var/lib/rancher/k3s/agent/pod-manifests"
I0325 05:31:13.811773    2321 kubelet.go:309] "Adding apiserver pod source"
I0325 05:31:13.811925    2321 apiserver.go:42] "Waiting for node sync before watching apiserver pods"
I0325 05:31:13.813774    2321 kuberuntime_manager.go:257] "Container runtime initialized" containerRuntime="containerd" version="v1.7.11-k3s2" apiVersion="v1"
W0325 05:31:13.815800    2321 probe.go:268] Flexvolume plugin directory at /usr/libexec/kubernetes/kubelet-plugins/volume/exec/ does not exist. Recreating.
I0325 05:31:13.817202    2321 server.go:1227] "Started kubelet"
I0325 05:31:13.819813    2321 fs_resource_analyzer.go:67] "Starting FS ResourceAnalyzer"
I0325 05:31:13.836533    2321 server.go:162] "Starting to listen" address="0.0.0.0" port=10250
I0325 05:31:13.837963    2321 server.go:462] "Adding debug handlers to kubelet server"
I0325 05:31:13.838504    2321 ratelimit.go:65] "Setting rate limiting for podresources endpoint" qps=100 burstTokens=10
I0325 05:31:13.838648    2321 server.go:233] "Starting to serve the podresources API" endpoint="unix:/var/lib/kubelet/pod-resources/kubelet.sock"
I0325 05:31:13.842444    2321 volume_manager.go:291] "Starting Kubelet Volume Manager"
I0325 05:31:13.842977    2321 desired_state_of_world_populator.go:151] "Desired state populator starts to run"
I0325 05:31:13.843886    2321 reconciler_new.go:29] "Reconciler: start to sync state"
E0325 05:31:13.848780    2321 cri_stats_provider.go:448] "Failed to get the info of the filesystem with mountpoint" err="unable to find data in memory cache" mountpoint="/var/lib/rancher/k3s/agent/containerd/io.containerd.snapshotter.v1.overlayfs"
E0325 05:31:13.849182    2321 kubelet.go:1431] "Image garbage collection failed once. Stats initialization may not have completed yet" err="invalid capacity 0 on image filesystem"
I0325 05:31:13.893887    2321 kubelet_network_linux.go:50] "Initialized iptables rules." protocol="IPv4"
I0325 05:31:13.912354    2321 kubelet_network_linux.go:50] "Initialized iptables rules." protocol="IPv6"
I0325 05:31:13.912918    2321 status_manager.go:217] "Starting to sync pod status with apiserver"
I0325 05:31:13.912952    2321 kubelet.go:2303] "Starting kubelet main sync loop"
E0325 05:31:13.913040    2321 kubelet.go:2327] "Skipping pod synchronization" err="[container runtime status check may not have completed yet, PLEG is not healthy: pleg has yet to be successful]"
I0325 05:31:13.969729    2321 kubelet_node_status.go:70] "Attempting to register node" node="server"
E0325 05:31:13.980353    2321 nodelease.go:49] "Failed to get node when trying to set owner ref to the node lease" err="nodes \"server\" not found" node="server"
I0325 05:31:13.985216    2321 kubelet_node_status.go:73] "Successfully registered node" node="server"
I0325 05:31:13.987959    2321 cpu_manager.go:214] "Starting CPU manager" policy="none"
I0325 05:31:13.988107    2321 cpu_manager.go:215] "Reconciling" reconcilePeriod="10s"
I0325 05:31:13.988206    2321 state_mem.go:36] "Initialized new in-memory state store"
I0325 05:31:13.992179    2321 policy_none.go:49] "None policy: Start"
time="2024-03-25T05:31:13Z" level=info msg="Waiting for CRD helmchartconfigs.helm.cattle.io to become available"
I0325 05:31:13.996804    2321 handler.go:275] Adding GroupVersion helm.cattle.io v1 to ResourceManager
I0325 05:31:14.001628    2321 memory_manager.go:169] "Starting memorymanager" policy="None"
I0325 05:31:14.001661    2321 state_mem.go:35] "Initializing new in-memory state store"
time="2024-03-25T05:31:14Z" level=info msg="Annotations and labels have been set successfully on node: server"
time="2024-03-25T05:31:14Z" level=info msg="Starting flannel with backend vxlan"
I0325 05:31:14.024301    2321 serving.go:355] Generated self-signed cert in-memory
E0325 05:31:14.024753    2321 kubelet.go:2327] "Skipping pod synchronization" err="container runtime status check may not have completed yet"
I0325 05:31:14.046412    2321 manager.go:471] "Failed to read data from checkpoint" checkpoint="kubelet_internal_checkpoint" err="checkpoint is not found"
I0325 05:31:14.048263    2321 plugin_manager.go:118] "Starting Kubelet Plugin Manager"
W0325 05:31:14.048807    2321 watcher.go:93] Error while processing event ("/sys/fs/cgroup/devices/kubepods.slice/kubepods-besteffort.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/devices/kubepods.slice/kubepods-besteffort.slice: no such file or directory
I0325 05:31:14.065529    2321 kubelet_node_status.go:493] "Fast updating node status as it just became ready"
I0325 05:31:14.283948    2321 controllermanager.go:189] "Starting" version="v1.28.7+k3s1"
I0325 05:31:14.283968    2321 controllermanager.go:191] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0325 05:31:14.287012    2321 secure_serving.go:213] Serving securely on 127.0.0.1:10257
I0325 05:31:14.287170    2321 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0325 05:31:14.287203    2321 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I0325 05:31:14.287211    2321 shared_informer.go:311] Waiting for caches to sync for RequestHeaderAuthRequestController
I0325 05:31:14.287271    2321 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0325 05:31:14.287278    2321 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0325 05:31:14.287325    2321 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0325 05:31:14.287332    2321 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0325 05:31:14.294367    2321 shared_informer.go:311] Waiting for caches to sync for tokens
I0325 05:31:14.299288    2321 controller.go:624] quota admission added evaluator for: serviceaccounts
I0325 05:31:14.301019    2321 controllermanager.go:642] "Started controller" controller="token-cleaner-controller"
I0325 05:31:14.301138    2321 tokencleaner.go:112] "Starting token cleaner controller"
I0325 05:31:14.301146    2321 shared_informer.go:311] Waiting for caches to sync for token_cleaner
I0325 05:31:14.301152    2321 shared_informer.go:318] Caches are synced for token_cleaner
I0325 05:31:14.306902    2321 controllermanager.go:642] "Started controller" controller="ttl-after-finished-controller"
I0325 05:31:14.307110    2321 ttlafterfinished_controller.go:109] "Starting TTL after finished controller"
I0325 05:31:14.307263    2321 shared_informer.go:311] Waiting for caches to sync for TTL after finished
I0325 05:31:14.317120    2321 controllermanager.go:642] "Started controller" controller="garbage-collector-controller"
I0325 05:31:14.317134    2321 garbagecollector.go:155] "Starting controller" controller="garbagecollector"
I0325 05:31:14.317402    2321 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0325 05:31:14.317418    2321 graph_builder.go:294] "Running" component="GraphBuilder"
I0325 05:31:14.331989    2321 controllermanager.go:642] "Started controller" controller="cronjob-controller"
I0325 05:31:14.333198    2321 cronjob_controllerv2.go:139] "Starting cronjob controller v2"
I0325 05:31:14.333315    2321 shared_informer.go:311] Waiting for caches to sync for cronjob
I0325 05:31:14.339157    2321 controllermanager.go:642] "Started controller" controller="persistentvolume-binder-controller"
I0325 05:31:14.339512    2321 pv_controller_base.go:319] "Starting persistent volume controller"
I0325 05:31:14.345212    2321 shared_informer.go:311] Waiting for caches to sync for persistent volume
I0325 05:31:14.345881    2321 controllermanager.go:642] "Started controller" controller="clusterrole-aggregation-controller"
I0325 05:31:14.346089    2321 clusterroleaggregation_controller.go:189] "Starting ClusterRoleAggregator controller"
I0325 05:31:14.346216    2321 shared_informer.go:311] Waiting for caches to sync for ClusterRoleAggregator
I0325 05:31:14.352233    2321 controllermanager.go:642] "Started controller" controller="endpoints-controller"
I0325 05:31:14.352368    2321 endpoints_controller.go:174] "Starting endpoint controller"
I0325 05:31:14.352376    2321 shared_informer.go:311] Waiting for caches to sync for endpoint
I0325 05:31:14.359078    2321 controllermanager.go:642] "Started controller" controller="endpointslice-controller"
I0325 05:31:14.359311    2321 endpointslice_controller.go:264] "Starting endpoint slice controller"
I0325 05:31:14.359429    2321 shared_informer.go:311] Waiting for caches to sync for endpoint_slice
I0325 05:31:14.364844    2321 controllermanager.go:642] "Started controller" controller="serviceaccount-controller"
I0325 05:31:14.365145    2321 serviceaccounts_controller.go:111] "Starting service account controller"
I0325 05:31:14.365302    2321 shared_informer.go:311] Waiting for caches to sync for service account
I0325 05:31:14.387790    2321 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0325 05:31:14.387796    2321 shared_informer.go:318] Caches are synced for RequestHeaderAuthRequestController
I0325 05:31:14.387808    2321 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0325 05:31:14.401857    2321 shared_informer.go:318] Caches are synced for tokens
time="2024-03-25T05:31:14Z" level=info msg="Done waiting for CRD helmchartconfigs.helm.cattle.io to become available"
I0325 05:31:14.499363    2321 controllermanager.go:642] "Started controller" controller="ttl-controller"
I0325 05:31:14.499375    2321 ttl_controller.go:124] "Starting TTL controller"
I0325 05:31:14.499457    2321 shared_informer.go:311] Waiting for caches to sync for TTL
time="2024-03-25T05:31:14Z" level=info msg="Writing static file: /var/lib/rancher/k3s/server/static/charts/traefik-crd-25.0.2+up25.0.0.tgz"
time="2024-03-25T05:31:14Z" level=info msg="Writing static file: /var/lib/rancher/k3s/server/static/charts/traefik-25.0.2+up25.0.0.tgz"
time="2024-03-25T05:31:14Z" level=info msg="Failed to get existing traefik HelmChart" error="helmcharts.helm.cattle.io \"traefik\" not found"
time="2024-03-25T05:31:14Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/aggregated-metrics-reader.yaml"
time="2024-03-25T05:31:14Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/metrics-apiservice.yaml"
time="2024-03-25T05:31:14Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/coredns.yaml"
time="2024-03-25T05:31:14Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/resource-reader.yaml"
time="2024-03-25T05:31:14Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/runtimes.yaml"
time="2024-03-25T05:31:14Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-deployment.yaml"
time="2024-03-25T05:31:14Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-service.yaml"
time="2024-03-25T05:31:14Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/rolebindings.yaml"
time="2024-03-25T05:31:14Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/ccm.yaml"
time="2024-03-25T05:31:14Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/local-storage.yaml"
time="2024-03-25T05:31:14Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/auth-delegator.yaml"
time="2024-03-25T05:31:14Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/auth-reader.yaml"
time="2024-03-25T05:31:14Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/traefik.yaml"
time="2024-03-25T05:31:14Z" level=info msg="Starting dynamiclistener CN filter node controller"
time="2024-03-25T05:31:14Z" level=info msg="Tunnel server egress proxy mode: agent"
time="2024-03-25T05:31:14Z" level=info msg="Starting k3s.cattle.io/v1, Kind=Addon controller"
time="2024-03-25T05:31:14Z" level=info msg="Creating deploy event broadcaster"
time="2024-03-25T05:31:14Z" level=info msg="Starting /v1, Kind=Node controller"
time="2024-03-25T05:31:14Z" level=info msg="Creating helm-controller event broadcaster"
time="2024-03-25T05:31:14Z" level=warning msg="Unable to fetch coredns config map: configmaps \"coredns\" not found"
time="2024-03-25T05:31:14Z" level=info msg="Labels and annotations have been set successfully on node: server"
time="2024-03-25T05:31:14Z" level=info msg="Cluster dns configmap has been set successfully"
time="2024-03-25T05:31:14Z" level=info msg="Starting helm.cattle.io/v1, Kind=HelmChart controller"
time="2024-03-25T05:31:14Z" level=info msg="Starting helm.cattle.io/v1, Kind=HelmChartConfig controller"
time="2024-03-25T05:31:14Z" level=info msg="Starting rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding controller"
time="2024-03-25T05:31:14Z" level=info msg="Starting batch/v1, Kind=Job controller"
time="2024-03-25T05:31:14Z" level=info msg="Starting /v1, Kind=ConfigMap controller"
time="2024-03-25T05:31:14Z" level=info msg="Starting /v1, Kind=Secret controller"
time="2024-03-25T05:31:14Z" level=info msg="Starting /v1, Kind=ServiceAccount controller"
I0325 05:31:14.813316    2321 apiserver.go:52] "Watching apiserver"
I0325 05:31:14.847300    2321 desired_state_of_world_populator.go:159] "Finished populating initial desired state of world"
time="2024-03-25T05:31:14Z" level=warning msg="Unable to fetch coredns config map: configmaps \"coredns\" not found"
time="2024-03-25T05:31:15Z" level=warning msg="Unable to fetch coredns config map: configmaps \"coredns\" not found"
time="2024-03-25T05:31:15Z" level=info msg="Updating TLS secret for kube-system/k3s-serving (count: 10): map[listener.cattle.io/cn-10.43.0.1:10.43.0.1 listener.cattle.io/cn-127.0.0.1:127.0.0.1 listener.cattle.io/cn-192.168.56.110:192.168.56.110 listener.cattle.io/cn-__1-f16284:::1 listener.cattle.io/cn-kubernetes:kubernetes listener.cattle.io/cn-kubernetes.default:kubernetes.default listener.cattle.io/cn-kubernetes.default.svc:kubernetes.default.svc listener.cattle.io/cn-kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local listener.cattle.io/cn-localhost:localhost listener.cattle.io/cn-server:server listener.cattle.io/fingerprint:SHA1=94E17A61242B431FB41127365AC892EB2FC94C07]"
time="2024-03-25T05:31:16Z" level=info msg="Updating TLS secret for kube-system/k3s-serving (count: 10): map[listener.cattle.io/cn-10.43.0.1:10.43.0.1 listener.cattle.io/cn-127.0.0.1:127.0.0.1 listener.cattle.io/cn-192.168.56.110:192.168.56.110 listener.cattle.io/cn-__1-f16284:::1 listener.cattle.io/cn-kubernetes:kubernetes listener.cattle.io/cn-kubernetes.default:kubernetes.default listener.cattle.io/cn-kubernetes.default.svc:kubernetes.default.svc listener.cattle.io/cn-kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local listener.cattle.io/cn-localhost:localhost listener.cattle.io/cn-server:server listener.cattle.io/fingerprint:SHA1=94E17A61242B431FB41127365AC892EB2FC94C07]"
time="2024-03-25T05:31:16Z" level=warning msg="Unable to fetch coredns config map: configmaps \"coredns\" not found"
time="2024-03-25T05:31:16Z" level=info msg="Active TLS secret kube-system/k3s-serving (ver=237) (count 10): map[listener.cattle.io/cn-10.43.0.1:10.43.0.1 listener.cattle.io/cn-127.0.0.1:127.0.0.1 listener.cattle.io/cn-192.168.56.110:192.168.56.110 listener.cattle.io/cn-__1-f16284:::1 listener.cattle.io/cn-kubernetes:kubernetes listener.cattle.io/cn-kubernetes.default:kubernetes.default listener.cattle.io/cn-kubernetes.default.svc:kubernetes.default.svc listener.cattle.io/cn-kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local listener.cattle.io/cn-localhost:localhost listener.cattle.io/cn-server:server listener.cattle.io/fingerprint:SHA1=94E17A61242B431FB41127365AC892EB2FC94C07]"
I0325 05:31:16.691000    2321 controller.go:624] quota admission added evaluator for: addons.k3s.cattle.io
I0325 05:31:16.695448    2321 event.go:307] "Event occurred" object="kube-system/ccm" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/ccm.yaml\""
I0325 05:31:16.723828    2321 event.go:307] "Event occurred" object="kube-system/ccm" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/ccm.yaml\""
I0325 05:31:16.733403    2321 event.go:307] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/coredns.yaml\""
I0325 05:31:16.764480    2321 controller.go:624] quota admission added evaluator for: deployments.apps
I0325 05:31:16.775905    2321 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.43.0.10"}
I0325 05:31:16.777123    2321 event.go:307] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/coredns.yaml\""
I0325 05:31:16.786916    2321 event.go:307] "Event occurred" object="kube-system/local-storage" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/local-storage.yaml\""
I0325 05:31:16.820664    2321 event.go:307] "Event occurred" object="kube-system/local-storage" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/local-storage.yaml\""
I0325 05:31:16.828362    2321 event.go:307] "Event occurred" object="kube-system/aggregated-metrics-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/aggregated-metrics-reader.yaml\""
I0325 05:31:16.836023    2321 event.go:307] "Event occurred" object="kube-system/aggregated-metrics-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/aggregated-metrics-reader.yaml\""
I0325 05:31:16.843316    2321 event.go:307] "Event occurred" object="kube-system/auth-delegator" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/auth-delegator.yaml\""
I0325 05:31:16.849077    2321 event.go:307] "Event occurred" object="kube-system/auth-delegator" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/auth-delegator.yaml\""
I0325 05:31:16.855787    2321 event.go:307] "Event occurred" object="kube-system/auth-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/auth-reader.yaml\""
I0325 05:31:16.860707    2321 event.go:307] "Event occurred" object="kube-system/auth-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/auth-reader.yaml\""
I0325 05:31:16.917850    2321 event.go:307] "Event occurred" object="server" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodePasswordValidationComplete" message="Deferred node password secret validation complete"
I0325 05:31:17.128401    2321 event.go:307] "Event occurred" object="kube-system/metrics-apiservice" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-apiservice.yaml\""
I0325 05:31:17.135501    2321 event.go:307] "Event occurred" object="kube-system/metrics-apiservice" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-apiservice.yaml\""
I0325 05:31:17.136404    2321 handler.go:275] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
W0325 05:31:17.139453    2321 handler_proxy.go:93] no RequestInfo found in the context
E0325 05:31:17.139514    2321 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
E0325 05:31:17.139733    2321 handler_proxy.go:137] error resolving kube-system/metrics-server: service "metrics-server" not found
time="2024-03-25T05:31:17Z" level=info msg="Updated coredns node hosts entry [192.168.56.110 server]"
I0325 05:31:17.541140    2321 event.go:307] "Event occurred" object="kube-system/metrics-server-deployment" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-deployment.yaml\""
I0325 05:31:17.553825    2321 event.go:307] "Event occurred" object="kube-system/metrics-server-deployment" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-deployment.yaml\""
I0325 05:31:18.069289    2321 serving.go:355] Generated self-signed cert in-memory
time="2024-03-25T05:31:18Z" level=info msg="Running kube-proxy --cluster-cidr=10.42.0.0/16 --conntrack-max-per-core=0 --conntrack-tcp-timeout-close-wait=0s --conntrack-tcp-timeout-established=0s --healthz-bind-address=127.0.0.1 --hostname-override=server --kubeconfig=/var/lib/rancher/k3s/agent/kubeproxy.kubeconfig --proxy-mode=iptables"
I0325 05:31:18.104035    2321 event.go:307] "Event occurred" object="kube-system/metrics-server-service" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-service.yaml\""
I0325 05:31:18.115081    2321 node.go:141] Successfully retrieved node IP: 192.168.56.110
I0325 05:31:18.119351    2321 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0325 05:31:18.121810    2321 server_others.go:152] "Using iptables Proxier"
I0325 05:31:18.121840    2321 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0325 05:31:18.121846    2321 server_others.go:438] "Defaulting to no-op detect-local"
I0325 05:31:18.121861    2321 proxier.go:250] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0325 05:31:18.122011    2321 server.go:846] "Version info" version="v1.28.7+k3s1"
I0325 05:31:18.122021    2321 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0325 05:31:18.132679    2321 config.go:188] "Starting service config controller"
I0325 05:31:18.132709    2321 shared_informer.go:311] Waiting for caches to sync for service config
I0325 05:31:18.132993    2321 config.go:97] "Starting endpoint slice config controller"
I0325 05:31:18.133075    2321 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0325 05:31:18.133482    2321 config.go:315] "Starting node config controller"
I0325 05:31:18.133489    2321 shared_informer.go:311] Waiting for caches to sync for node config
I0325 05:31:18.148278    2321 alloc.go:330] "allocated clusterIPs" service="kube-system/metrics-server" clusterIPs={"IPv4":"10.43.131.71"}
I0325 05:31:18.150373    2321 event.go:307] "Event occurred" object="kube-system/metrics-server-service" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-service.yaml\""
W0325 05:31:18.158612    2321 handler_proxy.go:93] no RequestInfo found in the context
E0325 05:31:18.158775    2321 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0325 05:31:18.158787    2321 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0325 05:31:18.158833    2321 handler_proxy.go:93] no RequestInfo found in the context
E0325 05:31:18.158848    2321 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
W0325 05:31:18.159657    2321 handler_proxy.go:93] no RequestInfo found in the context
E0325 05:31:18.159804    2321 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
E0325 05:31:18.159976    2321 handler_proxy.go:137] error resolving kube-system/metrics-server: endpoints "metrics-server" not found
I0325 05:31:18.160256    2321 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0325 05:31:18.233736    2321 shared_informer.go:318] Caches are synced for node config
I0325 05:31:18.233894    2321 shared_informer.go:318] Caches are synced for endpoint slice config
I0325 05:31:18.235192    2321 shared_informer.go:318] Caches are synced for service config
I0325 05:31:18.330114    2321 event.go:307] "Event occurred" object="kube-system/resource-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/resource-reader.yaml\""
I0325 05:31:18.347835    2321 event.go:307] "Event occurred" object="kube-system/resource-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/resource-reader.yaml\""
I0325 05:31:18.484227    2321 serving.go:355] Generated self-signed cert in-memory
I0325 05:31:18.731099    2321 event.go:307] "Event occurred" object="kube-system/rolebindings" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/rolebindings.yaml\""
I0325 05:31:18.760570    2321 event.go:307] "Event occurred" object="kube-system/rolebindings" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/rolebindings.yaml\""
I0325 05:31:19.032722    2321 controllermanager.go:168] Version: v1.28.7+k3s1
I0325 05:31:19.036312    2321 secure_serving.go:213] Serving securely on 127.0.0.1:10258
I0325 05:31:19.036541    2321 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I0325 05:31:19.036715    2321 shared_informer.go:311] Waiting for caches to sync for RequestHeaderAuthRequestController
I0325 05:31:19.036558    2321 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0325 05:31:19.036764    2321 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0325 05:31:19.036570    2321 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0325 05:31:19.037027    2321 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0325 05:31:19.036624    2321 tlsconfig.go:240] "Starting DynamicServingCertificateController"
E0325 05:31:19.049354    2321 controllermanager.go:524] unable to get all supported resources from server: unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
time="2024-03-25T05:31:19Z" level=info msg="Creating  event broadcaster"
I0325 05:31:19.141120    2321 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0325 05:31:19.141260    2321 shared_informer.go:318] Caches are synced for RequestHeaderAuthRequestController
I0325 05:31:19.141483    2321 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0325 05:31:19.147275    2321 event.go:307] "Event occurred" object="kube-system/runtimes" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/runtimes.yaml\""
W0325 05:31:19.159969    2321 handler_proxy.go:93] no RequestInfo found in the context
E0325 05:31:19.160205    2321 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0325 05:31:19.160276    2321 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
time="2024-03-25T05:31:19Z" level=info msg="Starting /v1, Kind=Node controller"
time="2024-03-25T05:31:19Z" level=info msg="Starting /v1, Kind=Pod controller"
time="2024-03-25T05:31:19Z" level=info msg="Starting apps/v1, Kind=DaemonSet controller"
I0325 05:31:19.176584    2321 controllermanager.go:337] Started "cloud-node-lifecycle-controller"
I0325 05:31:19.177105    2321 controllermanager.go:337] Started "service-lb-controller"
W0325 05:31:19.177121    2321 controllermanager.go:314] "node-route-controller" is disabled
I0325 05:31:19.177380    2321 controllermanager.go:337] Started "cloud-node-controller"
time="2024-03-25T05:31:19Z" level=info msg="Starting discovery.k8s.io/v1, Kind=EndpointSlice controller"
I0325 05:31:19.178067    2321 node_lifecycle_controller.go:113] Sending events to api server
I0325 05:31:19.178208    2321 controller.go:231] Starting service controller
I0325 05:31:19.178225    2321 shared_informer.go:311] Waiting for caches to sync for service
I0325 05:31:19.178289    2321 node_controller.go:165] Sending events to api server.
I0325 05:31:19.178346    2321 node_controller.go:174] Waiting for informer caches to sync
I0325 05:31:19.279280    2321 shared_informer.go:318] Caches are synced for service
I0325 05:31:19.279292    2321 node_controller.go:431] Initializing node server with cloud provider
I0325 05:31:19.284667    2321 node_controller.go:502] Successfully initialized node server with cloud provider
I0325 05:31:19.284945    2321 event.go:307] "Event occurred" object="server" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="Synced" message="Node synced successfully"
I0325 05:31:19.384970    2321 event.go:307] "Event occurred" object="kube-system/runtimes" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/runtimes.yaml\""
I0325 05:31:19.530482    2321 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/traefik.yaml\""
I0325 05:31:19.536937    2321 controller.go:624] quota admission added evaluator for: helmcharts.helm.cattle.io
I0325 05:31:19.548556    2321 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/traefik.yaml\""
I0325 05:31:19.569968    2321 event.go:307] "Event occurred" object="kube-system/traefik-crd" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik-crd"
I0325 05:31:19.570085    2321 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
I0325 05:31:19.619154    2321 controller.go:624] quota admission added evaluator for: jobs.batch
time="2024-03-25T05:31:19Z" level=error msg="error syncing 'kube-system/traefik': handler helm-controller-chart-registration: helmcharts.helm.cattle.io \"traefik\" not found, requeuing"
time="2024-03-25T05:31:19Z" level=error msg="error syncing 'kube-system/traefik-crd': handler helm-controller-chart-registration: helmcharts.helm.cattle.io \"traefik-crd\" not found, requeuing"
I0325 05:31:19.647739    2321 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
I0325 05:31:19.647979    2321 event.go:307] "Event occurred" object="kube-system/traefik-crd" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik-crd"
I0325 05:31:19.666690    2321 serving.go:355] Generated self-signed cert in-memory
time="2024-03-25T05:31:19Z" level=info msg="Tunnel authorizer set Kubelet Port 10250"
I0325 05:31:19.771186    2321 event.go:307] "Event occurred" object="kube-system/traefik-crd" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik-crd"
I0325 05:31:19.796704    2321 event.go:307] "Event occurred" object="kube-system/traefik-crd" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik-crd"
I0325 05:31:19.947791    2321 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
I0325 05:31:19.973466    2321 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
I0325 05:31:20.041408    2321 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.7+k3s1"
I0325 05:31:20.041537    2321 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0325 05:31:20.045039    2321 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0325 05:31:20.045107    2321 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I0325 05:31:20.045301    2321 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0325 05:31:20.045990    2321 shared_informer.go:311] Waiting for caches to sync for RequestHeaderAuthRequestController
I0325 05:31:20.046763    2321 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0325 05:31:20.046817    2321 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0325 05:31:20.046831    2321 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0325 05:31:20.046835    2321 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0325 05:31:20.146728    2321 shared_informer.go:318] Caches are synced for RequestHeaderAuthRequestController
I0325 05:31:20.147094    2321 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0325 05:31:20.147294    2321 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
time="2024-03-25T05:31:20Z" level=info msg="Stopped tunnel to 127.0.0.1:6443"
time="2024-03-25T05:31:20Z" level=info msg="Connecting to proxy" url="wss://192.168.56.110:6443/v1-k3s/connect"
time="2024-03-25T05:31:20Z" level=info msg="Proxy done" err="context canceled" url="wss://127.0.0.1:6443/v1-k3s/connect"
time="2024-03-25T05:31:20Z" level=info msg="error in remotedialer server [400]: websocket: close 1006 (abnormal closure): unexpected EOF"
time="2024-03-25T05:31:20Z" level=info msg="Handling backend connection request [server]"
I0325 05:31:24.683064    2321 range_allocator.go:111] "No Secondary Service CIDR provided. Skipping filtering out secondary service addresses"
I0325 05:31:24.683774    2321 controllermanager.go:642] "Started controller" controller="node-ipam-controller"
I0325 05:31:24.684126    2321 node_ipam_controller.go:162] "Starting ipam controller"
I0325 05:31:24.689345    2321 shared_informer.go:311] Waiting for caches to sync for node
I0325 05:31:24.694937    2321 controllermanager.go:642] "Started controller" controller="persistentvolume-protection-controller"
I0325 05:31:24.695124    2321 pv_protection_controller.go:78] "Starting PV protection controller"
I0325 05:31:24.695137    2321 shared_informer.go:311] Waiting for caches to sync for PV protection
I0325 05:31:24.702700    2321 controllermanager.go:642] "Started controller" controller="replicationcontroller-controller"
I0325 05:31:24.702879    2321 replica_set.go:214] "Starting controller" name="replicationcontroller"
I0325 05:31:24.702888    2321 shared_informer.go:311] Waiting for caches to sync for ReplicationController
E0325 05:31:24.716297    2321 namespaced_resources_deleter.go:162] unable to get all supported resources from server: unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I0325 05:31:24.716530    2321 controllermanager.go:642] "Started controller" controller="namespace-controller"
I0325 05:31:24.716645    2321 namespace_controller.go:197] "Starting namespace controller"
I0325 05:31:24.716665    2321 shared_informer.go:311] Waiting for caches to sync for namespace
I0325 05:31:24.722646    2321 controllermanager.go:642] "Started controller" controller="certificatesigningrequest-approving-controller"
I0325 05:31:24.722737    2321 certificate_controller.go:115] "Starting certificate controller" name="csrapproving"
I0325 05:31:24.722739    2321 controllermanager.go:607] "Warning: controller is disabled" controller="cloud-node-lifecycle-controller"
I0325 05:31:24.722748    2321 shared_informer.go:311] Waiting for caches to sync for certificate-csrapproving
I0325 05:31:24.728833    2321 controllermanager.go:642] "Started controller" controller="ephemeral-volume-controller"
I0325 05:31:24.728900    2321 controller.go:169] "Starting ephemeral volume controller"
I0325 05:31:24.728907    2321 shared_informer.go:311] Waiting for caches to sync for ephemeral
I0325 05:31:24.734908    2321 controllermanager.go:642] "Started controller" controller="daemonset-controller"
I0325 05:31:24.735053    2321 daemon_controller.go:291] "Starting daemon sets controller"
I0325 05:31:24.735062    2321 shared_informer.go:311] Waiting for caches to sync for daemon sets
I0325 05:31:24.737545    2321 certificate_controller.go:115] "Starting certificate controller" name="csrsigning-kubelet-serving"
I0325 05:31:24.737642    2321 shared_informer.go:311] Waiting for caches to sync for certificate-csrsigning-kubelet-serving
I0325 05:31:24.737780    2321 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/var/lib/rancher/k3s/server/tls/server-ca.nochain.crt::/var/lib/rancher/k3s/server/tls/server-ca.key"
I0325 05:31:24.737910    2321 controllermanager.go:642] "Started controller" controller="certificatesigningrequest-signing-controller"
I0325 05:31:24.738050    2321 certificate_controller.go:115] "Starting certificate controller" name="csrsigning-kubelet-client"
I0325 05:31:24.738060    2321 shared_informer.go:311] Waiting for caches to sync for certificate-csrsigning-kubelet-client
I0325 05:31:24.738078    2321 certificate_controller.go:115] "Starting certificate controller" name="csrsigning-kube-apiserver-client"
I0325 05:31:24.738084    2321 shared_informer.go:311] Waiting for caches to sync for certificate-csrsigning-kube-apiserver-client
I0325 05:31:24.738094    2321 certificate_controller.go:115] "Starting certificate controller" name="csrsigning-legacy-unknown"
I0325 05:31:24.738097    2321 shared_informer.go:311] Waiting for caches to sync for certificate-csrsigning-legacy-unknown
I0325 05:31:24.738115    2321 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/var/lib/rancher/k3s/server/tls/client-ca.nochain.crt::/var/lib/rancher/k3s/server/tls/client-ca.key"
I0325 05:31:24.738175    2321 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/var/lib/rancher/k3s/server/tls/client-ca.nochain.crt::/var/lib/rancher/k3s/server/tls/client-ca.key"
I0325 05:31:24.738212    2321 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/var/lib/rancher/k3s/server/tls/server-ca.nochain.crt::/var/lib/rancher/k3s/server/tls/server-ca.key"
I0325 05:31:24.740445    2321 node_lifecycle_controller.go:431] "Controller will reconcile labels"
I0325 05:31:24.740581    2321 controllermanager.go:642] "Started controller" controller="node-lifecycle-controller"
I0325 05:31:24.740809    2321 node_lifecycle_controller.go:465] "Sending events to api server"
I0325 05:31:24.740968    2321 node_lifecycle_controller.go:476] "Starting node controller"
I0325 05:31:24.741052    2321 shared_informer.go:311] Waiting for caches to sync for taint
I0325 05:31:24.746304    2321 controllermanager.go:642] "Started controller" controller="endpointslice-mirroring-controller"
I0325 05:31:24.746546    2321 endpointslicemirroring_controller.go:223] "Starting EndpointSliceMirroring controller"
I0325 05:31:24.746640    2321 shared_informer.go:311] Waiting for caches to sync for endpoint_slice_mirroring
I0325 05:31:24.752904    2321 controllermanager.go:642] "Started controller" controller="persistentvolumeclaim-protection-controller"
I0325 05:31:24.752990    2321 pvc_protection_controller.go:102] "Starting PVC protection controller"
I0325 05:31:24.752999    2321 shared_informer.go:311] Waiting for caches to sync for PVC protection
I0325 05:31:24.846255    2321 controllermanager.go:642] "Started controller" controller="root-ca-certificate-publisher-controller"
I0325 05:31:24.847045    2321 publisher.go:102] "Starting root CA cert publisher controller"
I0325 05:31:24.847357    2321 shared_informer.go:311] Waiting for caches to sync for crt configmap
I0325 05:31:24.968099    2321 controllermanager.go:642] "Started controller" controller="persistentvolume-attach-detach-controller"
I0325 05:31:24.968900    2321 attach_detach_controller.go:337] "Starting attach detach controller"
I0325 05:31:24.968971    2321 shared_informer.go:311] Waiting for caches to sync for attach detach
E0325 05:31:25.279771    2321 resource_quota_controller.go:169] initial discovery check failure, continuing and counting on future sync update: unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I0325 05:31:25.281104    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="roles.rbac.authorization.k8s.io"
I0325 05:31:25.281196    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="limitranges"
I0325 05:31:25.281226    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="endpoints"
I0325 05:31:25.281256    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="controllerrevisions.apps"
I0325 05:31:25.281284    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="cronjobs.batch"
I0325 05:31:25.281329    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="leases.coordination.k8s.io"
I0325 05:31:25.281357    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="endpointslices.discovery.k8s.io"
I0325 05:31:25.281399    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="csistoragecapacities.storage.k8s.io"
I0325 05:31:25.281514    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="horizontalpodautoscalers.autoscaling"
I0325 05:31:25.281570    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="rolebindings.rbac.authorization.k8s.io"
I0325 05:31:25.281611    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="helmchartconfigs.helm.cattle.io"
I0325 05:31:25.281649    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="jobs.batch"
I0325 05:31:25.281683    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="networkpolicies.networking.k8s.io"
I0325 05:31:25.281713    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="poddisruptionbudgets.policy"
I0325 05:31:25.281744    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="replicasets.apps"
I0325 05:31:25.281773    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="deployments.apps"
I0325 05:31:25.281806    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="addons.k3s.cattle.io"
I0325 05:31:25.281846    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="podtemplates"
I0325 05:31:25.281913    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="statefulsets.apps"
I0325 05:31:25.281942    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="daemonsets.apps"
I0325 05:31:25.282020    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="serviceaccounts"
I0325 05:31:25.282051    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingresses.networking.k8s.io"
I0325 05:31:25.282081    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="helmcharts.helm.cattle.io"
I0325 05:31:25.282106    2321 controllermanager.go:642] "Started controller" controller="resourcequota-controller"
E0325 05:31:25.285818    2321 resource_quota_controller.go:440] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I0325 05:31:25.286105    2321 shared_informer.go:311] Waiting for caches to sync for resource quota
I0325 05:31:25.286346    2321 resource_quota_controller.go:294] "Starting resource quota controller"
I0325 05:31:25.286507    2321 shared_informer.go:311] Waiting for caches to sync for resource quota
I0325 05:31:25.286612    2321 resource_quota_monitor.go:305] "QuotaMonitor running"
I0325 05:31:25.451473    2321 controllermanager.go:642] "Started controller" controller="job-controller"
I0325 05:31:25.452127    2321 job_controller.go:226] "Starting job controller"
I0325 05:31:25.452203    2321 shared_informer.go:311] Waiting for caches to sync for job
I0325 05:31:25.638298    2321 controllermanager.go:642] "Started controller" controller="replicaset-controller"
I0325 05:31:25.638414    2321 controllermanager.go:607] "Warning: controller is disabled" controller="service-lb-controller"
I0325 05:31:25.639092    2321 replica_set.go:214] "Starting controller" name="replicaset"
I0325 05:31:25.639173    2321 shared_informer.go:311] Waiting for caches to sync for ReplicaSet
I0325 05:31:25.719889    2321 controllermanager.go:642] "Started controller" controller="persistentvolume-expander-controller"
I0325 05:31:25.720531    2321 expand_controller.go:328] "Starting expand controller"
I0325 05:31:25.720976    2321 shared_informer.go:311] Waiting for caches to sync for expand
I0325 05:31:25.867784    2321 controllermanager.go:642] "Started controller" controller="deployment-controller"
I0325 05:31:25.868704    2321 deployment_controller.go:168] "Starting controller" controller="deployment"
I0325 05:31:25.869158    2321 shared_informer.go:311] Waiting for caches to sync for deployment
I0325 05:31:26.031508    2321 controllermanager.go:642] "Started controller" controller="statefulset-controller"
I0325 05:31:26.037137    2321 stateful_set.go:161] "Starting stateful set controller"
I0325 05:31:26.037562    2321 shared_informer.go:311] Waiting for caches to sync for stateful set
I0325 05:31:26.068426    2321 controllermanager.go:642] "Started controller" controller="certificatesigningrequest-cleaner-controller"
I0325 05:31:26.068527    2321 controllermanager.go:607] "Warning: controller is disabled" controller="bootstrap-signer-controller"
I0325 05:31:26.068552    2321 controllermanager.go:607] "Warning: controller is disabled" controller="node-route-controller"
I0325 05:31:26.068690    2321 cleaner.go:83] "Starting CSR cleaner controller"
I0325 05:31:26.259370    2321 controllermanager.go:642] "Started controller" controller="pod-garbage-collector-controller"
I0325 05:31:26.260023    2321 gc_controller.go:101] "Starting GC controller"
I0325 05:31:26.260064    2321 shared_informer.go:311] Waiting for caches to sync for GC
I0325 05:31:26.513302    2321 controllermanager.go:642] "Started controller" controller="horizontal-pod-autoscaler-controller"
I0325 05:31:26.513670    2321 horizontal.go:200] "Starting HPA controller"
I0325 05:31:26.513833    2321 shared_informer.go:311] Waiting for caches to sync for HPA
I0325 05:31:26.718331    2321 controllermanager.go:642] "Started controller" controller="disruption-controller"
I0325 05:31:26.719291    2321 disruption.go:433] "Sending events to api server."
I0325 05:31:26.719506    2321 disruption.go:444] "Starting disruption controller"
I0325 05:31:26.719544    2321 shared_informer.go:311] Waiting for caches to sync for disruption
I0325 05:31:26.772140    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0325 05:31:26.772682    2321 actual_state_of_world.go:547] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"server\" does not exist"
I0325 05:31:26.773303    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0325 05:31:26.784191    2321 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0325 05:31:26.789409    2321 shared_informer.go:318] Caches are synced for node
I0325 05:31:26.789541    2321 range_allocator.go:174] "Sending events to api server"
I0325 05:31:26.789641    2321 range_allocator.go:178] "Starting range CIDR allocator"
I0325 05:31:26.789724    2321 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I0325 05:31:26.789734    2321 shared_informer.go:318] Caches are synced for cidrallocator
I0325 05:31:26.795218    2321 shared_informer.go:318] Caches are synced for PV protection
I0325 05:31:26.795519    2321 range_allocator.go:380] "Set node PodCIDR" node="server" podCIDRs=["10.42.0.0/24"]
time="2024-03-25T05:31:26Z" level=info msg="Flannel found PodCIDR assigned for node server"
I0325 05:31:26.800327    2321 shared_informer.go:318] Caches are synced for TTL
time="2024-03-25T05:31:26Z" level=info msg="The interface enp0s3 with ipv4 address 10.0.2.15 will be used by flannel"
I0325 05:31:26.807077    2321 kube.go:139] Waiting 10m0s for node controller to sync
I0325 05:31:26.807813    2321 kube.go:461] Starting kube subnet manager
I0325 05:31:26.814175    2321 shared_informer.go:318] Caches are synced for HPA
I0325 05:31:26.814780    2321 shared_informer.go:318] Caches are synced for ReplicationController
I0325 05:31:26.814797    2321 shared_informer.go:318] Caches are synced for TTL after finished
I0325 05:31:26.816731    2321 shared_informer.go:318] Caches are synced for namespace
I0325 05:31:26.821879    2321 shared_informer.go:318] Caches are synced for expand
I0325 05:31:26.822778    2321 shared_informer.go:318] Caches are synced for certificate-csrapproving
I0325 05:31:26.829091    2321 shared_informer.go:318] Caches are synced for ephemeral
I0325 05:31:26.833532    2321 shared_informer.go:318] Caches are synced for cronjob
I0325 05:31:26.835895    2321 shared_informer.go:318] Caches are synced for daemon sets
I0325 05:31:26.838515    2321 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I0325 05:31:26.838543    2321 shared_informer.go:318] Caches are synced for stateful set
I0325 05:31:26.838560    2321 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I0325 05:31:26.838573    2321 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0325 05:31:26.838579    2321 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I0325 05:31:26.841117    2321 shared_informer.go:318] Caches are synced for ReplicaSet
I0325 05:31:26.841603    2321 shared_informer.go:318] Caches are synced for taint
I0325 05:31:26.841949    2321 node_lifecycle_controller.go:1225] "Initializing eviction metric for zone" zone=""
I0325 05:31:26.842283    2321 node_lifecycle_controller.go:877] "Missing timestamp for Node. Assuming now as a timestamp" node="server"
I0325 05:31:26.842525    2321 node_lifecycle_controller.go:1071] "Controller detected that zone is now in new state" zone="" newState="Normal"
I0325 05:31:26.843179    2321 taint_manager.go:205] "Starting NoExecuteTaintManager"
I0325 05:31:26.843591    2321 taint_manager.go:210] "Sending events to api server"
I0325 05:31:26.843599    2321 event.go:307] "Event occurred" object="server" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node server event: Registered Node server in Controller"
I0325 05:31:26.845935    2321 shared_informer.go:318] Caches are synced for persistent volume
I0325 05:31:26.847105    2321 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I0325 05:31:26.848429    2321 shared_informer.go:318] Caches are synced for crt configmap
I0325 05:31:26.849469    2321 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I0325 05:31:26.852654    2321 shared_informer.go:318] Caches are synced for job
I0325 05:31:26.853135    2321 shared_informer.go:318] Caches are synced for PVC protection
I0325 05:31:26.853760    2321 shared_informer.go:318] Caches are synced for endpoint
I0325 05:31:26.859497    2321 shared_informer.go:318] Caches are synced for endpoint_slice
I0325 05:31:26.860172    2321 shared_informer.go:318] Caches are synced for GC
I0325 05:31:26.865844    2321 shared_informer.go:318] Caches are synced for service account
I0325 05:31:26.869101    2321 shared_informer.go:318] Caches are synced for attach detach
I0325 05:31:26.870435    2321 shared_informer.go:318] Caches are synced for deployment
time="2024-03-25T05:31:26Z" level=info msg="Starting network policy controller version v2.0.1, built on 2024-02-29T20:36:21Z, go1.21.7"
I0325 05:31:26.933303    2321 network_policy_controller.go:164] Starting network policy controller
I0325 05:31:26.986463    2321 shared_informer.go:318] Caches are synced for resource quota
I0325 05:31:26.986761    2321 shared_informer.go:318] Caches are synced for resource quota
I0325 05:31:26.990058    2321 network_policy_controller.go:176] Starting network policy controller full sync goroutine
I0325 05:31:27.019739    2321 shared_informer.go:318] Caches are synced for disruption
I0325 05:31:27.318073    2321 event.go:307] "Event occurred" object="kube-system/helm-install-traefik" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: helm-install-traefik-9pn9j"
I0325 05:31:27.318097    2321 event.go:307] "Event occurred" object="kube-system/helm-install-traefik-crd" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: helm-install-traefik-crd-grh22"
I0325 05:31:27.319830    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0325 05:31:27.322401    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0325 05:31:27.331134    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0325 05:31:27.331256    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0325 05:31:27.332458    2321 topology_manager.go:215] "Topology Admit Handler" podUID="e02d41e1-eeb4-4804-b989-7bcd4c998148" podNamespace="kube-system" podName="helm-install-traefik-crd-grh22"
I0325 05:31:27.332663    2321 topology_manager.go:215] "Topology Admit Handler" podUID="e173886a-a2c5-423f-bb17-cad2b3891e1e" podNamespace="kube-system" podName="helm-install-traefik-9pn9j"
I0325 05:31:27.334080    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0325 05:31:27.334349    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0325 05:31:27.359160    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/e173886a-a2c5-423f-bb17-cad2b3891e1e-klipper-helm\") pod \"helm-install-traefik-9pn9j\" (UID: \"e173886a-a2c5-423f-bb17-cad2b3891e1e\") " pod="kube-system/helm-install-traefik-9pn9j"
I0325 05:31:27.384607    2321 shared_informer.go:318] Caches are synced for garbage collector
W0325 05:31:27.385671    2321 handler_proxy.go:93] no RequestInfo found in the context
E0325 05:31:27.385881    2321 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
E0325 05:31:27.386077    2321 handler_proxy.go:137] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
I0325 05:31:27.387462    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0325 05:31:27.396175    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0325 05:31:27.438684    2321 shared_informer.go:318] Caches are synced for garbage collector
I0325 05:31:27.438712    2321 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I0325 05:31:27.460745    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/e02d41e1-eeb4-4804-b989-7bcd4c998148-klipper-cache\") pod \"helm-install-traefik-crd-grh22\" (UID: \"e02d41e1-eeb4-4804-b989-7bcd4c998148\") " pod="kube-system/helm-install-traefik-crd-grh22"
I0325 05:31:27.460781    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/e02d41e1-eeb4-4804-b989-7bcd4c998148-klipper-config\") pod \"helm-install-traefik-crd-grh22\" (UID: \"e02d41e1-eeb4-4804-b989-7bcd4c998148\") " pod="kube-system/helm-install-traefik-crd-grh22"
I0325 05:31:27.460818    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"values\" (UniqueName: \"kubernetes.io/secret/e173886a-a2c5-423f-bb17-cad2b3891e1e-values\") pod \"helm-install-traefik-9pn9j\" (UID: \"e173886a-a2c5-423f-bb17-cad2b3891e1e\") " pod="kube-system/helm-install-traefik-9pn9j"
I0325 05:31:27.460877    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/e02d41e1-eeb4-4804-b989-7bcd4c998148-klipper-helm\") pod \"helm-install-traefik-crd-grh22\" (UID: \"e02d41e1-eeb4-4804-b989-7bcd4c998148\") " pod="kube-system/helm-install-traefik-crd-grh22"
I0325 05:31:27.460894    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/e02d41e1-eeb4-4804-b989-7bcd4c998148-tmp\") pod \"helm-install-traefik-crd-grh22\" (UID: \"e02d41e1-eeb4-4804-b989-7bcd4c998148\") " pod="kube-system/helm-install-traefik-crd-grh22"
I0325 05:31:27.460925    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"values\" (UniqueName: \"kubernetes.io/secret/e02d41e1-eeb4-4804-b989-7bcd4c998148-values\") pod \"helm-install-traefik-crd-grh22\" (UID: \"e02d41e1-eeb4-4804-b989-7bcd4c998148\") " pod="kube-system/helm-install-traefik-crd-grh22"
I0325 05:31:27.460949    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-vfwlw\" (UniqueName: \"kubernetes.io/projected/e02d41e1-eeb4-4804-b989-7bcd4c998148-kube-api-access-vfwlw\") pod \"helm-install-traefik-crd-grh22\" (UID: \"e02d41e1-eeb4-4804-b989-7bcd4c998148\") " pod="kube-system/helm-install-traefik-crd-grh22"
I0325 05:31:27.460965    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/e173886a-a2c5-423f-bb17-cad2b3891e1e-tmp\") pod \"helm-install-traefik-9pn9j\" (UID: \"e173886a-a2c5-423f-bb17-cad2b3891e1e\") " pod="kube-system/helm-install-traefik-9pn9j"
I0325 05:31:27.460984    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"content\" (UniqueName: \"kubernetes.io/configmap/e173886a-a2c5-423f-bb17-cad2b3891e1e-content\") pod \"helm-install-traefik-9pn9j\" (UID: \"e173886a-a2c5-423f-bb17-cad2b3891e1e\") " pod="kube-system/helm-install-traefik-9pn9j"
I0325 05:31:27.461017    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-tsthj\" (UniqueName: \"kubernetes.io/projected/e173886a-a2c5-423f-bb17-cad2b3891e1e-kube-api-access-tsthj\") pod \"helm-install-traefik-9pn9j\" (UID: \"e173886a-a2c5-423f-bb17-cad2b3891e1e\") " pod="kube-system/helm-install-traefik-9pn9j"
I0325 05:31:27.461033    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/e173886a-a2c5-423f-bb17-cad2b3891e1e-klipper-config\") pod \"helm-install-traefik-9pn9j\" (UID: \"e173886a-a2c5-423f-bb17-cad2b3891e1e\") " pod="kube-system/helm-install-traefik-9pn9j"
I0325 05:31:27.461054    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"content\" (UniqueName: \"kubernetes.io/configmap/e02d41e1-eeb4-4804-b989-7bcd4c998148-content\") pod \"helm-install-traefik-crd-grh22\" (UID: \"e02d41e1-eeb4-4804-b989-7bcd4c998148\") " pod="kube-system/helm-install-traefik-crd-grh22"
I0325 05:31:27.461101    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/e173886a-a2c5-423f-bb17-cad2b3891e1e-klipper-cache\") pod \"helm-install-traefik-9pn9j\" (UID: \"e173886a-a2c5-423f-bb17-cad2b3891e1e\") " pod="kube-system/helm-install-traefik-9pn9j"
I0325 05:31:27.514033    2321 controller.go:624] quota admission added evaluator for: replicasets.apps
I0325 05:31:27.517206    2321 event.go:307] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set coredns-6799fbcd5 to 1"
I0325 05:31:27.520157    2321 event.go:307] "Event occurred" object="kube-system/local-path-provisioner" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set local-path-provisioner-6c86858495 to 1"
I0325 05:31:27.520179    2321 event.go:307] "Event occurred" object="kube-system/metrics-server" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set metrics-server-67c658944b to 1"
I0325 05:31:27.651493    2321 event.go:307] "Event occurred" object="kube-system/local-path-provisioner-6c86858495" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: local-path-provisioner-6c86858495-77f7f"
I0325 05:31:27.651528    2321 event.go:307] "Event occurred" object="kube-system/metrics-server-67c658944b" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: metrics-server-67c658944b-zcmj6"
I0325 05:31:27.651536    2321 event.go:307] "Event occurred" object="kube-system/coredns-6799fbcd5" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: coredns-6799fbcd5-v7fsw"
I0325 05:31:27.667555    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="146.504015ms"
I0325 05:31:27.677147    2321 topology_manager.go:215] "Topology Admit Handler" podUID="4dbbb38e-6ee1-4fe5-9ad0-df4dae0d4725" podNamespace="kube-system" podName="coredns-6799fbcd5-v7fsw"
I0325 05:31:27.689718    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="173.020649ms"
I0325 05:31:27.690265    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="169.057342ms"
I0325 05:31:27.692317    2321 topology_manager.go:215] "Topology Admit Handler" podUID="d33433d8-efd2-4292-b857-6c973c0dc7e3" podNamespace="kube-system" podName="local-path-provisioner-6c86858495-77f7f"
I0325 05:31:27.692585    2321 topology_manager.go:215] "Topology Admit Handler" podUID="ce1bf72c-74ed-4215-8a4c-f02117b0c5a9" podNamespace="kube-system" podName="metrics-server-67c658944b-zcmj6"
I0325 05:31:27.712476    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="43.497599ms"
I0325 05:31:27.712573    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="67.925s"
I0325 05:31:27.737007    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="46.672351ms"
I0325 05:31:27.737491    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="47.651186ms"
I0325 05:31:27.762568    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-vhcr7\" (UniqueName: \"kubernetes.io/projected/ce1bf72c-74ed-4215-8a4c-f02117b0c5a9-kube-api-access-vhcr7\") pod \"metrics-server-67c658944b-zcmj6\" (UID: \"ce1bf72c-74ed-4215-8a4c-f02117b0c5a9\") " pod="kube-system/metrics-server-67c658944b-zcmj6"
I0325 05:31:27.762605    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-swpgg\" (UniqueName: \"kubernetes.io/projected/d33433d8-efd2-4292-b857-6c973c0dc7e3-kube-api-access-swpgg\") pod \"local-path-provisioner-6c86858495-77f7f\" (UID: \"d33433d8-efd2-4292-b857-6c973c0dc7e3\") " pod="kube-system/local-path-provisioner-6c86858495-77f7f"
I0325 05:31:27.762633    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/4dbbb38e-6ee1-4fe5-9ad0-df4dae0d4725-config-volume\") pod \"coredns-6799fbcd5-v7fsw\" (UID: \"4dbbb38e-6ee1-4fe5-9ad0-df4dae0d4725\") " pod="kube-system/coredns-6799fbcd5-v7fsw"
I0325 05:31:27.762650    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"custom-config-volume\" (UniqueName: \"kubernetes.io/configmap/4dbbb38e-6ee1-4fe5-9ad0-df4dae0d4725-custom-config-volume\") pod \"coredns-6799fbcd5-v7fsw\" (UID: \"4dbbb38e-6ee1-4fe5-9ad0-df4dae0d4725\") " pod="kube-system/coredns-6799fbcd5-v7fsw"
I0325 05:31:27.762666    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/d33433d8-efd2-4292-b857-6c973c0dc7e3-config-volume\") pod \"local-path-provisioner-6c86858495-77f7f\" (UID: \"d33433d8-efd2-4292-b857-6c973c0dc7e3\") " pod="kube-system/local-path-provisioner-6c86858495-77f7f"
I0325 05:31:27.762683    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp-dir\" (UniqueName: \"kubernetes.io/empty-dir/ce1bf72c-74ed-4215-8a4c-f02117b0c5a9-tmp-dir\") pod \"metrics-server-67c658944b-zcmj6\" (UID: \"ce1bf72c-74ed-4215-8a4c-f02117b0c5a9\") " pod="kube-system/metrics-server-67c658944b-zcmj6"
I0325 05:31:27.762701    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-njssm\" (UniqueName: \"kubernetes.io/projected/4dbbb38e-6ee1-4fe5-9ad0-df4dae0d4725-kube-api-access-njssm\") pod \"coredns-6799fbcd5-v7fsw\" (UID: \"4dbbb38e-6ee1-4fe5-9ad0-df4dae0d4725\") " pod="kube-system/coredns-6799fbcd5-v7fsw"
I0325 05:31:27.774157    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="48.901s"
I0325 05:31:27.775472    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="37.914271ms"
I0325 05:31:27.777085    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="40.67s"
I0325 05:31:27.775747    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="38.584089ms"
I0325 05:31:27.777124    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="18.904s"
I0325 05:31:27.782522    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="34.887s"
I0325 05:31:27.808814    2321 kube.go:146] Node controller sync successful
I0325 05:31:27.808865    2321 vxlan.go:141] VXLAN config: VNI=1 Port=0 GBP=false Learning=false DirectRouting=false
I0325 05:31:27.811456    2321 kube.go:621] List of node(server) annotations: map[string]string{"alpha.kubernetes.io/provided-node-ip":"192.168.56.110", "k3s.io/hostname":"server", "k3s.io/internal-ip":"192.168.56.110", "k3s.io/node-args":"[\"server\",\"--node-ip\",\"192.168.56.110\",\"--write-kubeconfig-mode\",\"644\",\"--log\",\"/vagrant/logs/server.logs\"]", "k3s.io/node-config-hash":"OEDLXPLCFN65I5K4IWF2M6JMEKD34L47WZJH7YZKLHVRXJTJG6TQ====", "k3s.io/node-env":"{\"K3S_DATA_DIR\":\"/var/lib/rancher/k3s/data/a3b46c0299091b71bfcc617b1e1fec1845c13bdd848584ceb39d2e700e702a4b\"}", "node.alpha.kubernetes.io/ttl":"0", "volumes.kubernetes.io/controller-managed-attach-detach":"true"}
I0325 05:31:27.848598    2321 kube.go:482] Creating the node lease for IPv4. This is the n.Spec.PodCIDRs: [10.42.0.0/24]
time="2024-03-25T05:31:27Z" level=info msg="Wrote flannel subnet file to /run/flannel/subnet.env"
time="2024-03-25T05:31:27Z" level=info msg="Running flannel backend."
I0325 05:31:27.853179    2321 vxlan_network.go:65] watching for new subnet leases
I0325 05:31:27.855366    2321 iptables.go:290] generated 7 rules
I0325 05:31:27.859307    2321 iptables.go:290] generated 3 rules
I0325 05:31:27.888273    2321 iptables.go:283] bootstrap done
E0325 05:31:27.902209    2321 iptables.go:320] Failed to ensure iptables rules: error checking rule existence: failed to check rule existence: running [/usr/sbin/iptables -t filter -C FORWARD -m comment --comment flanneld forward -j FLANNEL-FWD --wait]: exit status 2: iptables v1.8.4 (legacy): Couldn't load target `FLANNEL-FWD':No such file or directory

Try `iptables -h' or 'iptables --help' for more information.
I0325 05:31:27.905618    2321 iptables.go:283] bootstrap done
W0325 05:31:28.409838    2321 handler_proxy.go:93] no RequestInfo found in the context
W0325 05:31:28.410208    2321 handler_proxy.go:93] no RequestInfo found in the context
E0325 05:31:28.410298    2321 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I0325 05:31:28.410466    2321 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E0325 05:31:28.410401    2321 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0325 05:31:28.412049    2321 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0325 05:31:34.611130    2321 kuberuntime_manager.go:1528] "Updating runtime config through cri with podcidr" CIDR="10.42.0.0/24"
I0325 05:31:34.613013    2321 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.42.0.0/24"
I0325 05:31:49.661702    2321 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/coredns-6799fbcd5-v7fsw" containerName="coredns"
I0325 05:31:50.011898    2321 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/local-path-provisioner-6c86858495-77f7f" containerName="local-path-provisioner"
I0325 05:31:50.269815    2321 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/coredns-6799fbcd5-v7fsw" podStartSLOduration=5.7592565570000005 podCreationTimestamp="2024-03-25 05:31:27 +0000 UTC" firstStartedPulling="2024-03-25 05:31:32.142634977 +0000 UTC m=+22.190309596" lastFinishedPulling="2024-03-25 05:31:49.650811887 +0000 UTC m=+39.698486512" observedRunningTime="2024-03-25 05:31:50.266213021 +0000 UTC m=+40.313887657" watchObservedRunningTime="2024-03-25 05:31:50.267433473 +0000 UTC m=+40.315108110"
I0325 05:31:50.283159    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="62.033s"
I0325 05:31:50.316903    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="28.376277ms"
I0325 05:31:50.316983    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="51.699s"
I0325 05:31:50.355877    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="8.983059ms"
I0325 05:31:50.355984    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="61.763s"
I0325 05:31:56.298083    2321 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/metrics-server-67c658944b-zcmj6" containerName="metrics-server"
E0325 05:31:57.044249    2321 resource_quota_controller.go:440] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I0325 05:31:57.270851    2321 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/helm-install-traefik-crd-grh22" containerName="helm"
I0325 05:31:57.275666    2321 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/helm-install-traefik-9pn9j" containerName="helm"
I0325 05:31:57.345877    2321 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/local-path-provisioner-6c86858495-77f7f" podStartSLOduration=12.44808458 podCreationTimestamp="2024-03-25 05:31:27 +0000 UTC" firstStartedPulling="2024-03-25 05:31:32.113820168 +0000 UTC m=+22.161494787" lastFinishedPulling="2024-03-25 05:31:50.009134352 +0000 UTC m=+40.056808977" observedRunningTime="2024-03-25 05:31:50.34728227 +0000 UTC m=+40.394956904" watchObservedRunningTime="2024-03-25 05:31:57.34339877 +0000 UTC m=+47.391073397"
I0325 05:31:57.358739    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="3.604554ms"
I0325 05:31:57.482203    2321 garbagecollector.go:816] "failed to discover some groups" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"
I0325 05:31:58.337985    2321 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/helm-install-traefik-crd-grh22" podStartSLOduration=6.283539242 podCreationTimestamp="2024-03-25 05:31:27 +0000 UTC" firstStartedPulling="2024-03-25 05:31:32.211119504 +0000 UTC m=+22.258794128" lastFinishedPulling="2024-03-25 05:31:57.265528443 +0000 UTC m=+47.313203064" observedRunningTime="2024-03-25 05:31:58.337793894 +0000 UTC m=+48.385468528" watchObservedRunningTime="2024-03-25 05:31:58.337948178 +0000 UTC m=+48.385622801"
I0325 05:31:58.338224    2321 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/metrics-server-67c658944b-zcmj6" podStartSLOduration=7.168657018 podCreationTimestamp="2024-03-25 05:31:27 +0000 UTC" firstStartedPulling="2024-03-25 05:31:32.114027912 +0000 UTC m=+22.161702530" lastFinishedPulling="2024-03-25 05:31:56.283575336 +0000 UTC m=+46.331249968" observedRunningTime="2024-03-25 05:31:57.346118154 +0000 UTC m=+47.393792781" watchObservedRunningTime="2024-03-25 05:31:58.338204456 +0000 UTC m=+48.385879086"
I0325 05:31:58.345425    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0325 05:31:58.361746    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
W0325 05:31:59.040456    2321 aggregator.go:166] failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0325 05:31:59.343116    2321 scope.go:117] "RemoveContainer" containerID="541eee045b9ac152c14c1d239e6c1c9c015d9206aa9d50e0fd1ed414e3481f63"
I0325 05:31:59.403178    2321 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/helm-install-traefik-9pn9j" containerName="helm"
I0325 05:31:59.422629    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0325 05:31:59.424288    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0325 05:31:59.425163    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0325 05:32:00.173768    2321 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0325 05:32:00.221458    2321 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0325 05:32:00.250409    2321 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0325 05:32:00.264519    2321 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0325 05:32:00.275665    2321 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0325 05:32:00.299260    2321 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0325 05:32:00.357931    2321 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0325 05:32:00.369410    2321 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0325 05:32:00.380611    2321 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0325 05:32:00.484355    2321 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0325 05:32:00.494652    2321 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0325 05:32:00.549044    2321 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0325 05:32:00.647517    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0325 05:32:00.765353    2321 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/helm-install-traefik-9pn9j" podStartSLOduration=8.701094175 podCreationTimestamp="2024-03-25 05:31:27 +0000 UTC" firstStartedPulling="2024-03-25 05:31:32.211233103 +0000 UTC m=+22.258907722" lastFinishedPulling="2024-03-25 05:31:57.272240298 +0000 UTC m=+47.319914917" observedRunningTime="2024-03-25 05:31:58.363656451 +0000 UTC m=+48.411331086" watchObservedRunningTime="2024-03-25 05:32:00.76210137 +0000 UTC m=+50.809775996"
I0325 05:32:00.769207    2321 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0325 05:32:00.782034    2321 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0325 05:32:00.785811    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0325 05:32:00.789438    2321 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0325 05:32:00.807675    2321 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0325 05:32:00.847973    2321 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0325 05:32:00.859350    2321 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0325 05:32:00.869048    2321 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
W0325 05:32:01.026638    2321 aggregator.go:166] failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0325 05:32:01.565835    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0325 05:32:01.666389    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0325 05:32:02.017462    2321 alloc.go:330] "allocated clusterIPs" service="kube-system/traefik" clusterIPs={"IPv4":"10.43.41.10"}
I0325 05:32:02.070128    2321 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Service" apiVersion="v1" type="Normal" reason="EnsuringLoadBalancer" message="Ensuring load balancer"
I0325 05:32:02.155192    2321 controller.go:624] quota admission added evaluator for: ingressroutes.traefik.io
I0325 05:32:02.185829    2321 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set traefik-f4564c4f4 to 1"
I0325 05:32:02.241646    2321 controller.go:624] quota admission added evaluator for: daemonsets.apps
I0325 05:32:02.250212    2321 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Service" apiVersion="v1" type="Normal" reason="AppliedDaemonSet" message="Applied LoadBalancer DaemonSet kube-system/svclb-traefik-a3320499"
I0325 05:32:02.304137    2321 event.go:307] "Event occurred" object="kube-system/traefik-f4564c4f4" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: traefik-f4564c4f4-9jp8p"
I0325 05:32:02.346334    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="154.444129ms"
I0325 05:32:02.384032    2321 controller.go:624] quota admission added evaluator for: controllerrevisions.apps
I0325 05:32:02.384208    2321 topology_manager.go:215] "Topology Admit Handler" podUID="1d6c47ab-b13f-4827-b7e5-6940831075a8" podNamespace="kube-system" podName="traefik-f4564c4f4-9jp8p"
I0325 05:32:02.432168    2321 event.go:307] "Event occurred" object="kube-system/svclb-traefik-a3320499" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: svclb-traefik-a3320499-m8r59"
I0325 05:32:02.468248    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="121.867282ms"
I0325 05:32:02.475583    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/1d6c47ab-b13f-4827-b7e5-6940831075a8-tmp\") pod \"traefik-f4564c4f4-9jp8p\" (UID: \"1d6c47ab-b13f-4827-b7e5-6940831075a8\") " pod="kube-system/traefik-f4564c4f4-9jp8p"
I0325 05:32:02.475772    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-d2w47\" (UniqueName: \"kubernetes.io/projected/1d6c47ab-b13f-4827-b7e5-6940831075a8-kube-api-access-d2w47\") pod \"traefik-f4564c4f4-9jp8p\" (UID: \"1d6c47ab-b13f-4827-b7e5-6940831075a8\") " pod="kube-system/traefik-f4564c4f4-9jp8p"
I0325 05:32:02.477590    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"data\" (UniqueName: \"kubernetes.io/empty-dir/1d6c47ab-b13f-4827-b7e5-6940831075a8-data\") pod \"traefik-f4564c4f4-9jp8p\" (UID: \"1d6c47ab-b13f-4827-b7e5-6940831075a8\") " pod="kube-system/traefik-f4564c4f4-9jp8p"
I0325 05:32:02.489429    2321 topology_manager.go:215] "Topology Admit Handler" podUID="08ff47c7-65c6-4e3e-859b-1bdc2cbe82ed" podNamespace="kube-system" podName="svclb-traefik-a3320499-m8r59"
I0325 05:32:02.601821    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="133.057768ms"
I0325 05:32:02.602018    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="146.143s"
I0325 05:32:02.665116    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0325 05:32:03.271591    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0325 05:32:03.312738    2321 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"content\" (UniqueName: \"kubernetes.io/configmap/e02d41e1-eeb4-4804-b989-7bcd4c998148-content\") pod \"e02d41e1-eeb4-4804-b989-7bcd4c998148\" (UID: \"e02d41e1-eeb4-4804-b989-7bcd4c998148\") "
I0325 05:32:03.312820    2321 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/e02d41e1-eeb4-4804-b989-7bcd4c998148-klipper-cache\") pod \"e02d41e1-eeb4-4804-b989-7bcd4c998148\" (UID: \"e02d41e1-eeb4-4804-b989-7bcd4c998148\") "
I0325 05:32:03.312846    2321 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/e02d41e1-eeb4-4804-b989-7bcd4c998148-klipper-helm\") pod \"e02d41e1-eeb4-4804-b989-7bcd4c998148\" (UID: \"e02d41e1-eeb4-4804-b989-7bcd4c998148\") "
I0325 05:32:03.312865    2321 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"values\" (UniqueName: \"kubernetes.io/secret/e02d41e1-eeb4-4804-b989-7bcd4c998148-values\") pod \"e02d41e1-eeb4-4804-b989-7bcd4c998148\" (UID: \"e02d41e1-eeb4-4804-b989-7bcd4c998148\") "
I0325 05:32:03.312883    2321 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-vfwlw\" (UniqueName: \"kubernetes.io/projected/e02d41e1-eeb4-4804-b989-7bcd4c998148-kube-api-access-vfwlw\") pod \"e02d41e1-eeb4-4804-b989-7bcd4c998148\" (UID: \"e02d41e1-eeb4-4804-b989-7bcd4c998148\") "
I0325 05:32:03.312901    2321 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/e02d41e1-eeb4-4804-b989-7bcd4c998148-klipper-config\") pod \"e02d41e1-eeb4-4804-b989-7bcd4c998148\" (UID: \"e02d41e1-eeb4-4804-b989-7bcd4c998148\") "
I0325 05:32:03.312916    2321 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/e02d41e1-eeb4-4804-b989-7bcd4c998148-tmp\") pod \"e02d41e1-eeb4-4804-b989-7bcd4c998148\" (UID: \"e02d41e1-eeb4-4804-b989-7bcd4c998148\") "
I0325 05:32:03.322784    2321 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/configmap/e02d41e1-eeb4-4804-b989-7bcd4c998148-content" (OuterVolumeSpecName: "content") pod "e02d41e1-eeb4-4804-b989-7bcd4c998148" (UID: "e02d41e1-eeb4-4804-b989-7bcd4c998148"). InnerVolumeSpecName "content". PluginName "kubernetes.io/configmap", VolumeGidValue ""
I0325 05:32:03.355710    2321 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/e02d41e1-eeb4-4804-b989-7bcd4c998148-tmp" (OuterVolumeSpecName: "tmp") pod "e02d41e1-eeb4-4804-b989-7bcd4c998148" (UID: "e02d41e1-eeb4-4804-b989-7bcd4c998148"). InnerVolumeSpecName "tmp". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0325 05:32:03.387371    2321 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/e02d41e1-eeb4-4804-b989-7bcd4c998148-values" (OuterVolumeSpecName: "values") pod "e02d41e1-eeb4-4804-b989-7bcd4c998148" (UID: "e02d41e1-eeb4-4804-b989-7bcd4c998148"). InnerVolumeSpecName "values". PluginName "kubernetes.io/secret", VolumeGidValue ""
I0325 05:32:03.390779    2321 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/e02d41e1-eeb4-4804-b989-7bcd4c998148-klipper-cache" (OuterVolumeSpecName: "klipper-cache") pod "e02d41e1-eeb4-4804-b989-7bcd4c998148" (UID: "e02d41e1-eeb4-4804-b989-7bcd4c998148"). InnerVolumeSpecName "klipper-cache". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0325 05:32:03.391254    2321 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/e02d41e1-eeb4-4804-b989-7bcd4c998148-klipper-helm" (OuterVolumeSpecName: "klipper-helm") pod "e02d41e1-eeb4-4804-b989-7bcd4c998148" (UID: "e02d41e1-eeb4-4804-b989-7bcd4c998148"). InnerVolumeSpecName "klipper-helm". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0325 05:32:03.394308    2321 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/e02d41e1-eeb4-4804-b989-7bcd4c998148-kube-api-access-vfwlw" (OuterVolumeSpecName: "kube-api-access-vfwlw") pod "e02d41e1-eeb4-4804-b989-7bcd4c998148" (UID: "e02d41e1-eeb4-4804-b989-7bcd4c998148"). InnerVolumeSpecName "kube-api-access-vfwlw". PluginName "kubernetes.io/projected", VolumeGidValue ""
I0325 05:32:03.416044    2321 reconciler_common.go:300] "Volume detached for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/e02d41e1-eeb4-4804-b989-7bcd4c998148-klipper-helm\") on node \"server\" DevicePath \"\""
I0325 05:32:03.416076    2321 reconciler_common.go:300] "Volume detached for volume \"values\" (UniqueName: \"kubernetes.io/secret/e02d41e1-eeb4-4804-b989-7bcd4c998148-values\") on node \"server\" DevicePath \"\""
I0325 05:32:03.416088    2321 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-vfwlw\" (UniqueName: \"kubernetes.io/projected/e02d41e1-eeb4-4804-b989-7bcd4c998148-kube-api-access-vfwlw\") on node \"server\" DevicePath \"\""
I0325 05:32:03.416096    2321 reconciler_common.go:300] "Volume detached for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/e02d41e1-eeb4-4804-b989-7bcd4c998148-tmp\") on node \"server\" DevicePath \"\""
I0325 05:32:03.416103    2321 reconciler_common.go:300] "Volume detached for volume \"content\" (UniqueName: \"kubernetes.io/configmap/e02d41e1-eeb4-4804-b989-7bcd4c998148-content\") on node \"server\" DevicePath \"\""
I0325 05:32:03.416111    2321 reconciler_common.go:300] "Volume detached for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/e02d41e1-eeb4-4804-b989-7bcd4c998148-klipper-cache\") on node \"server\" DevicePath \"\""
I0325 05:32:03.421899    2321 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/e02d41e1-eeb4-4804-b989-7bcd4c998148-klipper-config" (OuterVolumeSpecName: "klipper-config") pod "e02d41e1-eeb4-4804-b989-7bcd4c998148" (UID: "e02d41e1-eeb4-4804-b989-7bcd4c998148"). InnerVolumeSpecName "klipper-config". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0325 05:32:03.516784    2321 reconciler_common.go:300] "Volume detached for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/e02d41e1-eeb4-4804-b989-7bcd4c998148-klipper-config\") on node \"server\" DevicePath \"\""
I0325 05:32:03.616977    2321 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="bf9dc934d782245ce38b0d8aeb9b6b9e72d386aa5e8e2fa92c4ed23e262dac5c"
I0325 05:32:03.642644    2321 scope.go:117] "RemoveContainer" containerID="541eee045b9ac152c14c1d239e6c1c9c015d9206aa9d50e0fd1ed414e3481f63"
I0325 05:32:03.645758    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0325 05:32:03.677637    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0325 05:32:03.685008    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0325 05:32:03.698358    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0325 05:32:03.709729    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0325 05:32:03.712283    2321 event.go:307] "Event occurred" object="kube-system/helm-install-traefik-crd" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="Completed" message="Job completed"
I0325 05:32:04.704040    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0325 05:32:04.712192    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0325 05:32:04.851209    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0325 05:32:04.866151    2321 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/e173886a-a2c5-423f-bb17-cad2b3891e1e-tmp\") pod \"e173886a-a2c5-423f-bb17-cad2b3891e1e\" (UID: \"e173886a-a2c5-423f-bb17-cad2b3891e1e\") "
I0325 05:32:04.866189    2321 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"content\" (UniqueName: \"kubernetes.io/configmap/e173886a-a2c5-423f-bb17-cad2b3891e1e-content\") pod \"e173886a-a2c5-423f-bb17-cad2b3891e1e\" (UID: \"e173886a-a2c5-423f-bb17-cad2b3891e1e\") "
I0325 05:32:04.866220    2321 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-tsthj\" (UniqueName: \"kubernetes.io/projected/e173886a-a2c5-423f-bb17-cad2b3891e1e-kube-api-access-tsthj\") pod \"e173886a-a2c5-423f-bb17-cad2b3891e1e\" (UID: \"e173886a-a2c5-423f-bb17-cad2b3891e1e\") "
I0325 05:32:04.866243    2321 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/e173886a-a2c5-423f-bb17-cad2b3891e1e-klipper-helm\") pod \"e173886a-a2c5-423f-bb17-cad2b3891e1e\" (UID: \"e173886a-a2c5-423f-bb17-cad2b3891e1e\") "
I0325 05:32:04.866259    2321 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/e173886a-a2c5-423f-bb17-cad2b3891e1e-klipper-cache\") pod \"e173886a-a2c5-423f-bb17-cad2b3891e1e\" (UID: \"e173886a-a2c5-423f-bb17-cad2b3891e1e\") "
I0325 05:32:04.866275    2321 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"values\" (UniqueName: \"kubernetes.io/secret/e173886a-a2c5-423f-bb17-cad2b3891e1e-values\") pod \"e173886a-a2c5-423f-bb17-cad2b3891e1e\" (UID: \"e173886a-a2c5-423f-bb17-cad2b3891e1e\") "
I0325 05:32:04.866380    2321 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/e173886a-a2c5-423f-bb17-cad2b3891e1e-klipper-config\") pod \"e173886a-a2c5-423f-bb17-cad2b3891e1e\" (UID: \"e173886a-a2c5-423f-bb17-cad2b3891e1e\") "
I0325 05:32:04.874050    2321 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/e173886a-a2c5-423f-bb17-cad2b3891e1e-tmp" (OuterVolumeSpecName: "tmp") pod "e173886a-a2c5-423f-bb17-cad2b3891e1e" (UID: "e173886a-a2c5-423f-bb17-cad2b3891e1e"). InnerVolumeSpecName "tmp". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0325 05:32:04.875445    2321 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/configmap/e173886a-a2c5-423f-bb17-cad2b3891e1e-content" (OuterVolumeSpecName: "content") pod "e173886a-a2c5-423f-bb17-cad2b3891e1e" (UID: "e173886a-a2c5-423f-bb17-cad2b3891e1e"). InnerVolumeSpecName "content". PluginName "kubernetes.io/configmap", VolumeGidValue ""
I0325 05:32:04.881777    2321 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/e173886a-a2c5-423f-bb17-cad2b3891e1e-kube-api-access-tsthj" (OuterVolumeSpecName: "kube-api-access-tsthj") pod "e173886a-a2c5-423f-bb17-cad2b3891e1e" (UID: "e173886a-a2c5-423f-bb17-cad2b3891e1e"). InnerVolumeSpecName "kube-api-access-tsthj". PluginName "kubernetes.io/projected", VolumeGidValue ""
I0325 05:32:04.888324    2321 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/e173886a-a2c5-423f-bb17-cad2b3891e1e-klipper-cache" (OuterVolumeSpecName: "klipper-cache") pod "e173886a-a2c5-423f-bb17-cad2b3891e1e" (UID: "e173886a-a2c5-423f-bb17-cad2b3891e1e"). InnerVolumeSpecName "klipper-cache". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0325 05:32:04.892181    2321 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/e173886a-a2c5-423f-bb17-cad2b3891e1e-values" (OuterVolumeSpecName: "values") pod "e173886a-a2c5-423f-bb17-cad2b3891e1e" (UID: "e173886a-a2c5-423f-bb17-cad2b3891e1e"). InnerVolumeSpecName "values". PluginName "kubernetes.io/secret", VolumeGidValue ""
I0325 05:32:04.892491    2321 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/e173886a-a2c5-423f-bb17-cad2b3891e1e-klipper-config" (OuterVolumeSpecName: "klipper-config") pod "e173886a-a2c5-423f-bb17-cad2b3891e1e" (UID: "e173886a-a2c5-423f-bb17-cad2b3891e1e"). InnerVolumeSpecName "klipper-config". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0325 05:32:04.892789    2321 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/e173886a-a2c5-423f-bb17-cad2b3891e1e-klipper-helm" (OuterVolumeSpecName: "klipper-helm") pod "e173886a-a2c5-423f-bb17-cad2b3891e1e" (UID: "e173886a-a2c5-423f-bb17-cad2b3891e1e"). InnerVolumeSpecName "klipper-helm". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0325 05:32:04.966526    2321 reconciler_common.go:300] "Volume detached for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/e173886a-a2c5-423f-bb17-cad2b3891e1e-tmp\") on node \"server\" DevicePath \"\""
I0325 05:32:04.966563    2321 reconciler_common.go:300] "Volume detached for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/e173886a-a2c5-423f-bb17-cad2b3891e1e-klipper-helm\") on node \"server\" DevicePath \"\""
I0325 05:32:04.966578    2321 reconciler_common.go:300] "Volume detached for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/e173886a-a2c5-423f-bb17-cad2b3891e1e-klipper-cache\") on node \"server\" DevicePath \"\""
I0325 05:32:04.966590    2321 reconciler_common.go:300] "Volume detached for volume \"content\" (UniqueName: \"kubernetes.io/configmap/e173886a-a2c5-423f-bb17-cad2b3891e1e-content\") on node \"server\" DevicePath \"\""
I0325 05:32:04.966605    2321 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-tsthj\" (UniqueName: \"kubernetes.io/projected/e173886a-a2c5-423f-bb17-cad2b3891e1e-kube-api-access-tsthj\") on node \"server\" DevicePath \"\""
I0325 05:32:04.966619    2321 reconciler_common.go:300] "Volume detached for volume \"values\" (UniqueName: \"kubernetes.io/secret/e173886a-a2c5-423f-bb17-cad2b3891e1e-values\") on node \"server\" DevicePath \"\""
I0325 05:32:04.966632    2321 reconciler_common.go:300] "Volume detached for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/e173886a-a2c5-423f-bb17-cad2b3891e1e-klipper-config\") on node \"server\" DevicePath \"\""
I0325 05:32:05.681158    2321 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="e7cdf38e4e355fdb5d0bbb91498e0612ddf354f85650638bf6c55c630324fb8b"
I0325 05:32:05.729035    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0325 05:32:05.733563    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0325 05:32:05.746057    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0325 05:32:05.758931    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0325 05:32:05.759274    2321 event.go:307] "Event occurred" object="kube-system/helm-install-traefik" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="Completed" message="Job completed"
I0325 05:32:09.235868    2321 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/svclb-traefik-a3320499-m8r59" containerName="lb-tcp-80"
I0325 05:32:09.425264    2321 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/svclb-traefik-a3320499-m8r59" containerName="lb-tcp-443"
I0325 05:32:09.772556    2321 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/svclb-traefik-a3320499-m8r59" podStartSLOduration=1.9998413830000001 podCreationTimestamp="2024-03-25 05:32:02 +0000 UTC" firstStartedPulling="2024-03-25 05:32:03.45791529 +0000 UTC m=+53.505589913" lastFinishedPulling="2024-03-25 05:32:09.227064874 +0000 UTC m=+59.274739554" observedRunningTime="2024-03-25 05:32:09.764185912 +0000 UTC m=+59.811860541" watchObservedRunningTime="2024-03-25 05:32:09.768991024 +0000 UTC m=+59.816665676"
I0325 05:32:09.840115    2321 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Service" apiVersion="v1" type="Normal" reason="UpdatedLoadBalancer" message="Updated LoadBalancer with new IPs: [] -> [192.168.56.110]"
E0325 05:32:11.449971    2321 handler_proxy.go:137] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
I0325 05:32:11.566771    2321 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/traefik-f4564c4f4-9jp8p" containerName="traefik"
I0325 05:32:12.790652    2321 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/traefik-f4564c4f4-9jp8p" podStartSLOduration=2.5842458710000002 podCreationTimestamp="2024-03-25 05:32:02 +0000 UTC" firstStartedPulling="2024-03-25 05:32:03.359542435 +0000 UTC m=+53.407217053" lastFinishedPulling="2024-03-25 05:32:11.55903403 +0000 UTC m=+61.606708696" observedRunningTime="2024-03-25 05:32:12.780722113 +0000 UTC m=+62.828396741" watchObservedRunningTime="2024-03-25 05:32:12.783737514 +0000 UTC m=+62.831412151"
I0325 05:32:12.797200    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="5.110261ms"
I0325 05:32:13.861903    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="24.052233ms"
I0325 05:32:13.862194    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="148.379s"
I0325 05:32:14.142199    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="8.111646ms"
I0325 05:32:14.151230    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="63.068s"
I0325 05:32:14.284330    2321 handler.go:275] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0325 05:32:17.955543    2321 iptables.go:421] Some iptables rules are missing; deleting and recreating rules
I0325 05:32:17.983218    2321 iptables.go:283] bootstrap done
I0325 05:32:27.080044    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressroutes.traefik.containo.us"
I0325 05:32:27.080256    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="tlsstores.traefik.io"
I0325 05:32:27.080341    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressroutes.traefik.io"
I0325 05:32:27.080428    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="middlewares.traefik.containo.us"
I0325 05:32:27.080504    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="tlsoptions.traefik.containo.us"
I0325 05:32:27.080569    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="serverstransports.traefik.containo.us"
I0325 05:32:27.080764    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="serverstransporttcps.traefik.io"
I0325 05:32:27.080838    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="tlsstores.traefik.containo.us"
I0325 05:32:27.080897    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressrouteudps.traefik.containo.us"
I0325 05:32:27.081042    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="serverstransports.traefik.io"
I0325 05:32:27.081524    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressroutetcps.traefik.containo.us"
I0325 05:32:27.081658    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressroutetcps.traefik.io"
I0325 05:32:27.081733    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="traefikservices.traefik.containo.us"
I0325 05:32:27.081943    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="middlewares.traefik.io"
I0325 05:32:27.082026    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="traefikservices.traefik.io"
I0325 05:32:27.082093    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="middlewaretcps.traefik.containo.us"
I0325 05:32:27.082152    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="tlsoptions.traefik.io"
I0325 05:32:27.082374    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="middlewaretcps.traefik.io"
I0325 05:32:27.082687    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressrouteudps.traefik.io"
I0325 05:32:27.096793    2321 shared_informer.go:311] Waiting for caches to sync for resource quota
I0325 05:32:27.213528    2321 shared_informer.go:318] Caches are synced for resource quota
I0325 05:32:27.516794    2321 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0325 05:32:27.517118    2321 shared_informer.go:318] Caches are synced for garbage collector
E0325 05:32:45.979806    2321 kubelet_node_status.go:701] "Failed to set some node status fields" err="failed to validate nodeIP: node IP: \"192.168.56.110\" not found in the host's network interfaces" node="server"
I0325 05:34:19.618964    2321 event.go:307] "Event occurred" object="default/nginx-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set nginx-deployment-7c79c4bf97 to 3"
I0325 05:34:19.655965    2321 event.go:307] "Event occurred" object="default/nginx-deployment-7c79c4bf97" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: nginx-deployment-7c79c4bf97-2ctv9"
I0325 05:34:19.665819    2321 topology_manager.go:215] "Topology Admit Handler" podUID="2b65a183-a852-4fb5-aecf-d0cfe630a463" podNamespace="default" podName="nginx-deployment-7c79c4bf97-2ctv9"
E0325 05:34:19.667340    2321 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="e173886a-a2c5-423f-bb17-cad2b3891e1e" containerName="helm"
E0325 05:34:19.667368    2321 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="e173886a-a2c5-423f-bb17-cad2b3891e1e" containerName="helm"
E0325 05:34:19.667375    2321 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="e02d41e1-eeb4-4804-b989-7bcd4c998148" containerName="helm"
I0325 05:34:19.667784    2321 memory_manager.go:346] "RemoveStaleState removing state" podUID="e173886a-a2c5-423f-bb17-cad2b3891e1e" containerName="helm"
I0325 05:34:19.667798    2321 memory_manager.go:346] "RemoveStaleState removing state" podUID="e173886a-a2c5-423f-bb17-cad2b3891e1e" containerName="helm"
I0325 05:34:19.667804    2321 memory_manager.go:346] "RemoveStaleState removing state" podUID="e02d41e1-eeb4-4804-b989-7bcd4c998148" containerName="helm"
I0325 05:34:19.686155    2321 event.go:307] "Event occurred" object="default/nginx-deployment-7c79c4bf97" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: nginx-deployment-7c79c4bf97-nf8m8"
I0325 05:34:19.686269    2321 event.go:307] "Event occurred" object="default/nginx-deployment-7c79c4bf97" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: nginx-deployment-7c79c4bf97-jgqw5"
I0325 05:34:19.700511    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/nginx-deployment-7c79c4bf97" duration="84.326875ms"
I0325 05:34:19.701416    2321 topology_manager.go:215] "Topology Admit Handler" podUID="72ac6b06-3351-4ee2-9ee6-230a3d3ad027" podNamespace="default" podName="nginx-deployment-7c79c4bf97-jgqw5"
I0325 05:34:19.702008    2321 topology_manager.go:215] "Topology Admit Handler" podUID="ef3fd1ba-1e0a-474f-89d6-f9f539904e95" podNamespace="default" podName="nginx-deployment-7c79c4bf97-nf8m8"
I0325 05:34:19.750170    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/nginx-deployment-7c79c4bf97" duration="49.615694ms"
I0325 05:34:19.750375    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/nginx-deployment-7c79c4bf97" duration="63.855s"
I0325 05:34:19.750581    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/nginx-deployment-7c79c4bf97" duration="108.657s"
I0325 05:34:19.760524    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/nginx-deployment-7c79c4bf97" duration="35.035s"
I0325 05:34:19.780010    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-52fr4\" (UniqueName: \"kubernetes.io/projected/2b65a183-a852-4fb5-aecf-d0cfe630a463-kube-api-access-52fr4\") pod \"nginx-deployment-7c79c4bf97-2ctv9\" (UID: \"2b65a183-a852-4fb5-aecf-d0cfe630a463\") " pod="default/nginx-deployment-7c79c4bf97-2ctv9"
I0325 05:34:19.880504    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-hlpdh\" (UniqueName: \"kubernetes.io/projected/72ac6b06-3351-4ee2-9ee6-230a3d3ad027-kube-api-access-hlpdh\") pod \"nginx-deployment-7c79c4bf97-jgqw5\" (UID: \"72ac6b06-3351-4ee2-9ee6-230a3d3ad027\") " pod="default/nginx-deployment-7c79c4bf97-jgqw5"
I0325 05:34:19.881125    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-pm5kt\" (UniqueName: \"kubernetes.io/projected/ef3fd1ba-1e0a-474f-89d6-f9f539904e95-kube-api-access-pm5kt\") pod \"nginx-deployment-7c79c4bf97-nf8m8\" (UID: \"ef3fd1ba-1e0a-474f-89d6-f9f539904e95\") " pod="default/nginx-deployment-7c79c4bf97-nf8m8"
I0325 05:34:45.281760    2321 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="default/nginx-deployment-7c79c4bf97-nf8m8" containerName="nginx"
I0325 05:34:45.294509    2321 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="default/nginx-deployment-7c79c4bf97-2ctv9" containerName="nginx"
I0325 05:34:45.295381    2321 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="default/nginx-deployment-7c79c4bf97-jgqw5" containerName="nginx"
I0325 05:34:45.956046    2321 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/nginx-deployment-7c79c4bf97-jgqw5" podStartSLOduration=2.378639464 podCreationTimestamp="2024-03-25 05:34:19 +0000 UTC" firstStartedPulling="2024-03-25 05:34:20.707735967 +0000 UTC m=+190.755410591" lastFinishedPulling="2024-03-25 05:34:45.282647125 +0000 UTC m=+215.330321749" observedRunningTime="2024-03-25 05:34:45.934750088 +0000 UTC m=+215.982424725" watchObservedRunningTime="2024-03-25 05:34:45.953550622 +0000 UTC m=+216.001225250"
I0325 05:34:46.009413    2321 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/nginx-deployment-7c79c4bf97-nf8m8" podStartSLOduration=2.455098084 podCreationTimestamp="2024-03-25 05:34:19 +0000 UTC" firstStartedPulling="2024-03-25 05:34:20.705298624 +0000 UTC m=+190.752973242" lastFinishedPulling="2024-03-25 05:34:45.259557615 +0000 UTC m=+215.307232240" observedRunningTime="2024-03-25 05:34:45.994674408 +0000 UTC m=+216.042349044" watchObservedRunningTime="2024-03-25 05:34:46.009357082 +0000 UTC m=+216.057031710"
I0325 05:34:46.017555    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/nginx-deployment-7c79c4bf97" duration="70.302472ms"
I0325 05:34:46.036926    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/nginx-deployment-7c79c4bf97" duration="19.323654ms"
I0325 05:34:46.037379    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/nginx-deployment-7c79c4bf97" duration="192.5s"
I0325 05:34:46.037800    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/nginx-deployment-7c79c4bf97" duration="47.041s"
time="2024-03-25T05:36:10Z" level=info msg="COMPACT revision 0 has already been compacted"
I0325 05:38:02.507154    2321 alloc.go:330] "allocated clusterIPs" service="default/nginx-service" clusterIPs={"IPv4":"10.43.172.56"}
E0325 05:39:15.298742    2321 kubelet_node_status.go:701] "Failed to set some node status fields" err="failed to validate nodeIP: node IP: \"192.168.56.110\" not found in the host's network interfaces" node="server"
E0325 07:16:41.340047    2321 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0325 07:16:41.340036    2321 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0325 07:16:41.344510    2321 resource_quota_controller.go:440] failed to discover resources: the server has asked for the client to provide credentials
I0325 07:16:41.345132    2321 garbagecollector.go:818] "failed to discover preferred resources" error="the server has asked for the client to provide credentials"
time="2024-03-25T08:05:13Z" level=info msg="COMPACT revision 0 has already been compacted"
time="2024-03-25T08:10:13Z" level=info msg="COMPACT revision 0 has already been compacted"
time="2024-03-25T08:15:13Z" level=info msg="COMPACT compactRev=0 targetCompactRev=60 currentRev=1060"
time="2024-03-25T08:15:13Z" level=info msg="COMPACT deleted 6 rows from 60 revisions in 5.469988ms - compacted to 60/1060"
E0325 08:18:33.519260    2321 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
I0325 08:18:33.535885    2321 garbagecollector.go:818] "failed to discover preferred resources" error="the server has asked for the client to provide credentials"
E0325 08:18:33.542811    2321 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0325 08:18:33.544821    2321 resource_quota_controller.go:440] failed to discover resources: the server has asked for the client to provide credentials
time="2024-03-25T08:20:13Z" level=info msg="COMPACT compactRev=60 targetCompactRev=151 currentRev=1151"
time="2024-03-25T08:20:13Z" level=info msg="COMPACT deleted 8 rows from 91 revisions in 13.766126ms - compacted to 151/1151"
time="2024-03-25T08:25:13Z" level=info msg="COMPACT compactRev=151 targetCompactRev=240 currentRev=1240"
time="2024-03-25T08:25:13Z" level=info msg="COMPACT deleted 16 rows from 89 revisions in 8.356185ms - compacted to 240/1240"
time="2024-03-25T08:30:13Z" level=info msg="COMPACT compactRev=240 targetCompactRev=329 currentRev=1329"
time="2024-03-25T08:30:13Z" level=info msg="COMPACT deleted 16 rows from 89 revisions in 17.920168ms - compacted to 329/1329"
time="2024-03-25T08:35:13Z" level=info msg="COMPACT compactRev=329 targetCompactRev=419 currentRev=1419"
time="2024-03-25T08:35:13Z" level=info msg="COMPACT deleted 31 rows from 90 revisions in 27.834228ms - compacted to 419/1419"
time="2024-03-25T08:40:13Z" level=info msg="COMPACT compactRev=419 targetCompactRev=508 currentRev=1508"
time="2024-03-25T08:40:13Z" level=info msg="COMPACT deleted 51 rows from 89 revisions in 60.86566ms - compacted to 508/1508"
time="2024-03-25T08:45:13Z" level=info msg="COMPACT compactRev=508 targetCompactRev=597 currentRev=1597"
time="2024-03-25T08:45:13Z" level=info msg="COMPACT deleted 54 rows from 89 revisions in 97.555092ms - compacted to 597/1597"
time="2024-03-25T08:50:13Z" level=info msg="COMPACT compactRev=597 targetCompactRev=687 currentRev=1687"
time="2024-03-25T08:50:13Z" level=info msg="COMPACT deleted 66 rows from 90 revisions in 82.423333ms - compacted to 687/1687"
time="2024-03-25T08:55:13Z" level=info msg="COMPACT compactRev=687 targetCompactRev=777 currentRev=1777"
time="2024-03-25T08:55:13Z" level=info msg="COMPACT deleted 66 rows from 90 revisions in 9.398991ms - compacted to 777/1777"
time="2024-03-25T09:00:13Z" level=info msg="COMPACT compactRev=777 targetCompactRev=980 currentRev=1980"
time="2024-03-25T09:00:13Z" level=info msg="COMPACT deleted 200 rows from 203 revisions in 76.018649ms - compacted to 980/1980"
time="2024-03-27T14:19:21Z" level=info msg="Starting k3s v1.28.7+k3s1 (051b14b2)"
time="2024-03-27T14:19:21Z" level=info msg="Configuring sqlite3 database connection pooling: maxIdleConns=2, maxOpenConns=0, connMaxLifetime=0s"
time="2024-03-27T14:19:21Z" level=info msg="Configuring database table schema and indexes, this may take a moment..."
time="2024-03-27T14:19:21Z" level=info msg="Database tables and indexes are up to date"
time="2024-03-27T14:19:21Z" level=info msg="Kine available at unix://kine.sock"
time="2024-03-27T14:19:21Z" level=info msg="generated self-signed CA certificate CN=k3s-client-ca@1711549161: notBefore=2024-03-27 14:19:21.134900466 +0000 UTC notAfter=2034-03-25 14:19:21.134900466 +0000 UTC"
time="2024-03-27T14:19:21Z" level=info msg="certificate CN=system:admin,O=system:masters signed by CN=k3s-client-ca@1711549161: notBefore=2024-03-27 14:19:21 +0000 UTC notAfter=2025-03-27 14:19:21 +0000 UTC"
time="2024-03-27T14:19:21Z" level=info msg="certificate CN=system:k3s-supervisor,O=system:masters signed by CN=k3s-client-ca@1711549161: notBefore=2024-03-27 14:19:21 +0000 UTC notAfter=2025-03-27 14:19:21 +0000 UTC"
time="2024-03-27T14:19:21Z" level=info msg="certificate CN=system:kube-controller-manager signed by CN=k3s-client-ca@1711549161: notBefore=2024-03-27 14:19:21 +0000 UTC notAfter=2025-03-27 14:19:21 +0000 UTC"
time="2024-03-27T14:19:21Z" level=info msg="certificate CN=system:kube-scheduler signed by CN=k3s-client-ca@1711549161: notBefore=2024-03-27 14:19:21 +0000 UTC notAfter=2025-03-27 14:19:21 +0000 UTC"
time="2024-03-27T14:19:21Z" level=info msg="certificate CN=system:apiserver,O=system:masters signed by CN=k3s-client-ca@1711549161: notBefore=2024-03-27 14:19:21 +0000 UTC notAfter=2025-03-27 14:19:21 +0000 UTC"
time="2024-03-27T14:19:21Z" level=info msg="certificate CN=system:kube-proxy signed by CN=k3s-client-ca@1711549161: notBefore=2024-03-27 14:19:21 +0000 UTC notAfter=2025-03-27 14:19:21 +0000 UTC"
time="2024-03-27T14:19:21Z" level=info msg="certificate CN=system:k3s-controller signed by CN=k3s-client-ca@1711549161: notBefore=2024-03-27 14:19:21 +0000 UTC notAfter=2025-03-27 14:19:21 +0000 UTC"
time="2024-03-27T14:19:21Z" level=info msg="certificate CN=k3s-cloud-controller-manager signed by CN=k3s-client-ca@1711549161: notBefore=2024-03-27 14:19:21 +0000 UTC notAfter=2025-03-27 14:19:21 +0000 UTC"
time="2024-03-27T14:19:21Z" level=info msg="generated self-signed CA certificate CN=k3s-server-ca@1711549161: notBefore=2024-03-27 14:19:21.145419815 +0000 UTC notAfter=2034-03-25 14:19:21.145419815 +0000 UTC"
time="2024-03-27T14:19:21Z" level=info msg="certificate CN=kube-apiserver signed by CN=k3s-server-ca@1711549161: notBefore=2024-03-27 14:19:21 +0000 UTC notAfter=2025-03-27 14:19:21 +0000 UTC"
time="2024-03-27T14:19:21Z" level=info msg="generated self-signed CA certificate CN=k3s-request-header-ca@1711549161: notBefore=2024-03-27 14:19:21.147659193 +0000 UTC notAfter=2034-03-25 14:19:21.147659193 +0000 UTC"
time="2024-03-27T14:19:21Z" level=info msg="certificate CN=system:auth-proxy signed by CN=k3s-request-header-ca@1711549161: notBefore=2024-03-27 14:19:21 +0000 UTC notAfter=2025-03-27 14:19:21 +0000 UTC"
time="2024-03-27T14:19:21Z" level=info msg="generated self-signed CA certificate CN=etcd-server-ca@1711549161: notBefore=2024-03-27 14:19:21.149725659 +0000 UTC notAfter=2034-03-25 14:19:21.149725659 +0000 UTC"
time="2024-03-27T14:19:21Z" level=info msg="certificate CN=etcd-client signed by CN=etcd-server-ca@1711549161: notBefore=2024-03-27 14:19:21 +0000 UTC notAfter=2025-03-27 14:19:21 +0000 UTC"
time="2024-03-27T14:19:21Z" level=info msg="generated self-signed CA certificate CN=etcd-peer-ca@1711549161: notBefore=2024-03-27 14:19:21.15160593 +0000 UTC notAfter=2034-03-25 14:19:21.15160593 +0000 UTC"
time="2024-03-27T14:19:21Z" level=info msg="certificate CN=etcd-peer signed by CN=etcd-peer-ca@1711549161: notBefore=2024-03-27 14:19:21 +0000 UTC notAfter=2025-03-27 14:19:21 +0000 UTC"
time="2024-03-27T14:19:21Z" level=info msg="certificate CN=etcd-server signed by CN=etcd-server-ca@1711549161: notBefore=2024-03-27 14:19:21 +0000 UTC notAfter=2025-03-27 14:19:21 +0000 UTC"
time="2024-03-27T14:19:21Z" level=info msg="Saving cluster bootstrap data to datastore"
time="2024-03-27T14:19:21Z" level=info msg="certificate CN=k3s,O=k3s signed by CN=k3s-server-ca@1711549161: notBefore=2024-03-27 14:19:21 +0000 UTC notAfter=2025-03-27 14:19:21 +0000 UTC"
time="2024-03-27T14:19:21Z" level=warning msg="dynamiclistener [::]:6443: no cached certificate available for preload - deferring certificate load until storage initialization or first client request"
time="2024-03-27T14:19:21Z" level=info msg="Active TLS secret / (ver=) (count 10): map[listener.cattle.io/cn-10.43.0.1:10.43.0.1 listener.cattle.io/cn-127.0.0.1:127.0.0.1 listener.cattle.io/cn-192.168.56.110:192.168.56.110 listener.cattle.io/cn-__1-f16284:::1 listener.cattle.io/cn-kubernetes:kubernetes listener.cattle.io/cn-kubernetes.default:kubernetes.default listener.cattle.io/cn-kubernetes.default.svc:kubernetes.default.svc listener.cattle.io/cn-kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local listener.cattle.io/cn-localhost:localhost listener.cattle.io/cn-server:server listener.cattle.io/fingerprint:SHA1=CD8FBC2467851DDA0EA873553D78030E43366624]"
time="2024-03-27T14:19:22Z" level=info msg="Running kube-apiserver --advertise-address=192.168.56.110 --advertise-port=6443 --allow-privileged=true --anonymous-auth=false --api-audiences=https://kubernetes.default.svc.cluster.local,k3s --authorization-mode=Node,RBAC --bind-address=127.0.0.1 --cert-dir=/var/lib/rancher/k3s/server/tls/temporary-certs --client-ca-file=/var/lib/rancher/k3s/server/tls/client-ca.crt --egress-selector-config-file=/var/lib/rancher/k3s/server/etc/egress-selector-config.yaml --enable-admission-plugins=NodeRestriction --enable-aggregator-routing=true --enable-bootstrap-token-auth=true --etcd-servers=unix://kine.sock --kubelet-certificate-authority=/var/lib/rancher/k3s/server/tls/server-ca.crt --kubelet-client-certificate=/var/lib/rancher/k3s/server/tls/client-kube-apiserver.crt --kubelet-client-key=/var/lib/rancher/k3s/server/tls/client-kube-apiserver.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --profiling=false --proxy-client-cert-file=/var/lib/rancher/k3s/server/tls/client-auth-proxy.crt --proxy-client-key-file=/var/lib/rancher/k3s/server/tls/client-auth-proxy.key --requestheader-allowed-names=system:auth-proxy --requestheader-client-ca-file=/var/lib/rancher/k3s/server/tls/request-header-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6444 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/var/lib/rancher/k3s/server/tls/service.key --service-account-signing-key-file=/var/lib/rancher/k3s/server/tls/service.current.key --service-cluster-ip-range=10.43.0.0/16 --service-node-port-range=30000-32767 --storage-backend=etcd3 --tls-cert-file=/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.crt --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 --tls-private-key-file=/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.key"
time="2024-03-27T14:19:22Z" level=info msg="Running kube-scheduler --authentication-kubeconfig=/var/lib/rancher/k3s/server/cred/scheduler.kubeconfig --authorization-kubeconfig=/var/lib/rancher/k3s/server/cred/scheduler.kubeconfig --bind-address=127.0.0.1 --kubeconfig=/var/lib/rancher/k3s/server/cred/scheduler.kubeconfig --leader-elect=false --profiling=false --secure-port=10259"
I0327 14:19:22.371269    2321 options.go:220] external host was not specified, using 192.168.56.110
time="2024-03-27T14:19:22Z" level=info msg="Running kube-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/var/lib/rancher/k3s/server/cred/controller.kubeconfig --authorization-kubeconfig=/var/lib/rancher/k3s/server/cred/controller.kubeconfig --bind-address=127.0.0.1 --cluster-cidr=10.42.0.0/16 --cluster-signing-kube-apiserver-client-cert-file=/var/lib/rancher/k3s/server/tls/client-ca.nochain.crt --cluster-signing-kube-apiserver-client-key-file=/var/lib/rancher/k3s/server/tls/client-ca.key --cluster-signing-kubelet-client-cert-file=/var/lib/rancher/k3s/server/tls/client-ca.nochain.crt --cluster-signing-kubelet-client-key-file=/var/lib/rancher/k3s/server/tls/client-ca.key --cluster-signing-kubelet-serving-cert-file=/var/lib/rancher/k3s/server/tls/server-ca.nochain.crt --cluster-signing-kubelet-serving-key-file=/var/lib/rancher/k3s/server/tls/server-ca.key --cluster-signing-legacy-unknown-cert-file=/var/lib/rancher/k3s/server/tls/server-ca.nochain.crt --cluster-signing-legacy-unknown-key-file=/var/lib/rancher/k3s/server/tls/server-ca.key --configure-cloud-routes=false --controllers=*,tokencleaner,-service,-route,-cloud-node-lifecycle --kubeconfig=/var/lib/rancher/k3s/server/cred/controller.kubeconfig --leader-elect=false --profiling=false --root-ca-file=/var/lib/rancher/k3s/server/tls/server-ca.crt --secure-port=10257 --service-account-private-key-file=/var/lib/rancher/k3s/server/tls/service.current.key --service-cluster-ip-range=10.43.0.0/16 --use-service-account-credentials=true"
time="2024-03-27T14:19:22Z" level=info msg="Running cloud-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/var/lib/rancher/k3s/server/cred/cloud-controller.kubeconfig --authorization-kubeconfig=/var/lib/rancher/k3s/server/cred/cloud-controller.kubeconfig --bind-address=127.0.0.1 --cloud-config=/var/lib/rancher/k3s/server/etc/cloud-config.yaml --cloud-provider=k3s --cluster-cidr=10.42.0.0/16 --configure-cloud-routes=false --controllers=*,-route --feature-gates=CloudDualStackNodeIPs=true --kubeconfig=/var/lib/rancher/k3s/server/cred/cloud-controller.kubeconfig --leader-elect=false --leader-elect-resource-name=k3s-cloud-controller-manager --node-status-update-frequency=1m0s --profiling=false"
I0327 14:19:22.372927    2321 server.go:156] Version: v1.28.7+k3s1
I0327 14:19:22.372977    2321 server.go:158] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
time="2024-03-27T14:19:22Z" level=info msg="Server node token is available at /var/lib/rancher/k3s/server/token"
time="2024-03-27T14:19:22Z" level=info msg="To join server node to cluster: k3s server -s https://10.0.2.15:6443 -t ${SERVER_NODE_TOKEN}"
time="2024-03-27T14:19:22Z" level=info msg="Agent node token is available at /var/lib/rancher/k3s/server/agent-token"
time="2024-03-27T14:19:22Z" level=info msg="To join agent node to cluster: k3s agent -s https://10.0.2.15:6443 -t ${AGENT_NODE_TOKEN}"
time="2024-03-27T14:19:22Z" level=info msg="Wrote kubeconfig /etc/rancher/k3s/k3s.yaml"
time="2024-03-27T14:19:22Z" level=info msg="Waiting for API server to become available"
time="2024-03-27T14:19:22Z" level=info msg="Run: k3s kubectl"
I0327 14:19:23.708999    2321 shared_informer.go:311] Waiting for caches to sync for node_authorizer
I0327 14:19:23.754933    2321 plugins.go:158] Loaded 12 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,TaintNodesByCondition,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,MutatingAdmissionWebhook.
I0327 14:19:23.755030    2321 plugins.go:161] Loaded 13 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,ClusterTrustBundleAttest,CertificateSubjectRestriction,ValidatingAdmissionPolicy,ValidatingAdmissionWebhook,ResourceQuota.
I0327 14:19:23.766316    2321 instance.go:298] Using reconciler: lease
I0327 14:19:23.817099    2321 handler.go:275] Adding GroupVersion apiextensions.k8s.io v1 to ResourceManager
W0327 14:19:23.817220    2321 genericapiserver.go:744] Skipping API apiextensions.k8s.io/v1beta1 because it has no resources.
I0327 14:19:24.139764    2321 handler.go:275] Adding GroupVersion  v1 to ResourceManager
I0327 14:19:24.149872    2321 instance.go:709] API group "internal.apiserver.k8s.io" is not enabled, skipping.
time="2024-03-27T14:19:24Z" level=info msg="Password verified locally for node server"
time="2024-03-27T14:19:24Z" level=info msg="certificate CN=server signed by CN=k3s-server-ca@1711549161: notBefore=2024-03-27 14:19:21 +0000 UTC notAfter=2025-03-27 14:19:24 +0000 UTC"
I0327 14:19:24.641250    2321 instance.go:709] API group "resource.k8s.io" is not enabled, skipping.
I0327 14:19:24.771527    2321 handler.go:275] Adding GroupVersion authentication.k8s.io v1 to ResourceManager
W0327 14:19:24.771961    2321 genericapiserver.go:744] Skipping API authentication.k8s.io/v1beta1 because it has no resources.
W0327 14:19:24.771999    2321 genericapiserver.go:744] Skipping API authentication.k8s.io/v1alpha1 because it has no resources.
I0327 14:19:24.773361    2321 handler.go:275] Adding GroupVersion authorization.k8s.io v1 to ResourceManager
W0327 14:19:24.773422    2321 genericapiserver.go:744] Skipping API authorization.k8s.io/v1beta1 because it has no resources.
I0327 14:19:24.776048    2321 handler.go:275] Adding GroupVersion autoscaling v2 to ResourceManager
I0327 14:19:24.778175    2321 handler.go:275] Adding GroupVersion autoscaling v1 to ResourceManager
W0327 14:19:24.778235    2321 genericapiserver.go:744] Skipping API autoscaling/v2beta1 because it has no resources.
W0327 14:19:24.778251    2321 genericapiserver.go:744] Skipping API autoscaling/v2beta2 because it has no resources.
I0327 14:19:24.782248    2321 handler.go:275] Adding GroupVersion batch v1 to ResourceManager
W0327 14:19:24.782308    2321 genericapiserver.go:744] Skipping API batch/v1beta1 because it has no resources.
I0327 14:19:24.784809    2321 handler.go:275] Adding GroupVersion certificates.k8s.io v1 to ResourceManager
W0327 14:19:24.784873    2321 genericapiserver.go:744] Skipping API certificates.k8s.io/v1beta1 because it has no resources.
W0327 14:19:24.784889    2321 genericapiserver.go:744] Skipping API certificates.k8s.io/v1alpha1 because it has no resources.
I0327 14:19:24.786605    2321 handler.go:275] Adding GroupVersion coordination.k8s.io v1 to ResourceManager
W0327 14:19:24.786666    2321 genericapiserver.go:744] Skipping API coordination.k8s.io/v1beta1 because it has no resources.
W0327 14:19:24.787191    2321 genericapiserver.go:744] Skipping API discovery.k8s.io/v1beta1 because it has no resources.
I0327 14:19:24.788960    2321 handler.go:275] Adding GroupVersion discovery.k8s.io v1 to ResourceManager
I0327 14:19:24.793240    2321 handler.go:275] Adding GroupVersion networking.k8s.io v1 to ResourceManager
W0327 14:19:24.793301    2321 genericapiserver.go:744] Skipping API networking.k8s.io/v1beta1 because it has no resources.
W0327 14:19:24.793317    2321 genericapiserver.go:744] Skipping API networking.k8s.io/v1alpha1 because it has no resources.
I0327 14:19:24.794926    2321 handler.go:275] Adding GroupVersion node.k8s.io v1 to ResourceManager
W0327 14:19:24.794979    2321 genericapiserver.go:744] Skipping API node.k8s.io/v1beta1 because it has no resources.
W0327 14:19:24.794994    2321 genericapiserver.go:744] Skipping API node.k8s.io/v1alpha1 because it has no resources.
I0327 14:19:24.797550    2321 handler.go:275] Adding GroupVersion policy v1 to ResourceManager
W0327 14:19:24.797624    2321 genericapiserver.go:744] Skipping API policy/v1beta1 because it has no resources.
I0327 14:19:24.802428    2321 handler.go:275] Adding GroupVersion rbac.authorization.k8s.io v1 to ResourceManager
W0327 14:19:24.802499    2321 genericapiserver.go:744] Skipping API rbac.authorization.k8s.io/v1beta1 because it has no resources.
W0327 14:19:24.802518    2321 genericapiserver.go:744] Skipping API rbac.authorization.k8s.io/v1alpha1 because it has no resources.
I0327 14:19:24.803755    2321 handler.go:275] Adding GroupVersion scheduling.k8s.io v1 to ResourceManager
W0327 14:19:24.803806    2321 genericapiserver.go:744] Skipping API scheduling.k8s.io/v1beta1 because it has no resources.
W0327 14:19:24.803821    2321 genericapiserver.go:744] Skipping API scheduling.k8s.io/v1alpha1 because it has no resources.
I0327 14:19:24.810847    2321 handler.go:275] Adding GroupVersion storage.k8s.io v1 to ResourceManager
W0327 14:19:24.810923    2321 genericapiserver.go:744] Skipping API storage.k8s.io/v1beta1 because it has no resources.
W0327 14:19:24.810940    2321 genericapiserver.go:744] Skipping API storage.k8s.io/v1alpha1 because it has no resources.
I0327 14:19:24.815361    2321 handler.go:275] Adding GroupVersion flowcontrol.apiserver.k8s.io v1beta3 to ResourceManager
I0327 14:19:24.819065    2321 handler.go:275] Adding GroupVersion flowcontrol.apiserver.k8s.io v1beta2 to ResourceManager
W0327 14:19:24.819130    2321 genericapiserver.go:744] Skipping API flowcontrol.apiserver.k8s.io/v1beta1 because it has no resources.
W0327 14:19:24.819147    2321 genericapiserver.go:744] Skipping API flowcontrol.apiserver.k8s.io/v1alpha1 because it has no resources.
I0327 14:19:24.828357    2321 handler.go:275] Adding GroupVersion apps v1 to ResourceManager
W0327 14:19:24.828420    2321 genericapiserver.go:744] Skipping API apps/v1beta2 because it has no resources.
W0327 14:19:24.828437    2321 genericapiserver.go:744] Skipping API apps/v1beta1 because it has no resources.
I0327 14:19:24.831211    2321 handler.go:275] Adding GroupVersion admissionregistration.k8s.io v1 to ResourceManager
W0327 14:19:24.831268    2321 genericapiserver.go:744] Skipping API admissionregistration.k8s.io/v1beta1 because it has no resources.
W0327 14:19:24.831284    2321 genericapiserver.go:744] Skipping API admissionregistration.k8s.io/v1alpha1 because it has no resources.
I0327 14:19:24.832978    2321 handler.go:275] Adding GroupVersion events.k8s.io v1 to ResourceManager
W0327 14:19:24.833033    2321 genericapiserver.go:744] Skipping API events.k8s.io/v1beta1 because it has no resources.
I0327 14:19:24.841177    2321 handler.go:275] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
W0327 14:19:24.841243    2321 genericapiserver.go:744] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
time="2024-03-27T14:19:26Z" level=info msg="certificate CN=system:node:server,O=system:nodes signed by CN=k3s-client-ca@1711549161: notBefore=2024-03-27 14:19:21 +0000 UTC notAfter=2025-03-27 14:19:26 +0000 UTC"
I0327 14:19:26.612096    2321 secure_serving.go:213] Serving securely on 127.0.0.1:6444
I0327 14:19:26.612510    2321 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/rancher/k3s/server/tls/request-header-ca.crt"
I0327 14:19:26.612889    2321 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.crt::/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.key"
I0327 14:19:26.613147    2321 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0327 14:19:26.614898    2321 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/rancher/k3s/server/tls/client-auth-proxy.crt::/var/lib/rancher/k3s/server/tls/client-auth-proxy.key"
I0327 14:19:26.617659    2321 aggregator.go:164] waiting for initial CRD sync...
I0327 14:19:26.617746    2321 controller.go:78] Starting OpenAPI AggregationController
I0327 14:19:26.618584    2321 customresource_discovery_controller.go:289] Starting DiscoveryController
I0327 14:19:26.619670    2321 gc_controller.go:78] Starting apiserver lease garbage collector
I0327 14:19:26.619842    2321 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0327 14:19:26.619862    2321 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0327 14:19:26.620470    2321 gc_controller.go:78] Starting apiserver lease garbage collector
I0327 14:19:26.621115    2321 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0327 14:19:26.621163    2321 shared_informer.go:311] Waiting for caches to sync for cluster_authentication_trust_controller
I0327 14:19:26.621228    2321 available_controller.go:423] Starting AvailableConditionController
I0327 14:19:26.621241    2321 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0327 14:19:26.621297    2321 controller.go:80] Starting OpenAPI V3 AggregationController
I0327 14:19:26.622441    2321 apf_controller.go:374] Starting API Priority and Fairness config controller
I0327 14:19:26.622610    2321 system_namespaces_controller.go:67] Starting system namespaces controller
I0327 14:19:26.623240    2321 controller.go:116] Starting legacy_token_tracking_controller
I0327 14:19:26.623288    2321 shared_informer.go:311] Waiting for caches to sync for configmaps
I0327 14:19:26.623353    2321 handler_discovery.go:412] Starting ResourceDiscoveryManager
I0327 14:19:26.623485    2321 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/rancher/k3s/server/tls/client-ca.crt"
I0327 14:19:26.675699    2321 crdregistration_controller.go:111] Starting crd-autoregister controller
I0327 14:19:26.675754    2321 shared_informer.go:311] Waiting for caches to sync for crd-autoregister
I0327 14:19:26.675886    2321 controller.go:134] Starting OpenAPI controller
I0327 14:19:26.676086    2321 controller.go:85] Starting OpenAPI V3 controller
I0327 14:19:26.676127    2321 naming_controller.go:291] Starting NamingConditionController
I0327 14:19:26.676200    2321 establishing_controller.go:76] Starting EstablishingController
I0327 14:19:26.676245    2321 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0327 14:19:26.676312    2321 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0327 14:19:26.676341    2321 crd_finalizer.go:266] Starting CRDFinalizer
I0327 14:19:26.677679    2321 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/rancher/k3s/server/tls/client-ca.crt"
I0327 14:19:26.678187    2321 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/rancher/k3s/server/tls/request-header-ca.crt"
I0327 14:19:26.984154    2321 shared_informer.go:318] Caches are synced for crd-autoregister
I0327 14:19:26.984333    2321 aggregator.go:166] initial CRD sync complete...
I0327 14:19:26.984352    2321 autoregister_controller.go:141] Starting autoregister controller
I0327 14:19:26.984365    2321 cache.go:32] Waiting for caches to sync for autoregister controller
I0327 14:19:26.984378    2321 cache.go:39] Caches are synced for autoregister controller
I0327 14:19:27.009383    2321 shared_informer.go:318] Caches are synced for node_authorizer
I0327 14:19:27.024673    2321 shared_informer.go:318] Caches are synced for configmaps
I0327 14:19:27.028616    2321 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0327 14:19:27.028851    2321 shared_informer.go:318] Caches are synced for cluster_authentication_trust_controller
I0327 14:19:27.028927    2321 cache.go:39] Caches are synced for AvailableConditionController controller
I0327 14:19:27.028999    2321 apf_controller.go:379] Running API Priority and Fairness config worker
I0327 14:19:27.029029    2321 apf_controller.go:382] Running API Priority and Fairness periodic rebalancing process
I0327 14:19:27.055433    2321 controller.go:624] quota admission added evaluator for: namespaces
E0327 14:19:27.157654    2321 controller.go:95] Unable to perform initial Kubernetes service initialization: namespaces "default" not found
I0327 14:19:27.232390    2321 controller.go:624] quota admission added evaluator for: leases.coordination.k8s.io
time="2024-03-27T14:19:27Z" level=info msg="Module overlay was already loaded"
time="2024-03-27T14:19:27Z" level=info msg="Module br_netfilter was already loaded"
time="2024-03-27T14:19:27Z" level=info msg="Set sysctl 'net/netfilter/nf_conntrack_tcp_timeout_established' to 86400"
time="2024-03-27T14:19:27Z" level=info msg="Set sysctl 'net/netfilter/nf_conntrack_tcp_timeout_close_wait' to 3600"
time="2024-03-27T14:19:27Z" level=info msg="Set sysctl 'net/ipv4/conf/all/forwarding' to 1"
I0327 14:19:27.650282    2321 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
time="2024-03-27T14:19:27Z" level=info msg="Set sysctl 'net/netfilter/nf_conntrack_max' to 131072"
time="2024-03-27T14:19:27Z" level=info msg="Logging containerd to /var/lib/rancher/k3s/agent/containerd/containerd.log"
time="2024-03-27T14:19:27Z" level=info msg="Running containerd -c /var/lib/rancher/k3s/agent/etc/containerd/config.toml -a /run/k3s/containerd/containerd.sock --state /run/k3s/containerd --root /var/lib/rancher/k3s/agent/containerd"
I0327 14:19:27.668744    2321 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0327 14:19:27.669001    2321 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
time="2024-03-27T14:19:28Z" level=info msg="containerd is now running"
time="2024-03-27T14:19:28Z" level=info msg="Connecting to proxy" url="wss://127.0.0.1:6443/v1-k3s/connect"
time="2024-03-27T14:19:28Z" level=info msg="Running kubelet --address=0.0.0.0 --allowed-unsafe-sysctls=net.ipv4.ip_forward,net.ipv6.conf.all.forwarding --anonymous-auth=false --authentication-token-webhook=true --authorization-mode=Webhook --cgroup-driver=systemd --client-ca-file=/var/lib/rancher/k3s/agent/client-ca.crt --cloud-provider=external --cluster-dns=10.43.0.10 --cluster-domain=cluster.local --container-runtime-endpoint=unix:///run/k3s/containerd/containerd.sock --containerd=/run/k3s/containerd/containerd.sock --eviction-hard=imagefs.available<5%,nodefs.available<5% --eviction-minimum-reclaim=imagefs.available=10%,nodefs.available=10% --fail-swap-on=false --feature-gates=CloudDualStackNodeIPs=true --healthz-bind-address=127.0.0.1 --hostname-override=server --kubeconfig=/var/lib/rancher/k3s/agent/kubelet.kubeconfig --node-ip=192.168.56.110 --node-labels= --pod-infra-container-image=rancher/mirrored-pause:3.6 --pod-manifest-path=/var/lib/rancher/k3s/agent/pod-manifests --read-only-port=0 --resolv-conf=/run/systemd/resolve/resolv.conf --serialize-image-pulls=false --tls-cert-file=/var/lib/rancher/k3s/agent/serving-kubelet.crt --tls-private-key-file=/var/lib/rancher/k3s/agent/serving-kubelet.key"
time="2024-03-27T14:19:28Z" level=info msg="Handling backend connection request [server]"
time="2024-03-27T14:19:29Z" level=info msg="Waiting to retrieve kube-proxy configuration; server is not ready: https://127.0.0.1:6443/v1-k3s/readyz: 500 Internal Server Error"
I0327 14:19:29.323459    2321 controller.go:624] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0327 14:19:29.428432    2321 controller.go:624] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0327 14:19:29.687002    2321 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.43.0.1"}
W0327 14:19:29.699456    2321 lease.go:263] Resetting endpoints for master service "kubernetes" to [192.168.56.110]
I0327 14:19:29.701802    2321 controller.go:624] quota admission added evaluator for: endpoints
I0327 14:19:29.712766    2321 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
Flag --cloud-provider has been deprecated, will be removed in 1.25 or later, in favor of removing cloud provider code from Kubelet.
Flag --containerd has been deprecated, This is a cadvisor flag that was mistakenly registered with the Kubelet. Due to legacy concerns, it will follow the standard CLI deprecation timeline before being removed.
Flag --pod-infra-container-image has been deprecated, will be removed in a future release. Image garbage collector will get sandbox image information from CRI.
I0327 14:19:29.831084    2321 server.go:202] "--pod-infra-container-image will not be pruned by the image garbage collector in kubelet and should also be set in the remote runtime"
I0327 14:19:29.835465    2321 server.go:462] "Kubelet version" kubeletVersion="v1.28.7+k3s1"
I0327 14:19:29.835516    2321 server.go:464] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0327 14:19:29.840341    2321 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/rancher/k3s/agent/client-ca.crt"
I0327 14:19:29.858573    2321 server.go:720] "--cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /"
I0327 14:19:29.861575    2321 container_manager_linux.go:265] "Container manager verified user specified cgroup-root exists" cgroupRoot=[]
I0327 14:19:29.862466    2321 container_manager_linux.go:270] "Creating Container Manager object based on Node Config" nodeConfig={"RuntimeCgroupsName":"","SystemCgroupsName":"","KubeletCgroupsName":"","KubeletOOMScoreAdj":-999,"ContainerRuntime":"","CgroupsPerQOS":true,"CgroupRoot":"/","CgroupDriver":"systemd","KubeletRootDir":"/var/lib/kubelet","ProtectKernelDefaults":false,"KubeReservedCgroupName":"","SystemReservedCgroupName":"","ReservedSystemCPUs":{},"EnforceNodeAllocatable":{"pods":{}},"KubeReserved":null,"SystemReserved":null,"HardEvictionThresholds":[{"Signal":"imagefs.available","Operator":"LessThan","Value":{"Quantity":null,"Percentage":0.05},"GracePeriod":0,"MinReclaim":null},{"Signal":"nodefs.available","Operator":"LessThan","Value":{"Quantity":null,"Percentage":0.05},"GracePeriod":0,"MinReclaim":null}],"QOSReserved":{},"CPUManagerPolicy":"none","CPUManagerPolicyOptions":null,"TopologyManagerScope":"container","CPUManagerReconcilePeriod":10000000000,"ExperimentalMemoryManagerPolicy":"None","ExperimentalMemoryManagerReservedMemory":null,"PodPidsLimit":-1,"EnforceCPULimits":true,"CPUCFSQuotaPeriod":100000000,"TopologyManagerPolicy":"none","TopologyManagerPolicyOptions":null}
I0327 14:19:29.863533    2321 topology_manager.go:138] "Creating topology manager with none policy"
I0327 14:19:29.863610    2321 container_manager_linux.go:301] "Creating device plugin manager"
I0327 14:19:29.864912    2321 state_mem.go:36] "Initialized new in-memory state store"
I0327 14:19:29.871967    2321 kubelet.go:393] "Attempting to sync node with API server"
I0327 14:19:29.872193    2321 kubelet.go:298] "Adding static pod path" path="/var/lib/rancher/k3s/agent/pod-manifests"
I0327 14:19:29.872538    2321 kubelet.go:309] "Adding apiserver pod source"
I0327 14:19:29.872784    2321 apiserver.go:42] "Waiting for node sync before watching apiserver pods"
I0327 14:19:29.873288    2321 apiserver.go:52] "Watching apiserver"
I0327 14:19:29.878641    2321 kuberuntime_manager.go:257] "Container runtime initialized" containerRuntime="containerd" version="v1.7.11-k3s2" apiVersion="v1"
W0327 14:19:29.883983    2321 probe.go:268] Flexvolume plugin directory at /usr/libexec/kubernetes/kubelet-plugins/volume/exec/ does not exist. Recreating.
I0327 14:19:29.888117    2321 server.go:1227] "Started kubelet"
I0327 14:19:29.894318    2321 fs_resource_analyzer.go:67] "Starting FS ResourceAnalyzer"
E0327 14:19:29.897722    2321 cri_stats_provider.go:448] "Failed to get the info of the filesystem with mountpoint" err="unable to find data in memory cache" mountpoint="/var/lib/rancher/k3s/agent/containerd/io.containerd.snapshotter.v1.overlayfs"
E0327 14:19:29.898039    2321 kubelet.go:1431] "Image garbage collection failed once. Stats initialization may not have completed yet" err="invalid capacity 0 on image filesystem"
I0327 14:19:29.908559    2321 server.go:162] "Starting to listen" address="0.0.0.0" port=10250
I0327 14:19:29.910159    2321 ratelimit.go:65] "Setting rate limiting for podresources endpoint" qps=100 burstTokens=10
I0327 14:19:29.910877    2321 server.go:233] "Starting to serve the podresources API" endpoint="unix:/var/lib/kubelet/pod-resources/kubelet.sock"
I0327 14:19:29.917760    2321 volume_manager.go:291] "Starting Kubelet Volume Manager"
I0327 14:19:29.922226    2321 server.go:462] "Adding debug handlers to kubelet server"
I0327 14:19:29.925346    2321 desired_state_of_world_populator.go:151] "Desired state populator starts to run"
I0327 14:19:29.926943    2321 reconciler_new.go:29] "Reconciler: start to sync state"
E0327 14:19:30.023352    2321 nodelease.go:49] "Failed to get node when trying to set owner ref to the node lease" err="nodes \"server\" not found" node="server"
I0327 14:19:30.046142    2321 kubelet_node_status.go:70] "Attempting to register node" node="server"
I0327 14:19:30.118723    2321 kubelet_node_status.go:73] "Successfully registered node" node="server"
time="2024-03-27T14:19:30Z" level=info msg="Creating k3s-supervisor event broadcaster"
time="2024-03-27T14:19:30Z" level=info msg="Kube API server is now running"
time="2024-03-27T14:19:30Z" level=info msg="ETCD server is now running"
time="2024-03-27T14:19:30Z" level=info msg="k3s is up and running"
time="2024-03-27T14:19:30Z" level=info msg="Waiting for cloud-controller-manager privileges to become available"
I0327 14:19:30.144812    2321 kubelet_network_linux.go:50] "Initialized iptables rules." protocol="IPv4"
I0327 14:19:30.221462    2321 kubelet_network_linux.go:50] "Initialized iptables rules." protocol="IPv6"
I0327 14:19:30.239721    2321 status_manager.go:217] "Starting to sync pod status with apiserver"
I0327 14:19:30.239837    2321 kubelet.go:2303] "Starting kubelet main sync loop"
E0327 14:19:30.240109    2321 kubelet.go:2327] "Skipping pod synchronization" err="[container runtime status check may not have completed yet, PLEG is not healthy: pleg has yet to be successful]"
time="2024-03-27T14:19:30Z" level=info msg="Annotations and labels have been set successfully on node: server"
time="2024-03-27T14:19:30Z" level=info msg="Starting flannel with backend vxlan"
E0327 14:19:30.362210    2321 kubelet.go:2327] "Skipping pod synchronization" err="container runtime status check may not have completed yet"
time="2024-03-27T14:19:30Z" level=info msg="Applying CRD addons.k3s.cattle.io"
I0327 14:19:30.471580    2321 cpu_manager.go:214] "Starting CPU manager" policy="none"
I0327 14:19:30.471809    2321 cpu_manager.go:215] "Reconciling" reconcilePeriod="10s"
I0327 14:19:30.472217    2321 state_mem.go:36] "Initialized new in-memory state store"
I0327 14:19:30.477066    2321 policy_none.go:49] "None policy: Start"
I0327 14:19:30.496839    2321 memory_manager.go:169] "Starting memorymanager" policy="None"
I0327 14:19:30.497085    2321 state_mem.go:35] "Initializing new in-memory state store"
time="2024-03-27T14:19:30Z" level=info msg="Applying CRD etcdsnapshotfiles.k3s.cattle.io"
E0327 14:19:30.564574    2321 kubelet.go:2327] "Skipping pod synchronization" err="container runtime status check may not have completed yet"
I0327 14:19:30.573795    2321 handler.go:275] Adding GroupVersion k3s.cattle.io v1 to ResourceManager
time="2024-03-27T14:19:30Z" level=info msg="Applying CRD helmcharts.helm.cattle.io"
I0327 14:19:30.648674    2321 handler.go:275] Adding GroupVersion k3s.cattle.io v1 to ResourceManager
W0327 14:19:30.669341    2321 watcher.go:93] Error while processing event ("/sys/fs/cgroup/devices/kubepods.slice/kubepods-besteffort.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/devices/kubepods.slice/kubepods-besteffort.slice: no such file or directory
I0327 14:19:30.723004    2321 manager.go:471] "Failed to read data from checkpoint" checkpoint="kubelet_internal_checkpoint" err="checkpoint is not found"
I0327 14:19:30.727717    2321 plugin_manager.go:118] "Starting Kubelet Plugin Manager"
I0327 14:19:30.781998    2321 handler.go:275] Adding GroupVersion helm.cattle.io v1 to ResourceManager
time="2024-03-27T14:19:30Z" level=info msg="Applying CRD helmchartconfigs.helm.cattle.io"
I0327 14:19:30.783269    2321 kubelet_node_status.go:493] "Fast updating node status as it just became ready"
I0327 14:19:30.892788    2321 handler.go:275] Adding GroupVersion helm.cattle.io v1 to ResourceManager
time="2024-03-27T14:19:30Z" level=info msg="Writing static file: /var/lib/rancher/k3s/server/static/charts/traefik-25.0.2+up25.0.0.tgz"
time="2024-03-27T14:19:30Z" level=info msg="Writing static file: /var/lib/rancher/k3s/server/static/charts/traefik-crd-25.0.2+up25.0.0.tgz"
time="2024-03-27T14:19:30Z" level=info msg="Failed to get existing traefik HelmChart" error="helmcharts.helm.cattle.io \"traefik\" not found"
time="2024-03-27T14:19:30Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/metrics-apiservice.yaml"
time="2024-03-27T14:19:30Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-service.yaml"
time="2024-03-27T14:19:30Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/resource-reader.yaml"
time="2024-03-27T14:19:30Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/rolebindings.yaml"
time="2024-03-27T14:19:30Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/traefik.yaml"
time="2024-03-27T14:19:30Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/auth-delegator.yaml"
time="2024-03-27T14:19:30Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/auth-reader.yaml"
time="2024-03-27T14:19:30Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/runtimes.yaml"
time="2024-03-27T14:19:30Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/ccm.yaml"
time="2024-03-27T14:19:30Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/coredns.yaml"
time="2024-03-27T14:19:30Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/local-storage.yaml"
time="2024-03-27T14:19:30Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/aggregated-metrics-reader.yaml"
time="2024-03-27T14:19:30Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-deployment.yaml"
time="2024-03-27T14:19:30Z" level=info msg="Starting dynamiclistener CN filter node controller"
time="2024-03-27T14:19:30Z" level=info msg="Tunnel server egress proxy mode: agent"
I0327 14:19:31.026863    2321 desired_state_of_world_populator.go:159] "Finished populating initial desired state of world"
time="2024-03-27T14:19:31Z" level=info msg="Starting k3s.cattle.io/v1, Kind=Addon controller"
time="2024-03-27T14:19:31Z" level=info msg="Creating deploy event broadcaster"
time="2024-03-27T14:19:31Z" level=info msg="Creating helm-controller event broadcaster"
time="2024-03-27T14:19:31Z" level=info msg="Starting /v1, Kind=Node controller"
time="2024-03-27T14:19:31Z" level=warning msg="Unable to fetch coredns config map: configmaps \"coredns\" not found"
time="2024-03-27T14:19:31Z" level=info msg="Cluster dns configmap has been set successfully"
time="2024-03-27T14:19:31Z" level=info msg="Labels and annotations have been set successfully on node: server"
time="2024-03-27T14:19:31Z" level=warning msg="Unable to fetch coredns config map: configmaps \"coredns\" not found"
time="2024-03-27T14:19:31Z" level=info msg="Starting helm.cattle.io/v1, Kind=HelmChartConfig controller"
time="2024-03-27T14:19:31Z" level=info msg="Starting helm.cattle.io/v1, Kind=HelmChart controller"
time="2024-03-27T14:19:31Z" level=info msg="Starting rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding controller"
time="2024-03-27T14:19:31Z" level=info msg="Starting batch/v1, Kind=Job controller"
time="2024-03-27T14:19:31Z" level=info msg="Starting /v1, Kind=Secret controller"
time="2024-03-27T14:19:31Z" level=info msg="Starting /v1, Kind=ConfigMap controller"
time="2024-03-27T14:19:31Z" level=info msg="Starting /v1, Kind=ServiceAccount controller"
time="2024-03-27T14:19:31Z" level=warning msg="Unable to fetch coredns config map: configmaps \"coredns\" not found"
I0327 14:19:31.962773    2321 serving.go:355] Generated self-signed cert in-memory
time="2024-03-27T14:19:32Z" level=info msg="Updating TLS secret for kube-system/k3s-serving (count: 10): map[listener.cattle.io/cn-10.43.0.1:10.43.0.1 listener.cattle.io/cn-127.0.0.1:127.0.0.1 listener.cattle.io/cn-192.168.56.110:192.168.56.110 listener.cattle.io/cn-__1-f16284:::1 listener.cattle.io/cn-kubernetes:kubernetes listener.cattle.io/cn-kubernetes.default:kubernetes.default listener.cattle.io/cn-kubernetes.default.svc:kubernetes.default.svc listener.cattle.io/cn-kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local listener.cattle.io/cn-localhost:localhost listener.cattle.io/cn-server:server listener.cattle.io/fingerprint:SHA1=CD8FBC2467851DDA0EA873553D78030E43366624]"
time="2024-03-27T14:19:32Z" level=info msg="Updating TLS secret for kube-system/k3s-serving (count: 10): map[listener.cattle.io/cn-10.43.0.1:10.43.0.1 listener.cattle.io/cn-127.0.0.1:127.0.0.1 listener.cattle.io/cn-192.168.56.110:192.168.56.110 listener.cattle.io/cn-__1-f16284:::1 listener.cattle.io/cn-kubernetes:kubernetes listener.cattle.io/cn-kubernetes.default:kubernetes.default listener.cattle.io/cn-kubernetes.default.svc:kubernetes.default.svc listener.cattle.io/cn-kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local listener.cattle.io/cn-localhost:localhost listener.cattle.io/cn-server:server listener.cattle.io/fingerprint:SHA1=CD8FBC2467851DDA0EA873553D78030E43366624]"
time="2024-03-27T14:19:32Z" level=info msg="Active TLS secret kube-system/k3s-serving (ver=223) (count 10): map[listener.cattle.io/cn-10.43.0.1:10.43.0.1 listener.cattle.io/cn-127.0.0.1:127.0.0.1 listener.cattle.io/cn-192.168.56.110:192.168.56.110 listener.cattle.io/cn-__1-f16284:::1 listener.cattle.io/cn-kubernetes:kubernetes listener.cattle.io/cn-kubernetes.default:kubernetes.default listener.cattle.io/cn-kubernetes.default.svc:kubernetes.default.svc listener.cattle.io/cn-kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local listener.cattle.io/cn-localhost:localhost listener.cattle.io/cn-server:server listener.cattle.io/fingerprint:SHA1=CD8FBC2467851DDA0EA873553D78030E43366624]"
I0327 14:19:33.084675    2321 controller.go:624] quota admission added evaluator for: addons.k3s.cattle.io
I0327 14:19:33.096848    2321 event.go:307] "Event occurred" object="kube-system/ccm" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/ccm.yaml\""
I0327 14:19:33.149505    2321 controllermanager.go:189] "Starting" version="v1.28.7+k3s1"
I0327 14:19:33.149933    2321 controllermanager.go:191] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0327 14:19:33.163280    2321 secure_serving.go:213] Serving securely on 127.0.0.1:10257
I0327 14:19:33.165433    2321 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I0327 14:19:33.165676    2321 shared_informer.go:311] Waiting for caches to sync for RequestHeaderAuthRequestController
I0327 14:19:33.165731    2321 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0327 14:19:33.166084    2321 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0327 14:19:33.166249    2321 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0327 14:19:33.166348    2321 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0327 14:19:33.166371    2321 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0327 14:19:33.199484    2321 event.go:307] "Event occurred" object="kube-system/ccm" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/ccm.yaml\""
I0327 14:19:33.206657    2321 controllermanager.go:607] "Warning: controller is disabled" controller="service-lb-controller"
I0327 14:19:33.207468    2321 shared_informer.go:311] Waiting for caches to sync for tokens
I0327 14:19:33.245811    2321 event.go:307] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/coredns.yaml\""
I0327 14:19:33.248926    2321 controller.go:624] quota admission added evaluator for: serviceaccounts
I0327 14:19:33.263198    2321 controllermanager.go:642] "Started controller" controller="persistentvolume-binder-controller"
I0327 14:19:33.264454    2321 pv_controller_base.go:319] "Starting persistent volume controller"
I0327 14:19:33.264518    2321 shared_informer.go:311] Waiting for caches to sync for persistent volume
I0327 14:19:33.285912    2321 controllermanager.go:642] "Started controller" controller="ephemeral-volume-controller"
I0327 14:19:33.286523    2321 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0327 14:19:33.286952    2321 controller.go:169] "Starting ephemeral volume controller"
I0327 14:19:33.287285    2321 shared_informer.go:311] Waiting for caches to sync for ephemeral
I0327 14:19:33.290082    2321 shared_informer.go:318] Caches are synced for RequestHeaderAuthRequestController
I0327 14:19:33.290522    2321 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0327 14:19:33.307759    2321 shared_informer.go:318] Caches are synced for tokens
I0327 14:19:33.413772    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="endpoints"
I0327 14:19:33.415163    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="horizontalpodautoscalers.autoscaling"
I0327 14:19:33.415578    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="rolebindings.rbac.authorization.k8s.io"
I0327 14:19:33.415963    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="addons.k3s.cattle.io"
I0327 14:19:33.416188    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="statefulsets.apps"
I0327 14:19:33.416256    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="jobs.batch"
W0327 14:19:33.416286    2321 shared_informer.go:593] resyncPeriod 14h53m32.432156851s is smaller than resyncCheckPeriod 23h45m28.741202608s and the informer has already started. Changing it to 23h45m28.741202608s
W0327 14:19:33.416374    2321 shared_informer.go:593] resyncPeriod 23h30m27.82094274s is smaller than resyncCheckPeriod 23h45m28.741202608s and the informer has already started. Changing it to 23h45m28.741202608s
I0327 14:19:33.416424    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="serviceaccounts"
I0327 14:19:33.416473    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="podtemplates"
I0327 14:19:33.416524    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="networkpolicies.networking.k8s.io"
I0327 14:19:33.416606    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="helmchartconfigs.helm.cattle.io"
I0327 14:19:33.416654    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="limitranges"
I0327 14:19:33.416701    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="daemonsets.apps"
I0327 14:19:33.416742    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="controllerrevisions.apps"
I0327 14:19:33.416787    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingresses.networking.k8s.io"
I0327 14:19:33.416838    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="poddisruptionbudgets.policy"
I0327 14:19:33.416905    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="replicasets.apps"
I0327 14:19:33.416961    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="cronjobs.batch"
I0327 14:19:33.417013    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="roles.rbac.authorization.k8s.io"
I0327 14:19:33.417071    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="csistoragecapacities.storage.k8s.io"
I0327 14:19:33.417119    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="leases.coordination.k8s.io"
I0327 14:19:33.417166    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="endpointslices.discovery.k8s.io"
I0327 14:19:33.417229    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="deployments.apps"
I0327 14:19:33.417276    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="helmcharts.helm.cattle.io"
I0327 14:19:33.417321    2321 controllermanager.go:642] "Started controller" controller="resourcequota-controller"
I0327 14:19:33.417663    2321 resource_quota_controller.go:294] "Starting resource quota controller"
I0327 14:19:33.417689    2321 shared_informer.go:311] Waiting for caches to sync for resource quota
I0327 14:19:33.418016    2321 resource_quota_monitor.go:305] "QuotaMonitor running"
I0327 14:19:33.492527    2321 controllermanager.go:642] "Started controller" controller="serviceaccount-controller"
I0327 14:19:33.493345    2321 serviceaccounts_controller.go:111] "Starting service account controller"
I0327 14:19:33.493868    2321 shared_informer.go:311] Waiting for caches to sync for service account
I0327 14:19:33.517681    2321 controller.go:624] quota admission added evaluator for: deployments.apps
I0327 14:19:33.546001    2321 controllermanager.go:642] "Started controller" controller="certificatesigningrequest-approving-controller"
I0327 14:19:33.546395    2321 controllermanager.go:607] "Warning: controller is disabled" controller="bootstrap-signer-controller"
I0327 14:19:33.547810    2321 certificate_controller.go:115] "Starting certificate controller" name="csrapproving"
I0327 14:19:33.548383    2321 shared_informer.go:311] Waiting for caches to sync for certificate-csrapproving
I0327 14:19:33.615171    2321 node_lifecycle_controller.go:431] "Controller will reconcile labels"
I0327 14:19:33.615426    2321 controllermanager.go:642] "Started controller" controller="node-lifecycle-controller"
I0327 14:19:33.615465    2321 controllermanager.go:607] "Warning: controller is disabled" controller="node-route-controller"
I0327 14:19:33.616736    2321 node_lifecycle_controller.go:465] "Sending events to api server"
I0327 14:19:33.616843    2321 node_lifecycle_controller.go:476] "Starting node controller"
I0327 14:19:33.616865    2321 shared_informer.go:311] Waiting for caches to sync for taint
I0327 14:19:33.620756    2321 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.43.0.10"}
I0327 14:19:33.625335    2321 event.go:307] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/coredns.yaml\""
I0327 14:19:33.675354    2321 controllermanager.go:642] "Started controller" controller="root-ca-certificate-publisher-controller"
I0327 14:19:33.676170    2321 event.go:307] "Event occurred" object="kube-system/local-storage" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/local-storage.yaml\""
I0327 14:19:33.676595    2321 publisher.go:102] "Starting root CA cert publisher controller"
I0327 14:19:33.676637    2321 shared_informer.go:311] Waiting for caches to sync for crt configmap
I0327 14:19:33.731491    2321 controllermanager.go:642] "Started controller" controller="endpoints-controller"
I0327 14:19:33.732378    2321 endpoints_controller.go:174] "Starting endpoint controller"
I0327 14:19:33.732431    2321 shared_informer.go:311] Waiting for caches to sync for endpoint
I0327 14:19:33.774647    2321 controllermanager.go:642] "Started controller" controller="daemonset-controller"
I0327 14:19:33.775533    2321 daemon_controller.go:291] "Starting daemon sets controller"
I0327 14:19:33.775584    2321 shared_informer.go:311] Waiting for caches to sync for daemon sets
I0327 14:19:33.820448    2321 controllermanager.go:642] "Started controller" controller="deployment-controller"
I0327 14:19:33.821248    2321 deployment_controller.go:168] "Starting controller" controller="deployment"
I0327 14:19:33.821296    2321 shared_informer.go:311] Waiting for caches to sync for deployment
I0327 14:19:33.863791    2321 controllermanager.go:642] "Started controller" controller="replicaset-controller"
I0327 14:19:33.864984    2321 replica_set.go:214] "Starting controller" name="replicaset"
I0327 14:19:33.865038    2321 shared_informer.go:311] Waiting for caches to sync for ReplicaSet
I0327 14:19:33.874323    2321 event.go:307] "Event occurred" object="kube-system/local-storage" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/local-storage.yaml\""
I0327 14:19:33.907827    2321 event.go:307] "Event occurred" object="kube-system/aggregated-metrics-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/aggregated-metrics-reader.yaml\""
I0327 14:19:33.940613    2321 event.go:307] "Event occurred" object="kube-system/aggregated-metrics-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/aggregated-metrics-reader.yaml\""
I0327 14:19:33.966621    2321 controllermanager.go:642] "Started controller" controller="token-cleaner-controller"
I0327 14:19:33.967185    2321 tokencleaner.go:112] "Starting token cleaner controller"
I0327 14:19:33.967229    2321 shared_informer.go:311] Waiting for caches to sync for token_cleaner
I0327 14:19:33.967246    2321 shared_informer.go:318] Caches are synced for token_cleaner
I0327 14:19:33.983094    2321 event.go:307] "Event occurred" object="kube-system/auth-delegator" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/auth-delegator.yaml\""
I0327 14:19:34.022781    2321 controllermanager.go:642] "Started controller" controller="endpointslice-mirroring-controller"
I0327 14:19:34.025625    2321 endpointslicemirroring_controller.go:223] "Starting EndpointSliceMirroring controller"
I0327 14:19:34.025691    2321 shared_informer.go:311] Waiting for caches to sync for endpoint_slice_mirroring
I0327 14:19:34.026567    2321 event.go:307] "Event occurred" object="kube-system/auth-delegator" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/auth-delegator.yaml\""
I0327 14:19:34.074582    2321 event.go:307] "Event occurred" object="kube-system/auth-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/auth-reader.yaml\""
I0327 14:19:34.107324    2321 event.go:307] "Event occurred" object="kube-system/auth-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/auth-reader.yaml\""
I0327 14:19:34.131550    2321 controllermanager.go:642] "Started controller" controller="garbage-collector-controller"
I0327 14:19:34.133105    2321 garbagecollector.go:155] "Starting controller" controller="garbagecollector"
I0327 14:19:34.133323    2321 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0327 14:19:34.133855    2321 graph_builder.go:294] "Running" component="GraphBuilder"
I0327 14:19:34.151472    2321 event.go:307] "Event occurred" object="kube-system/metrics-apiservice" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-apiservice.yaml\""
I0327 14:19:34.189907    2321 handler.go:275] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0327 14:19:34.191521    2321 event.go:307] "Event occurred" object="kube-system/metrics-apiservice" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-apiservice.yaml\""
I0327 14:19:34.238197    2321 event.go:307] "Event occurred" object="kube-system/metrics-server-deployment" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-deployment.yaml\""
W0327 14:19:34.350464    2321 handler_proxy.go:93] no RequestInfo found in the context
E0327 14:19:34.350956    2321 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
E0327 14:19:34.352320    2321 handler_proxy.go:137] error resolving kube-system/metrics-server: service "metrics-server" not found
I0327 14:19:34.484565    2321 controllermanager.go:642] "Started controller" controller="statefulset-controller"
I0327 14:19:34.487886    2321 stateful_set.go:161] "Starting stateful set controller"
I0327 14:19:34.488239    2321 shared_informer.go:311] Waiting for caches to sync for stateful set
I0327 14:19:34.727302    2321 event.go:307] "Event occurred" object="kube-system/metrics-server-deployment" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-deployment.yaml\""
I0327 14:19:34.871461    2321 garbagecollector.go:816] "failed to discover some groups" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"
I0327 14:19:35.106903    2321 event.go:307] "Event occurred" object="kube-system/metrics-server-service" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-service.yaml\""
I0327 14:19:35.182215    2321 serving.go:355] Generated self-signed cert in-memory
W0327 14:19:35.200640    2321 handler_proxy.go:93] no RequestInfo found in the context
E0327 14:19:35.201389    2321 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0327 14:19:35.201730    2321 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0327 14:19:35.202510    2321 handler_proxy.go:93] no RequestInfo found in the context
E0327 14:19:35.202809    2321 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I0327 14:19:35.202843    2321 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0327 14:19:35.231868    2321 controllermanager.go:642] "Started controller" controller="persistentvolume-attach-detach-controller"
I0327 14:19:35.234314    2321 attach_detach_controller.go:337] "Starting attach detach controller"
I0327 14:19:35.234598    2321 shared_informer.go:311] Waiting for caches to sync for attach detach
I0327 14:19:35.439130    2321 event.go:307] "Event occurred" object="server" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodePasswordValidationComplete" message="Deferred node password secret validation complete"
time="2024-03-27T14:19:35Z" level=info msg="Running kube-proxy --cluster-cidr=10.42.0.0/16 --conntrack-max-per-core=0 --conntrack-tcp-timeout-close-wait=0s --conntrack-tcp-timeout-established=0s --healthz-bind-address=127.0.0.1 --hostname-override=server --kubeconfig=/var/lib/rancher/k3s/agent/kubeproxy.kubeconfig --proxy-mode=iptables"
I0327 14:19:35.544718    2321 alloc.go:330] "allocated clusterIPs" service="kube-system/metrics-server" clusterIPs={"IPv4":"10.43.168.195"}
I0327 14:19:35.553013    2321 controllermanager.go:642] "Started controller" controller="ttl-after-finished-controller"
I0327 14:19:35.557333    2321 ttlafterfinished_controller.go:109] "Starting TTL after finished controller"
I0327 14:19:35.557573    2321 shared_informer.go:311] Waiting for caches to sync for TTL after finished
I0327 14:19:35.558347    2321 event.go:307] "Event occurred" object="kube-system/metrics-server-service" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-service.yaml\""
I0327 14:19:35.583961    2321 node.go:141] Successfully retrieved node IP: 192.168.56.110
I0327 14:19:35.602313    2321 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
W0327 14:19:35.604309    2321 handler_proxy.go:93] no RequestInfo found in the context
E0327 14:19:35.604499    2321 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
E0327 14:19:35.604962    2321 handler_proxy.go:137] error resolving kube-system/metrics-server: endpoints "metrics-server" not found
I0327 14:19:35.609290    2321 server_others.go:152] "Using iptables Proxier"
I0327 14:19:35.609368    2321 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0327 14:19:35.609390    2321 server_others.go:438] "Defaulting to no-op detect-local"
I0327 14:19:35.609447    2321 proxier.go:250] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0327 14:19:35.609897    2321 server.go:846] "Version info" version="v1.28.7+k3s1"
I0327 14:19:35.609925    2321 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0327 14:19:35.643112    2321 config.go:188] "Starting service config controller"
I0327 14:19:35.643175    2321 shared_informer.go:311] Waiting for caches to sync for service config
I0327 14:19:35.643296    2321 config.go:97] "Starting endpoint slice config controller"
I0327 14:19:35.643315    2321 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0327 14:19:35.644778    2321 config.go:315] "Starting node config controller"
I0327 14:19:35.644825    2321 shared_informer.go:311] Waiting for caches to sync for node config
I0327 14:19:35.693245    2321 event.go:307] "Event occurred" object="kube-system/resource-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/resource-reader.yaml\""
I0327 14:19:35.737179    2321 controllermanager.go:642] "Started controller" controller="job-controller"
I0327 14:19:35.737950    2321 job_controller.go:226] "Starting job controller"
I0327 14:19:35.738131    2321 shared_informer.go:311] Waiting for caches to sync for job
I0327 14:19:35.761428    2321 event.go:307] "Event occurred" object="kube-system/resource-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/resource-reader.yaml\""
I0327 14:19:35.780344    2321 shared_informer.go:318] Caches are synced for service config
I0327 14:19:35.780530    2321 shared_informer.go:318] Caches are synced for endpoint slice config
I0327 14:19:35.787088    2321 shared_informer.go:318] Caches are synced for node config
I0327 14:19:35.950332    2321 controllermanager.go:642] "Started controller" controller="cronjob-controller"
I0327 14:19:35.954840    2321 cronjob_controllerv2.go:139] "Starting cronjob controller v2"
I0327 14:19:35.960849    2321 shared_informer.go:311] Waiting for caches to sync for cronjob
I0327 14:19:35.961762    2321 event.go:307] "Event occurred" object="kube-system/rolebindings" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/rolebindings.yaml\""
I0327 14:19:35.994502    2321 controllermanager.go:642] "Started controller" controller="certificatesigningrequest-signing-controller"
I0327 14:19:35.999541    2321 certificate_controller.go:115] "Starting certificate controller" name="csrsigning-kubelet-serving"
I0327 14:19:35.999608    2321 shared_informer.go:311] Waiting for caches to sync for certificate-csrsigning-kubelet-serving
I0327 14:19:35.999746    2321 certificate_controller.go:115] "Starting certificate controller" name="csrsigning-kubelet-client"
I0327 14:19:36.000039    2321 shared_informer.go:311] Waiting for caches to sync for certificate-csrsigning-kubelet-client
I0327 14:19:36.000160    2321 certificate_controller.go:115] "Starting certificate controller" name="csrsigning-kube-apiserver-client"
I0327 14:19:36.000375    2321 shared_informer.go:311] Waiting for caches to sync for certificate-csrsigning-kube-apiserver-client
I0327 14:19:36.000967    2321 certificate_controller.go:115] "Starting certificate controller" name="csrsigning-legacy-unknown"
I0327 14:19:36.001014    2321 shared_informer.go:311] Waiting for caches to sync for certificate-csrsigning-legacy-unknown
I0327 14:19:36.001452    2321 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/var/lib/rancher/k3s/server/tls/server-ca.nochain.crt::/var/lib/rancher/k3s/server/tls/server-ca.key"
I0327 14:19:36.001911    2321 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/var/lib/rancher/k3s/server/tls/client-ca.nochain.crt::/var/lib/rancher/k3s/server/tls/client-ca.key"
I0327 14:19:36.002308    2321 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/var/lib/rancher/k3s/server/tls/client-ca.nochain.crt::/var/lib/rancher/k3s/server/tls/client-ca.key"
I0327 14:19:36.015164    2321 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/var/lib/rancher/k3s/server/tls/server-ca.nochain.crt::/var/lib/rancher/k3s/server/tls/server-ca.key"
I0327 14:19:36.037221    2321 controllermanager.go:642] "Started controller" controller="certificatesigningrequest-cleaner-controller"
I0327 14:19:36.038019    2321 cleaner.go:83] "Starting CSR cleaner controller"
I0327 14:19:36.098849    2321 controllermanager.go:642] "Started controller" controller="persistentvolumeclaim-protection-controller"
I0327 14:19:36.103598    2321 pvc_protection_controller.go:102] "Starting PVC protection controller"
I0327 14:19:36.103986    2321 shared_informer.go:311] Waiting for caches to sync for PVC protection
I0327 14:19:36.134697    2321 event.go:307] "Event occurred" object="kube-system/rolebindings" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/rolebindings.yaml\""
I0327 14:19:36.333551    2321 event.go:307] "Event occurred" object="kube-system/runtimes" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/runtimes.yaml\""
I0327 14:19:36.369617    2321 controllermanager.go:642] "Started controller" controller="persistentvolume-protection-controller"
I0327 14:19:36.370850    2321 pv_protection_controller.go:78] "Starting PV protection controller"
I0327 14:19:36.370903    2321 shared_informer.go:311] Waiting for caches to sync for PV protection
I0327 14:19:36.429726    2321 controllermanager.go:642] "Started controller" controller="replicationcontroller-controller"
I0327 14:19:36.430865    2321 replica_set.go:214] "Starting controller" name="replicationcontroller"
I0327 14:19:36.430921    2321 shared_informer.go:311] Waiting for caches to sync for ReplicationController
I0327 14:19:36.497605    2321 controllermanager.go:642] "Started controller" controller="pod-garbage-collector-controller"
I0327 14:19:36.498791    2321 gc_controller.go:101] "Starting GC controller"
I0327 14:19:36.498845    2321 shared_informer.go:311] Waiting for caches to sync for GC
W0327 14:19:36.617887    2321 handler_proxy.go:93] no RequestInfo found in the context
W0327 14:19:36.618120    2321 handler_proxy.go:93] no RequestInfo found in the context
E0327 14:19:36.618171    2321 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I0327 14:19:36.618421    2321 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E0327 14:19:36.619029    2321 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0327 14:19:36.619501    2321 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0327 14:19:36.634679    2321 event.go:307] "Event occurred" object="kube-system/runtimes" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/runtimes.yaml\""
I0327 14:19:36.723337    2321 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/traefik.yaml\""
E0327 14:19:36.801684    2321 namespaced_resources_deleter.go:162] unable to get all supported resources from server: unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I0327 14:19:36.801963    2321 controllermanager.go:642] "Started controller" controller="namespace-controller"
I0327 14:19:36.802291    2321 controllermanager.go:607] "Warning: controller is disabled" controller="cloud-node-lifecycle-controller"
I0327 14:19:36.805481    2321 controller.go:624] quota admission added evaluator for: helmcharts.helm.cattle.io
I0327 14:19:36.805855    2321 namespace_controller.go:197] "Starting namespace controller"
I0327 14:19:36.805902    2321 shared_informer.go:311] Waiting for caches to sync for namespace
time="2024-03-27T14:19:36Z" level=info msg="Tunnel authorizer set Kubelet Port 10250"
I0327 14:19:36.905111    2321 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/traefik.yaml\""
I0327 14:19:36.942916    2321 controllermanager.go:642] "Started controller" controller="clusterrole-aggregation-controller"
I0327 14:19:36.945050    2321 clusterroleaggregation_controller.go:189] "Starting ClusterRoleAggregator controller"
I0327 14:19:36.945101    2321 shared_informer.go:311] Waiting for caches to sync for ClusterRoleAggregator
I0327 14:19:36.993521    2321 event.go:307] "Event occurred" object="kube-system/traefik-crd" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik-crd"
I0327 14:19:36.993602    2321 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
I0327 14:19:37.154507    2321 controllermanager.go:642] "Started controller" controller="horizontal-pod-autoscaler-controller"
I0327 14:19:37.159744    2321 horizontal.go:200] "Starting HPA controller"
I0327 14:19:37.159801    2321 shared_informer.go:311] Waiting for caches to sync for HPA
I0327 14:19:37.196504    2321 controller.go:624] quota admission added evaluator for: jobs.batch
I0327 14:19:37.256638    2321 controllermanager.go:642] "Started controller" controller="ttl-controller"
I0327 14:19:37.258216    2321 ttl_controller.go:124] "Starting TTL controller"
I0327 14:19:37.258260    2321 shared_informer.go:311] Waiting for caches to sync for TTL
I0327 14:19:37.274882    2321 serving.go:355] Generated self-signed cert in-memory
time="2024-03-27T14:19:37Z" level=error msg="error syncing 'kube-system/traefik': handler helm-controller-chart-registration: helmcharts.helm.cattle.io \"traefik\" not found, requeuing"
I0327 14:19:37.322721    2321 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
time="2024-03-27T14:19:37Z" level=error msg="error syncing 'kube-system/traefik-crd': handler helm-controller-chart-registration: helmcharts.helm.cattle.io \"traefik-crd\" not found, requeuing"
I0327 14:19:37.344491    2321 event.go:307] "Event occurred" object="kube-system/traefik-crd" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik-crd"
I0327 14:19:37.368853    2321 event.go:307] "Event occurred" object="kube-system/traefik-crd" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik-crd"
I0327 14:19:37.470342    2321 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
I0327 14:19:37.470886    2321 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
time="2024-03-27T14:19:37Z" level=info msg="Stopped tunnel to 127.0.0.1:6443"
time="2024-03-27T14:19:37Z" level=info msg="Connecting to proxy" url="wss://192.168.56.110:6443/v1-k3s/connect"
time="2024-03-27T14:19:37Z" level=info msg="Proxy done" err="context canceled" url="wss://127.0.0.1:6443/v1-k3s/connect"
time="2024-03-27T14:19:37Z" level=info msg="error in remotedialer server [400]: websocket: close 1006 (abnormal closure): unexpected EOF"
time="2024-03-27T14:19:37Z" level=info msg="Handling backend connection request [server]"
I0327 14:19:38.766781    2321 controllermanager.go:168] Version: v1.28.7+k3s1
I0327 14:19:38.775490    2321 secure_serving.go:213] Serving securely on 127.0.0.1:10258
I0327 14:19:38.775857    2321 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0327 14:19:38.776297    2321 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I0327 14:19:38.776522    2321 shared_informer.go:311] Waiting for caches to sync for RequestHeaderAuthRequestController
I0327 14:19:38.776916    2321 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0327 14:19:38.777200    2321 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0327 14:19:38.777426    2321 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0327 14:19:38.777633    2321 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
E0327 14:19:38.806875    2321 controllermanager.go:524] unable to get all supported resources from server: unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
time="2024-03-27T14:19:38Z" level=info msg="Creating  event broadcaster"
I0327 14:19:38.877564    2321 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0327 14:19:38.877725    2321 shared_informer.go:318] Caches are synced for RequestHeaderAuthRequestController
I0327 14:19:38.877932    2321 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
time="2024-03-27T14:19:38Z" level=info msg="Starting /v1, Kind=Node controller"
time="2024-03-27T14:19:38Z" level=info msg="Starting /v1, Kind=Pod controller"
I0327 14:19:38.988606    2321 controllermanager.go:337] Started "cloud-node-controller"
I0327 14:19:38.989402    2321 node_controller.go:165] Sending events to api server.
I0327 14:19:38.989776    2321 node_controller.go:174] Waiting for informer caches to sync
I0327 14:19:38.989989    2321 node_lifecycle_controller.go:113] Sending events to api server
time="2024-03-27T14:19:38Z" level=info msg="Starting apps/v1, Kind=DaemonSet controller"
I0327 14:19:38.989798    2321 controllermanager.go:337] Started "cloud-node-lifecycle-controller"
time="2024-03-27T14:19:38Z" level=info msg="Starting discovery.k8s.io/v1, Kind=EndpointSlice controller"
I0327 14:19:38.991627    2321 controllermanager.go:337] Started "service-lb-controller"
W0327 14:19:38.991885    2321 controllermanager.go:314] "node-route-controller" is disabled
I0327 14:19:38.993665    2321 controller.go:231] Starting service controller
I0327 14:19:38.993959    2321 shared_informer.go:311] Waiting for caches to sync for service
I0327 14:19:39.090241    2321 node_controller.go:431] Initializing node server with cloud provider
I0327 14:19:39.094725    2321 shared_informer.go:318] Caches are synced for service
I0327 14:19:39.104571    2321 node_controller.go:502] Successfully initialized node server with cloud provider
I0327 14:19:39.105501    2321 event.go:307] "Event occurred" object="server" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="Synced" message="Node synced successfully"
time="2024-03-27T14:19:39Z" level=info msg="Updated coredns node hosts entry [192.168.56.110 server]"
I0327 14:19:41.145393    2321 serving.go:355] Generated self-signed cert in-memory
I0327 14:19:42.311619    2321 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.7+k3s1"
I0327 14:19:42.311694    2321 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0327 14:19:42.319986    2321 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I0327 14:19:42.320036    2321 shared_informer.go:311] Waiting for caches to sync for RequestHeaderAuthRequestController
I0327 14:19:42.320089    2321 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0327 14:19:42.320106    2321 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0327 14:19:42.320129    2321 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0327 14:19:42.320146    2321 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0327 14:19:42.321950    2321 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0327 14:19:42.322272    2321 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0327 14:19:42.421021    2321 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0327 14:19:42.421111    2321 shared_informer.go:318] Caches are synced for RequestHeaderAuthRequestController
I0327 14:19:42.421256    2321 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0327 14:19:47.332300    2321 range_allocator.go:111] "No Secondary Service CIDR provided. Skipping filtering out secondary service addresses"
I0327 14:19:47.332458    2321 controllermanager.go:642] "Started controller" controller="node-ipam-controller"
I0327 14:19:47.333007    2321 node_ipam_controller.go:162] "Starting ipam controller"
I0327 14:19:47.333059    2321 shared_informer.go:311] Waiting for caches to sync for node
I0327 14:19:47.390349    2321 controllermanager.go:642] "Started controller" controller="persistentvolume-expander-controller"
I0327 14:19:47.392324    2321 expand_controller.go:328] "Starting expand controller"
I0327 14:19:47.404226    2321 shared_informer.go:311] Waiting for caches to sync for expand
I0327 14:19:47.439159    2321 controllermanager.go:642] "Started controller" controller="endpointslice-controller"
I0327 14:19:47.440286    2321 endpointslice_controller.go:264] "Starting endpoint slice controller"
I0327 14:19:47.440367    2321 shared_informer.go:311] Waiting for caches to sync for endpoint_slice
I0327 14:19:47.477496    2321 controllermanager.go:642] "Started controller" controller="disruption-controller"
I0327 14:19:47.506777    2321 disruption.go:433] "Sending events to api server."
I0327 14:19:47.506970    2321 disruption.go:444] "Starting disruption controller"
I0327 14:19:47.507007    2321 shared_informer.go:311] Waiting for caches to sync for disruption
I0327 14:19:47.531017    2321 shared_informer.go:311] Waiting for caches to sync for resource quota
I0327 14:19:47.635924    2321 shared_informer.go:318] Caches are synced for PVC protection
I0327 14:19:47.637703    2321 shared_informer.go:318] Caches are synced for node
I0327 14:19:47.637815    2321 range_allocator.go:174] "Sending events to api server"
I0327 14:19:47.637869    2321 range_allocator.go:178] "Starting range CIDR allocator"
I0327 14:19:47.637906    2321 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I0327 14:19:47.637921    2321 shared_informer.go:318] Caches are synced for cidrallocator
I0327 14:19:47.648638    2321 shared_informer.go:318] Caches are synced for certificate-csrapproving
I0327 14:19:47.654179    2321 actual_state_of_world.go:547] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"server\" does not exist"
I0327 14:19:47.657402    2321 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I0327 14:19:47.659866    2321 shared_informer.go:318] Caches are synced for HPA
I0327 14:19:47.660436    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0327 14:19:47.663643    2321 shared_informer.go:318] Caches are synced for TTL
I0327 14:19:47.666511    2321 shared_informer.go:318] Caches are synced for ReplicaSet
I0327 14:19:47.676206    2321 shared_informer.go:318] Caches are synced for TTL after finished
I0327 14:19:47.688410    2321 shared_informer.go:318] Caches are synced for stateful set
I0327 14:19:47.691451    2321 shared_informer.go:318] Caches are synced for ephemeral
I0327 14:19:47.692333    2321 shared_informer.go:318] Caches are synced for daemon sets
I0327 14:19:47.695629    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0327 14:19:47.696080    2321 shared_informer.go:318] Caches are synced for cronjob
I0327 14:19:47.700081    2321 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I0327 14:19:47.700178    2321 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I0327 14:19:47.700762    2321 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0327 14:19:47.701369    2321 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I0327 14:19:47.702242    2321 shared_informer.go:318] Caches are synced for GC
I0327 14:19:47.702704    2321 shared_informer.go:318] Caches are synced for crt configmap
I0327 14:19:47.705390    2321 shared_informer.go:318] Caches are synced for expand
I0327 14:19:47.717140    2321 shared_informer.go:318] Caches are synced for taint
I0327 14:19:47.718147    2321 node_lifecycle_controller.go:1225] "Initializing eviction metric for zone" zone=""
I0327 14:19:47.719121    2321 node_lifecycle_controller.go:877] "Missing timestamp for Node. Assuming now as a timestamp" node="server"
I0327 14:19:47.719703    2321 node_lifecycle_controller.go:1071] "Controller detected that zone is now in new state" zone="" newState="Normal"
I0327 14:19:47.721663    2321 shared_informer.go:318] Caches are synced for deployment
I0327 14:19:47.726603    2321 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I0327 14:19:47.730199    2321 taint_manager.go:205] "Starting NoExecuteTaintManager"
I0327 14:19:47.730586    2321 taint_manager.go:210] "Sending events to api server"
I0327 14:19:47.731069    2321 shared_informer.go:318] Caches are synced for ReplicationController
I0327 14:19:47.732593    2321 shared_informer.go:318] Caches are synced for endpoint
I0327 14:19:47.754393    2321 shared_informer.go:318] Caches are synced for job
I0327 14:19:47.756607    2321 event.go:307] "Event occurred" object="server" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node server event: Registered Node server in Controller"
I0327 14:19:47.757002    2321 shared_informer.go:318] Caches are synced for endpoint_slice
time="2024-03-27T14:19:47Z" level=info msg="Flannel found PodCIDR assigned for node server"
time="2024-03-27T14:19:47Z" level=info msg="The interface enp0s3 with ipv4 address 10.0.2.15 will be used by flannel"
I0327 14:19:47.815550    2321 kube.go:139] Waiting 10m0s for node controller to sync
I0327 14:19:47.819337    2321 range_allocator.go:380] "Set node PodCIDR" node="server" podCIDRs=["10.42.0.0/24"]
I0327 14:19:47.823767    2321 kube.go:461] Starting kube subnet manager
I0327 14:19:47.831418    2321 shared_informer.go:318] Caches are synced for resource quota
I0327 14:19:47.837275    2321 shared_informer.go:318] Caches are synced for attach detach
I0327 14:19:47.865868    2321 shared_informer.go:318] Caches are synced for persistent volume
I0327 14:19:47.871763    2321 shared_informer.go:318] Caches are synced for PV protection
I0327 14:19:47.888542    2321 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0327 14:19:47.894043    2321 shared_informer.go:318] Caches are synced for service account
I0327 14:19:47.906955    2321 shared_informer.go:318] Caches are synced for namespace
I0327 14:19:47.907554    2321 shared_informer.go:318] Caches are synced for disruption
I0327 14:19:47.919400    2321 shared_informer.go:318] Caches are synced for resource quota
I0327 14:19:48.096826    2321 controller.go:624] quota admission added evaluator for: replicasets.apps
time="2024-03-27T14:19:48Z" level=info msg="Starting network policy controller version v2.0.1, built on 2024-02-29T20:36:21Z, go1.21.7"
I0327 14:19:48.162989    2321 network_policy_controller.go:164] Starting network policy controller
I0327 14:19:48.232714    2321 event.go:307] "Event occurred" object="kube-system/local-path-provisioner" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set local-path-provisioner-6c86858495 to 1"
I0327 14:19:48.233382    2321 event.go:307] "Event occurred" object="kube-system/metrics-server" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set metrics-server-67c658944b to 1"
I0327 14:19:48.233503    2321 event.go:307] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set coredns-6799fbcd5 to 1"
I0327 14:19:48.237809    2321 shared_informer.go:318] Caches are synced for garbage collector
I0327 14:19:48.237898    2321 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
W0327 14:19:48.259517    2321 handler_proxy.go:93] no RequestInfo found in the context
E0327 14:19:48.261376    2321 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
E0327 14:19:48.263534    2321 handler_proxy.go:137] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
I0327 14:19:48.293646    2321 shared_informer.go:318] Caches are synced for garbage collector
I0327 14:19:48.345766    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0327 14:19:48.349428    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0327 14:19:48.364674    2321 event.go:307] "Event occurred" object="kube-system/helm-install-traefik" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: helm-install-traefik-rqbh9"
I0327 14:19:48.380910    2321 event.go:307] "Event occurred" object="kube-system/helm-install-traefik-crd" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: helm-install-traefik-crd-dtrfg"
I0327 14:19:48.526275    2321 topology_manager.go:215] "Topology Admit Handler" podUID="6b8c7f1e-049d-469a-a179-56363f323e70" podNamespace="kube-system" podName="helm-install-traefik-crd-dtrfg"
I0327 14:19:48.538836    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0327 14:19:48.539926    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0327 14:19:48.594838    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0327 14:19:48.617222    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-b7dbw\" (UniqueName: \"kubernetes.io/projected/6b8c7f1e-049d-469a-a179-56363f323e70-kube-api-access-b7dbw\") pod \"helm-install-traefik-crd-dtrfg\" (UID: \"6b8c7f1e-049d-469a-a179-56363f323e70\") " pod="kube-system/helm-install-traefik-crd-dtrfg"
I0327 14:19:48.617405    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/6b8c7f1e-049d-469a-a179-56363f323e70-klipper-helm\") pod \"helm-install-traefik-crd-dtrfg\" (UID: \"6b8c7f1e-049d-469a-a179-56363f323e70\") " pod="kube-system/helm-install-traefik-crd-dtrfg"
I0327 14:19:48.617513    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/6b8c7f1e-049d-469a-a179-56363f323e70-klipper-cache\") pod \"helm-install-traefik-crd-dtrfg\" (UID: \"6b8c7f1e-049d-469a-a179-56363f323e70\") " pod="kube-system/helm-install-traefik-crd-dtrfg"
I0327 14:19:48.617622    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/6b8c7f1e-049d-469a-a179-56363f323e70-klipper-config\") pod \"helm-install-traefik-crd-dtrfg\" (UID: \"6b8c7f1e-049d-469a-a179-56363f323e70\") " pod="kube-system/helm-install-traefik-crd-dtrfg"
I0327 14:19:48.617710    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/6b8c7f1e-049d-469a-a179-56363f323e70-tmp\") pod \"helm-install-traefik-crd-dtrfg\" (UID: \"6b8c7f1e-049d-469a-a179-56363f323e70\") " pod="kube-system/helm-install-traefik-crd-dtrfg"
I0327 14:19:48.617796    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"values\" (UniqueName: \"kubernetes.io/secret/6b8c7f1e-049d-469a-a179-56363f323e70-values\") pod \"helm-install-traefik-crd-dtrfg\" (UID: \"6b8c7f1e-049d-469a-a179-56363f323e70\") " pod="kube-system/helm-install-traefik-crd-dtrfg"
I0327 14:19:48.617883    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"content\" (UniqueName: \"kubernetes.io/configmap/6b8c7f1e-049d-469a-a179-56363f323e70-content\") pod \"helm-install-traefik-crd-dtrfg\" (UID: \"6b8c7f1e-049d-469a-a179-56363f323e70\") " pod="kube-system/helm-install-traefik-crd-dtrfg"
I0327 14:19:48.618343    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0327 14:19:48.658046    2321 topology_manager.go:215] "Topology Admit Handler" podUID="87543c2b-a2be-4644-8b8b-960585860c4d" podNamespace="kube-system" podName="helm-install-traefik-rqbh9"
W0327 14:19:48.735263    2321 watcher.go:93] Error while processing event ("/sys/fs/cgroup/devices/kubepods.slice/kubepods-besteffort.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/devices/kubepods.slice/kubepods-besteffort.slice: no such file or directory
I0327 14:19:48.784173    2321 event.go:307] "Event occurred" object="kube-system/local-path-provisioner-6c86858495" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: local-path-provisioner-6c86858495-znp6t"
I0327 14:19:48.784252    2321 event.go:307] "Event occurred" object="kube-system/coredns-6799fbcd5" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: coredns-6799fbcd5-mc67c"
I0327 14:19:48.784277    2321 event.go:307] "Event occurred" object="kube-system/metrics-server-67c658944b" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: metrics-server-67c658944b-6t6cb"
I0327 14:19:48.817334    2321 kube.go:146] Node controller sync successful
I0327 14:19:48.862468    2321 vxlan.go:141] VXLAN config: VNI=1 Port=0 GBP=false Learning=false DirectRouting=false
I0327 14:19:48.821450    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/87543c2b-a2be-4644-8b8b-960585860c4d-klipper-config\") pod \"helm-install-traefik-rqbh9\" (UID: \"87543c2b-a2be-4644-8b8b-960585860c4d\") " pod="kube-system/helm-install-traefik-rqbh9"
I0327 14:19:48.868516    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/87543c2b-a2be-4644-8b8b-960585860c4d-klipper-cache\") pod \"helm-install-traefik-rqbh9\" (UID: \"87543c2b-a2be-4644-8b8b-960585860c4d\") " pod="kube-system/helm-install-traefik-rqbh9"
I0327 14:19:48.868622    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-w2kxz\" (UniqueName: \"kubernetes.io/projected/87543c2b-a2be-4644-8b8b-960585860c4d-kube-api-access-w2kxz\") pod \"helm-install-traefik-rqbh9\" (UID: \"87543c2b-a2be-4644-8b8b-960585860c4d\") " pod="kube-system/helm-install-traefik-rqbh9"
I0327 14:19:48.868859    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/87543c2b-a2be-4644-8b8b-960585860c4d-klipper-helm\") pod \"helm-install-traefik-rqbh9\" (UID: \"87543c2b-a2be-4644-8b8b-960585860c4d\") " pod="kube-system/helm-install-traefik-rqbh9"
I0327 14:19:48.869242    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"values\" (UniqueName: \"kubernetes.io/secret/87543c2b-a2be-4644-8b8b-960585860c4d-values\") pod \"helm-install-traefik-rqbh9\" (UID: \"87543c2b-a2be-4644-8b8b-960585860c4d\") " pod="kube-system/helm-install-traefik-rqbh9"
I0327 14:19:48.869507    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"content\" (UniqueName: \"kubernetes.io/configmap/87543c2b-a2be-4644-8b8b-960585860c4d-content\") pod \"helm-install-traefik-rqbh9\" (UID: \"87543c2b-a2be-4644-8b8b-960585860c4d\") " pod="kube-system/helm-install-traefik-rqbh9"
I0327 14:19:48.869793    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/87543c2b-a2be-4644-8b8b-960585860c4d-tmp\") pod \"helm-install-traefik-rqbh9\" (UID: \"87543c2b-a2be-4644-8b8b-960585860c4d\") " pod="kube-system/helm-install-traefik-rqbh9"
I0327 14:19:48.950880    2321 network_policy_controller.go:176] Starting network policy controller full sync goroutine
I0327 14:19:49.161693    2321 kube.go:621] List of node(server) annotations: map[string]string{"alpha.kubernetes.io/provided-node-ip":"192.168.56.110", "k3s.io/hostname":"server", "k3s.io/internal-ip":"192.168.56.110", "k3s.io/node-args":"[\"server\",\"--node-ip\",\"192.168.56.110\",\"--write-kubeconfig-mode\",\"644\",\"--log\",\"/vagrant/logs/server.logs\"]", "k3s.io/node-config-hash":"OEDLXPLCFN65I5K4IWF2M6JMEKD34L47WZJH7YZKLHVRXJTJG6TQ====", "k3s.io/node-env":"{\"K3S_DATA_DIR\":\"/var/lib/rancher/k3s/data/a3b46c0299091b71bfcc617b1e1fec1845c13bdd848584ceb39d2e700e702a4b\"}", "node.alpha.kubernetes.io/ttl":"0", "volumes.kubernetes.io/controller-managed-attach-detach":"true"}
I0327 14:19:49.272405    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="1.052630453s"
W0327 14:19:49.296293    2321 handler_proxy.go:93] no RequestInfo found in the context
E0327 14:19:49.296543    2321 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0327 14:19:49.296580    2321 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0327 14:19:49.296743    2321 handler_proxy.go:93] no RequestInfo found in the context
E0327 14:19:49.296799    2321 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I0327 14:19:49.297735    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="1.079506626s"
I0327 14:19:49.298317    2321 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0327 14:19:49.326441    2321 trace.go:236] Trace[956318249]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:037e20f9-713c-4c01-88ab-edb6dd852175,client:127.0.0.1,protocol:HTTP/2.0,resource:replicasets,scope:resource,url:/apis/apps/v1/namespaces/kube-system/replicasets/coredns-6799fbcd5/status,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:replicaset-controller,verb:PUT (27-Mar-2024 14:19:48.804) (total time: 521ms):
Trace[956318249]: ---"limitedReadBody succeeded" len:5924 29ms (14:19:48.834)
Trace[956318249]: ---"Writing http response done" 21ms (14:19:49.326)
Trace[956318249]: [521.656611ms] [521.656611ms] END
I0327 14:19:49.382600    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="1.1633727s"
I0327 14:19:49.389307    2321 topology_manager.go:215] "Topology Admit Handler" podUID="8522bcad-f41d-4fe3-a8c8-2d7b94f9a12e" podNamespace="kube-system" podName="metrics-server-67c658944b-6t6cb"
I0327 14:19:49.390209    2321 topology_manager.go:215] "Topology Admit Handler" podUID="cce2b8c7-8d6c-4de5-8363-9be3c75c4856" podNamespace="kube-system" podName="coredns-6799fbcd5-mc67c"
I0327 14:19:49.390869    2321 topology_manager.go:215] "Topology Admit Handler" podUID="78ee5377-1a05-4777-b618-8bac10b0e6ac" podNamespace="kube-system" podName="local-path-provisioner-6c86858495-znp6t"
I0327 14:19:49.402504    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0327 14:19:49.497875    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp-dir\" (UniqueName: \"kubernetes.io/empty-dir/8522bcad-f41d-4fe3-a8c8-2d7b94f9a12e-tmp-dir\") pod \"metrics-server-67c658944b-6t6cb\" (UID: \"8522bcad-f41d-4fe3-a8c8-2d7b94f9a12e\") " pod="kube-system/metrics-server-67c658944b-6t6cb"
I0327 14:19:49.498011    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-kshw9\" (UniqueName: \"kubernetes.io/projected/8522bcad-f41d-4fe3-a8c8-2d7b94f9a12e-kube-api-access-kshw9\") pod \"metrics-server-67c658944b-6t6cb\" (UID: \"8522bcad-f41d-4fe3-a8c8-2d7b94f9a12e\") " pod="kube-system/metrics-server-67c658944b-6t6cb"
I0327 14:19:49.498093    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"custom-config-volume\" (UniqueName: \"kubernetes.io/configmap/cce2b8c7-8d6c-4de5-8363-9be3c75c4856-custom-config-volume\") pod \"coredns-6799fbcd5-mc67c\" (UID: \"cce2b8c7-8d6c-4de5-8363-9be3c75c4856\") " pod="kube-system/coredns-6799fbcd5-mc67c"
I0327 14:19:49.498181    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/78ee5377-1a05-4777-b618-8bac10b0e6ac-config-volume\") pod \"local-path-provisioner-6c86858495-znp6t\" (UID: \"78ee5377-1a05-4777-b618-8bac10b0e6ac\") " pod="kube-system/local-path-provisioner-6c86858495-znp6t"
I0327 14:19:49.498791    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/cce2b8c7-8d6c-4de5-8363-9be3c75c4856-config-volume\") pod \"coredns-6799fbcd5-mc67c\" (UID: \"cce2b8c7-8d6c-4de5-8363-9be3c75c4856\") " pod="kube-system/coredns-6799fbcd5-mc67c"
I0327 14:19:49.498886    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-gn4cm\" (UniqueName: \"kubernetes.io/projected/78ee5377-1a05-4777-b618-8bac10b0e6ac-kube-api-access-gn4cm\") pod \"local-path-provisioner-6c86858495-znp6t\" (UID: \"78ee5377-1a05-4777-b618-8bac10b0e6ac\") " pod="kube-system/local-path-provisioner-6c86858495-znp6t"
I0327 14:19:49.498959    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-z675j\" (UniqueName: \"kubernetes.io/projected/cce2b8c7-8d6c-4de5-8363-9be3c75c4856-kube-api-access-z675j\") pod \"coredns-6799fbcd5-mc67c\" (UID: \"cce2b8c7-8d6c-4de5-8363-9be3c75c4856\") " pod="kube-system/coredns-6799fbcd5-mc67c"
I0327 14:19:49.573956    2321 kube.go:482] Creating the node lease for IPv4. This is the n.Spec.PodCIDRs: [10.42.0.0/24]
time="2024-03-27T14:19:49Z" level=info msg="Wrote flannel subnet file to /run/flannel/subnet.env"
time="2024-03-27T14:19:49Z" level=info msg="Running flannel backend."
I0327 14:19:49.648127    2321 vxlan_network.go:65] watching for new subnet leases
I0327 14:19:49.648770    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0327 14:19:49.653323    2321 iptables.go:290] generated 3 rules
I0327 14:19:49.673811    2321 iptables.go:290] generated 7 rules
I0327 14:19:49.679956    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="382.087071ms"
I0327 14:19:49.681345    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="408.782596ms"
W0327 14:19:49.697087    2321 watcher.go:93] Error while processing event ("/sys/fs/cgroup/devices/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podcce2b8c7_8d6c_4de5_8363_9be3c75c4856.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/devices/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podcce2b8c7_8d6c_4de5_8363_9be3c75c4856.slice: no such file or directory
I0327 14:19:49.944101    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="560.804334ms"
I0327 14:19:49.944568    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="192.856s"
I0327 14:19:50.013580    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="238.169s"
I0327 14:19:50.021637    2321 iptables.go:283] bootstrap done
I0327 14:19:50.092926    2321 iptables.go:421] Some iptables rules are missing; deleting and recreating rules
I0327 14:19:50.114193    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="434.069303ms"
I0327 14:19:50.114511    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="177.897s"
I0327 14:19:50.138568    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="457.059543ms"
I0327 14:19:50.139749    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="196.598s"
I0327 14:19:50.141119    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="114.891s"
I0327 14:19:50.196304    2321 iptables.go:283] bootstrap done
I0327 14:19:50.209235    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="157.246s"
I0327 14:19:50.239001    2321 iptables.go:283] bootstrap done
I0327 14:19:50.690198    2321 kuberuntime_manager.go:1528] "Updating runtime config through cri with podcidr" CIDR="10.42.0.0/24"
I0327 14:19:50.692400    2321 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.42.0.0/24"
E0327 14:20:11.969581    2321 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = failed to pull and unpack image \"docker.io/rancher/local-path-provisioner:v0.0.26\": failed to copy: httpReadSeeker: failed open: failed to do request: Get \"https://production.cloudflare.docker.com/registry-v2/docker/registry/v2/blobs/sha256/c9/c926b61bad3b94ae7351bafd0c184c159ebf0643b085f7ef1d47ecdc7316833c/data?verify=1711552203-GvvzQtBgVf8GspAJTd3%2BxstgV5U%3D\": net/http: TLS handshake timeout" image="rancher/local-path-provisioner:v0.0.26"
E0327 14:20:11.969714    2321 kuberuntime_image.go:53] "Failed to pull image" err="failed to pull and unpack image \"docker.io/rancher/local-path-provisioner:v0.0.26\": failed to copy: httpReadSeeker: failed open: failed to do request: Get \"https://production.cloudflare.docker.com/registry-v2/docker/registry/v2/blobs/sha256/c9/c926b61bad3b94ae7351bafd0c184c159ebf0643b085f7ef1d47ecdc7316833c/data?verify=1711552203-GvvzQtBgVf8GspAJTd3%2BxstgV5U%3D\": net/http: TLS handshake timeout" image="rancher/local-path-provisioner:v0.0.26"
E0327 14:20:11.971470    2321 kuberuntime_manager.go:1261] container &Container{Name:local-path-provisioner,Image:rancher/local-path-provisioner:v0.0.26,Command:[local-path-provisioner start --config /etc/config/config.json],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:POD_NAMESPACE,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:config-volume,ReadOnly:false,MountPath:/etc/config/,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-gn4cm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod local-path-provisioner-6c86858495-znp6t_kube-system(78ee5377-1a05-4777-b618-8bac10b0e6ac): ErrImagePull: failed to pull and unpack image "docker.io/rancher/local-path-provisioner:v0.0.26": failed to copy: httpReadSeeker: failed open: failed to do request: Get "https://production.cloudflare.docker.com/registry-v2/docker/registry/v2/blobs/sha256/c9/c926b61bad3b94ae7351bafd0c184c159ebf0643b085f7ef1d47ecdc7316833c/data?verify=1711552203-GvvzQtBgVf8GspAJTd3%2BxstgV5U%3D": net/http: TLS handshake timeout
E0327 14:20:11.971633    2321 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"local-path-provisioner\" with ErrImagePull: \"failed to pull and unpack image \\\"docker.io/rancher/local-path-provisioner:v0.0.26\\\": failed to copy: httpReadSeeker: failed open: failed to do request: Get \\\"https://production.cloudflare.docker.com/registry-v2/docker/registry/v2/blobs/sha256/c9/c926b61bad3b94ae7351bafd0c184c159ebf0643b085f7ef1d47ecdc7316833c/data?verify=1711552203-GvvzQtBgVf8GspAJTd3%2BxstgV5U%3D\\\": net/http: TLS handshake timeout\"" pod="kube-system/local-path-provisioner-6c86858495-znp6t" podUID="78ee5377-1a05-4777-b618-8bac10b0e6ac"
E0327 14:20:12.281340    2321 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"local-path-provisioner\" with ImagePullBackOff: \"Back-off pulling image \\\"rancher/local-path-provisioner:v0.0.26\\\"\"" pod="kube-system/local-path-provisioner-6c86858495-znp6t" podUID="78ee5377-1a05-4777-b618-8bac10b0e6ac"
I0327 14:20:12.321614    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="231.324s"
E0327 14:20:17.843813    2321 resource_quota_controller.go:440] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I0327 14:20:18.404823    2321 garbagecollector.go:816] "failed to discover some groups" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"
I0327 14:20:23.321153    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="210.359s"
E0327 14:20:26.623929    2321 handler_proxy.go:137] error resolving kube-system/metrics-server: no endpoints available for service "metrics-server"
I0327 14:20:31.533946    2321 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/local-path-provisioner-6c86858495-znp6t" containerName="local-path-provisioner"
I0327 14:20:32.014739    2321 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/metrics-server-67c658944b-6t6cb" containerName="metrics-server"
I0327 14:20:33.898238    2321 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/metrics-server-67c658944b-6t6cb" podStartSLOduration=9.099955767 podCreationTimestamp="2024-03-27 14:19:48 +0000 UTC" firstStartedPulling="2024-03-27 14:19:55.218696476 +0000 UTC m=+34.401913556" lastFinishedPulling="2024-03-27 14:20:32.006333593 +0000 UTC m=+71.189550683" observedRunningTime="2024-03-27 14:20:33.870354454 +0000 UTC m=+73.053571576" watchObservedRunningTime="2024-03-27 14:20:33.887592894 +0000 UTC m=+73.070809991"
I0327 14:20:33.928103    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="2.642233ms"
I0327 14:20:33.965136    2321 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/local-path-provisioner-6c86858495-znp6t" podStartSLOduration=9.855064382 podCreationTimestamp="2024-03-27 14:19:48 +0000 UTC" firstStartedPulling="2024-03-27 14:19:55.413093486 +0000 UTC m=+34.596310540" lastFinishedPulling="2024-03-27 14:20:31.523060071 +0000 UTC m=+70.706277143" observedRunningTime="2024-03-27 14:20:33.964405765 +0000 UTC m=+73.147622882" watchObservedRunningTime="2024-03-27 14:20:33.965030985 +0000 UTC m=+73.148248074"
I0327 14:20:34.036322    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="63.03425ms"
I0327 14:20:34.047403    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/local-path-provisioner-6c86858495" duration="172.365s"
E0327 14:20:47.863780    2321 resource_quota_controller.go:440] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I0327 14:20:48.434492    2321 garbagecollector.go:816] "failed to discover some groups" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"
W0327 14:20:49.314580    2321 handler_proxy.go:93] no RequestInfo found in the context
W0327 14:20:49.315308    2321 handler_proxy.go:93] no RequestInfo found in the context
E0327 14:20:49.315748    2321 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I0327 14:20:49.315832    2321 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E0327 14:20:49.316447    2321 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0327 14:20:49.359435    2321 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0327 14:20:54.089501    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="81.999993ms"
I0327 14:20:54.093459    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-67c658944b" duration="125.839s"
I0327 14:20:54.435401    2321 handler.go:275] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
time="2024-03-27T14:24:21Z" level=info msg="COMPACT revision 0 has already been compacted"
E0327 14:25:05.093968    2321 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Canceled desc = failed to pull and unpack image \"docker.io/rancher/klipper-helm:v0.8.2-build20230815\": context canceled" image="rancher/klipper-helm:v0.8.2-build20230815"
E0327 14:25:05.094362    2321 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Canceled desc = failed to pull and unpack image \"docker.io/rancher/klipper-helm:v0.8.2-build20230815\": context canceled" image="rancher/klipper-helm:v0.8.2-build20230815"
E0327 14:25:05.110133    2321 kuberuntime_manager.go:1261] container &Container{Name:helm,Image:rancher/klipper-helm:v0.8.2-build20230815,Command:[],Args:[install --set-string global.systemDefaultRegistry=],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:NAME,Value:traefik,ValueFrom:nil,},EnvVar{Name:VERSION,Value:,ValueFrom:nil,},EnvVar{Name:REPO,Value:,ValueFrom:nil,},EnvVar{Name:HELM_DRIVER,Value:secret,ValueFrom:nil,},EnvVar{Name:CHART_NAMESPACE,Value:kube-system,ValueFrom:nil,},EnvVar{Name:CHART,Value:https://%{KUBERNETES_API}%/static/charts/traefik-25.0.2+up25.0.0.tgz,ValueFrom:nil,},EnvVar{Name:HELM_VERSION,Value:,ValueFrom:nil,},EnvVar{Name:TARGET_NAMESPACE,Value:kube-system,ValueFrom:nil,},EnvVar{Name:AUTH_PASS_CREDENTIALS,Value:false,ValueFrom:nil,},EnvVar{Name:NO_PROXY,Value:.svc,.cluster.local,10.42.0.0/16,10.43.0.0/16,ValueFrom:nil,},EnvVar{Name:FAILURE_POLICY,Value:reinstall,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:klipper-helm,ReadOnly:false,MountPath:/home/klipper-helm/.helm,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:klipper-cache,ReadOnly:false,MountPath:/home/klipper-helm/.cache,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:klipper-config,ReadOnly:false,MountPath:/home/klipper-helm/.config,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:tmp,ReadOnly:false,MountPath:/tmp,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:values,ReadOnly:false,MountPath:/config,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:content,ReadOnly:false,MountPath:/chart,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-w2kxz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[ALL],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:*true,AllowPrivilegeEscalation:*false,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod helm-install-traefik-rqbh9_kube-system(87543c2b-a2be-4644-8b8b-960585860c4d): ErrImagePull: rpc error: code = Canceled desc = failed to pull and unpack image "docker.io/rancher/klipper-helm:v0.8.2-build20230815": context canceled
E0327 14:25:05.111154    2321 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"helm\" with ErrImagePull: \"rpc error: code = Canceled desc = failed to pull and unpack image \\\"docker.io/rancher/klipper-helm:v0.8.2-build20230815\\\": context canceled\"" pod="kube-system/helm-install-traefik-rqbh9" podUID="87543c2b-a2be-4644-8b8b-960585860c4d"
E0327 14:25:05.134756    2321 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"helm\" with ImagePullBackOff: \"Back-off pulling image \\\"rancher/klipper-helm:v0.8.2-build20230815\\\"\"" pod="kube-system/helm-install-traefik-rqbh9" podUID="87543c2b-a2be-4644-8b8b-960585860c4d"
I0327 14:25:05.196664    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0327 14:25:16.361661    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0327 14:25:16.811377    2321 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/helm-install-traefik-crd-dtrfg" containerName="helm"
I0327 14:25:17.309857    2321 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/helm-install-traefik-rqbh9" containerName="helm"
I0327 14:25:18.357079    2321 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/helm-install-traefik-rqbh9" podStartSLOduration=8.094277461 podCreationTimestamp="2024-03-27 14:19:48 +0000 UTC" firstStartedPulling="2024-03-27 14:19:55.055318909 +0000 UTC m=+34.238535989" lastFinishedPulling="2024-03-27 14:25:17.302454302 +0000 UTC m=+356.485671359" observedRunningTime="2024-03-27 14:25:18.338867093 +0000 UTC m=+357.522084193" watchObservedRunningTime="2024-03-27 14:25:18.341412831 +0000 UTC m=+357.524629925"
I0327 14:25:18.368359    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0327 14:25:18.393209    2321 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/helm-install-traefik-crd-dtrfg" podStartSLOduration=8.932549007 podCreationTimestamp="2024-03-27 14:19:48 +0000 UTC" firstStartedPulling="2024-03-27 14:19:55.34115879 +0000 UTC m=+34.524375858" lastFinishedPulling="2024-03-27 14:25:16.801723304 +0000 UTC m=+355.984940367" observedRunningTime="2024-03-27 14:25:18.381504983 +0000 UTC m=+357.564722088" watchObservedRunningTime="2024-03-27 14:25:18.393113516 +0000 UTC m=+357.576330622"
I0327 14:25:18.396040    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0327 14:25:19.464341    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0327 14:25:19.477775    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0327 14:25:21.406082    2321 scope.go:117] "RemoveContainer" containerID="5dc5098959ca9418a36ecfdc1deccb57becdbd4eff21a520e7872e0466c14878"
I0327 14:25:21.482878    2321 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/helm-install-traefik-rqbh9" containerName="helm"
I0327 14:25:21.618086    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0327 14:25:22.557970    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0327 14:25:23.773462    2321 trace.go:236] Trace[1294050675]: "Create" accept:application/json,audit-id:216981d6-1eb6-4979-9517-d857a2ff57be,client:10.42.0.5,protocol:HTTP/2.0,resource:customresourcedefinitions,scope:resource,url:/apis/apiextensions.k8s.io/v1/customresourcedefinitions,user-agent:Helm/3.12.3,verb:POST (27-Mar-2024 14:25:23.213) (total time: 555ms):
Trace[1294050675]: ---"limitedReadBody succeeded" len:3200 170ms (14:25:23.383)
Trace[1294050675]: [555.482667ms] [555.482667ms] END
I0327 14:25:23.809736    2321 trace.go:236] Trace[1358460433]: "Create" accept:application/json,audit-id:439fe358-e602-47b7-8507-a22fbad15ef8,client:10.42.0.5,protocol:HTTP/2.0,resource:customresourcedefinitions,scope:resource,url:/apis/apiextensions.k8s.io/v1/customresourcedefinitions,user-agent:Helm/3.12.3,verb:POST (27-Mar-2024 14:25:23.196) (total time: 613ms):
Trace[1358460433]: ---"limitedReadBody succeeded" len:3938 109ms (14:25:23.305)
Trace[1358460433]: ---"Writing http response done" 54ms (14:25:23.809)
Trace[1358460433]: [613.514256ms] [613.514256ms] END
I0327 14:25:23.815172    2321 trace.go:236] Trace[1768776491]: "Create" accept:application/json,audit-id:74b32216-9c72-430b-a15f-2c38cfbc2979,client:10.42.0.5,protocol:HTTP/2.0,resource:customresourcedefinitions,scope:resource,url:/apis/apiextensions.k8s.io/v1/customresourcedefinitions,user-agent:Helm/3.12.3,verb:POST (27-Mar-2024 14:25:23.214) (total time: 600ms):
Trace[1768776491]: ---"limitedReadBody succeeded" len:13949 142ms (14:25:23.357)
Trace[1768776491]: ---"Writing http response done" 36ms (14:25:23.814)
Trace[1768776491]: [600.55958ms] [600.55958ms] END
I0327 14:25:23.817462    2321 trace.go:236] Trace[1777233811]: "Create" accept:application/json,audit-id:a2ae0334-6734-4335-a8da-c32cda63e6e4,client:10.42.0.5,protocol:HTTP/2.0,resource:customresourcedefinitions,scope:resource,url:/apis/apiextensions.k8s.io/v1/customresourcedefinitions,user-agent:Helm/3.12.3,verb:POST (27-Mar-2024 14:25:23.170) (total time: 646ms):
Trace[1777233811]: ---"limitedReadBody succeeded" len:13967 43ms (14:25:23.214)
Trace[1777233811]: ---"Conversion done" 104ms (14:25:23.318)
Trace[1777233811]: ---"Write to database call succeeded" len:13967 24ms (14:25:23.790)
Trace[1777233811]: ---"Writing http response done" 25ms (14:25:23.816)
Trace[1777233811]: [646.235231ms] [646.235231ms] END
I0327 14:25:23.842756    2321 trace.go:236] Trace[779747107]: "Create" accept:application/json,audit-id:f359fb59-7975-4ce9-b559-dce7de8c5cfa,client:10.42.0.5,protocol:HTTP/2.0,resource:customresourcedefinitions,scope:resource,url:/apis/apiextensions.k8s.io/v1/customresourcedefinitions,user-agent:Helm/3.12.3,verb:POST (27-Mar-2024 14:25:23.196) (total time: 645ms):
Trace[779747107]: ---"limitedReadBody succeeded" len:3218 291ms (14:25:23.488)
Trace[779747107]: ---"Write to database call succeeded" len:3218 20ms (14:25:23.841)
Trace[779747107]: [645.719276ms] [645.719276ms] END
I0327 14:25:23.861273    2321 trace.go:236] Trace[802863011]: "Create" accept:application/json,audit-id:d4643f72-ecaf-4a5d-bc4b-3385b3e99ebe,client:10.42.0.5,protocol:HTTP/2.0,resource:customresourcedefinitions,scope:resource,url:/apis/apiextensions.k8s.io/v1/customresourcedefinitions,user-agent:Helm/3.12.3,verb:POST (27-Mar-2024 14:25:23.198) (total time: 662ms):
Trace[802863011]: ---"limitedReadBody succeeded" len:9166 328ms (14:25:23.526)
Trace[802863011]: ---"Writing http response done" 14ms (14:25:23.861)
Trace[802863011]: [662.741484ms] [662.741484ms] END
I0327 14:25:23.913867    2321 trace.go:236] Trace[1675966055]: "Create" accept:application/json,audit-id:101aa093-88bd-4650-94a3-9d0352f5d6ae,client:10.42.0.5,protocol:HTTP/2.0,resource:customresourcedefinitions,scope:resource,url:/apis/apiextensions.k8s.io/v1/customresourcedefinitions,user-agent:Helm/3.12.3,verb:POST (27-Mar-2024 14:25:23.197) (total time: 716ms):
Trace[1675966055]: ---"limitedReadBody succeeded" len:2364 249ms (14:25:23.447)
Trace[1675966055]: [716.072688ms] [716.072688ms] END
I0327 14:25:23.920039    2321 trace.go:236] Trace[1243453734]: "Create" accept:application/json,audit-id:e5382d1f-1fcc-4f6f-8917-895a2f460bf5,client:10.42.0.5,protocol:HTTP/2.0,resource:customresourcedefinitions,scope:resource,url:/apis/apiextensions.k8s.io/v1/customresourcedefinitions,user-agent:Helm/3.12.3,verb:POST (27-Mar-2024 14:25:23.199) (total time: 720ms):
Trace[1243453734]: ---"limitedReadBody succeeded" len:3238 290ms (14:25:23.490)
Trace[1243453734]: ---"Write to database call succeeded" len:3238 44ms (14:25:23.918)
Trace[1243453734]: [720.346169ms] [720.346169ms] END
I0327 14:25:23.920518    2321 trace.go:236] Trace[1999448517]: "Create" accept:application/json,audit-id:0772635d-d90c-4cb2-b238-5661a295fae3,client:10.42.0.5,protocol:HTTP/2.0,resource:customresourcedefinitions,scope:resource,url:/apis/apiextensions.k8s.io/v1/customresourcedefinitions,user-agent:Helm/3.12.3,verb:POST (27-Mar-2024 14:25:23.194) (total time: 725ms):
Trace[1999448517]: ---"limitedReadBody succeeded" len:2382 65ms (14:25:23.260)
Trace[1999448517]: ["Create etcd3" audit-id:0772635d-d90c-4cb2-b238-5661a295fae3,key:/apiextensions.k8s.io/customresourcedefinitions/middlewaretcps.traefik.containo.us,type:*apiextensions.CustomResourceDefinition,resource:customresourcedefinitions.apiextensions.k8s.io 573ms (14:25:23.346)
Trace[1999448517]:  ---"Txn call succeeded" 458ms (14:25:23.874)]
Trace[1999448517]: ---"Write to database call succeeded" len:2382 45ms (14:25:23.920)
Trace[1999448517]: [725.53771ms] [725.53771ms] END
I0327 14:25:23.953208    2321 trace.go:236] Trace[164711829]: "Create" accept:application/json,audit-id:1ddecb32-095d-4be8-aa8a-fe7225f6b729,client:10.42.0.5,protocol:HTTP/2.0,resource:customresourcedefinitions,scope:resource,url:/apis/apiextensions.k8s.io/v1/customresourcedefinitions,user-agent:Helm/3.12.3,verb:POST (27-Mar-2024 14:25:23.198) (total time: 754ms):
Trace[164711829]: ---"limitedReadBody succeeded" len:6911 297ms (14:25:23.496)
Trace[164711829]: ---"Writing http response done" 44ms (14:25:23.953)
Trace[164711829]: [754.082142ms] [754.082142ms] END
I0327 14:25:23.954330    2321 trace.go:236] Trace[1758235709]: "Create" accept:application/json,audit-id:37ba2311-3656-45b0-b554-4526491b6548,client:10.42.0.5,protocol:HTTP/2.0,resource:customresourcedefinitions,scope:resource,url:/apis/apiextensions.k8s.io/v1/customresourcedefinitions,user-agent:Helm/3.12.3,verb:POST (27-Mar-2024 14:25:23.212) (total time: 741ms):
Trace[1758235709]: ---"limitedReadBody succeeded" len:3920 184ms (14:25:23.397)
Trace[1758235709]: ["Create etcd3" audit-id:37ba2311-3656-45b0-b554-4526491b6548,key:/apiextensions.k8s.io/customresourcedefinitions/tlsoptions.traefik.io,type:*apiextensions.CustomResourceDefinition,resource:customresourcedefinitions.apiextensions.k8s.io 553ms (14:25:23.400)
Trace[1758235709]:  ---"Txn call succeeded" 529ms (14:25:23.930)]
Trace[1758235709]: ---"Writing http response done" 22ms (14:25:23.954)
Trace[1758235709]: [741.067397ms] [741.067397ms] END
I0327 14:25:23.955420    2321 trace.go:236] Trace[2035971992]: "Create" accept:application/json,audit-id:b8d5bc42-f7e1-4f43-8d2f-34f4a5798358,client:10.42.0.5,protocol:HTTP/2.0,resource:customresourcedefinitions,scope:resource,url:/apis/apiextensions.k8s.io/v1/customresourcedefinitions,user-agent:Helm/3.12.3,verb:POST (27-Mar-2024 14:25:23.212) (total time: 743ms):
Trace[2035971992]: ---"limitedReadBody succeeded" len:4247 240ms (14:25:23.452)
Trace[2035971992]: ---"Writing http response done" 38ms (14:25:23.955)
Trace[2035971992]: [743.01194ms] [743.01194ms] END
I0327 14:25:23.956389    2321 trace.go:236] Trace[471155640]: "Create" accept:application/json,audit-id:e3e90bf6-6209-496d-a2f1-1d2b17ee9848,client:10.42.0.5,protocol:HTTP/2.0,resource:customresourcedefinitions,scope:resource,url:/apis/apiextensions.k8s.io/v1/customresourcedefinitions,user-agent:Helm/3.12.3,verb:POST (27-Mar-2024 14:25:23.317) (total time: 638ms):
Trace[471155640]: ---"limitedReadBody succeeded" len:33989 87ms (14:25:23.405)
Trace[471155640]: ---"Write to database call succeeded" len:33989 39ms (14:25:23.912)
Trace[471155640]: ---"Writing http response done" 44ms (14:25:23.956)
Trace[471155640]: [638.391178ms] [638.391178ms] END
I0327 14:25:23.976383    2321 trace.go:236] Trace[978207641]: "Create" accept:application/json,audit-id:06c0bb22-d049-46fd-ac6c-7b0f9e65984f,client:10.42.0.5,protocol:HTTP/2.0,resource:customresourcedefinitions,scope:resource,url:/apis/apiextensions.k8s.io/v1/customresourcedefinitions,user-agent:Helm/3.12.3,verb:POST (27-Mar-2024 14:25:23.317) (total time: 659ms):
Trace[978207641]: ---"limitedReadBody succeeded" len:6929 127ms (14:25:23.444)
Trace[978207641]: ---"Write to database call succeeded" len:6929 20ms (14:25:23.932)
Trace[978207641]: ---"Writing http response done" 43ms (14:25:23.976)
Trace[978207641]: [659.126007ms] [659.126007ms] END
I0327 14:25:23.977009    2321 trace.go:236] Trace[632466059]: "Create" accept:application/json,audit-id:318f2d77-6518-430b-8d93-abd8dd9ab616,client:10.42.0.5,protocol:HTTP/2.0,resource:customresourcedefinitions,scope:resource,url:/apis/apiextensions.k8s.io/v1/customresourcedefinitions,user-agent:Helm/3.12.3,verb:POST (27-Mar-2024 14:25:23.176) (total time: 800ms):
Trace[632466059]: ---"limitedReadBody succeeded" len:9184 362ms (14:25:23.539)
Trace[632466059]: ---"Write to database call succeeded" len:9184 16ms (14:25:23.946)
Trace[632466059]: ---"Writing http response done" 30ms (14:25:23.976)
Trace[632466059]: [800.132106ms] [800.132106ms] END
I0327 14:25:24.006383    2321 trace.go:236] Trace[2063378998]: "Create" accept:application/json,audit-id:88f5f745-eb00-4986-a0da-cc028c48dfb5,client:10.42.0.5,protocol:HTTP/2.0,resource:customresourcedefinitions,scope:resource,url:/apis/apiextensions.k8s.io/v1/customresourcedefinitions,user-agent:Helm/3.12.3,verb:POST (27-Mar-2024 14:25:23.318) (total time: 687ms):
Trace[2063378998]: ---"limitedReadBody succeeded" len:4142 216ms (14:25:23.534)
Trace[2063378998]: ---"Writing http response done" 42ms (14:25:24.006)
Trace[2063378998]: [687.754851ms] [687.754851ms] END
I0327 14:25:24.020108    2321 trace.go:236] Trace[181357111]: "Create" accept:application/json,audit-id:17f1cb2d-0796-45bc-a6ed-ac31c34c041a,client:10.42.0.5,protocol:HTTP/2.0,resource:customresourcedefinitions,scope:resource,url:/apis/apiextensions.k8s.io/v1/customresourcedefinitions,user-agent:Helm/3.12.3,verb:POST (27-Mar-2024 14:25:23.193) (total time: 826ms):
Trace[181357111]: ---"limitedReadBody succeeded" len:34007 370ms (14:25:23.564)
Trace[181357111]: [826.002274ms] [826.002274ms] END
I0327 14:25:24.197140    2321 trace.go:236] Trace[1506633099]: "Create" accept:application/json,audit-id:c1c225f2-7ddd-44da-a552-ab62da9eae47,client:10.42.0.5,protocol:HTTP/2.0,resource:customresourcedefinitions,scope:resource,url:/apis/apiextensions.k8s.io/v1/customresourcedefinitions,user-agent:Helm/3.12.3,verb:POST (27-Mar-2024 14:25:23.188) (total time: 1008ms):
Trace[1506633099]: ---"limitedReadBody succeeded" len:3256 290ms (14:25:23.479)
Trace[1506633099]: ["Create etcd3" audit-id:c1c225f2-7ddd-44da-a552-ab62da9eae47,key:/apiextensions.k8s.io/customresourcedefinitions/ingressrouteudps.traefik.containo.us,type:*apiextensions.CustomResourceDefinition,resource:customresourcedefinitions.apiextensions.k8s.io 709ms (14:25:23.487)
Trace[1506633099]:  ---"Txn call succeeded" 699ms (14:25:24.188)]
Trace[1506633099]: [1.008349691s] [1.008349691s] END
I0327 14:25:24.506248    2321 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0327 14:25:24.679291    2321 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0327 14:25:24.849702    2321 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0327 14:25:24.958967    2321 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0327 14:25:25.062279    2321 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0327 14:25:25.187456    2321 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0327 14:25:25.264907    2321 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
E0327 14:25:25.377916    2321 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Canceled desc = failed to pull and unpack image \"docker.io/rancher/mirrored-coredns-coredns:1.10.1\": context canceled" image="rancher/mirrored-coredns-coredns:1.10.1"
E0327 14:25:25.379092    2321 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Canceled desc = failed to pull and unpack image \"docker.io/rancher/mirrored-coredns-coredns:1.10.1\": context canceled" image="rancher/mirrored-coredns-coredns:1.10.1"
E0327 14:25:25.384036    2321 kuberuntime_manager.go:1261] container &Container{Name:coredns,Image:rancher/mirrored-coredns-coredns:1.10.1,Command:[],Args:[-conf /etc/coredns/Corefile],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:dns,HostPort:0,ContainerPort:53,Protocol:UDP,HostIP:,},ContainerPort{Name:dns-tcp,HostPort:0,ContainerPort:53,Protocol:TCP,HostIP:,},ContainerPort{Name:metrics,HostPort:0,ContainerPort:9153,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{memory: {{178257920 0} {<nil>} 170Mi BinarySI},},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{73400320 0} {<nil>} 70Mi BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:config-volume,ReadOnly:true,MountPath:/etc/coredns,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:custom-config-volume,ReadOnly:true,MountPath:/etc/coredns/custom,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-z675j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/health,Port:{0 8080 },Host:,Scheme:HTTP,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:60,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,TerminationGracePeriodSeconds:nil,},ReadinessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/ready,Port:{0 8181 },Host:,Scheme:HTTP,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:2,SuccessThreshold:1,FailureThreshold:3,TerminationGracePeriodSeconds:nil,},Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[NET_BIND_SERVICE],Drop:[all],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:*true,AllowPrivilegeEscalation:*false,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod coredns-6799fbcd5-mc67c_kube-system(cce2b8c7-8d6c-4de5-8363-9be3c75c4856): ErrImagePull: rpc error: code = Canceled desc = failed to pull and unpack image "docker.io/rancher/mirrored-coredns-coredns:1.10.1": context canceled
E0327 14:25:25.395054    2321 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"coredns\" with ErrImagePull: \"rpc error: code = Canceled desc = failed to pull and unpack image \\\"docker.io/rancher/mirrored-coredns-coredns:1.10.1\\\": context canceled\"" pod="kube-system/coredns-6799fbcd5-mc67c" podUID="cce2b8c7-8d6c-4de5-8363-9be3c75c4856"
I0327 14:25:25.417817    2321 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0327 14:25:25.460204    2321 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0327 14:25:25.483902    2321 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0327 14:25:25.519315    2321 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0327 14:25:25.544718    2321 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0327 14:25:25.571278    2321 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0327 14:25:25.653448    2321 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0327 14:25:25.717328    2321 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0327 14:25:25.746285    2321 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0327 14:25:25.755936    2321 handler.go:275] Adding GroupVersion traefik.io v1alpha1 to ResourceManager
I0327 14:25:25.835121    2321 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0327 14:25:25.939492    2321 handler.go:275] Adding GroupVersion traefik.containo.us v1alpha1 to ResourceManager
I0327 14:25:26.046102    2321 scope.go:117] "RemoveContainer" containerID="5dc5098959ca9418a36ecfdc1deccb57becdbd4eff21a520e7872e0466c14878"
I0327 14:25:26.052304    2321 scope.go:117] "RemoveContainer" containerID="39844b69e05342553a8eee13b1667caba13cbfe9ee4b373d3f42e7b83611b90e"
E0327 14:25:26.055692    2321 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"helm\" with CrashLoopBackOff: \"back-off 10s restarting failed container=helm pod=helm-install-traefik-rqbh9_kube-system(87543c2b-a2be-4644-8b8b-960585860c4d)\"" pod="kube-system/helm-install-traefik-rqbh9" podUID="87543c2b-a2be-4644-8b8b-960585860c4d"
E0327 14:25:26.164828    2321 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"coredns\" with ImagePullBackOff: \"Back-off pulling image \\\"rancher/mirrored-coredns-coredns:1.10.1\\\"\"" pod="kube-system/coredns-6799fbcd5-mc67c" podUID="cce2b8c7-8d6c-4de5-8363-9be3c75c4856"
I0327 14:25:26.185825    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0327 14:25:26.338444    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="3.64591ms"
I0327 14:25:26.421258    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0327 14:25:27.235772    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0327 14:25:27.440289    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0327 14:25:27.653283    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0327 14:25:27.657460    2321 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"content\" (UniqueName: \"kubernetes.io/configmap/6b8c7f1e-049d-469a-a179-56363f323e70-content\") pod \"6b8c7f1e-049d-469a-a179-56363f323e70\" (UID: \"6b8c7f1e-049d-469a-a179-56363f323e70\") "
I0327 14:25:27.657582    2321 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-b7dbw\" (UniqueName: \"kubernetes.io/projected/6b8c7f1e-049d-469a-a179-56363f323e70-kube-api-access-b7dbw\") pod \"6b8c7f1e-049d-469a-a179-56363f323e70\" (UID: \"6b8c7f1e-049d-469a-a179-56363f323e70\") "
I0327 14:25:27.657638    2321 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"values\" (UniqueName: \"kubernetes.io/secret/6b8c7f1e-049d-469a-a179-56363f323e70-values\") pod \"6b8c7f1e-049d-469a-a179-56363f323e70\" (UID: \"6b8c7f1e-049d-469a-a179-56363f323e70\") "
I0327 14:25:27.657698    2321 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/6b8c7f1e-049d-469a-a179-56363f323e70-tmp\") pod \"6b8c7f1e-049d-469a-a179-56363f323e70\" (UID: \"6b8c7f1e-049d-469a-a179-56363f323e70\") "
I0327 14:25:27.657755    2321 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/6b8c7f1e-049d-469a-a179-56363f323e70-klipper-config\") pod \"6b8c7f1e-049d-469a-a179-56363f323e70\" (UID: \"6b8c7f1e-049d-469a-a179-56363f323e70\") "
I0327 14:25:27.657807    2321 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/6b8c7f1e-049d-469a-a179-56363f323e70-klipper-helm\") pod \"6b8c7f1e-049d-469a-a179-56363f323e70\" (UID: \"6b8c7f1e-049d-469a-a179-56363f323e70\") "
I0327 14:25:27.657858    2321 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/6b8c7f1e-049d-469a-a179-56363f323e70-klipper-cache\") pod \"6b8c7f1e-049d-469a-a179-56363f323e70\" (UID: \"6b8c7f1e-049d-469a-a179-56363f323e70\") "
I0327 14:25:27.703425    2321 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/configmap/6b8c7f1e-049d-469a-a179-56363f323e70-content" (OuterVolumeSpecName: "content") pod "6b8c7f1e-049d-469a-a179-56363f323e70" (UID: "6b8c7f1e-049d-469a-a179-56363f323e70"). InnerVolumeSpecName "content". PluginName "kubernetes.io/configmap", VolumeGidValue ""
I0327 14:25:27.773638    2321 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/6b8c7f1e-049d-469a-a179-56363f323e70-tmp" (OuterVolumeSpecName: "tmp") pod "6b8c7f1e-049d-469a-a179-56363f323e70" (UID: "6b8c7f1e-049d-469a-a179-56363f323e70"). InnerVolumeSpecName "tmp". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0327 14:25:27.775248    2321 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/6b8c7f1e-049d-469a-a179-56363f323e70-values" (OuterVolumeSpecName: "values") pod "6b8c7f1e-049d-469a-a179-56363f323e70" (UID: "6b8c7f1e-049d-469a-a179-56363f323e70"). InnerVolumeSpecName "values". PluginName "kubernetes.io/secret", VolumeGidValue ""
I0327 14:25:27.776161    2321 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/6b8c7f1e-049d-469a-a179-56363f323e70-klipper-helm" (OuterVolumeSpecName: "klipper-helm") pod "6b8c7f1e-049d-469a-a179-56363f323e70" (UID: "6b8c7f1e-049d-469a-a179-56363f323e70"). InnerVolumeSpecName "klipper-helm". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0327 14:25:27.777718    2321 reconciler_common.go:300] "Volume detached for volume \"content\" (UniqueName: \"kubernetes.io/configmap/6b8c7f1e-049d-469a-a179-56363f323e70-content\") on node \"server\" DevicePath \"\""
I0327 14:25:27.777785    2321 reconciler_common.go:300] "Volume detached for volume \"values\" (UniqueName: \"kubernetes.io/secret/6b8c7f1e-049d-469a-a179-56363f323e70-values\") on node \"server\" DevicePath \"\""
I0327 14:25:27.777813    2321 reconciler_common.go:300] "Volume detached for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/6b8c7f1e-049d-469a-a179-56363f323e70-tmp\") on node \"server\" DevicePath \"\""
I0327 14:25:27.777876    2321 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/6b8c7f1e-049d-469a-a179-56363f323e70-kube-api-access-b7dbw" (OuterVolumeSpecName: "kube-api-access-b7dbw") pod "6b8c7f1e-049d-469a-a179-56363f323e70" (UID: "6b8c7f1e-049d-469a-a179-56363f323e70"). InnerVolumeSpecName "kube-api-access-b7dbw". PluginName "kubernetes.io/projected", VolumeGidValue ""
I0327 14:25:27.778329    2321 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/6b8c7f1e-049d-469a-a179-56363f323e70-klipper-cache" (OuterVolumeSpecName: "klipper-cache") pod "6b8c7f1e-049d-469a-a179-56363f323e70" (UID: "6b8c7f1e-049d-469a-a179-56363f323e70"). InnerVolumeSpecName "klipper-cache". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0327 14:25:27.794419    2321 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/6b8c7f1e-049d-469a-a179-56363f323e70-klipper-config" (OuterVolumeSpecName: "klipper-config") pod "6b8c7f1e-049d-469a-a179-56363f323e70" (UID: "6b8c7f1e-049d-469a-a179-56363f323e70"). InnerVolumeSpecName "klipper-config". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0327 14:25:27.878498    2321 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-b7dbw\" (UniqueName: \"kubernetes.io/projected/6b8c7f1e-049d-469a-a179-56363f323e70-kube-api-access-b7dbw\") on node \"server\" DevicePath \"\""
I0327 14:25:27.878562    2321 reconciler_common.go:300] "Volume detached for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/6b8c7f1e-049d-469a-a179-56363f323e70-klipper-helm\") on node \"server\" DevicePath \"\""
I0327 14:25:27.878589    2321 reconciler_common.go:300] "Volume detached for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/6b8c7f1e-049d-469a-a179-56363f323e70-klipper-cache\") on node \"server\" DevicePath \"\""
I0327 14:25:27.878614    2321 reconciler_common.go:300] "Volume detached for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/6b8c7f1e-049d-469a-a179-56363f323e70-klipper-config\") on node \"server\" DevicePath \"\""
I0327 14:25:28.161080    2321 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="28974efc7a8fc184924b8943c5b4ca3ee2400c67fb80af8d18e0817ca880b865"
I0327 14:25:28.198913    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0327 14:25:28.463039    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0327 14:25:28.495680    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0327 14:25:28.509822    2321 event.go:307] "Event occurred" object="kube-system/helm-install-traefik-crd" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="Completed" message="Job completed"
I0327 14:25:28.510441    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik-crd"
I0327 14:25:40.246353    2321 scope.go:117] "RemoveContainer" containerID="39844b69e05342553a8eee13b1667caba13cbfe9ee4b373d3f42e7b83611b90e"
I0327 14:25:40.284491    2321 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/helm-install-traefik-rqbh9" containerName="helm"
I0327 14:25:40.349462    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="222.298s"
I0327 14:25:40.387145    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0327 14:25:41.500139    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0327 14:25:42.568530    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0327 14:25:46.861480    2321 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/coredns-6799fbcd5-mc67c" containerName="coredns"
I0327 14:25:47.188523    2321 alloc.go:330] "allocated clusterIPs" service="kube-system/traefik" clusterIPs={"IPv4":"10.43.215.126"}
I0327 14:25:47.264056    2321 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Service" apiVersion="v1" type="Normal" reason="EnsuringLoadBalancer" message="Ensuring load balancer"
I0327 14:25:47.649159    2321 controller.go:624] quota admission added evaluator for: daemonsets.apps
I0327 14:25:47.745369    2321 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set traefik-f4564c4f4 to 1"
I0327 14:25:47.790235    2321 controller.go:624] quota admission added evaluator for: ingressroutes.traefik.io
I0327 14:25:47.826574    2321 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Service" apiVersion="v1" type="Normal" reason="AppliedDaemonSet" message="Applied LoadBalancer DaemonSet kube-system/svclb-traefik-54dd7185"
I0327 14:25:48.329935    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="tlsstores.traefik.io"
I0327 14:25:48.330926    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressroutes.traefik.containo.us"
I0327 14:25:48.335071    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressroutes.traefik.io"
I0327 14:25:48.336712    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="tlsoptions.traefik.io"
I0327 14:25:48.350909    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="traefikservices.traefik.io"
I0327 14:25:48.351157    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="middlewaretcps.traefik.io"
I0327 14:25:48.351252    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressroutetcps.traefik.containo.us"
I0327 14:25:48.351311    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="tlsstores.traefik.containo.us"
I0327 14:25:48.351510    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="tlsoptions.traefik.containo.us"
I0327 14:25:48.351561    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressrouteudps.traefik.io"
I0327 14:25:48.360202    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressroutetcps.traefik.io"
I0327 14:25:48.360331    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="serverstransports.traefik.io"
I0327 14:25:48.360394    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ingressrouteudps.traefik.containo.us"
I0327 14:25:48.360729    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="serverstransporttcps.traefik.io"
I0327 14:25:48.361298    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="middlewares.traefik.containo.us"
I0327 14:25:48.361375    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="middlewaretcps.traefik.containo.us"
I0327 14:25:48.361426    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="traefikservices.traefik.containo.us"
I0327 14:25:48.361639    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="serverstransports.traefik.containo.us"
I0327 14:25:48.362909    2321 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="middlewares.traefik.io"
I0327 14:25:48.368017    2321 shared_informer.go:311] Waiting for caches to sync for resource quota
I0327 14:25:48.585211    2321 controller.go:624] quota admission added evaluator for: controllerrevisions.apps
I0327 14:25:48.713026    2321 trace.go:236] Trace[1853608363]: "Create" accept:application/vnd.kubernetes.protobuf, */*,audit-id:4cdbd964-7e52-4f8c-be68-4767b3a6bc20,client:127.0.0.1,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/kube-system/pods,user-agent:k3s/v1.28.7+k3s1 (linux/amd64) kubernetes/051b14b/system:serviceaccount:kube-system:replicaset-controller,verb:POST (27-Mar-2024 14:25:47.969) (total time: 743ms):
Trace[1853608363]: ---"limitedReadBody succeeded" len:1682 20ms (14:25:47.990)
Trace[1853608363]: ---"Writing http response done" 35ms (14:25:48.712)
Trace[1853608363]: [743.639626ms] [743.639626ms] END
I0327 14:25:48.766205    2321 event.go:307] "Event occurred" object="kube-system/traefik-f4564c4f4" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: traefik-f4564c4f4-55xj2"
I0327 14:25:49.013029    2321 topology_manager.go:215] "Topology Admit Handler" podUID="3a47c73b-34df-408a-9b19-2d04d3548e3a" podNamespace="kube-system" podName="traefik-f4564c4f4-55xj2"
E0327 14:25:49.019637    2321 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="6b8c7f1e-049d-469a-a179-56363f323e70" containerName="helm"
I0327 14:25:49.022386    2321 memory_manager.go:346] "RemoveStaleState removing state" podUID="6b8c7f1e-049d-469a-a179-56363f323e70" containerName="helm"
I0327 14:25:49.131699    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="1.31228792s"
I0327 14:25:49.175812    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-sf56j\" (UniqueName: \"kubernetes.io/projected/3a47c73b-34df-408a-9b19-2d04d3548e3a-kube-api-access-sf56j\") pod \"traefik-f4564c4f4-55xj2\" (UID: \"3a47c73b-34df-408a-9b19-2d04d3548e3a\") " pod="kube-system/traefik-f4564c4f4-55xj2"
I0327 14:25:49.176384    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"data\" (UniqueName: \"kubernetes.io/empty-dir/3a47c73b-34df-408a-9b19-2d04d3548e3a-data\") pod \"traefik-f4564c4f4-55xj2\" (UID: \"3a47c73b-34df-408a-9b19-2d04d3548e3a\") " pod="kube-system/traefik-f4564c4f4-55xj2"
I0327 14:25:49.176786    2321 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/3a47c73b-34df-408a-9b19-2d04d3548e3a-tmp\") pod \"traefik-f4564c4f4-55xj2\" (UID: \"3a47c73b-34df-408a-9b19-2d04d3548e3a\") " pod="kube-system/traefik-f4564c4f4-55xj2"
I0327 14:25:49.256460    2321 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0327 14:25:49.405991    2321 event.go:307] "Event occurred" object="kube-system/svclb-traefik-54dd7185" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: svclb-traefik-54dd7185-mxvbc"
I0327 14:25:49.603130    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="471.285898ms"
I0327 14:25:49.604659    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="1.417453ms"
I0327 14:25:49.634990    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="1.302069ms"
I0327 14:25:49.712012    2321 topology_manager.go:215] "Topology Admit Handler" podUID="2521a10d-8727-4c40-9391-07a05ff06ab6" podNamespace="kube-system" podName="svclb-traefik-54dd7185-mxvbc"
I0327 14:25:49.895098    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="438.011s"
I0327 14:25:50.580972    2321 trace.go:236] Trace[173114074]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.56.110,type:*v1.Endpoints,resource:apiServerIPInfo (27-Mar-2024 14:25:49.966) (total time: 614ms):
Trace[173114074]: ---"initial value restored" 119ms (14:25:50.086)
Trace[173114074]: ---"Transaction prepared" 92ms (14:25:50.179)
Trace[173114074]: ---"Txn call completed" 401ms (14:25:50.580)
Trace[173114074]: [614.515459ms] [614.515459ms] END
I0327 14:25:51.157500    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="535.715238ms"
I0327 14:25:51.264138    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-6799fbcd5" duration="182.199s"
I0327 14:25:51.371289    2321 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/coredns-6799fbcd5-mc67c" podStartSLOduration=11.880006534 podCreationTimestamp="2024-03-27 14:19:48 +0000 UTC" firstStartedPulling="2024-03-27 14:19:55.330299112 +0000 UTC m=+34.513516186" lastFinishedPulling="2024-03-27 14:25:46.812775516 +0000 UTC m=+385.995992584" observedRunningTime="2024-03-27 14:25:50.604119612 +0000 UTC m=+389.787336735" watchObservedRunningTime="2024-03-27 14:25:51.362482932 +0000 UTC m=+390.545700008"
I0327 14:25:51.420171    2321 scope.go:117] "RemoveContainer" containerID="39844b69e05342553a8eee13b1667caba13cbfe9ee4b373d3f42e7b83611b90e"
I0327 14:25:52.021874    2321 shared_informer.go:318] Caches are synced for resource quota
I0327 14:25:52.076927    2321 shared_informer.go:318] Caches are synced for garbage collector
I0327 14:25:52.174364    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0327 14:25:52.469761    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0327 14:25:52.811504    2321 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"content\" (UniqueName: \"kubernetes.io/configmap/87543c2b-a2be-4644-8b8b-960585860c4d-content\") pod \"87543c2b-a2be-4644-8b8b-960585860c4d\" (UID: \"87543c2b-a2be-4644-8b8b-960585860c4d\") "
I0327 14:25:52.812274    2321 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/87543c2b-a2be-4644-8b8b-960585860c4d-tmp\") pod \"87543c2b-a2be-4644-8b8b-960585860c4d\" (UID: \"87543c2b-a2be-4644-8b8b-960585860c4d\") "
I0327 14:25:52.812722    2321 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-w2kxz\" (UniqueName: \"kubernetes.io/projected/87543c2b-a2be-4644-8b8b-960585860c4d-kube-api-access-w2kxz\") pod \"87543c2b-a2be-4644-8b8b-960585860c4d\" (UID: \"87543c2b-a2be-4644-8b8b-960585860c4d\") "
I0327 14:25:52.813149    2321 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/87543c2b-a2be-4644-8b8b-960585860c4d-klipper-helm\") pod \"87543c2b-a2be-4644-8b8b-960585860c4d\" (UID: \"87543c2b-a2be-4644-8b8b-960585860c4d\") "
I0327 14:25:52.813570    2321 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"values\" (UniqueName: \"kubernetes.io/secret/87543c2b-a2be-4644-8b8b-960585860c4d-values\") pod \"87543c2b-a2be-4644-8b8b-960585860c4d\" (UID: \"87543c2b-a2be-4644-8b8b-960585860c4d\") "
I0327 14:25:52.813969    2321 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/87543c2b-a2be-4644-8b8b-960585860c4d-klipper-config\") pod \"87543c2b-a2be-4644-8b8b-960585860c4d\" (UID: \"87543c2b-a2be-4644-8b8b-960585860c4d\") "
I0327 14:25:52.814384    2321 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/87543c2b-a2be-4644-8b8b-960585860c4d-klipper-cache\") pod \"87543c2b-a2be-4644-8b8b-960585860c4d\" (UID: \"87543c2b-a2be-4644-8b8b-960585860c4d\") "
I0327 14:25:52.823327    2321 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/configmap/87543c2b-a2be-4644-8b8b-960585860c4d-content" (OuterVolumeSpecName: "content") pod "87543c2b-a2be-4644-8b8b-960585860c4d" (UID: "87543c2b-a2be-4644-8b8b-960585860c4d"). InnerVolumeSpecName "content". PluginName "kubernetes.io/configmap", VolumeGidValue ""
I0327 14:25:52.847026    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0327 14:25:52.860735    2321 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/87543c2b-a2be-4644-8b8b-960585860c4d-tmp" (OuterVolumeSpecName: "tmp") pod "87543c2b-a2be-4644-8b8b-960585860c4d" (UID: "87543c2b-a2be-4644-8b8b-960585860c4d"). InnerVolumeSpecName "tmp". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0327 14:25:52.874697    2321 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/87543c2b-a2be-4644-8b8b-960585860c4d-kube-api-access-w2kxz" (OuterVolumeSpecName: "kube-api-access-w2kxz") pod "87543c2b-a2be-4644-8b8b-960585860c4d" (UID: "87543c2b-a2be-4644-8b8b-960585860c4d"). InnerVolumeSpecName "kube-api-access-w2kxz". PluginName "kubernetes.io/projected", VolumeGidValue ""
I0327 14:25:52.876518    2321 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/87543c2b-a2be-4644-8b8b-960585860c4d-klipper-config" (OuterVolumeSpecName: "klipper-config") pod "87543c2b-a2be-4644-8b8b-960585860c4d" (UID: "87543c2b-a2be-4644-8b8b-960585860c4d"). InnerVolumeSpecName "klipper-config". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0327 14:25:52.877425    2321 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/87543c2b-a2be-4644-8b8b-960585860c4d-klipper-helm" (OuterVolumeSpecName: "klipper-helm") pod "87543c2b-a2be-4644-8b8b-960585860c4d" (UID: "87543c2b-a2be-4644-8b8b-960585860c4d"). InnerVolumeSpecName "klipper-helm". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0327 14:25:52.879482    2321 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/87543c2b-a2be-4644-8b8b-960585860c4d-klipper-cache" (OuterVolumeSpecName: "klipper-cache") pod "87543c2b-a2be-4644-8b8b-960585860c4d" (UID: "87543c2b-a2be-4644-8b8b-960585860c4d"). InnerVolumeSpecName "klipper-cache". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0327 14:25:52.902441    2321 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/87543c2b-a2be-4644-8b8b-960585860c4d-values" (OuterVolumeSpecName: "values") pod "87543c2b-a2be-4644-8b8b-960585860c4d" (UID: "87543c2b-a2be-4644-8b8b-960585860c4d"). InnerVolumeSpecName "values". PluginName "kubernetes.io/secret", VolumeGidValue ""
I0327 14:25:52.915703    2321 reconciler_common.go:300] "Volume detached for volume \"klipper-cache\" (UniqueName: \"kubernetes.io/empty-dir/87543c2b-a2be-4644-8b8b-960585860c4d-klipper-cache\") on node \"server\" DevicePath \"\""
I0327 14:25:52.915938    2321 reconciler_common.go:300] "Volume detached for volume \"klipper-helm\" (UniqueName: \"kubernetes.io/empty-dir/87543c2b-a2be-4644-8b8b-960585860c4d-klipper-helm\") on node \"server\" DevicePath \"\""
I0327 14:25:52.915979    2321 reconciler_common.go:300] "Volume detached for volume \"values\" (UniqueName: \"kubernetes.io/secret/87543c2b-a2be-4644-8b8b-960585860c4d-values\") on node \"server\" DevicePath \"\""
I0327 14:25:52.916007    2321 reconciler_common.go:300] "Volume detached for volume \"klipper-config\" (UniqueName: \"kubernetes.io/empty-dir/87543c2b-a2be-4644-8b8b-960585860c4d-klipper-config\") on node \"server\" DevicePath \"\""
I0327 14:25:52.916033    2321 reconciler_common.go:300] "Volume detached for volume \"content\" (UniqueName: \"kubernetes.io/configmap/87543c2b-a2be-4644-8b8b-960585860c4d-content\") on node \"server\" DevicePath \"\""
I0327 14:25:52.916059    2321 reconciler_common.go:300] "Volume detached for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/87543c2b-a2be-4644-8b8b-960585860c4d-tmp\") on node \"server\" DevicePath \"\""
I0327 14:25:52.916091    2321 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-w2kxz\" (UniqueName: \"kubernetes.io/projected/87543c2b-a2be-4644-8b8b-960585860c4d-kube-api-access-w2kxz\") on node \"server\" DevicePath \"\""
I0327 14:25:53.285448    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0327 14:25:53.321482    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0327 14:25:53.348434    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0327 14:25:53.350247    2321 event.go:307] "Event occurred" object="kube-system/helm-install-traefik" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="Completed" message="Job completed"
I0327 14:25:53.439122    2321 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="4fcc42480b5a222435e1fcde154476bc808b0f0b4e8ce8ab247fccfc850d8d1e"
I0327 14:25:53.518339    2321 job_controller.go:562] "enqueueing job" key="kube-system/helm-install-traefik"
I0327 14:25:57.666913    2321 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/svclb-traefik-54dd7185-mxvbc" containerName="lb-tcp-80"
I0327 14:25:58.412199    2321 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/svclb-traefik-54dd7185-mxvbc" containerName="lb-tcp-443"
I0327 14:25:59.755642    2321 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/svclb-traefik-54dd7185-mxvbc" podStartSLOduration=5.37646161 podCreationTimestamp="2024-03-27 14:25:49 +0000 UTC" firstStartedPulling="2024-03-27 14:25:52.29311844 +0000 UTC m=+391.476335499" lastFinishedPulling="2024-03-27 14:25:57.64584801 +0000 UTC m=+396.829065086" observedRunningTime="2024-03-27 14:25:59.725698493 +0000 UTC m=+398.908915613" watchObservedRunningTime="2024-03-27 14:25:59.729191197 +0000 UTC m=+398.912408319"
I0327 14:25:59.879286    2321 event.go:307] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Service" apiVersion="v1" type="Normal" reason="UpdatedLoadBalancer" message="Updated LoadBalancer with new IPs: [] -> [192.168.56.110]"
I0327 14:26:03.241293    2321 kuberuntime_container_linux.go:167] "No swap cgroup controller present" swapBehavior="" pod="kube-system/traefik-f4564c4f4-55xj2" containerName="traefik"
I0327 14:26:04.806436    2321 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/traefik-f4564c4f4-55xj2" podStartSLOduration=5.089973169 podCreationTimestamp="2024-03-27 14:25:48 +0000 UTC" firstStartedPulling="2024-03-27 14:25:51.533327376 +0000 UTC m=+390.716544447" lastFinishedPulling="2024-03-27 14:26:03.21005788 +0000 UTC m=+402.393275023" observedRunningTime="2024-03-27 14:26:04.750233361 +0000 UTC m=+403.933450482" watchObservedRunningTime="2024-03-27 14:26:04.766703745 +0000 UTC m=+403.949920823"
I0327 14:26:04.892714    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="6.550112ms"
I0327 14:26:06.860376    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="74.892503ms"
I0327 14:26:06.861602    2321 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/traefik-f4564c4f4" duration="502.913s"
